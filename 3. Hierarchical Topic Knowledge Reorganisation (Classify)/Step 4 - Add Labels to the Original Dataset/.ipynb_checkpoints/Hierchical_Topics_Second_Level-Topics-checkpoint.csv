Cluster_Label,Human_Readable_Topic,Representation,Representative_Docs,Higher_Topic_Label,Representative_Document
1,"Chinese Language Models Evaluation , Evaluating Language Models with Benchmarks , Evaluating Large Language Models , Evaluating Large Language Models , Evaluating Large Language Models for NLP Tasks , ""Large Language Model Benchmarking and Evaluation""","['mandarin', 'chinese', 'languages', 'linguistic', 'language', 'nlp', 'taiwanese', 'english', 'subjects', 'conversational'] , ['benchmarking', 'evaluations', 'language', 'benchmarks', 'benchmark', 'evaluation', 'assessment', 'vocabulary', 'evaluating', 'tests'] , ['evaluations', 'evaluation', 'evaluator', 'evaluating', 'assessment', 'evaluators', 'assessing', 'assessments', 'judgments', 'ranking'] , ['ranking', 'evaluations', 'rankings', 'evaluation', 'language', 'assessment', 'responses', 'bias', 'questioner', 'prompts'] , ['textual', 'nlp', 'summarization', 'linguistic', 'texts', 'evaluation', 'text', 'generated', 'language', 'lingual'] , ['benchmarking', 'tools', 'benchmarks', 'benchmark', 'tool', 'evaluations', 'language', 'answering', 'software', 'openai']","['  Chinese Large Language Models (LLMs) have recently demonstrated impressive\ncapabilities across various NLP benchmarks and real-world applications.\nHowever, the existing benchmarks for comprehensively evaluating these LLMs are\nstill insufficient, particularly in terms of measuring knowledge that LLMs\ncapture. Current datasets collect questions from Chinese examinations across\ndifferent subjects and educational levels to address this issue. Yet, these\nbenchmarks primarily focus on objective questions such as multiple-choice\nquestions, leading to a lack of diversity in question types. To tackle this\nproblem, we propose LHMKE, a Large-scale, Holistic, and Multi-subject Knowledge\nEvaluation benchmark in this paper. LHMKE is designed to provide a\ncomprehensive evaluation of the knowledge acquisition capabilities of Chinese\nLLMs. It encompasses 10,465 questions across 75 tasks covering 30 subjects,\nranging from primary school to professional certification exams. Notably, LHMKE\nincludes both objective and subjective questions, offering a more holistic\nevaluation of the knowledge level of LLMs. We have assessed 11 Chinese LLMs\nunder the zero-shot setting, which aligns with real examinations, and compared\ntheir performance across different subjects. We also conduct an in-depth\nanalysis to check whether GPT-4 can automatically score subjective predictions.\nOur findings suggest that LHMKE is a challenging and advanced testbed for\nChinese LLMs.\n', ""  Large Language Models (LLMs), such as ChatGPT and GPT-4, have dramatically\ntransformed natural language processing research and shown promising strides\ntowards Artificial General Intelligence (AGI). Nonetheless, the high costs\nassociated with training and deploying LLMs present substantial obstacles to\ntransparent, accessible academic research. While several large language models,\nsuch as LLaMA, have been open-sourced by the community, these predominantly\nfocus on English corpora, limiting their usefulness for other languages. In\nthis paper, we propose a method to augment LLaMA with capabilities for\nunderstanding and generating Chinese text and its ability to follow\ninstructions. We achieve this by extending LLaMA's existing vocabulary with an\nadditional 20,000 Chinese tokens, thereby improving its encoding efficiency and\nsemantic understanding of Chinese. We further incorporate secondary\npre-training using Chinese data and fine-tune the model with Chinese\ninstruction datasets, significantly enhancing the model's ability to comprehend\nand execute instructions. Our experimental results indicate that the newly\nproposed model markedly enhances the original LLaMA's proficiency in\nunderstanding and generating Chinese content. Additionally, the results on the\nC-Eval dataset yield competitive performance among the models with several\ntimes the size of ours. We have made our pre-trained models, training scripts,\nand other resources available through GitHub, fostering open research for our\ncommunity. Chinese LLaMA series:\n\\url{https://github.com/ymcui/Chinese-LLaMA-Alpaca} and Chinese Llama-2 series:\n\\url{https://github.com/ymcui/Chinese-LLaMA-Alpaca-2}\n"", '  With the accelerating development of Large Language Models (LLMs), many LLMs\nare beginning to be used in the Chinese K-12 education domain. The integration\nof LLMs and education is getting closer and closer, however, there is currently\nno benchmark for evaluating LLMs that focuses on the Chinese K-12 education\ndomain. Therefore, there is an urgent need for a comprehensive natural language\nprocessing benchmark to accurately assess the capabilities of various LLMs in\nthe Chinese K-12 education domain. To address this, we introduce the E-EVAL,\nthe first comprehensive evaluation benchmark specifically designed for the\nChinese K-12 education field. The E-EVAL consists of 4,351 multiple-choice\nquestions at the primary, middle, and high school levels across a wide range of\nsubjects, including Chinese, English, Politics, History, Ethics, Physics,\nChemistry, Mathematics, and Geography. We conducted a comprehensive evaluation\nof E-EVAL on advanced LLMs, including both English-dominant and\nChinese-dominant models. Findings show that Chinese-dominant models perform\nwell compared to English-dominant models, with many scoring even above the GPT\n4.0. However, almost all models perform poorly in complex subjects such as\nmathematics. We also found that most Chinese-dominant LLMs did not achieve\nhigher scores at the primary school level compared to the middle school level.\nWe observe that the mastery of higher-order knowledge by the model does not\nnecessarily imply the mastery of lower-order knowledge as well. Additionally,\nthe experimental results indicate that the Chain of Thought (CoT) technique is\neffective only for the challenging science subjects, while Few-shot prompting\nis more beneficial for liberal arts subjects. With E-EVAL, we aim to analyze\nthe strengths and limitations of LLMs in educational applications, and to\ncontribute to the progress and development of Chinese K-12 education and LLMs.\n'] , ['  Recent advancements in Large Language Models (LLMs) have sparked interest in\ntheir potential applications across various fields. This paper embarked on a\npivotal inquiry: Can existing LLMs effectively serve as ""water expert models""\nfor water engineering and research tasks? This study was the first to evaluate\nLLMs\' contributions across various water engineering and research tasks by\nestablishing a domain-specific benchmark suite, namely, WaterER. Herein, we\nprepared 983 tasks related to water engineering and research, categorized into\n""wastewater treatment"", ""environmental restoration"", ""drinking water treatment\nand distribution"", ""sanitation"", ""anaerobic digestion"" and ""contaminants\nassessment"". We evaluated the performance of seven LLMs (i.e., GPT-4, GPT-3.5,\nGemini, GLM-4, ERNIE, QWEN and Llama3) on these tasks. We highlighted the\nstrengths of GPT-4 in handling diverse and complex tasks of water engineering\nand water research, the specialized capabilities of Gemini in academic\ncontexts, Llama3\'s strongest capacity to answer Chinese water engineering\nquestions and the competitive performance of Chinese-oriented models like\nGLM-4, ERNIE and QWEN in some water engineering tasks. More specifically,\ncurrent LLMs excelled particularly in generating precise research gaps for\npapers on ""contaminants and related water quality monitoring and assessment"".\nAdditionally, they were more adept at creating appropriate titles for research\npapers on ""treatment processes for wastewaters"", ""environmental restoration"",\nand ""drinking water treatment"". Overall, this study pioneered evaluating LLMs\nin water engineering and research by introducing the WaterER benchmark to\nassess the trustworthiness of their predictions. This standardized evaluation\nframework would also drive future advancements in LLM technology by using\ntargeting datasets, propelling these models towards becoming true ""water\nexpert"".\n', ""  Large language models (LLMs) are increasingly relied upon for complex\nmulti-turn conversations across diverse real-world applications. However,\nexisting benchmarks predominantly focus on single-turn evaluations, overlooking\nthe models' capabilities in multi-turn interactions. To address this gap, we\nintroduce MT-Eval, a comprehensive benchmark designed to evaluate multi-turn\nconversational abilities. By analyzing human-LLM conversations, we categorize\ninteraction patterns into four types: recollection, expansion, refinement, and\nfollow-up. We construct multi-turn queries for each category either by\naugmenting existing datasets or by creating new examples with GPT-4 to avoid\ndata leakage. To study the factors impacting multi-turn abilities, we create\nsingle-turn versions of the 1170 multi-turn queries and compare performance.\nOur evaluation of 11 well-known LLMs shows that while closed-source models\ngenerally surpass open-source ones, certain open-source models exceed\nGPT-3.5-Turbo in specific tasks. We observe significant performance degradation\nin multi-turn settings compared to single-turn settings in most models, which\nis not correlated with the models' fundamental capabilities. Moreover, we\nidentify the distance to relevant content and susceptibility to error\npropagation as the key factors influencing multi-turn performance. MT-Eval is\nreleased publicly to encourage future research towards more robust\nconversational models.\n"", ""  Recent advancements in Language Models (LMs) have catalyzed the creation of\nmultiple benchmarks, designed to assess these models' general capabilities. A\ncrucial task, however, is assessing the validity of the benchmarks themselves.\nThis is most commonly done via Benchmark Agreement Testing (BAT), where new\nbenchmarks are validated against established ones using some agreement metric\n(e.g., rank correlation). Despite the crucial role of BAT for benchmark\nbuilders and consumers, there are no standardized procedures for such agreement\ntesting. This deficiency can lead to invalid conclusions, fostering mistrust in\nbenchmarks and upending the ability to properly choose the appropriate\nbenchmark to use. By analyzing over 40 prominent benchmarks, we demonstrate how\nsome overlooked methodological choices can significantly influence BAT results,\npotentially undermining the validity of conclusions. To address these\ninconsistencies, we propose a set of best practices for BAT and demonstrate how\nutilizing these methodologies greatly improves BAT robustness and validity. To\nfoster adoption and facilitate future research,, we introduce BenchBench, a\npython package for BAT, and release the BenchBench-leaderboard, a\nmeta-benchmark designed to evaluate benchmarks using their peers. Our findings\nunderscore the necessity for standardized BAT, ensuring the robustness and\nvalidity of benchmark evaluations in the evolving landscape of language model\nresearch.\n  BenchBench Package: https://github.com/IBM/BenchBench\n  Leaderboard: https://huggingface.co/spaces/per/BenchBench\n""] , [""  Large Language Models (LLMs) excel in various Natural Language Processing\n(NLP) tasks, yet their evaluation, particularly in languages beyond the top\n$20$, remains inadequate due to existing benchmarks and metrics limitations.\nEmploying LLMs as evaluators to rank or score other models' outputs emerges as\na viable solution, addressing the constraints tied to human annotators and\nestablished benchmarks. In this study, we explore the potential of LLM-based\nevaluators, specifically GPT-4 in enhancing multilingual evaluation by\ncalibrating them against $20$K human judgments across three text-generation\ntasks, five metrics, and eight languages. Our analysis reveals a bias in\nGPT4-based evaluators towards higher scores, underscoring the necessity of\ncalibration with native speaker judgments, especially in low-resource and\nnon-Latin script languages, to ensure accurate evaluation of LLM performance\nacross diverse languages.\n"", '  Large Language Models (LLMs) have demonstrated promising capabilities as\nautomatic evaluators in assessing the quality of generated natural language.\nHowever, LLMs still exhibit biases in evaluation and often struggle to generate\ncoherent evaluations that align with human assessments. In this work, we first\nconduct a systematic study of the misalignment between LLM evaluators and human\njudgement, revealing that existing calibration methods aimed at mitigating\nbiases are insufficient for effectively aligning LLM evaluators. Inspired by\nthe use of preference data in RLHF, we formulate the evaluation as a ranking\nproblem and introduce Pairwise-preference Search (PairS), an uncertainty-guided\nsearch method that employs LLMs to conduct pairwise comparisons and efficiently\nranks candidate texts. PairS achieves state-of-the-art performance on\nrepresentative evaluation tasks and demonstrates significant improvements over\ndirect scoring. Furthermore, we provide insights into the role of pairwise\npreference in quantifying the transitivity of LLMs and demonstrate how PairS\nbenefits from calibration.\n', '  The zero-shot capability of Large Language Models (LLMs) has enabled highly\nflexible, reference-free metrics for various tasks, making LLM evaluators\ncommon tools in NLP. However, the robustness of these LLM evaluators remains\nrelatively understudied; existing work mainly pursued optimal performance in\nterms of correlating LLM scores with human expert scores. In this paper, we\nconduct a series of analyses using the SummEval dataset and confirm that LLMs\nare biased evaluators as they: (1) exhibit familiarity bias-a preference for\ntext with lower perplexity, (2) show skewed and biased distributions of\nratings, and (3) experience anchoring effects for multi-attribute judgments. We\nalso found that LLMs are inconsistent evaluators, showing low ""inter-sample""\nagreement and sensitivity to prompt differences that are insignificant to human\nunderstanding of text quality. Furthermore, we share recipes for configuring\nLLM evaluators to mitigate these limitations. Experimental results on the RoSE\ndataset demonstrate improvements over the state-of-the-art LLM evaluators.\n'] , [""  Multiple-choice questions (MCQ) are frequently used to assess large language\nmodels (LLMs). Typically, an LLM is given a question and selects the answer\ndeemed most probable after adjustments for factors like length. Unfortunately,\nLLMs may inherently favor certain answer choice IDs, such as A/B/C/D, due to\ninherent biases of priori unbalanced probabilities, influencing the prediction\nof answers based on these IDs. Previous research has introduced methods to\nreduce this ''selection bias'' by simply permutating options on a few test\nsamples and applying to new ones. Another problem of MCQ is the lottery ticket\nchoice by ''random guessing''. The LLM does not learn particular knowledge, but\nthe option is guessed correctly. This situation is especially serious for those\nsmall-scale LLMs. To address them, a more thorough approach involves shifting\nfrom MCQ to open-style questions, which can fundamentally eliminate selection\nbias and random guessing issues. However, transitioning causes its own set of\nchallenges in (1) identifying suitable open-style questions and (2) validating\nthe correctness of LLM open-style responses against human-annotated\nground-truths. This work aims to tackle these significant difficulties, and\nestablish a new LLM evaluation benchmark through entirely open-style questions.\nConsequently, we introduce the Open-LLM-Leaderboard to track various LLMs'\nperformance and reflect true capability of them, such as GPT-4o/4/3.5, Claude\n3, Gemini, etc. Our code and dataset are available at\nhttps://github.com/VILA-Lab/Open-LLM-Leaderboard.\n"", '  The rapid evolution of language models has necessitated the development of\nmore challenging benchmarks. Current static benchmarks often struggle to\nconsistently distinguish between the capabilities of different models and fail\nto align with real-world user preferences. On the other hand, live\ncrowd-sourced platforms like the Chatbot Arena collect a wide range of natural\nprompts and user feedback. However, these prompts vary in sophistication and\nthe feedback cannot be applied offline to new models. In order to ensure that\nbenchmarks keep up with the pace of LLM development, we address how one can\nevaluate benchmarks on their ability to confidently separate models and their\nalignment with human preference. Under these principles, we developed\nBenchBuilder, a living benchmark that filters high-quality prompts from live\ndata sources to enable offline evaluation on fresh, challenging prompts.\nBenchBuilder identifies seven indicators of a high-quality prompt, such as the\nrequirement for domain knowledge, and utilizes an LLM annotator to select a\nhigh-quality subset of prompts from various topic clusters. The LLM evaluation\nprocess employs an LLM judge to ensure a fully automated, high-quality, and\nconstantly updating benchmark. We apply BenchBuilder on prompts from the\nChatbot Arena to create Arena-Hard-Auto v0.1: 500 challenging user prompts from\na wide range of tasks. Arena-Hard-Auto v0.1 offers 3x tighter confidence\nintervals than MT-Bench and achieves a state-of-the-art 89.1% agreement with\nhuman preference rankings, all at a cost of only $25 and without human\nlabelers. The BenchBuilder pipeline enhances evaluation benchmarks and provides\na valuable tool for developers, enabling them to extract high-quality\nbenchmarks from extensive data with minimal effort.\n', '  Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious applications, fundamentally reshaping the landscape of natural language\nprocessing (NLP) research. However, recent evaluation frameworks often rely on\nthe output probabilities of LLMs for predictions, primarily due to\ncomputational constraints, diverging from real-world LLM usage scenarios. While\nwidely employed, the efficacy of these probability-based evaluation strategies\nremains an open research question. This study aims to scrutinize the validity\nof such probability-based evaluation methods within the context of using LLMs\nfor Multiple Choice Questions (MCQs), highlighting their inherent limitations.\nOur empirical investigation reveals that the prevalent probability-based\nevaluation method inadequately aligns with generation-based prediction.\nFurthermore, current evaluation frameworks typically assess LLMs through\npredictive tasks based on output probabilities rather than directly generating\nresponses, owing to computational limitations. We illustrate that these\nprobability-based approaches do not effectively correspond with generative\npredictions. The outcomes of our study can enhance the understanding of LLM\nevaluation methodologies and provide insights for future research in this\ndomain.\n'] , ['  Instruction-tuned Large Language Models (LLMs) have recently showcased\nremarkable advancements in their ability to generate fitting responses to\nnatural language instructions. However, many current works rely on manual\nevaluation to judge the quality of generated responses. Since such manual\nevaluation is time-consuming, it does not easily scale to the evaluation of\nmultiple models and model variants. In this short paper, we propose a\nstraightforward but remarkably effective evaluation metric called SemScore, in\nwhich we directly compare model outputs to gold target responses using semantic\ntextual similarity (STS). We conduct a comparative evaluation of the model\noutputs of 12 prominent instruction-tuned LLMs using 8 widely-used evaluation\nmetrics for text generation. We find that our proposed SemScore metric\noutperforms all other, in many cases more complex, evaluation metrics in terms\nof correlation to human evaluation. These findings indicate the utility of our\nproposed metric for the evaluation of instruction-tuned LLMs.\n', ""  Natural Language Processing (NLP) is witnessing a remarkable breakthrough\ndriven by the success of Large Language Models (LLMs). LLMs have gained\nsignificant attention across academia and industry for their versatile\napplications in text generation, question answering, and text summarization. As\nthe landscape of NLP evolves with an increasing number of domain-specific LLMs\nemploying diverse techniques and trained on various corpus, evaluating\nperformance of these models becomes paramount. To quantify the performance,\nit's crucial to have a comprehensive grasp of existing metrics. Among the\nevaluation, metrics which quantifying the performance of LLMs play a pivotal\nrole. This paper offers a comprehensive exploration of LLM evaluation from a\nmetrics perspective, providing insights into the selection and interpretation\nof metrics currently in use. Our main goal is to elucidate their mathematical\nformulations and statistical interpretations. We shed light on the application\nof these metrics using recent Biomedical LLMs. Additionally, we offer a\nsuccinct comparison of these metrics, aiding researchers in selecting\nappropriate metrics for diverse tasks. The overarching goal is to furnish\nresearchers with a pragmatic guide for effective LLM evaluation and metric\nselection, thereby advancing the understanding and application of these large\nlanguage models.\n"", '  Automatic evaluation of generated textual content presents an ongoing\nchallenge within the field of NLP. Given the impressive capabilities of modern\nlanguage models (LMs) across diverse NLP tasks, there is a growing trend to\nemploy these models in creating innovative evaluation metrics for automated\nassessment of generation tasks. This paper investigates a pivotal question: Do\nlanguage model-driven evaluation metrics inherently exhibit bias favoring texts\ngenerated by the same underlying language model? Specifically, we assess\nwhether prominent LM-based evaluation metrics (e.g. BARTScore, T5Score, and\nGPTScore) demonstrate a favorable bias toward their respective underlying LMs\nin the context of summarization tasks. Our findings unveil a latent bias,\nparticularly pronounced when such evaluation metrics are used in a\nreference-free manner without leveraging gold summaries. These results\nunderscore that assessments provided by generative evaluation models can be\ninfluenced by factors beyond the inherent text quality, highlighting the\nnecessity of developing more reliable evaluation protocols in the future.\n'] , ['  Log analysis is crucial for ensuring the orderly and stable operation of\ninformation systems, particularly in the field of Artificial Intelligence for\nIT Operations (AIOps). Large Language Models (LLMs) have demonstrated\nsignificant potential in natural language processing tasks. In the AIOps\ndomain, they excel in tasks such as anomaly detection, root cause analysis of\nfaults, operations and maintenance script generation, and alert information\nsummarization. However, the performance of current LLMs in log analysis tasks\nremains inadequately validated. To address this gap, we introduce LogEval, a\ncomprehensive benchmark suite designed to evaluate the capabilities of LLMs in\nvarious log analysis tasks for the first time. This benchmark covers tasks such\nas log parsing, log anomaly detection, log fault diagnosis, and log\nsummarization. LogEval evaluates each task using 4,000 publicly available log\ndata entries and employs 15 different prompts for each task to ensure a\nthorough and fair assessment. By rigorously evaluating leading LLMs, we\ndemonstrate the impact of various LLM technologies on log analysis performance,\nfocusing on aspects such as self-consistency and few-shot contextual learning.\nWe also discuss findings related to model quantification, Chinese-English\nquestion-answering evaluation, and prompt engineering. These findings provide\ninsights into the strengths and weaknesses of LLMs in multilingual environments\nand the effectiveness of different prompt strategies. Various evaluation\nmethods are employed for different tasks to accurately measure the performance\nof LLMs in log analysis, ensuring a comprehensive assessment. The insights\ngained from LogEvals evaluation reveal the strengths and limitations of LLMs in\nlog analysis tasks, providing valuable guidance for researchers and\npractitioners.\n', ""  With the rapid advancement of large language models (LLMs), there is a\npressing need for a comprehensive evaluation suite to assess their capabilities\nand limitations. Existing LLM leaderboards often reference scores reported in\nother papers without consistent settings and prompts, which may inadvertently\nencourage cherry-picking favored settings and prompts for better results. In\nthis work, we introduce GPT-Fathom, an open-source and reproducible LLM\nevaluation suite built on top of OpenAI Evals. We systematically evaluate 10+\nleading LLMs as well as OpenAI's legacy models on 20+ curated benchmarks across\n7 capability categories, all under aligned settings. Our retrospective study on\nOpenAI's earlier models offers valuable insights into the evolutionary path\nfrom GPT-3 to GPT-4. Currently, the community is eager to know how GPT-3\nprogressively improves to GPT-4, including technical details like whether\nadding code data improves LLM's reasoning capability, which aspects of LLM\ncapability can be improved by SFT and RLHF, how much is the alignment tax, etc.\nOur analysis sheds light on many of these questions, aiming to improve the\ntransparency of advanced LLMs.\n"", ""  To solve complex tasks, large language models (LLMs) often require multiple\nrounds of interactions with the user, sometimes assisted by external tools.\nHowever, current evaluation protocols often emphasize benchmark performance\nwith single-turn exchanges, neglecting the nuanced interactions among the user,\nLLMs, and external tools, while also underestimating the importance of natural\nlanguage feedback from users. These oversights contribute to discrepancies\nbetween research benchmark evaluations and real-world use cases. We introduce\nMINT, a benchmark that evaluates LLMs' ability to solve tasks with multi-turn\ninteractions by (1) using tools and (2) leveraging natural language feedback.\nTo ensure reproducibility, we provide an evaluation framework where LLMs can\naccess tools by executing Python code and receive users' natural language\nfeedback simulated by GPT-4. We repurpose a diverse set of established\nevaluation datasets focusing on reasoning, coding, and decision-making and\ncarefully curate them into a compact subset for efficient evaluation. Our\nanalysis of 20 open- and closed-source LLMs offers intriguing findings. (a)\nLLMs generally benefit from tools and language feedback, with performance gains\n(absolute, same below) of 1-8% for each turn of tool use and 2-17% with natural\nlanguage feedback. (b) Better single-turn performance does not guarantee better\nmulti-turn performance. (c) Surprisingly, on the LLMs evaluated, supervised\ninstruction-finetuning (SIFT) and reinforcement learning from human feedback\n(RLHF) generally hurt multi-turn capabilities. We expect MINT can help measure\nprogress and incentivize research in improving LLMs' capabilities in multi-turn\ninteractions, especially for open-source communities where multi-turn human\nevaluation can be less accessible compared to commercial LLMs with a larger\nuser base.\n""]",Evaluating Large Language Models,Evaluating Large Language Models
2,"Quantization Techniques for Large Language Models , ""Optimizing Cache Memory for Large Language Models"" , ""Efficient Pruning Methods for Large Language Models"" , Evolutionary Algorithms for Large Language Models , Optimizing Large Language Model Serving Systems , Log Parsing with Large Language Models , Efficient Cache Compression for Large Language Models , ""Efficient Large Language Models (LLMs) and Decoding Techniques"" , ""Large Language Models (LLMs) Training and Optimization"" , ""Optimizing Large Language Models on GPUs and FPGAs"" , ""Large Language Models for Personalized Search"" , Optimization Problem Solving with Large Language Models","['quantization', 'quantizing', 'quantize', 'quantized', 'memory', 'compression', 'compressed', 'weights', 'compressing', 'bits'] , ['memory', 'attention', 'cachegen', 'caches', 'cache', 'caching', 'bottlenecks', 'cacheblend', 'bottleneck', 'decoding'] , ['pruning', 'pruningbench', 'pruner', 'prune', 'pruned', 'compression', 'sparse', 'sparsegpt', 'compressing', 'efficient'] , ['heuristics', 'heuristic', 'optimized', 'generating', 'automate', 'evolutionary', 'algorithms', 'automated', 'generates', 'evolving'] , ['throughput', 'streaming', 'chatbot', 'batching', 'workloads', 'chatbots', 'serving', 'scheduling', 'concurrent', 'sglang'] , ['logs', 'parsers', 'log', 'openlogparser', 'parsing', 'parser', 'parses', 'logprompt', 'loghub', 'logelectra'] , ['caching', 'cache', 'caches', 'memory', 'cachedattention', 'quantization', 'storage', 'attention', 'compression', 'efficient'] , ['language', 'models', 'lms', 'decoding', 'llms', 'deepseek', 'llama2', 'llm', 'throughput', 'computational'] , ['llms', 'language', 'modelizer', 'llm', 'benchmarks', 'expertise', 'models', 'knowledge', 'training', 'optimizers'] , ['gpu', 'gpus', 'fpga', 'memory', 'compilers', 'implementations', 'sram', 'bottleneck', 'hardware', 'dataflow'] , ['retrieval', 'search', 'personalization', 'personalized', 'queries', 'personalize', 'profiles', 'mindsearch', 'seeking', 'cosearchagent'] , ['programming', 'interpreter', 'solvers', 'automate', 'solver', 'programs', 'code', 'tools', 'maxmind', 'optimization']","[""  The growing demand for Large Language Models (LLMs) in applications such as\ncontent generation, intelligent chatbots, and sentiment analysis poses\nconsiderable challenges for LLM service providers. To efficiently use GPU\nresources and boost throughput, batching multiple requests has emerged as a\npopular paradigm; to further speed up batching, LLM quantization techniques\nreduce memory consumption and increase computing capacity. However, prevalent\nquantization schemes (e.g., 8-bit weight-activation quantization) cannot fully\nleverage the capabilities of modern GPUs, such as 4-bit integer operators,\nresulting in sub-optimal performance.\n  To maximize LLMs' serving throughput, we introduce Atom, a low-bit\nquantization method that achieves high throughput improvements with negligible\naccuracy loss. Atom significantly boosts serving throughput by using low-bit\noperators and considerably reduces memory consumption via low-bit quantization.\nIt attains high accuracy by applying a novel mixed-precision and fine-grained\nquantization process. We evaluate Atom on 4-bit weight-activation quantization\nin the serving context. Atom improves end-to-end throughput (token/s) by up to\n$7.7\\times$ compared to the FP16 and by $2.5\\times$ compared to INT8\nquantization, while maintaining the same latency target.\n"", '  Quantization has emerged as a promising technique for improving the memory\nand computational efficiency of large language models (LLMs). Though the\ntrade-off between performance and efficiency is well-known, there is still much\nto be learned about the relationship between quantization and LLM performance.\nTo shed light on this relationship, we propose a new perspective on\nquantization, viewing it as perturbations added to the weights and activations\nof LLMs. We call this approach ""the lens of perturbation"". Using this lens, we\nconduct experiments with various artificial perturbations to explore their\nimpact on LLM performance. Our findings reveal several connections between the\nproperties of perturbations and LLM performance, providing insights into the\nfailure cases of uniform quantization and suggesting potential solutions to\nimprove the robustness of LLM quantization. To demonstrate the significance of\nour findings, we implement a simple non-uniform quantization approach based on\nour insights. Our experiments show that this approach achieves minimal\nperformance degradation on both 4-bit weight quantization and 8-bit\nquantization for weights and activations. These results validate the\ncorrectness of our approach and highlight its potential to improve the\nefficiency of LLMs without sacrificing performance.\n', ""  Due to the high memory and computational costs associated with Large Language\nModels, model compression via quantization and parameter-efficient fine-tuning\n(PEFT) methods, such as low-rank adaptation (LoRA), are gaining popularity.\nThis has led to active research on quantization-aware PEFT techniques, which\naim to create models with high accuracy and low memory overhead. Among\nquantization methods, post-training quantization (PTQ) is more commonly used in\nprevious works than quantization-aware training (QAT), despite QAT's potential\nfor higher accuracy. This preference is due to PTQ's low training overhead.\nHowever, PTQ-based PEFT methods often utilize high-precision parameters, making\nit difficult to fully exploit the efficiency of quantization. Additionally,\nthey have limited adaptation ability due to a reduced and constrained LoRA\nparameter structure. To overcome these challenges, we propose L4Q, which\nleverages joint quantization and fine-tuning to reduce QAT's memory overhead\nand produce models that consist entirely of quantized weights while achieving\neffective adaptation to downstream tasks. By design, L4Q allows quantization\nparameters to reflect weight updates, while weight updates reduce quantization\nerrors. Our experiments demonstrate that this coupled quantization and\nfine-tuning approach yields superior accuracy compared to decoupled fine-tuning\nschemes in sub-4-bit quantization. Using the LLaMA model families and\ninstructional datasets, we showcase L4Q's capabilities in language tasks and\nfew-shot in-context learning.\n""] , ['  The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy.\n', '  In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusin on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques achieving up to a 20.5 absolute accuracy improvement on\nTREC.\n', '  How to efficiently serve Large Language Models (LLMs) has become a pressing\nissue because of their huge computational cost in their autoregressive\ngeneration process. To mitigate computational costs, LLMs often employ the KV\nCache technique to improve the generation speed. While improving the\ncomputational efficiency, the storage requirements of the KV cache are\nsubstantial, particularly in long-context scenarios, leading to significant\nmemory consumption. Existing KV cache eviction methods often degrade the\nperformance of LLMs in long-context scenarios due to the information loss\nintroduced by eviction. In this paper, we propose a novel KV cache merging\napproach, called KVMerger, to achieve adaptive KV cache compression for\nlong-context tasks without significant performance degradation under\nconstrained memory budgets. Our approach is inspired by the intriguing\nobservation that key states exhibit high similarity at the token level within a\nsingle sequence. To facilitate merging, we develop an effective yet\nstraightforward merging set identification algorithm to identify suitable KV\nstates for merging. Our merging set identification algorithm stimulates the\nsecond observation that KV cache sparsity, from similarity perspective, is\nindependent of the dataset and remains persistent at the model level.\nSubsequently, we propose a Gaussian kernel weighted merging algorithm to\nselectively merge all states within each merging set. We conduct extensive\nexperiments to demonstrate the effectiveness of KVMerger for long-context tasks\nunder constrained memory budgets, applying it to models including\nLlama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll\nbenchmarks, we compare our method with other KV cache compression techniques,\nincluding H2O and CaM, showing that our method achieves superior performance\nacross tasks with both 50% and 35% KV cache budgets.\n'] , ['  To remove redundant components of large language models (LLMs) without\nincurring significant computational costs, this work focuses on single-shot\npruning without a retraining phase. We simplify the pruning process for\nTransformer-based LLMs by identifying a depth-2 pruning structure that\nfunctions independently. Additionally, we propose two inference-aware pruning\ncriteria derived from the optimization perspective of output approximation,\nwhich outperforms traditional training-aware metrics such as gradient and\nHessian. We also introduce a two-step reconstruction technique to mitigate\npruning errors without model retraining. Experimental results demonstrate that\nour approach significantly reduces computational costs and hardware\nrequirements while maintaining superior performance across various datasets and\nmodels.\n', '  Large Vision-Language Models (LVLMs) can understand the world comprehensively\nby integrating rich information from different modalities, achieving remarkable\nadvancements on various multimodal downstream tasks. However, deploying LVLMs\nis often problematic due to their massive computational/energy costs and carbon\nconsumption. Such issues make it infeasible to adopt conventional iterative\nglobal pruning, which is costly due to computing the Hessian matrix of the\nentire large model for sparsification. Alternatively, several studies have\nrecently proposed layer-wise pruning approaches to avoid the expensive\ncomputation of global pruning and efficiently compress model weights according\nto their importance within a layer. However, they often suffer from suboptimal\nmodel compression due to their lack of a global perspective. To address this\nlimitation in recent efficient pruning methods for large models, we propose\nEfficient Coarse-to-Fine LayerWise Pruning (ECoFLaP), a two-stage\ncoarse-to-fine weight pruning approach for LVLMs. We first determine the\nsparsity ratios of different layers or blocks by leveraging the global\nimportance score, which is efficiently computed based on the zeroth-order\napproximation of the global model gradients. Then, the model performs local\nlayer-wise unstructured weight pruning based on globally-informed sparsity\nratios. We validate our proposed method across various multimodal and unimodal\nmodels and datasets, demonstrating significant performance improvements over\nprevalent pruning techniques in the high-sparsity regime.\n', '  Despite the remarkable capabilities, Large Language Models (LLMs) face\ndeployment challenges due to their extensive size. Pruning methods drop a\nsubset of weights to accelerate, but many of them require retraining, which is\nprohibitively expensive and computationally demanding. Recently, post-training\npruning approaches introduced novel metrics, enabling the pruning of LLMs\nwithout retraining. However, these metrics require the involvement of human\nexperts and tedious trial and error. To efficiently identify superior pruning\nmetrics, we develop an automatic framework for searching symbolic pruning\nmetrics using genetic programming. In particular, we devise an elaborate search\nspace encompassing the existing pruning metrics to discover the potential\nsymbolic pruning metric. We propose an opposing operation simplification\nstrategy to increase the diversity of the population. In this way, Pruner-Zero\nallows auto-generation of symbolic pruning metrics. Based on the searched\nresults, we explore the correlation between pruning metrics and performance\nafter pruning and summarize some principles. Extensive experiments on LLaMA and\nLLaMA-2 on language modeling and zero-shot tasks demonstrate that our\nPruner-Zero obtains superior performance than SOTA post-training pruning\nmethods. Code at: \\url{https://github.com/pprp/Pruner-Zero}.\n'] , ['  Pre-trained large language models (LLMs) have powerful capabilities for\ngenerating creative natural text. Evolutionary algorithms (EAs) can discover\ndiverse solutions to complex real-world problems. Motivated by the common\ncollective and directionality of text generation and evolution, this paper\nillustrates the parallels between LLMs and EAs, which includes multiple\none-to-one key characteristics: token representation and individual\nrepresentation, position encoding and fitness shaping, position embedding and\nselection, Transformers block and reproduction, and model training and\nparameter adaptation. By examining these parallels, we analyze existing\ninterdisciplinary research, with a specific focus on evolutionary fine-tuning\nand LLM-enhanced EAs. Drawing from these insights, valuable future directions\nare presented for advancing the integration of LLMs and EAs, while highlighting\nkey challenges along the way. These parallels not only reveal the evolution\nmechanism behind LLMs but also facilitate the development of evolved artificial\nagents that approach or surpass biological organisms.\n', '  Heuristics are widely used for dealing with complex search and optimization\nproblems. However, manual design of heuristics can be often very labour\nextensive and requires rich working experience and knowledge. This paper\nproposes Evolution of Heuristic (EoH), a novel evolutionary paradigm that\nleverages both Large Language Models (LLMs) and Evolutionary Computation (EC)\nmethods for Automatic Heuristic Design (AHD). EoH represents the ideas of\nheuristics in natural language, termed thoughts. They are then translated into\nexecutable codes by LLMs. The evolution of both thoughts and codes in an\nevolutionary search framework makes it very effective and efficient for\ngenerating high-performance heuristics. Experiments on three widely studied\ncombinatorial optimization benchmark problems demonstrate that EoH outperforms\ncommonly used handcrafted heuristics and other recent AHD methods including\nFunSearch. Particularly, the heuristic produced by EoH with a low computational\nbudget (in terms of the number of queries to LLMs) significantly outperforms\nwidely-used human hand-crafted baseline algorithms for the online bin packing\nproblem.\n', '  Large Language Models (LLMs) excel in various tasks, but they rely on\ncarefully crafted prompts that often demand substantial human effort. To\nautomate this process, in this paper, we propose a novel framework for discrete\nprompt optimization, called EvoPrompt, which borrows the idea of evolutionary\nalgorithms (EAs) as they exhibit good performance and fast convergence. To\nenable EAs to work on discrete prompts, which are natural language expressions\nthat need to be coherent and human-readable, we connect LLMs with EAs. This\napproach allows us to simultaneously leverage the powerful language processing\ncapabilities of LLMs and the efficient optimization performance of EAs.\nSpecifically, abstaining from any gradients or parameters, EvoPrompt starts\nfrom a population of prompts and iteratively generates new prompts with LLMs\nbased on the evolutionary operators, improving the population based on the\ndevelopment set. We optimize prompts for both closed- and open-source LLMs\nincluding GPT-3.5 and Alpaca, on 31 datasets covering language understanding,\ngeneration tasks, as well as BIG-Bench Hard (BBH) tasks. EvoPrompt\nsignificantly outperforms human-engineered prompts and existing methods for\nautomatic prompt generation (e.g., up to 25% on BBH). Furthermore, EvoPrompt\ndemonstrates that connecting LLMs with EAs creates synergies, which could\ninspire further research on the combination of LLMs and conventional\nalgorithms.\n'] , ['  The advent of large language models (LLMs) has transformed text-based\nservices, enabling capabilities ranging from real-time translation to AI-driven\nchatbots. However, existing serving systems primarily focus on optimizing\nserver-side aggregate metrics like token generation throughput, ignoring\nindividual user experience with streamed text. As a result, under high and/or\nbursty load, a significant number of users can receive unfavorable service\nquality or poor Quality-of-Experience (QoE). In this paper, we first formally\ndefine QoE of text streaming services, where text is delivered incrementally\nand interactively to users, by considering the end-to-end token delivery\nprocess throughout the entire interaction with the user. Thereafter, we propose\nAndes, a QoE-aware serving system that enhances user experience for LLM-enabled\ntext streaming services. At its core, Andes strategically allocates contended\nGPU resources among multiple requests over time to optimize their QoE. Our\nevaluations demonstrate that, compared to the state-of-the-art LLM serving\nsystems like vLLM, Andes improves the average QoE by up to 3.2$\\times$ under\nhigh request rate, or alternatively, it attains up to 1.6$\\times$ higher\nrequest rate while preserving high QoE.\n', '  High-demand LLM inference services (e.g., ChatGPT and BARD) support a wide\nrange of requests from short chat conversations to long document reading. To\nensure that all client requests are processed fairly, most major LLM inference\nservices have request rate limits, to ensure that no client can dominate the\nrequest queue. However, this rudimentary notion of fairness also results in\nunder-utilization of the resources and poor client experience when there is\nspare capacity. While there is a rich literature on fair scheduling, serving\nLLMs presents new challenges due to their unpredictable request lengths and\ntheir unique batching characteristics on parallel accelerators. This paper\nintroduces the definition of LLM serving fairness based on a cost function that\naccounts for the number of input and output tokens processed. To achieve\nfairness in serving, we propose a novel scheduling algorithm, the Virtual Token\nCounter (VTC), a fair scheduler based on the continuous batching mechanism. We\nprove a 2x tight upper bound on the service difference between two backlogged\nclients, adhering to the requirement of work-conserving. Through extensive\nexperiments, we demonstrate the superior performance of VTC in ensuring\nfairness, especially in contrast to other baseline methods, which exhibit\nshortcomings under various conditions. The reproducible code is available at\nhttps://github.com/Ying1123/VTC-artifact\n', '  Each LLM serving request goes through two phases. The first is prefill which\nprocesses the entire input prompt and produces the first output token and the\nsecond is decode which generates the rest of output tokens, one-at-a-time.\nPrefill iterations have high latency but saturate GPU compute due to parallel\nprocessing of the input prompt. In contrast, decode iterations have low latency\nbut also low compute utilization because a decode iteration processes only a\nsingle token per request. This makes batching highly effective for decodes and\nconsequently for overall throughput. However, batching multiple requests leads\nto an interleaving of prefill and decode iterations which makes it challenging\nto achieve both high throughput and low latency.\n  We introduce an efficient LLM inference scheduler, Sarathi-Serve, to address\nthis throughput-latency tradeoff. Sarathi-Serve introduces chunked-prefills\nwhich splits a prefill request into near equal sized chunks and creates\nstall-free schedules that adds new requests in a batch without pausing ongoing\ndecodes. Stall-free scheduling unlocks the opportunity to improve throughput\nwith large batch sizes while minimizing the effect of batching on latency.\nFurthermore, uniform batches in Sarathi-Serve ameliorate the imbalance between\niterations resulting in minimal pipeline bubbles.\n  Our techniques yield significant improvements in inference performance across\nmodels and hardware under tail latency constraints. For Mistral-7B on single\nA100 GPUs, we achieve 2.6x higher serving capacity and up to 3.7x higher\nserving capacity for the Yi-34B model on two A100 GPUs as compared to vLLM.\nWhen used with pipeline parallelism on Falcon-180B, Sarathi-Serve provides up\nto 5.6x gain in the end-to-end serving capacity. The source code for\nSarathi-Serve is available at https://github.com/microsoft/sarathi-serve.\n'] , ['  Logs are important in modern software development with runtime information.\nLog parsing is the first step in many log-based analyses, that involve\nextracting structured information from unstructured log data. Traditional log\nparsers face challenges in accurately parsing logs due to the diversity of log\nformats, which directly impacts the performance of downstream log-analysis\ntasks. In this paper, we explore the potential of using Large Language Models\n(LLMs) for log parsing and propose LLMParser, an LLM-based log parser based on\ngenerative LLMs and few-shot tuning. We leverage four LLMs, Flan-T5-small,\nFlan-T5-base, LLaMA-7B, and ChatGLM-6B in LLMParsers. Our evaluation of 16\nopen-source systems shows that LLMParser achieves statistically significantly\nhigher parsing accuracy than state-of-the-art parsers (a 96% average parsing\naccuracy). We further conduct a comprehensive empirical analysis on the effect\nof training size, model size, and pre-training LLM on log parsing accuracy. We\nfind that smaller LLMs may be more effective than more complex LLMs; for\ninstance where Flan-T5-base achieves comparable results as LLaMA-7B with a\nshorter inference time. We also find that using LLMs pre-trained using logs\nfrom other systems does not always improve parsing accuracy. While using\npre-trained Flan-T5-base shows an improvement in accuracy, pre-trained LLaMA\nresults in a decrease (decrease by almost 55% in group accuracy). In short, our\nstudy provides empirical evidence for using LLMs for log parsing and highlights\nthe limitations and future research direction of LLM-based log parsers.\n', '  Log parsing is a critical step that transforms unstructured log data into\nstructured formats, facilitating subsequent log-based analysis. Traditional\nsyntax-based log parsers are efficient and effective, but they often experience\ndecreased accuracy when processing logs that deviate from the predefined rules.\nRecently, large language models (LLM) based log parsers have shown superior\nparsing accuracy. However, existing LLM-based parsers face three main\nchallenges: 1)time-consuming and labor-intensive manual labeling for\nfine-tuning or in-context learning, 2)increased parsing costs due to the vast\nvolume of log data and limited context size of LLMs, and 3)privacy risks from\nusing commercial models like ChatGPT with sensitive log information. To\novercome these limitations, this paper introduces OpenLogParser, an\nunsupervised log parsing approach that leverages open-source LLMs (i.e.,\nLlama3-8B) to enhance privacy and reduce operational costs while achieving\nstate-of-the-art parsing accuracy. OpenLogParser first groups logs with similar\nstatic text but varying dynamic variables using a fixed-depth grouping tree. It\nthen parses logs within these groups using three components: i)similarity\nscoring-based retrieval augmented generation: selects diverse logs within each\ngroup based on Jaccard similarity, helping the LLM distinguish between static\ntext and dynamic variables; ii)self-reflection: iteratively query LLMs to\nrefine log templates to improve parsing accuracy; and iii) log template memory:\nstores parsed templates to reduce LLM queries for improved parsing efficiency.\nOur evaluation on LogHub-2.0 shows that OpenLogParser achieves 25% higher\nparsing accuracy and processes logs 2.7 times faster compared to\nstate-of-the-art LLM-based parsers. In short, OpenLogParser addresses privacy\nand cost concerns of using commercial LLMs while achieving state-of-the-arts\nparsing efficiency and accuracy.\n', ""  Logs are a first-hand source of information for software maintenance and\nfailure diagnosis. Log parsing, which converts semi-structured log messages\ninto structured templates, is a prerequisite for automated log analysis tasks\nsuch as anomaly detection, troubleshooting, and root cause analysis. However,\nexisting log parsers fail in real-world systems for three main reasons. First,\ntraditional heuristics-based parsers require handcrafted features and domain\nknowledge, which are difficult to generalize at scale. Second, existing large\nlanguage model-based parsers rely on periodic offline processing, limiting\ntheir effectiveness in real-time use cases. Third, existing online parsing\nalgorithms are susceptible to log drift, where slight log changes create false\npositives that drown out real anomalies. To address these challenges, we\npropose HELP, a Hierarchical Embeddings-based Log Parser. HELP is the first\nonline semantic-based parser to leverage LLMs for performant and cost-effective\nlog parsing. We achieve this through a novel hierarchical embeddings module,\nwhich fine-tunes a text embedding model to cluster logs before parsing,\nreducing querying costs by multiple orders of magnitude. To combat log drift,\nwe also develop an iterative rebalancing module, which periodically updates\nexisting log groupings. We evaluate HELP extensively on 14 public large-scale\ndatasets, showing that HELP achieves significantly higher F1-weighted grouping\nand parsing accuracy than current state-of-the-art online log parsers. We also\nimplement HELP into Iudex's production observability platform, confirming\nHELP's practicality in a production environment. Our results show that HELP is\neffective and efficient for high-throughput real-world log parsing.\n""] , ['  KV cache stores key and value states from previous tokens to avoid\nre-computation, yet it demands substantial storage space, especially for long\nsequences. Adaptive KV cache compression seeks to discern the saliency of\ntokens, preserving vital information while aggressively compressing those of\nless importance. However, previous methods of this approach exhibit significant\nperformance degradation at high compression ratios due to inaccuracies in\nidentifying salient tokens. In this paper, we present ZipCache, an accurate and\nefficient KV cache quantization method for LLMs. First, we construct a strong\nbaseline for quantizing KV cache. Through the proposed channel-separable\ntokenwise quantization scheme, the memory overhead of quantization parameters\nare substantially reduced compared to fine-grained groupwise quantization. To\nenhance the compression ratio, we propose normalized attention score as an\neffective metric for identifying salient tokens by considering the lower\ntriangle characteristics of the attention matrix. Moreover, we develop an\nefficient approximation method that decouples the saliency metric from full\nattention scores, enabling compatibility with fast attention implementations\nlike FlashAttention. Extensive experiments demonstrate that ZipCache achieves\nsuperior compression ratios, fast generation speed and minimal performance\nlosses compared with previous KV cache compression methods. For instance, when\nevaluating Mistral-7B model on GSM8k dataset, ZipCache is capable of\ncompressing the KV cache by $4.98\\times$, with only a $0.38\\%$ drop in\naccuracy. In terms of efficiency, ZipCache also showcases a $37.3\\%$ reduction\nin prefill-phase latency, a $56.9\\%$ reduction in decoding-phase latency, and a\n$19.8\\%$ reduction in GPU memory usage when evaluating LLaMA3-8B model with a\ninput length of $4096$.\n', '  Key-Value (KV) Caching has become an essential technique for accelerating the\ninference speed and throughput of generative Large Language Models~(LLMs).\nHowever, the memory footprint of the KV cache poses a critical bottleneck in\nLLM deployment as the cache size grows with batch size and sequence length,\noften surpassing even the size of the model itself. Although recent methods\nwere proposed to select and evict unimportant KV pairs from the cache to reduce\nmemory consumption, the potential ramifications of eviction on the generative\nprocess are yet to be thoroughly examined. In this paper, we examine the\ndetrimental impact of cache eviction and observe that unforeseen risks arise as\nthe information contained in the KV pairs is exhaustively discarded, resulting\nin safety breaches, hallucinations, and context loss. Surprisingly, we find\nthat preserving even a small amount of information contained in the evicted KV\npairs via reduced precision quantization substantially recovers the incurred\ndegradation. On the other hand, we observe that the important KV pairs must be\nkept at a relatively higher precision to safeguard the generation quality.\nMotivated by these observations, we propose \\textit{Mixed-precision KV\ncache}~(MiKV), a reliable cache compression method that simultaneously\npreserves the context details by retaining the evicted KV pairs in\nlow-precision and ensure generation quality by keeping the important KV pairs\nin high-precision. Experiments on diverse benchmarks and LLM backbones show\nthat our proposed method offers a state-of-the-art trade-off between\ncompression ratio and performance, compared to other baselines.\n', '  Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI.\n'] , ['  Large Language Models (LLMs) have achieved remarkable results, but their\nincreasing resource demand has become a major obstacle to the development of\npowerful and accessible super-human intelligence. This report introduces\nJetMoE-8B, a new LLM trained with less than $0.1 million, using 1.25T tokens\nfrom carefully mixed open-source corpora and 30,000 H100 GPU hours. Despite its\nlow cost, the JetMoE-8B demonstrates impressive performance, with JetMoE-8B\noutperforming the Llama2-7B model and JetMoE-8B-Chat surpassing the\nLlama2-13B-Chat model. These results suggest that LLM training can be much more\ncost-effective than generally thought. JetMoE-8B is based on an efficient\nSparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention\nand feedforward experts. Both layers are sparsely activated, allowing JetMoE-8B\nto have 8B parameters while only activating 2B for each input token, reducing\ninference computation by about 70% compared to Llama2-7B. Moreover, JetMoE-8B\nis highly open and academia-friendly, using only public datasets and training\ncode. All training parameters and data mixtures have been detailed in this\nreport to facilitate future efforts in the development of open foundation\nmodels. This transparency aims to encourage collaboration and further\nadvancements in the field of accessible and efficient LLMs. The model weights\nare publicly available at https://github.com/myshell-ai/JetMoE.\n', '  In this paper, we present Paramanu-Ganita, a 208 million parameter novel Auto\nRegressive (AR) decoder based language model on mathematics. The model is\npretrained from scratch at context size of 4096 on our curated mixed\nmathematical corpus. We evaluate our model on both perplexity metric and GSM8k\nmathematical benchmark. Paramanu-Ganita despite being 35 times smaller than 7B\nLLMs, outperformed generalist LLMs such as LLaMa-1 7B by 28.4% points, LLaMa-2\n7B by 27.6% points, Falcon 7B by 32.6% points, PaLM 8B by 35.3% points, and\nmath specialised LLMs such as Minerva 8B by 23.2% points, and LLEMMA-7B by 3.0%\npoints in GSM8k test accuracy metric respectively. Paramanu-Ganita also\noutperformed giant LLMs like PaLM 62B by 6.4% points, Falcon 40B by 19.8%\npoints, LLaMa-1 33B by 3.8% points and Vicuna 13B by 11.8% points respectively.\nThe large significant margin improvement in performance of our math model over\nthe existing LLMs signifies that reasoning capabilities of language model are\njust not restricted to LLMs with humongous number of parameters.\nParamanu-Ganita took 146 hours of A100 training whereas math specialised LLM,\nLLEMMA 7B, was trained for 23,000 A100 hours of training equivalent. Thus, our\napproach of pretraining powerful domain specialised language models from\nscratch for domain adaptation is much more cost-effective than performing\ncontinual training of LLMs for domain adaptation. Hence, we conclude that for\nstrong mathematical reasoning abilities of language model, we do not need giant\nLLMs and immense computing power to our end. In the end, we want to point out\nthat we have only trained Paramanu-Ganita only on a part of our entire\nmathematical corpus and yet to explore the full potential of our model.\n', ""  Despite the general capabilities of large pretrained language models, they\nconsistently benefit from further adaptation to better achieve desired\nbehaviors. However, tuning these models has become increasingly\nresource-intensive, or impossible when model weights are private. We introduce\nproxy-tuning, a lightweight decoding-time algorithm that operates on top of\nblack-box LMs to achieve the same end as direct tuning, but by accessing only\nits predictions over the output vocabulary, not its parameters. Our method\ntunes a smaller LM, then applies the difference between the predictions of the\nsmall tuned and untuned LMs to shift the original predictions of the larger\nuntuned model in the direction of tuning, while retaining the benefits of\nlarger-scale pretraining. In experiments, when we apply proxy-tuning to\nLlama2-70B using proxies of only 7B size, we can close 88% of the gap between\nLlama2-70B and its truly-tuned chat version, when evaluated across knowledge,\nreasoning, and safety benchmarks. Interestingly, on TruthfulQA, proxy-tuned\nmodels are actually more truthful than directly tuned models, possibly because\ndecoding-time guidance better retains the model's factual knowledge. We then\ndemonstrate the generality of proxy-tuning by applying it to domain adaptation\non code, and task-specific finetuning on question-answering and math problems.\nFinally, we show how to proxy-tune a truly black-box LM, GPT-3.5, for temporal\nadaptation, increasing its knowledge about recent events. Our work demonstrates\nthe promise of using small tuned LMs to efficiently customize large,\npotentially proprietary LMs through decoding-time guidance.\n""] , [""  Large language models (LLMs) hold the promise of solving diverse tasks when\nprovided with appropriate natural language prompts. However, prompting often\nleads models to make predictions with lower accuracy compared to finetuning a\nmodel with ample training data. On the other hand, while finetuning LLMs on\ntask-specific data generally improves their performance, abundant annotated\ndatasets are not available for all tasks. Previous work has explored generating\ntask-specific data from state-of-the-art LLMs and using this data to finetune\nsmaller models, but this approach requires access to a language model other\nthan the one being trained, which introduces cost, scalability challenges, and\nlegal hurdles associated with continuously relying on more powerful LLMs. In\nresponse to these, we propose SELF-GUIDE, a multi-stage mechanism in which we\nsynthesize task-specific input-output pairs from the student LLM, then use\nthese input-output pairs to finetune the student LLM itself. In our empirical\nevaluation of the Natural Instructions V2 benchmark, we find that SELF-GUIDE\nimproves the performance of LLM by a substantial margin. Specifically, we\nreport an absolute improvement of approximately 15% for classification tasks\nand 18% for generation tasks in the benchmark's metrics. This sheds light on\nthe promise of self-synthesized data guiding LLMs towards becoming\ntask-specific experts without any external learning signals.\n"", ""  Despite the strong performance of large language models (LLMs) across a wide\nrange of tasks, they still have reliability issues. Previous studies indicate\nthat strong LLMs like GPT-4-turbo excel in evaluating the reliability of\nresponses from LLMs, but face efficiency and local deployment issues. Thus, to\nenable weak LLMs to effectively assess the reliability of LLM responses, we\npropose a novel cross-query-comparison-based method called $\\textit{Meta\nRanking}$ (MR). Unlike previous few-shot methods that solely based on\nin-context learning capabilities in LLMs, MR assesses reliability by pairwisely\nranking the target query-response pair with multiple reference query-response\npairs. We found that MR is highly effective in error detection for LLM\nresponses, where weak LLMs, such as Phi-2, could surpass strong baselines like\nGPT-3.5-turbo, requiring only five reference samples and significantly\nimproving efficiency. We further demonstrate that MR can enhance strong LLMs'\nperformance in two practical applications: model cascading and instruction\ntuning. In model cascading, we combine open- and closed-source LLMs to achieve\nperformance comparable to GPT-4-turbo with lower costs. In instruction tuning,\nwe use MR for iterative training data filtering, significantly reducing data\nprocessing time and enabling LLaMA-7B and Phi-2 to surpass Alpaca-13B with\nfewer training tokens. These results underscore the high potential of MR in\nboth efficiency and effectiveness.\n"", '  While training large language models (LLMs) from scratch can indeed lead to\nmodels with distinct capabilities and strengths, it incurs substantial costs\nand may lead to redundancy in competencies. Knowledge fusion aims to integrate\nexisting LLMs of diverse architectures and capabilities into a more potent LLM\nthrough lightweight continual training, thereby reducing the need for costly\nLLM development. In this work, we propose a new framework for the knowledge\nfusion of chat LLMs through two main stages, resulting in FuseChat. Firstly, we\nconduct pairwise knowledge fusion on source chat LLMs of varying structures and\nscales to create multiple target LLMs with identical structure and size via\nlightweight fine-tuning. During this process, a statistics-based token\nalignment approach is introduced as the cornerstone for fusing LLMs with\ndifferent structures. Secondly, we merge these target LLMs within the parameter\nspace, where we propose a novel method for determining the merging coefficients\nbased on the magnitude of parameter updates before and after fine-tuning. We\nimplement and validate FuseChat using six prominent chat LLMs with diverse\narchitectures and scales, including OpenChat-3.5-7B, Starling-LM-7B-alpha,\nNH2-SOLAR-10.7B, InternLM2-Chat-20B, Mixtral-8x7B-Instruct, and\nQwen-1.5-Chat-72B. Experimental results on two instruction-following\nbenchmarks, AlpacaEval 2.0 and MT-Bench, demonstrate the superiority of\nFuseChat-7B over baselines of various sizes. Our model is even comparable to\nthe larger Mixtral-8x7B-Instruct and approaches GPT-3.5-Turbo-1106 on MT-Bench.\nOur code, model weights, and data are public at\n\\url{https://github.com/fanqiwan/FuseAI}.\n'] , [""  Large language models (LLMs) are increasingly integrated into many online\nservices, yet they remain cost-prohibitive to deploy due to the requirement of\nexpensive GPU instances. Prior work has addressed the high cost of LLM serving\nby improving the inference engine, but less attention has been given to\nselecting the most cost-efficient GPU type(s) for a specific LLM service. There\nis a large and growing landscape of GPU types and, within these options, higher\ncost does not always lead to increased performance. Instead, through a\ncomprehensive investigation, we find that three key LLM service characteristics\n(request size, request rate, SLO) strongly influence GPU cost efficiency, and\ndiffering GPU types are most cost efficient for differing LLM service settings.\nAs a result, the most cost-efficient allocation for a given service is\ntypically a mix of heterogeneous GPU types. Based on this analysis, we\nintroduce M\\'elange, a GPU allocation framework that navigates these diverse\nLLM service characteristics and heterogeneous GPU option space to automatically\nand efficiently derive the minimal-cost GPU allocation for a given LLM service.\nWe formulate the GPU allocation task as a cost-aware bin packing problem where\nGPUs are bins and items are slices of the service workload. Our formulation's\nconstraints account for a service's unique characteristics, allowing M\\'elange\nto be flexible to support diverse service settings and heterogeneity-aware to\nadapt the GPU allocation to a specific service. Compared to using only a single\nGPU type, M\\'elange reduces deployment costs by up to 77% in conversational\nsettings, 33% in document-based settings, and 51% in a mixed setting.\n"", ""  Transformer-based Large Language Models (LLMs) have made a significant impact\non various domains. However, LLMs' efficiency suffers from both heavy\ncomputation and memory overheads. Compression techniques like sparsification\nand quantization are commonly used to mitigate the gap between LLM's\ncomputation/memory overheads and hardware capacity. However, existing GPU and\ntransformer-based accelerators cannot efficiently process compressed LLMs, due\nto the following unresolved challenges: low computational efficiency,\nunderutilized memory bandwidth, and large compilation overheads.\n  This paper proposes FlightLLM, enabling efficient LLMs inference with a\ncomplete mapping flow on FPGAs. In FlightLLM, we highlight an innovative\nsolution that the computation and memory overhead of LLMs can be solved by\nutilizing FPGA-specific resources (e.g., DSP48 and heterogeneous memory\nhierarchy). We propose a configurable sparse DSP chain to support different\nsparsity patterns with high computation efficiency. Second, we propose an\nalways-on-chip decode scheme to boost memory bandwidth with mixed-precision\nsupport. Finally, to make FlightLLM available for real-world LLMs, we propose a\nlength adaptive compilation method to reduce the compilation overhead.\nImplemented on the Xilinx Alveo U280 FPGA, FlightLLM achieves 6.0$\\times$\nhigher energy efficiency and 1.8$\\times$ better cost efficiency against\ncommercial GPUs (e.g., NVIDIA V100S) on modern LLMs (e.g., LLaMA2-7B) using\nvLLM and SmoothQuant under the batch size of one. FlightLLM beats NVIDIA A100\nGPU with 1.2$\\times$ higher throughput using the latest Versal VHK158 FPGA.\n"", '  Recent advancements in large language models (LLMs) boasting billions of\nparameters have generated a significant demand for efficient deployment in\ninference workloads. The majority of existing approaches rely on temporal\narchitectures that reuse hardware units for different network layers and\noperators. However, these methods often encounter challenges in achieving low\nlatency due to considerable memory access overhead. This paper investigates the\nfeasibility and potential of model-specific spatial acceleration for LLM\ninference on FPGAs. Our approach involves the specialization of distinct\nhardware units for specific operators or layers, facilitating direct\ncommunication between them through a dataflow architecture while minimizing\noff-chip memory accesses. We introduce a comprehensive analytical model for\nestimating the performance of a spatial LLM accelerator, taking into account\nthe on-chip compute and memory resources available on an FPGA. Through our\nanalysis, we can determine the scenarios in which FPGA-based spatial\nacceleration can outperform its GPU-based counterpart. To enable more\nproductive implementations of an LLM model on FPGAs, we further provide a\nlibrary of high-level synthesis (HLS) kernels that are composable and reusable.\nThis library will be made available as open-source. To validate the\neffectiveness of both our analytical model and HLS library, we have implemented\nBERT and GPT2 on an AMD Alveo U280 FPGA device. Experimental results\ndemonstrate our approach can achieve up to 13.4x speedup when compared to\nprevious FPGA-based accelerators for the BERT model. For GPT generative\ninference, we attain a 2.2x speedup compared to DFX, an FPGA overlay, in the\nprefill stage, while achieving a 1.9x speedup and a 5.7x improvement in energy\nefficiency compared to the NVIDIA A100 GPU in the decode stage.\n'] , [""  Due to the advantages in the cost-efficiency and reproducibility, user\nsimulation has become a promising solution to the user-centric evaluation of\ninformation retrieval systems. Nonetheless, accurately simulating user search\nbehaviors has long been a challenge, because users' actions in search are\nhighly complex and driven by intricate cognitive processes such as learning,\nreasoning, and planning. Recently, Large Language Models (LLMs) have\ndemonstrated remarked potential in simulating human-level intelligence and have\nbeen used in building autonomous agents for various tasks. However, the\npotential of using LLMs in simulating search behaviors has not yet been fully\nexplored. In this paper, we introduce a LLM-based user search behavior\nsimulator, USimAgent. The proposed simulator can simulate users' querying,\nclicking, and stopping behaviors during search, and thus, is capable of\ngenerating complete search sessions for specific search tasks. Empirical\ninvestigation on a real user behavior dataset shows that the proposed simulator\noutperforms existing methods in query generation and is comparable to\ntraditional methods in predicting user clicks and stopping behaviors. These\nresults not only validate the effectiveness of using LLMs for user simulation\nbut also shed light on the development of a more robust and generic user\nsimulators.\n"", '  Due to the excellent capacities of large language models (LLMs), it becomes\nfeasible to develop LLM-based agents for reliable user simulation. Considering\nthe scarcity and limit (e.g., privacy issues) of real user data, in this paper,\nwe conduct large-scale user simulation for web search, to improve the analysis\nand modeling of user search behavior. Specially, we propose BASES, a novel user\nsimulation framework with LLM-based agents, designed to facilitate\ncomprehensive simulations of web search user behaviors. Our simulation\nframework can generate unique user profiles at scale, which subsequently leads\nto diverse search behaviors. To demonstrate the effectiveness of BASES, we\nconduct evaluation experiments based on two human benchmarks in both Chinese\nand English, demonstrating that BASES can effectively simulate large-scale\nhuman-like search behaviors. To further accommodate the research on web search,\nwe develop WARRIORS, a new large-scale dataset encompassing web search user\nbehaviors, including both Chinese and English versions, which can greatly\nbolster research in the field of information retrieval. Our code and data will\nbe publicly released soon.\n', ""  Large Language Models (LLMs) excel at tackling various natural language\ntasks. However, due to the significant costs involved in re-training or\nfine-tuning them, they remain largely static and difficult to personalize.\nNevertheless, a variety of applications could benefit from generations that are\ntailored to users' preferences, goals, and knowledge. Among them is web search,\nwhere knowing what a user is trying to accomplish, what they care about, and\nwhat they know can lead to improved search experiences. In this work, we\npropose a novel and general approach that augments an LLM with relevant context\nfrom users' interaction histories with a search engine in order to personalize\nits outputs. Specifically, we construct an entity-centric knowledge store for\neach user based on their search and browsing activities on the web, which is\nthen leveraged to provide contextually relevant LLM prompt augmentations. This\nknowledge store is light-weight, since it only produces user-specific aggregate\nprojections of interests and knowledge onto public knowledge graphs, and\nleverages existing search log infrastructure, thereby mitigating the privacy,\ncompliance, and scalability concerns associated with building deep user\nprofiles for personalization. We validate our approach on the task of\ncontextual query suggestion, which requires understanding not only the user's\ncurrent search context but also what they historically know and care about.\nThrough a number of experiments based on human evaluation, we show that our\napproach is significantly better than several other LLM-powered baselines,\ngenerating query suggestions that are contextually more relevant, personalized,\nand useful.\n""] , ['  Optimization problems are pervasive in sectors from manufacturing and\ndistribution to healthcare. However, most such problems are still solved\nheuristically by hand rather than optimally by state-of-the-art solvers because\nthe expertise required to formulate and solve these problems limits the\nwidespread adoption of optimization tools and techniques. This paper introduces\nOptiMUS, a Large Language Model (LLM)-based agent designed to formulate and\nsolve (mixed integer) linear programming problems from their natural language\ndescriptions. OptiMUS can develop mathematical models, write and debug solver\ncode, evaluate the generated solutions, and improve its model and code based on\nthese evaluations. OptiMUS utilizes a modular structure to process problems,\nallowing it to handle problems with long descriptions and complex data without\nlong prompts. Experiments demonstrate that OptiMUS outperforms existing\nstate-of-the-art methods on easy datasets by more than $20\\%$ and on hard\ndatasets (including a new dataset, NLP4LP, released with this paper that\nfeatures long and complex problems) by more than $30\\%$.\n', ""  Large language models (LLMs) have exhibited their problem-solving ability in\nmathematical reasoning. Solving realistic optimization (OPT) problems in\nindustrial application scenarios requires advanced and applied math ability.\nHowever, current OPT benchmarks that merely solve linear programming are far\nfrom complex realistic situations. In this work, we propose E-OPT, a benchmark\nfor end-to-end optimization problem-solving with human-readable inputs and\noutputs. E-OPT contains rich optimization problems, including linear/nonlinear\nprogramming with/without table data, which can comprehensively evaluate LLMs'\nsolving ability. In our benchmark, LLMs are required to correctly understand\nthe problem in E-OPT and call code solver to get precise numerical answers.\nFurthermore, to alleviate the data scarcity for optimization problems, and to\nbridge the gap between open-source LLMs on a small scale (e.g., Llama-2-7b and\nLlama-3-8b) and closed-source LLMs (e.g., GPT-4), we further propose a novel\ndata synthesis method namely ReSocratic. Unlike general data synthesis methods\nthat proceed from questions to answers, ReSocratic first incrementally\nsynthesizes optimization scenarios with mathematical formulations step by step\nand then back-translates the generated scenarios into questions. In such a way,\nwe construct the ReSocratic-29k dataset from a small seed sample pool with the\npowerful open-source large model DeepSeek-V2. To demonstrate the effectiveness\nof ReSocratic, we conduct supervised fine-tuning with ReSocratic-29k on\nmultiple open-source models. The results show that Llama3-8b is significantly\nimproved from 13.6% to 51.7% on E-OPT, while DeepSeek-V2 reaches 61.0%,\napproaching 65.5% of GPT-4.\n"", '  Optimization problems are pervasive in sectors from manufacturing and\ndistribution to healthcare. However, most such problems are still solved\nheuristically by hand rather than optimally by state-of-the art solvers because\nthe expertise required to formulate and solve these problems limits the\nwidespread adoption of optimization tools and techniques. We introduce a Large\nLanguage Model (LLM)-based system designed to formulate and solve (mixed\ninteger) linear programming problems from their natural language descriptions.\nOur system is capable of developing mathematical models, writing and debugging\nsolver code, evaluating the generated solutions, and improving efficiency and\ncorrectness of its model and code based on these evaluations. OptiMUS-0.3\nutilizes a modular structure to process problems, allowing it to handle\nproblems with long descriptions and complex data without long prompts.\nExperiments demonstrate that OptiMUS-0.3 outperforms existing state-of-the-art\nmethods on easy datasets by more than 12% and on hard datasets (including a new\ndataset, NLP4LP, released with this paper that features long and complex\nproblems) by more than 8%.\n']",Optimization and Efficiency of Large Language Models,Optimization Problem Solving with Large Language Models
3,"""Aligning Language Models with Human Preferences"" , Model Editing in Large Language Models , Confidence Estimation in Large Language Models , Scaling Laws for Large Language Models , ""Unlearning in Large Language Models"" , ""Tool Learning and Utilization in Large Language Models"" , Memorization in Large Language Models , Automated Alignment in Large Language Models , Evaluating Faithfulness in Large Language Models , Catastrophic Forgetting in Large Language Models , Sparse Activations in Large Language Models , Self-Correction in Large Language Models , Fine-Tuning Strategies for Large Language Models , Fine-Tuning Large Language Models","['reward', 'reinforcement', 'rewards', 'supervised', 'learning', 'trained', 'language', 'preference', 'ranking', 'sampling'] , ['editing', 'editor', 'editors', 'edits', 'modifying', 'modify', 'edit', 'retraining', 'edited', 'forgetting'] , ['confidence', 'confident', 'reliable', 'language', 'verbalized', 'reliability', 'models', 'uncertainties', 'prediction', 'semantic'] , ['scaling', 'models', 'smaller', 'scale', 'autoscale', 'larger', 'predicting', 'performance', 'predictable', 'language'] , ['unlearning', 'ununlearning', 'unlearn', 'memorization', 'forgetting', 'memorize', 'memorized', 'unlearned', 'corpus', 'forget'] , ['tools', 'toolkits', 'toollens', 'toolsets', 'tool', 'toolflow', 'toolnet', 'toolbench', 'toolset', 'toolname'] , ['memorization', 'memorisation', 'memorized', 'memorise', 'linguistic', 'neural', 'language', 'sentences', 'attentional', 'lexical'] , ['alignment', 'annotation', 'aligned', 'aligner', 'language', 'aligning', 'models', 'automated', 'position', 'aligncot'] , ['explanations', 'counterfactuals', 'reasonability', 'implicatures', 'explaining', 'counterfactual', 'explainer', 'readability', 'language', 'predictions'] , ['forgetting', 'continual', 'forget', 'languages', 'catastrophic', 'pretraining', 'training', 'language', 'tuning', 'tuned'] , ['activations', 'sparse', 'activation', 'neuron', 'neurons', 'progressive_gradient_flow_nm_sparsity', 'relu', 'sparsely', 'sparsity', 'sparsification'] , ['selfreflection', 'responses', 'corrections', 'self', 'mistakes', 'revise', 'improve', 'feedback', 'refine', 'criticize'] , ['pretraining', 'trained', 'unlearning', 'pretrained', 'training', 'benchmarks', 'tuning', 'datasets', 'dataset', 'tuned'] , ['memorization', 'pretrained', 'tuning', 'trained', 'language', 'tuned', 'training', 'generating', 'tasks', 'examples']","['  RLHF has emerged as a pivotal step in aligning language models with human\nobjectives and values. It typically involves learning a reward model from human\npreference data and then using reinforcement learning to update the generative\nmodel accordingly. Conversely, Direct Preference Optimization (DPO) directly\noptimizes the generative model with preference data, skipping reinforcement\nlearning. However, both RLHF and DPO assume uniform preferences, overlooking\nthe reality of diverse human annotators. This paper presents a new method to\nalign generative models with varied human preferences. We propose an\nExpectation-Maximization adaptation to DPO, generating a mixture of models\nbased on latent preference types of the annotators. We then introduce a min-max\nregret ensemble learning model to produce a single generative method to\nminimize worst-case regret among annotator subgroups with similar latent\nfactors. Our algorithms leverage the simplicity of DPO while accommodating\ndiverse preferences. Experimental results validate the effectiveness of our\napproach in producing equitable generative policies.\n', '  Aligning human preference and value is an important requirement for building\ncontemporary foundation models and embodied AI. However, popular approaches\nsuch as reinforcement learning with human feedback (RLHF) break down the task\ninto successive stages, such as supervised fine-tuning (SFT), reward modeling\n(RM), and reinforcement learning (RL), each performing one specific learning\ntask. Such a sequential approach results in serious issues such as significant\nunder-utilization of data and distribution mismatch between the learned reward\nmodel and generated policy, which eventually lead to poor alignment\nperformance. We develop a single stage approach named Alignment with Integrated\nHuman Feedback (AIHF), capable of integrating both human preference and\ndemonstration to train reward models and the policy. The proposed approach\nadmits a suite of efficient algorithms, which can easily reduce to, and\nleverage, popular alignment algorithms such as RLHF and Directly Policy\nOptimization (DPO), and only requires minor changes to the existing alignment\npipelines. We demonstrate the efficiency of the proposed solutions with\nextensive experiments involving alignment problems in LLMs and robotic control\nproblems in MuJoCo. We observe that the proposed solutions outperform the\nexisting alignment algorithms such as RLHF and DPO by large margins, especially\nwhen the amount of high-quality preference data is relatively limited.\n', '  The success of reinforcement learning from human feedback (RLHF) in language\nmodel alignment is strongly dependent on the quality of the underlying reward\nmodel. In this paper, we present a novel approach to improve reward model\nquality by generating synthetic preference data, thereby augmenting the\ntraining dataset with on-policy, high-quality preference pairs. Motivated by\nthe promising results of Best-of-N sampling strategies in language model\ntraining, we extend their application to reward model training. This results in\na self-training strategy to generate preference pairs by selecting the best and\nworst candidates in a pool of responses to a given query. Empirically, we find\nthat this approach improves the performance of any reward model, with an effect\ncomparable to the addition of a similar quantity of human preference data. This\nwork opens up new avenues of research for improving RLHF for language model\nalignment, by offering synthetic preference generation as a solution to reward\nmodeling challenges.\n'] , [""  Model editing is a technique that edits the large language models (LLMs) with\nupdated knowledge to alleviate hallucinations without resource-intensive\nretraining. While current model editing methods can effectively modify a\nmodel's behavior within a specific area of interest, they often overlook the\npotential unintended side effects on the general abilities of LLMs such as\nreasoning, natural language inference, and question answering. In this paper,\nwe raise concerns that model editing's improvements on factuality may come at\nthe cost of a significant degradation of the model's general abilities. We\nsystematically analyze the side effects by evaluating four popular editing\nmethods on three LLMs across eight representative tasks. Our extensive\nempirical experiments show that it is challenging for current editing methods\nto simultaneously improve factuality of LLMs and maintain their general\nabilities. Our analysis reveals that the side effects are caused by model\nediting altering the original model weights excessively, leading to overfitting\nto the edited facts. To mitigate this, a method named RECT (RElative Change in\nweighT) is proposed to regularize the edit update weights. Evaluation results\nshow that RECT can significantly mitigate the side effects of editing while\nstill maintaining over 94% editing performance.\n"", '  Knowledge editing aims to rectify inaccuracies in large language models\n(LLMs) without costly retraining for outdated or erroneous knowledge. However,\ncurrent knowledge editing methods primarily focus on single editing, failing to\nmeet the requirements for lifelong editing. This study reveals a performance\ndegradation encountered by knowledge editing in lifelong editing, characterized\nby toxicity buildup and toxicity flash, with the primary cause identified as\npattern unmatch. We introduce a knowledge editing approach named Wise-Layer\nKnowledge Editor (WilKE), which selects editing layer based on the pattern\nmatching degree of editing knowledge across different layers in language\nmodels. Experimental results demonstrate that, in lifelong editing, WilKE\nexhibits an average improvement of 46.2% and 67.8% on editing GPT2-XL and GPT-J\nrelative to state-of-the-art knowledge editing methods.\n', '  This study presents a targeted model editing analysis focused on the latest\nlarge language model, Llama-3. We explore the efficacy of popular model editing\ntechniques - ROME, MEMIT, and EMMET, which are designed for precise layer\ninterventions. We identify the most effective layers for targeted edits through\nan evaluation that encompasses up to 4096 edits across three distinct\nstrategies: sequential editing, batch editing, and a hybrid approach we call as\nsequential-batch editing. Our findings indicate that increasing edit\nbatch-sizes may degrade model performance more significantly than using smaller\nedit batches sequentially for equal number of edits. With this, we argue that\nsequential model editing is an important component for scaling model editing\nmethods and future research should focus on methods that combine both batched\nand sequential editing. This observation suggests a potential limitation in\ncurrent model editing methods which push towards bigger edit batch sizes, and\nwe hope it paves way for future investigations into optimizing batch sizes and\nmodel editing performance.\n'] , ['  Empowering large language models to accurately express confidence in their\nanswers is essential for trustworthy decision-making. Previous confidence\nelicitation methods, which primarily rely on white-box access to internal model\ninformation or model fine-tuning, have become less suitable for LLMs,\nespecially closed-source commercial APIs. This leads to a growing need to\nexplore the untapped area of black-box approaches for LLM uncertainty\nestimation. To better break down the problem, we define a systematic framework\nwith three components: prompting strategies for eliciting verbalized\nconfidence, sampling methods for generating multiple responses, and aggregation\ntechniques for computing consistency. We then benchmark these methods on two\nkey tasks-confidence calibration and failure prediction-across five types of\ndatasets (e.g., commonsense and arithmetic reasoning) and five widely-used LLMs\nincluding GPT-4 and LLaMA 2 Chat. Our analysis uncovers several key insights:\n1) LLMs, when verbalizing their confidence, tend to be overconfident,\npotentially imitating human patterns of expressing confidence. 2) As model\ncapability scales up, both calibration and failure prediction performance\nimprove. 3) Employing our proposed strategies, such as human-inspired prompts,\nconsistency among multiple responses, and better aggregation strategies can\nhelp mitigate this overconfidence from various perspectives. 4) Comparisons\nwith white-box methods indicate that while white-box methods perform better,\nthe gap is narrow, e.g., 0.522 to 0.605 in AUROC. Despite these advancements,\nnone of these techniques consistently outperform others, and all investigated\nmethods struggle in challenging tasks, such as those requiring professional\nknowledge, indicating significant scope for improvement. We believe this study\ncan serve as a strong baseline and provide insights for eliciting confidence in\nblack-box LLMs.\n', '  Although large language models (LLMs) are capable of performing various\ntasks, they still suffer from producing plausible but incorrect responses. To\nimprove the reliability of LLMs, recent research has focused on uncertainty\nquantification to predict whether a response is correct or not. However, most\nuncertainty quantification methods have been evaluated on questions requiring a\nsingle clear answer, ignoring the existence of data uncertainty that arises\nfrom irreducible randomness. Instead, these methods only consider model\nuncertainty, which arises from a lack of knowledge. In this paper, we\ninvestigate previous uncertainty quantification methods under the presence of\ndata uncertainty. Our contributions are two-fold: 1) proposing a new\nMulti-Answer Question Answering dataset, MAQA, consisting of world knowledge,\nmathematical reasoning, and commonsense reasoning tasks to evaluate uncertainty\nquantification regarding data uncertainty, and 2) assessing 5 uncertainty\nquantification methods of diverse white- and black-box LLMs. Our findings show\nthat entropy and consistency-based methods estimate the model uncertainty well\neven under data uncertainty, while other methods for white- and black-box LLMs\nstruggle depending on the tasks. Additionally, methods designed for white-box\nLLMs suffer from overconfidence in reasoning tasks compared to simple knowledge\nqueries. We believe our observations will pave the way for future work on\nuncertainty quantification in realistic setting.\n', ""  To enhance Large Language Models' (LLMs) reliability, calibration is\nessential -- the model's assessed confidence scores should align with the\nactual likelihood of its responses being correct. However, current confidence\nelicitation methods and calibration metrics typically rely on a binary\ntrue/false assessment of response correctness. This approach does not apply to\nlong-form generation, where an answer can be partially correct. Addressing this\ngap, we introduce a unified calibration framework, in which both the\ncorrectness of the LLMs' responses and their associated confidence levels are\ntreated as distributions across a range of scores. Within this framework, we\ndevelop three metrics to precisely evaluate LLM calibration and further propose\ntwo confidence elicitation methods based on self-consistency and\nself-evaluation. Our experiments, which include long-form QA and summarization\ntasks, demonstrate that larger models don't necessarily guarantee better\ncalibration, that calibration performance is found to be metric-dependent, and\nthat self-consistency methods excel in factoid datasets. We also find that\ncalibration can be enhanced through techniques such as fine-tuning, integrating\nrelevant source documents, scaling the temperature, and combining\nself-consistency with self-evaluation. Lastly, we showcase a practical\napplication of our system: selecting and cascading open-source models and\nChatGPT to optimize correctness given a limited API budget. This research not\nonly challenges existing notions of LLM calibration but also offers practical\nmethodologies for improving trustworthiness in long-form generation.\n""] , [""  The scientific scale-up of large language models (LLMs) necessitates a\ncomprehensive understanding of their scaling properties. However, the existing\nliterature on the scaling properties only yields an incomplete answer:\noptimization loss decreases predictably as the model size increases, in line\nwith established scaling law; yet no scaling law for task has been established\nand the task performances are far from predictable during scaling. Task\nperformances typically show minor gains on small models until they improve\ndramatically once models exceed a size threshold, exemplifying the ``emergent\nabilities''. In this study, we discover that small models, although they\nexhibit minor performance, demonstrate critical and consistent task performance\nimprovements that are not captured by conventional evaluation strategies due to\ninsufficient measurement resolution. To measure such improvements, we introduce\nPassUntil, an evaluation strategy with theoretically infinite resolution,\nthrough massive sampling in the decoding phase. With PassUntil, we conduct a\nquantitative investigation into the scaling law of task performance. The\ninvestigation contains two parts. Firstly, a strict task scaling law that is\nnot conventionally known to exist, is identified, enhancing the predictability\nof task performances. Remarkably, we are able to predict the performance of the\n2.4B model on code generation with merely 0.05\\% deviation before training\nstarts, which is the first systematic attempt to verify predictable scaling\nproposed by GPT-4's report. Secondly, we are able to study emergent abilities\nquantitatively. We identify a kind of accelerated emergence whose scaling curve\ncannot be fitted by standard scaling law function and has a increasing speed.\nWe then examine two hypothesis and imply that the ``multiple circuits\nhypothesis'' might be responsible for the accelerated emergence.\n"", '  Large language model pre-training has become increasingly expensive, with\nmost practitioners relying on scaling laws to allocate compute budgets for\nmodel size and training tokens, commonly referred to as Compute-Optimal or\nChinchilla Optimal. In this paper, we hypothesize a new scaling law that\nsuggests model performance depends mostly on the amount of compute spent for\ntransformer-based models, independent of the specific allocation to model size\nand dataset size. Using this unified scaling law, we predict that (a) for\ninference efficiency, training should prioritize smaller model sizes and larger\ntraining datasets, and (b) assuming the exhaustion of available web datasets,\nscaling the model size might be the only way to further improve model\nperformance.\n', '  Recently, Large Language Models (LLMs) have been widely adopted in a wide\nrange of tasks, leading to increasing attention towards the research on how\nscaling LLMs affects their performance. Existing works, termed Scaling Laws,\nhave discovered that the final test loss of LLMs scales as power-laws with\nmodel size, computational budget, and dataset size. However, the temporal\nchange of the test loss of an LLM throughout its pre-training process remains\nunexplored, though it is valuable in many aspects, such as selecting better\nhyperparameters \\textit{directly} on the target LLM. In this paper, we propose\nthe novel concept of Temporal Scaling Law, studying how the test loss of an LLM\nevolves as the training steps scale up. In contrast to modeling the test loss\nas a whole in a coarse-grained manner, we break it down and dive into the\nfine-grained test loss of each token position, and further develop a dynamic\nhyperbolic-law. Afterwards, we derive the much more precise temporal scaling\nlaw by studying the temporal patterns of the parameters in the dynamic\nhyperbolic-law. Results on both in-distribution (ID) and out-of-distribution\n(OOD) validation datasets demonstrate that our temporal scaling law accurately\npredicts the test loss of LLMs across training steps. Our temporal scaling law\nhas broad practical applications. First, it enables direct and efficient\nhyperparameter selection on the target LLM, such as data mixture proportions.\nSecondly, viewing the LLM pre-training dynamics from the token position\ngranularity provides some insights to enhance the understanding of LLM\npre-training.\n'] , ['  Large Language Models (LLMs) have highlighted the necessity of effective\nunlearning mechanisms to comply with data regulations and ethical AI practices.\nLLM unlearning aims at removing undesired data influences and associated model\ncapabilities without compromising utility beyond the scope of unlearning. While\ninterest in studying LLM unlearning is growing, the impact of the optimizer\nchoice for LLM unlearning remains unexplored. In this work, we shed light on\nthe significance of optimizer selection in LLM unlearning for the first time,\nestablishing a clear connection between second-order optimization and influence\nunlearning (a classical approach using influence functions to update the model\nfor data influence removal). This insight propels us to develop a second-order\noptimization-based LLM unlearning framework, termed Second-Order UnLearning\n(SOUL), which extends the static, one-shot model update using influence\nunlearning to a dynamic, iterative unlearning process. Our extensive\nexperiments show that SOUL consistently outperforms conventional first-order\nmethods across various unlearning tasks, models, and metrics, indicating that\nsecond-order optimization offers an effective and broadly applicable solution\nfor LLM unlearning. Codes are available at https://github.com/OPTML-Group/SOUL.\n', '  Despite the strong capabilities of Large Language Models (LLMs) to acquire\nknowledge from their training corpora, the memorization of sensitive\ninformation in the corpora such as copyrighted, harmful, and private content\nhas led to ethical and legal concerns. In response to these challenges,\nunlearning has emerged as a potential remedy for LLMs affected by problematic\ntraining data. However, previous unlearning techniques are either not\napplicable to black-box LLMs due to required access to model internal weights,\nor violate data protection principles by retaining sensitive data for\ninference-time correction. We propose $\\delta$-unlearning, an offset unlearning\nframework for black-box LLMs. Instead of tuning the black-box LLM itself,\n$\\delta$-unlearning learns the logit offset needed for unlearning by\ncontrasting the logits from a pair of smaller models. Experiments demonstrate\nthat $\\delta$-unlearning can effectively unlearn target data while maintaining\nsimilar or even stronger performance on general out-of-forget-scope tasks.\n$\\delta$-unlearning also effectively incorporates different unlearning\nalgorithms, making our approach a versatile solution to adapting various\nexisting unlearning algorithms to black-box LLMs.\n', '  Large language models trained on massive corpora of data from the web can\nmemorize and reproduce sensitive or private data raising both legal and ethical\nconcerns. Unlearning, or tuning models to forget information present in their\ntraining data, provides us with a way to protect private data after training.\nAlthough several methods exist for such unlearning, it is unclear to what\nextent they result in models equivalent to those where the data to be forgotten\nwas never learned in the first place. To address this challenge, we present\nTOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen\nour understanding of unlearning. We offer a dataset of 200 diverse synthetic\nauthor profiles, each consisting of 20 question-answer pairs, and a subset of\nthese profiles called the forget set that serves as the target for unlearning.\nWe compile a suite of metrics that work together to provide a holistic picture\nof unlearning efficacy. Finally, we provide a set of baseline results from\nexisting unlearning algorithms. Importantly, none of the baselines we consider\nshow effective unlearning motivating continued efforts to develop approaches\nfor unlearning that effectively tune models so that they truly behave as if\nthey were never trained on the forget data at all.\n'] , ['  Humans possess an extraordinary ability to create and utilize tools, allowing\nthem to overcome physical limitations and explore new frontiers. With the\nadvent of foundation models, AI systems have the potential to be equally adept\nin tool use as humans. This paradigm, i.e., tool learning with foundation\nmodels, combines the strengths of specialized tools and foundation models to\nachieve enhanced accuracy, efficiency, and automation in problem-solving.\nDespite its immense potential, there is still a lack of a comprehensive\nunderstanding of key challenges, opportunities, and future endeavors in this\nfield. To this end, we present a systematic investigation of tool learning in\nthis paper. We first introduce the background of tool learning, including its\ncognitive origins, the paradigm shift of foundation models, and the\ncomplementary roles of tools and models. Then we recapitulate existing tool\nlearning research into tool-augmented and tool-oriented learning. We formulate\na general tool learning framework: starting from understanding the user\ninstruction, models should learn to decompose a complex task into several\nsubtasks, dynamically adjust their plan through reasoning, and effectively\nconquer each sub-task by selecting appropriate tools. We also discuss how to\ntrain models for improved tool-use capabilities and facilitate the\ngeneralization in tool learning. Considering the lack of a systematic tool\nlearning evaluation in prior works, we experiment with 18 representative tools\nand show the potential of current foundation models in skillfully utilizing\ntools. Finally, we discuss several open problems that require further\ninvestigation for tool learning. In general, we hope this paper could inspire\nfuture research in integrating tools with foundation models.\n', '  Large language models (LLMs) have garnered significant attention due to their\nimpressive natural language processing (NLP) capabilities. Recently, many\nstudies have focused on the tool utilization ability of LLMs. They primarily\ninvestigated how LLMs effectively collaborate with given specific tools.\nHowever, in scenarios where LLMs serve as intelligent agents, as seen in\napplications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate\ndecision-making processes that involve deciding whether to employ a tool and\nselecting the most suitable tool(s) from a collection of available tools to\nfulfill user requests. Therefore, in this paper, we introduce MetaTool, a\nbenchmark designed to evaluate whether LLMs have tool usage awareness and can\ncorrectly choose tools. Specifically, we create a dataset called ToolE within\nthe benchmark. This dataset contains various types of user queries in the form\nof prompts that trigger LLMs to use tools, including both single-tool and\nmulti-tool scenarios. Subsequently, we set the tasks for both tool usage\nawareness and tool selection. We define four subtasks from different\nperspectives in tool selection, including tool selection with similar choices,\ntool selection in specific scenarios, tool selection with possible reliability\nissues, and multi-tool selection. We conduct experiments involving eight\npopular LLMs and find that the majority of them still struggle to effectively\nselect tools, highlighting the existing gaps between LLMs and genuine\nintelligent agents. However, through the error analysis, we found there is\nstill significant room for improvement. Finally, we conclude with insights for\ntool developers -- we strongly recommend that tool developers choose an\nappropriate rewrite model for generating new descriptions based on the\ndownstream LLM the tool will apply to. Our code is in\nhttps://github.com/HowieHwong/MetaTool.\n', ""  Tool learning aims to enhance and expand large language models' (LLMs)\ncapabilities with external tools, which has gained significant attention\nrecently. Current methods have shown that LLMs can effectively handle a certain\namount of tools through in-context learning or fine-tuning. However, in\nreal-world scenarios, the number of tools is typically extensive and\nirregularly updated, emphasizing the necessity for a dedicated tool retrieval\ncomponent. Tool retrieval is nontrivial due to the following challenges: 1)\ncomplex user instructions and tool descriptions; 2) misalignment between tool\nretrieval and tool usage models. To address the above issues, we propose to\nenhance tool retrieval with iterative feedback from the large language model.\nSpecifically, we prompt the tool usage model, i.e., the LLM, to provide\nfeedback for the tool retriever model in multi-round, which could progressively\nimprove the tool retriever's understanding of instructions and tools and reduce\nthe gap between the two standalone components. We build a unified and\ncomprehensive benchmark to evaluate tool retrieval models. The extensive\nexperiments indicate that our proposed approach achieves advanced performance\nin both in-domain evaluation and out-of-domain evaluation.\n""] , ['  Large Language Models (LLMs), trained on massive corpora with billions of\nparameters, show unprecedented performance in various fields. Though surprised\nby their excellent performances, researchers also noticed some special\nbehaviors of those LLMs. One of those behaviors is memorization, in which LLMs\ncan generate the same content used to train them. Though previous research has\ndiscussed memorization, the memorization of LLMs still lacks explanation,\nespecially the cause of memorization and the dynamics of generating them. In\nthis research, we comprehensively discussed memorization from various\nperspectives and extended the discussion scope to not only just the memorized\ncontent but also less and unmemorized content. Through various studies, we\nfound that: (1) Through experiments, we revealed the relation of memorization\nbetween model size, continuation size, and context size. Further, we showed how\nunmemorized sentences transition to memorized sentences. (2) Through embedding\nanalysis, we showed the distribution and decoding dynamics across model size in\nembedding space for sentences with different memorization scores. The n-gram\nstatistics analysis presents d (3) An analysis over n-gram and entropy decoding\ndynamics discovered a boundary effect when the model starts to generate\nmemorized sentences or unmemorized sentences. (4)We trained a Transformer model\nto predict the memorization of different models, showing that it is possible to\npredict memorizations by context.\n', '  Memorisation is a natural part of learning from real-world data: neural\nmodels pick up on atypical input-output combinations and store those training\nexamples in their parameter space. That this happens is well-known, but how and\nwhere are questions that remain largely unanswered. Given a multi-layered\nneural model, where does memorisation occur in the millions of parameters?\nRelated work reports conflicting findings: a dominant hypothesis based on image\nclassification is that lower layers learn generalisable features and that\ndeeper layers specialise and memorise. Work from NLP suggests this does not\napply to language models, but has been mainly focused on memorisation of facts.\nWe expand the scope of the localisation question to 12 natural language\nclassification tasks and apply 4 memorisation localisation techniques. Our\nresults indicate that memorisation is a gradual process rather than a localised\none, establish that memorisation is task-dependent, and give nuance to the\ngeneralisation first, memorisation second hypothesis.\n', ""  Understanding memorisation in language models has practical and societal\nimplications, e.g., studying models' training dynamics or preventing copyright\ninfringements. Prior work defines memorisation as the causal effect of training\nwith an instance on the model's ability to predict that instance. This\ndefinition relies on a counterfactual: the ability to observe what would have\nhappened had the model not seen that instance. Existing methods struggle to\nprovide computationally efficient and accurate estimates of this\ncounterfactual. Further, they often estimate memorisation for a model\narchitecture rather than for a specific model instance. This paper fills an\nimportant gap in the literature, proposing a new, principled, and efficient\nmethod to estimate memorisation based on the difference-in-differences design\nfrom econometrics. Using this method, we characterise a model's memorisation\nprofile--its memorisation trends across training--by only observing its\nbehaviour on a small set of instances throughout training. In experiments with\nthe Pythia model suite, we find that memorisation (i) is stronger and more\npersistent in larger models, (ii) is determined by data order and learning\nrate, and (iii) has stable trends across model sizes, thus making memorisation\nin larger models predictable from smaller ones.\n""] , ['  Alignment is the most critical step in building large language models (LLMs)\nthat meet human needs. With the rapid development of LLMs gradually surpassing\nhuman capabilities, traditional alignment methods based on human-annotation are\nincreasingly unable to meet the scalability demands. Therefore, there is an\nurgent need to explore new sources of automated alignment signals and technical\napproaches. In this paper, we systematically review the recently emerging\nmethods of automated alignment, attempting to explore how to achieve effective,\nscalable, automated alignment once the capabilities of LLMs exceed those of\nhumans. Specifically, we categorize existing automated alignment methods into 4\nmajor categories based on the sources of alignment signals and discuss the\ncurrent status and potential development of each category. Additionally, we\nexplore the underlying mechanisms that enable automated alignment and discuss\nthe essential factors that make automated alignment technologies feasible and\neffective from the fundamental role of alignment.\n', ""  The alignment process changes several properties of a large language model's\n(LLM's) output distribution. We analyze two aspects of post-alignment\ndistributional shift of LLM responses. First, we re-examine previously reported\nreductions in response diversity post-alignment. Our analysis suggests that an\napparent drop in the diversity of responses is largely explained by quality\ncontrol and information aggregation. Alignment suppresses irrelevant and\nunhelpful content while shifting the output distribution toward longer\nresponses that cover information spanning several responses from the base LLM,\nessentially presenting diverse information in a single response. Finding little\nevidence that alignment suppresses useful information, it is natural to ask the\nopposite question: do aligned models surface information that cannot be\nrecovered from base models? Our second investigation shows this is not the case\nand the behavior of aligned models is recoverable from base models without\nfine-tuning. A combination of in-context examples and lower-resolution semantic\nhints about response content can elicit responses from base LLMs that are as\nsimilar to alignment-tuned LLM responses as alignment-tuned LLM responses are\nto each other. Taken together, these results indicate that current alignment\ntechniques capture but do not extend the useful subset of assistant-like base\nLLM behavior, providing further evidence for the Superficial Alignment\nHypothesis. They also show that in-context alignment can go surprisingly far as\na strategy for imitating aligned LLMs without fine-tuning. Our code and data is\navailable at https://github.com/thomlake/investigating-alignment.\n"", '  Ensuring alignment with human preferences is a crucial characteristic of\nlarge language models (LLMs). Presently, the primary alignment methods, RLHF\nand DPO, require extensive human annotation, which is expensive despite their\nefficacy. The significant expenses associated with current alignment techniques\nmotivate researchers to investigate the development of annotation-free\nalignment training methods. In pursuit of improved alignment without relying on\nexternal annotation, we introduce Latent Distance Guided Alignment Training\n(LD-Align). This approach seeks to align the model with a high-quality\nsupervised fine-tune dataset using guidance from a latent space. The latent\nspace is generated through sample reconstruction, akin to auto-encoding.\nConsequently, we utilize the distance between sample pairs in the latent space\nto guide DPO-based alignment training. Extensive experimentation and evaluation\nshow the efficacy of our proposed method in achieving notable alignment.\n'] , [""  Large language models (LLMs) are becoming bigger to boost performance.\nHowever, little is known about how explainability is affected by this trend.\nThis work explores LIME explanations for DeBERTaV3 models of four different\nsizes on natural language inference (NLI) and zero-shot classification (ZSC)\ntasks. We evaluate the explanations based on their faithfulness to the models'\ninternal decision processes and their plausibility, i.e. their agreement with\nhuman explanations. The key finding is that increased model size does not\ncorrelate with plausibility despite improved model performance, suggesting a\nmisalignment between the LIME explanations and the models' internal processes\nas model size increases. Our results further suggest limitations regarding\nfaithfulness metrics in NLI contexts.\n"", ""  Instruction-tuned Large Language Models (LLMs) excel at many tasks and will\neven explain their reasoning, so-called self-explanations. However, convincing\nand wrong self-explanations can lead to unsupported confidence in LLMs, thus\nincreasing risk. Therefore, it's important to measure if self-explanations\ntruly reflect the model's behavior. Such a measure is called\ninterpretability-faithfulness and is challenging to perform since the ground\ntruth is inaccessible, and many LLMs only have an inference API. To address\nthis, we propose employing self-consistency checks to measure faithfulness. For\nexample, if an LLM says a set of words is important for making a prediction,\nthen it should not be able to make its prediction without these words. While\nself-consistency checks are a common approach to faithfulness, they have not\npreviously been successfully applied to LLM self-explanations for\ncounterfactual, feature attribution, and redaction explanations. Our results\ndemonstrate that faithfulness is explanation, model, and task-dependent,\nshowing self-explanations should not be trusted in general. For example, with\nsentiment classification, counterfactuals are more faithful for Llama2, feature\nattribution for Mistral, and redaction for Falcon 40B.\n"", '  Large Language Models (LLMs) are deployed as powerful tools for several\nnatural language processing (NLP) applications. Recent works show that modern\nLLMs can generate self-explanations (SEs), which elicit their intermediate\nreasoning steps for explaining their behavior. Self-explanations have seen\nwidespread adoption owing to their conversational and plausible nature.\nHowever, there is little to no understanding of their faithfulness. In this\nwork, we discuss the dichotomy between faithfulness and plausibility in SEs\ngenerated by LLMs. We argue that while LLMs are adept at generating plausible\nexplanations -- seemingly logical and coherent to human users -- these\nexplanations do not necessarily align with the reasoning processes of the LLMs,\nraising concerns about their faithfulness. We highlight that the current trend\ntowards increasing the plausibility of explanations, primarily driven by the\ndemand for user-friendly interfaces, may come at the cost of diminishing their\nfaithfulness. We assert that the faithfulness of explanations is critical in\nLLMs employed for high-stakes decision-making. Moreover, we emphasize the need\nfor a systematic characterization of faithfulness-plausibility requirements of\ndifferent real-world applications and ensure explanations meet those needs.\nWhile there are several approaches to improving plausibility, improving\nfaithfulness is an open challenge. We call upon the community to develop novel\nmethods to enhance the faithfulness of self explanations thereby enabling\ntransparent deployment of LLMs in diverse high-stakes settings.\n'] , ['  Recent advances in Large Language Models (LLMs) have exhibited remarkable\nproficiency across various tasks. Given the potent applications of LLMs in\nnumerous fields, there has been a surge in LLM development. In developing LLMs,\na common practice involves continual pre-training on previously fine-tuned\nmodels. However, this can lead to catastrophic forgetting. In our work, we\ninvestigate the phenomenon of forgetting that occurs during continual\npre-training on an existing fine-tuned LLM. We evaluate the impact of\ncontinuous pre-training on the fine-tuned LLM across various dimensions,\nincluding output format, knowledge, and reliability. Experiment results\nhighlight the non-trivial challenge of addressing catastrophic forgetting\nduring continual pre-training, especially the repetition issue.\n', '  We study and quantify the problem of forgetting when fine-tuning pre-trained\nlarge language models (LLMs) on a downstream task. We find that\nparameter-efficient fine-tuning (PEFT) strategies, such as Low-Rank Adapters\n(LoRA), still suffer from catastrophic forgetting. In particular, we identify a\nstrong inverse linear relationship between the fine-tuning performance and the\namount of forgetting when fine-tuning LLMs with LoRA. We further obtain precise\nscaling laws that show forgetting increases as a shifted power law in the\nnumber of parameters fine-tuned and the number of update steps. We also examine\nthe impact of forgetting on knowledge, reasoning, and the safety guardrails\ntrained into Llama 2 7B chat. Our study suggests that forgetting cannot be\navoided through early stopping or by varying the number of parameters\nfine-tuned. We believe this opens up an important safety-critical direction for\nfuture research to evaluate and develop fine-tuning schemes which mitigate\nforgetting\n', ""  Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning\nwhen a model forgets previously learned information while acquiring new\nknowledge. As large language models (LLMs) have demonstrated remarkable\nperformance, it is intriguing to investigate whether CF exists during the\ncontinual instruction tuning of LLMs. This study empirically evaluates the\nforgetting phenomenon in LLMs' knowledge during continual instruction tuning\nfrom the perspectives of domain knowledge, reasoning, and reading\ncomprehension. The experiments reveal that catastrophic forgetting is generally\nobserved in LLMs ranging from 1b to 7b parameters. Moreover, as the model scale\nincreases, the severity of forgetting intensifies. Comparing the decoder-only\nmodel BLOOMZ with the encoder-decoder model mT0, BLOOMZ exhibits less\nforgetting and retains more knowledge. Interestingly, we also observe that LLMs\ncan mitigate language biases, such as gender bias, during continual\nfine-tuning. Furthermore, our findings indicate that ALPACA maintains more\nknowledge and capacity compared to LLAMA during continual fine-tuning,\nsuggesting that general instruction tuning can help alleviate the forgetting\nphenomenon in LLMs during subsequent fine-tuning processes.\n""] , ['  In this work, we systematically investigate the efficacy of dynamic\nactivation mechanisms within the LLaMA family of language models. Despite the\npotential of dynamic activation methods to reduce computation and increase\nspeed in models using the ReLU activation function, our empirical findings have\nuncovered several inherent pitfalls in the current dynamic activation schemes.\nThrough extensive experiments across various dynamic activation strategies, we\ndemonstrate that LLaMA models usually underperform when compared to their ReLU\ncounterparts, particularly in scenarios demanding high sparsity ratio. We\nattribute these deficiencies to a combination of factors: 1) the inherent\ncomplexity of dynamically predicting activation heads and neurons; 2) the\ninadequate sparsity resulting from activation functions; 3) the insufficient\npreservation of information resulting from KV cache skipping. Our analysis not\nonly sheds light on the limitations of dynamic activation in the context of\nlarge-scale LLaMA models but also proposes roadmaps for enhancing the design of\nfuture sparsity schemes.\n', '  Sparse computation offers a compelling solution for the inference of Large\nLanguage Models (LLMs) in low-resource scenarios by dynamically skipping the\ncomputation of inactive neurons. While traditional approaches focus on\nReLU-based LLMs, leveraging zeros in activation values, we broaden the scope of\nsparse LLMs beyond zero activation values. We introduce a general method that\ndefines neuron activation through neuron output magnitudes and a tailored\nmagnitude threshold, demonstrating that non-ReLU LLMs also exhibit sparse\nactivation. To find the most efficient activation function for sparse\ncomputation, we propose a systematic framework to examine the sparsity of LLMs\nfrom three aspects: the trade-off between sparsity and performance, the\npredictivity of sparsity, and the hardware affinity. We conduct thorough\nexperiments on LLMs utilizing different activation functions, including ReLU,\nSwiGLU, ReGLU, and ReLU$^2$. The results indicate that models employing\nReLU$^2$ excel across all three evaluation aspects, highlighting its potential\nas an efficient activation function for sparse LLMs. We will release the code\nto facilitate future research.\n', '  Activation sparsity refers to the existence of considerable\nweakly-contributed elements among activation outputs. As a prevalent property\nof the models using the ReLU activation function, activation sparsity has been\nproven a promising paradigm to boost model inference efficiency. Nevertheless,\nmost large language models (LLMs) adopt activation functions without intrinsic\nactivation sparsity (e.g., GELU and Swish). Some recent efforts have explored\nintroducing ReLU or its variants as the substitutive activation function to\nhelp LLMs achieve activation sparsity and inference acceleration, but few can\nsimultaneously obtain high sparsity and comparable model performance. This\npaper introduces a simple and effective sparsification method named ""ProSparse""\nto push LLMs for higher activation sparsity while maintaining comparable\nperformance. Specifically, after substituting the activation function of LLMs\nwith ReLU, ProSparse adopts progressive sparsity regularization with a factor\nsmoothly increasing along the multi-stage sine curves. This can enhance\nactivation sparsity and mitigate performance degradation by avoiding radical\nshifts in activation distributions. With ProSparse, we obtain high sparsity of\n89.32% for LLaMA2-7B, 88.80% for LLaMA2-13B, and 87.89% for end-size\nMiniCPM-1B, respectively, achieving comparable performance to their original\nSwish-activated versions. These present the most sparsely activated models\namong open-source LLaMA versions and competitive end-size models, considerably\nsurpassing ReluLLaMA-7B (66.98%) and ReluLLaMA-13B (71.56%). Our inference\nacceleration experiments further demonstrate the significant practical\nacceleration potential of LLMs with higher activation sparsity, obtaining up to\n4.52$\\times$ inference speedup.\n'] , ['  Large Language Models (LLMs) can improve their responses when instructed to\ndo so, a capability known as self-correction. When these instructions lack\nspecific details about the issues in the response, this is referred to as\nleveraging the intrinsic self-correction capability. The empirical success of\nself-correction can be found in various applications, e.g., text detoxification\nand social bias mitigation. However, leveraging this self-correction capability\nmay not always be effective, as it has the potential to revise an initially\ncorrect response into an incorrect one. In this paper, we endeavor to\nunderstand how and why leveraging the self-correction capability is effective.\nWe identify that appropriate instructions can guide LLMs to a convergence\nstate, wherein additional self-correction steps do not yield further\nperformance improvements. We empirically demonstrate that model uncertainty and\nactivated latent concepts jointly characterize the effectiveness of\nself-correction. Furthermore, we provide a mathematical formulation indicating\nthat the activated latent concept drives the convergence of the model\nuncertainty and self-correction performance. Our analysis can also be\ngeneralized to the self-correction behaviors observed in Vision-Language Models\n(VLMs). Moreover, we highlight that task-agnostic debiasing can benefit from\nour principle in terms of selecting effective fine-tuning samples. Such initial\nsuccess demonstrates the potential extensibility for better instruction tuning\nand safety alignment.\n', ""  Large language models (LLMs) have attracted significant attention for their\nremarkable abilities in various natural language processing tasks, but they\nsuffer from hallucinations that will cause performance degradation. One\npromising solution to improve the LLMs' performance is to ask LLMs to revise\ntheir answer after generation, a technique known as self-correction. Among the\ntwo types of self-correction, intrinsic self-correction is considered a\npromising direction because it does not utilize external knowledge. However,\nrecent works doubt the validity of LLM's ability to conduct intrinsic\nself-correction. In this paper, we present a novel perspective on the intrinsic\nself-correction capabilities of LLMs through theoretical analyses and empirical\nexperiments. In addition, we identify two critical factors for successful\nself-correction: zero temperature and fair prompts. Leveraging these factors,\nwe demonstrate that intrinsic self-correction ability is exhibited across\nmultiple existing LLMs. Our findings offer insights into the fundamental\ntheories underlying the self-correction behavior of LLMs and remark on the\nimportance of unbiased prompts and zero temperature settings in harnessing\ntheir full potential.\n"", '  Self-correction is an approach to improving responses from large language\nmodels (LLMs) by refining the responses using LLMs during inference. Prior work\nhas proposed various self-correction frameworks using different sources of\nfeedback, including self-evaluation and external feedback. However, there is\nstill no consensus on the question of when LLMs can correct their own mistakes,\nas recent studies also report negative results. In this work, we critically\nsurvey broad papers and discuss the conditions required for successful\nself-correction. We first find that prior studies often do not define their\nresearch questions in detail and involve impractical frameworks or unfair\nevaluations that over-evaluate self-correction. To tackle these issues, we\ncategorize research questions in self-correction research and provide a\nchecklist for designing appropriate experiments. Our critical survey based on\nthe newly categorized research questions shows that (1) no prior work\ndemonstrates successful self-correction with feedback from prompted LLMs in\ngeneral tasks, (2) self-correction works well in tasks that can use reliable\nexternal feedback, and (3) large-scale fine-tuning enables self-correction.\n'] , [""  Fine-tuning on task-specific question-answer pairs is a predominant method\nfor enhancing the performance of instruction-tuned large language models (LLMs)\non downstream tasks. However, in certain specialized domains, such as\nhealthcare or harmless content generation, it is nearly impossible to obtain a\nlarge volume of high-quality data that matches the downstream distribution. To\nimprove the performance of LLMs in data-scarce domains with domain-mismatched\ndata, we re-evaluated the Transformer architecture and discovered that not all\nparameter updates during fine-tuning contribute positively to downstream\nperformance. Our analysis reveals that within the self-attention and\nfeed-forward networks, only the fine-tuned attention parameters are\nparticularly beneficial when the training set's distribution does not fully\nalign with the test set. Based on this insight, we propose an effective\ninference-time intervention method: Training All parameters but Inferring with\nonly Attention (\\trainallInfAttn). We empirically validate \\trainallInfAttn\nusing two general instruction-tuning datasets and evaluate it on seven\ndownstream tasks involving math, reasoning, and knowledge understanding across\nLLMs of different parameter sizes and fine-tuning techniques. Our comprehensive\nexperiments demonstrate that \\trainallInfAttn achieves superior improvements\ncompared to both the fully fine-tuned model and the base model in most\nscenarios, with significant performance gains. The high tolerance of\n\\trainallInfAttn to data mismatches makes it resistant to jailbreaking tuning\nand enhances specialized tasks using general data.\n"", '  This work focuses on leveraging and selecting from vast, unlabeled, open data\nto pre-fine-tune a pre-trained language model. The goal is to minimize the need\nfor costly domain-specific data for subsequent fine-tuning while achieving\ndesired performance levels. While many data selection algorithms have been\ndesigned for small-scale applications, rendering them unsuitable for our\ncontext, some emerging methods do cater to language data scales. However, they\noften prioritize data that aligns with the target distribution. While this\nstrategy may be effective when training a model from scratch, it can yield\nlimited results when the model has already been pre-trained on a different\ndistribution. Differing from prior work, our key idea is to select data that\nnudges the pre-training distribution closer to the target distribution. We show\nthe optimality of this approach for fine-tuning tasks under certain conditions.\nWe demonstrate the efficacy of our methodology across a diverse array of tasks\n(NLU, NLG, zero-shot) with models up to 2.7B, showing that it consistently\nsurpasses other selection methods. Moreover, our proposed method is\nsignificantly faster than existing techniques, scaling to millions of samples\nwithin a single GPU hour. Our code is open-sourced (Code repository:\nhttps://anonymous.4open.science/r/DV4LLM-D761/ ). While fine-tuning offers\nsignificant potential for enhancing performance across diverse tasks, its\nassociated costs often limit its widespread adoption; with this work, we hope\nto lay the groundwork for cost-effective fine-tuning, making its benefits more\naccessible.\n', '  In the domain of large language models (LLMs), arXiv:2305.16938 showed that\nfew-shot full-model fine-tuning -- namely Vanilla Fine Tuning (FT) and\nPattern-Based Fine Tuning (PBFT) --, and In-Context Learning (ICL) generalize\nsimilarly on Out-Of-Domain (OOD) datasets, but vary in terms of task\nadaptation. However, they both pose challenges, especially in term of memory\nrequirements. In this paper, we further try to push the understanding of\ndifferent fine-tuning strategies for LLM and aim to bring a myriad of these on\nthe same pedestal for an elaborate comparison with full-model fine-tuning on\ntwo diverse datasets. To that end, we conducted a series of experiments,\nbeginning with state-of-the-art methods like vanilla fine-tuning and\nPattern-Based Fine-Tuning (PBFT) on pre-trained models across two datasets,\nCOLA and MNLI. We then investigate adaptive fine-tuning and the efficiency of\nLoRA adapters in a few-shot setting. Finally, we also compare an alternative\napproach that has gained recent popularity -- context distillation -- with the\nvanilla FT and PBFT with and without few-shot setup.\n  Our findings suggest that these alternative strategies that we explored can\nexhibit out-of-domain generalization comparable to that of vanilla FT and PBFT.\nPBFT under-performs Vanilla FT on out-of-domain (OOD) data, emphasizing the\nneed for effective prompts. Further, our adaptive-fine tuning and LoRA\nexperiments perform comparable or slightly worse than the standard fine-tunings\nas anticipated, since standard fine-tunings involve tuning the entire model.\nFinally, our context distillation experiments out-perform the standard\nfine-tuning methods. These findings underscore that eventually the choice of an\nappropriate fine-tuning method depends on the available resources (memory,\ncompute, data) and task adaptability.\n'] , ['  High-quality instruction data is critical for aligning large language models\n(LLMs). Although some models, such as Llama-3-Instruct, have open weights,\ntheir alignment data remain private, which hinders the democratization of AI.\nHigh human labor costs and a limited, predefined scope for prompting prevent\nexisting open-source data creation methods from scaling effectively,\npotentially limiting the diversity and quality of public alignment datasets. Is\nit possible to synthesize high-quality instruction data at scale by extracting\nit directly from an aligned LLM? We present a self-synthesis method for\ngenerating large-scale alignment data named Magpie. Our key observation is that\naligned LLMs like Llama-3-Instruct can generate a user query when we input only\nthe left-side templates up to the position reserved for user messages, thanks\nto their auto-regressive nature. We use this method to prompt Llama-3-Instruct\nand generate 4 million instructions along with their corresponding responses.\nWe perform a comprehensive analysis of the extracted data and select 300K\nhigh-quality instances. To compare Magpie data with other public instruction\ndatasets, we fine-tune Llama-3-8B-Base with each dataset and evaluate the\nperformance of the fine-tuned models. Our results indicate that in some tasks,\nmodels fine-tuned with Magpie perform comparably to the official\nLlama-3-8B-Instruct, despite the latter being enhanced with 10 million data\npoints through supervised fine-tuning (SFT) and subsequent feedback learning.\nWe also show that using Magpie solely for SFT can surpass the performance of\nprevious public datasets utilized for both SFT and preference optimization,\nsuch as direct preference optimization with UltraFeedback. This advantage is\nevident on alignment benchmarks such as AlpacaEval, ArenaHard, and WildBench.\n', ""  Pretrained large language models (LLMs) are general purpose problem solvers\napplicable to a diverse set of tasks with prompts. They can be further improved\ntowards a specific task by fine-tuning on a specialized dataset. However,\nfine-tuning usually makes the model narrowly specialized on this dataset with\nreduced general in-context learning performances, which is undesirable whenever\nthe fine-tuned model needs to handle additional tasks where no fine-tuning data\nis available. In this work, we first demonstrate that fine-tuning on a single\ntask indeed decreases LLMs' general in-context learning performance. We\ndiscover one important cause of such forgetting, format specialization, where\nthe model overfits to the format of the fine-tuned task.We further show that\nformat specialization happens at the very beginning of fine-tuning. To solve\nthis problem, we propose Prompt Tuning with MOdel Tuning (ProMoT), a simple yet\neffective two-stage fine-tuning framework that reduces format specialization\nand improves generalization.ProMoT offloads task-specific format learning into\nadditional and removable parameters by first doing prompt tuning and then\nfine-tuning the model itself with this soft prompt attached. With experiments\non several fine-tuning tasks and 8 in-context evaluation tasks, we show that\nProMoT achieves comparable performance on fine-tuned tasks to standard\nfine-tuning, but with much less loss of in-context learning performances across\na board range of out-of-domain evaluation tasks. More importantly, ProMoT can\neven enhance generalization on in-context learning tasks that are semantically\nrelated to the fine-tuned task, e.g. ProMoT on En-Fr translation significantly\nimproves performance on other language pairs, and ProMoT on NLI improves\nperformance on summarization. Experiments also show that ProMoT can improve the\ngeneralization performance of multi-task training.\n"", ""  While Large Language Models (LLMs) have demonstrated exceptional multitasking\nabilities, fine-tuning these models on downstream, domain-specific datasets is\noften necessary to yield superior performance on test sets compared to their\ncounterparts without fine-tuning. However, the comprehensive effects of\nfine-tuning on the LLMs' generalization ability are not fully understood. This\npaper delves into the differences between original, unmodified LLMs and their\nfine-tuned variants. Our primary investigation centers on whether fine-tuning\naffects the generalization ability intrinsic to LLMs. To elaborate on this, we\nconduct extensive experiments across five distinct language tasks on various\ndatasets. Our main findings reveal that models fine-tuned on generation and\nclassification tasks exhibit dissimilar behaviors in generalizing to different\ndomains and tasks. Intriguingly, we observe that integrating the in-context\nlearning strategy during fine-tuning on generation tasks can enhance the\nmodel's generalization ability. Through this systematic investigation, we aim\nto contribute valuable insights into the evolving landscape of fine-tuning\npractices for LLMs.\n""]",Advances in Large Language Models,Fine-Tuning Large Language Models
4,"""Jailbreak Attacks on Large Language Models"" , ""Large Language Models for Cybersecurity"" , ""Large Language Model Safety and Risk Mitigation"" , Watermarking Techniques for Large Language Models , ""Large Language Models and Privacy Concerns"" , Data Contamination in Language Models , ""Efficient Large Language Models for Mobile Devices"" , Copyright Infringement in Large Language Models , ""Language Model Compression and Safety Evaluation""","['jailbreaks', 'jailbreaking', 'jailbreak', 'jailbreakbench', 'vulnerabilities', 'vulnerable', 'vulnerability', 'security', 'adversarial', 'attacks'] , ['cybersecurity', 'cyberattacks', 'cyberattack', 'cybercrime', 'llm4security', 'security', 'malware', 'cybermetric', 'threats', 'phishing'] , ['safety', 'unsafe', 'safepatching', 'safeguards', 'safeguarding', 'safeguard', 'harmfulness', 'safe', 'malicious', 'risks'] , ['watermarking', 'watermark', 'watermarked', 'watermarks', 'postmark', 'text', 'steganographic', 'tokens', 'steganography', 'embedding'] , ['privacy', 'anonymization', 'privacyrestore', 'anonymized', 'anonymisation', 'disclosures', 'security', 'obfuscation', 'disclosure', 'incognitext'] , ['contamination', 'language', 'corpora', 'contaminated', 'evading', 'benchmark', 'models', 'benchmarks', 'data', 'detection'] , ['memory', 'smartphone', 'devices', 'smartphones', 'mobile', 'ios', 'device', 'mobilellm', 'annotation', 'cloud'] , ['copyright', 'plagiarism', 'copying', 'infringement', 'infringing', 'copyrighted', 'watermarking', 'copybench', 'memorization', 'passphrases'] , ['compression', 'compressed', 'memory', 'softmax', 'decoding', 'tokenizers', 'decoder', 'tokenization', 'tokenizer', 'attention']","['  As Large Language Models (LLMs) are increasingly being deployed in\nsafety-critical applications, their vulnerability to potential jailbreaks --\nmalicious prompts that can disable the safety mechanism of LLMs -- has\nattracted growing research attention. While alignment methods have been\nproposed to protect LLMs from jailbreaks, many have found that aligned LLMs can\nstill be jailbroken by carefully crafted malicious prompts, producing content\nthat violates policy regulations. Existing jailbreak attacks on LLMs can be\ncategorized into prompt-level methods which make up stories/logic to circumvent\nsafety alignment and token-level attack methods which leverage gradient methods\nto find adversarial tokens. In this work, we introduce the concept of Ensemble\nJailbreak and explore methods that can integrate prompt-level and token-level\njailbreak into a more powerful hybrid jailbreak attack. Specifically, we\npropose a novel EnJa attack to hide harmful instructions using prompt-level\njailbreak, boost the attack success rate using a gradient-based attack, and\nconnect the two types of jailbreak attacks via a template-based connector. We\nevaluate the effectiveness of EnJa on several aligned models and show that it\nachieves a state-of-the-art attack success rate with fewer queries and is much\nstronger than any individual jailbreak.\n', '  Large Language Models (LLMs), like ChatGPT, have demonstrated vast potential\nbut also introduce challenges related to content constraints and potential\nmisuse. Our study investigates three key research questions: (1) the number of\ndifferent prompt types that can jailbreak LLMs, (2) the effectiveness of\njailbreak prompts in circumventing LLM constraints, and (3) the resilience of\nChatGPT against these jailbreak prompts. Initially, we develop a classification\nmodel to analyze the distribution of existing prompts, identifying ten distinct\npatterns and three categories of jailbreak prompts. Subsequently, we assess the\njailbreak capability of prompts with ChatGPT versions 3.5 and 4.0, utilizing a\ndataset of 3,120 jailbreak questions across eight prohibited scenarios.\nFinally, we evaluate the resistance of ChatGPT against jailbreak prompts,\nfinding that the prompts can consistently evade the restrictions in 40 use-case\nscenarios. The study underscores the importance of prompt structures in\njailbreaking LLMs and discusses the challenges of robust jailbreak prompt\ngeneration and prevention.\n', '  Misuse of the Large Language Models (LLMs) has raised widespread concern. To\naddress this issue, safeguards have been taken to ensure that LLMs align with\nsocial ethics. However, recent findings have revealed an unsettling\nvulnerability bypassing the safeguards of LLMs, known as jailbreak attacks. By\napplying techniques, such as employing role-playing scenarios, adversarial\nexamples, or subtle subversion of safety objectives as a prompt, LLMs can\nproduce an inappropriate or even harmful response. While researchers have\nstudied several categories of jailbreak attacks, they have done so in\nisolation. To fill this gap, we present the first large-scale measurement of\nvarious jailbreak attack methods. We concentrate on 13 cutting-edge jailbreak\nmethods from four categories, 160 questions from 16 violation categories, and\nsix popular LLMs. Our extensive experimental results demonstrate that the\noptimized jailbreak prompts consistently achieve the highest attack success\nrates, as well as exhibit robustness across different LLMs. Some jailbreak\nprompt datasets, available from the Internet, can also achieve high attack\nsuccess rates on many LLMs, such as ChatGLM3, GPT-3.5, and PaLM2. Despite the\nclaims from many organizations regarding the coverage of violation categories\nin their policies, the attack success rates from these categories remain high,\nindicating the challenges of effectively aligning LLM policies and the ability\nto counter jailbreak attacks. We also discuss the trade-off between the attack\nperformance and efficiency, as well as show that the transferability of the\njailbreak prompts is still viable, becoming an option for black-box models.\nOverall, our research highlights the necessity of evaluating different\njailbreak methods. We hope our study can provide insights for future research\non jailbreak attacks and serve as a benchmark tool for evaluating them for\npractitioners.\n'] , ['  The rapid advancement of Large Language Models (LLMs) has opened up new\nopportunities for leveraging artificial intelligence in various domains,\nincluding cybersecurity. As the volume and sophistication of cyber threats\ncontinue to grow, there is an increasing need for intelligent systems that can\nautomatically detect vulnerabilities, analyze malware, and respond to attacks.\nIn this survey, we conduct a comprehensive review of the literature on the\napplication of LLMs in cybersecurity (LLM4Security). By comprehensively\ncollecting over 30K relevant papers and systematically analyzing 127 papers\nfrom top security and software engineering venues, we aim to provide a holistic\nview of how LLMs are being used to solve diverse problems across the\ncybersecurity domain. Through our analysis, we identify several key findings.\nFirst, we observe that LLMs are being applied to a wide range of cybersecurity\ntasks, including vulnerability detection, malware analysis, network intrusion\ndetection, and phishing detection. Second, we find that the datasets used for\ntraining and evaluating LLMs in these tasks are often limited in size and\ndiversity, highlighting the need for more comprehensive and representative\ndatasets. Third, we identify several promising techniques for adapting LLMs to\nspecific cybersecurity domains, such as fine-tuning, transfer learning, and\ndomain-specific pre-training. Finally, we discuss the main challenges and\nopportunities for future research in LLM4Security, including the need for more\ninterpretable and explainable models, the importance of addressing data privacy\nand security concerns, and the potential for leveraging LLMs for proactive\ndefense and threat hunting. Overall, our survey provides a comprehensive\noverview of the current state-of-the-art in LLM4Security and identifies several\npromising directions for future research.\n', '  Cybersecurity researchers have contributed to the automated extraction of CTI\nfrom textual sources, such as threat reports and online articles, where\ncyberattack strategies, procedures, and tools are described. The goal of this\narticle is to aid cybersecurity researchers understand the current techniques\nused for cyberthreat intelligence extraction from text through a survey of\nrelevant studies in the literature. We systematically collect ""CTI extraction\nfrom text""-related studies from the literature and categorize the CTI\nextraction purposes. We propose a CTI extraction pipeline abstracted from these\nstudies. We identify the data sources, techniques, and CTI sharing formats\nutilized in the context of the proposed pipeline. Our work finds ten types of\nextraction purposes, such as extraction indicators of compromise extraction,\nTTPs (tactics, techniques, procedures of attack), and cybersecurity keywords.\nWe also identify seven types of textual sources for CTI extraction, and textual\ndata obtained from hacker forums, threat reports, social media posts, and\nonline news articles have been used by almost 90% of the studies. Natural\nlanguage processing along with both supervised and unsupervised machine\nlearning techniques such as named entity recognition, topic modelling,\ndependency parsing, supervised classification, and clustering are used for CTI\nextraction. We observe the technical challenges associated with these studies\nrelated to obtaining available clean, labelled data which could assure\nreplication, validation, and further extension of the studies. As we find the\nstudies focusing on CTI information extraction from text, we advocate for\nbuilding upon the current CTI extraction work to help cybersecurity\npractitioners with proactive decision making such as threat prioritization,\nautomated threat modelling to utilize knowledge from past cybersecurity\nincidents.\n', '  This paper provides a comprehensive review of the future of cybersecurity\nthrough Generative AI and Large Language Models (LLMs). We explore LLM\napplications across various domains, including hardware design security,\nintrusion detection, software engineering, design verification, cyber threat\nintelligence, malware detection, and phishing detection. We present an overview\nof LLM evolution and its current state, focusing on advancements in models such\nas GPT-4, GPT-3.5, Mixtral-8x7B, BERT, Falcon2, and LLaMA. Our analysis extends\nto LLM vulnerabilities, such as prompt injection, insecure output handling,\ndata poisoning, DDoS attacks, and adversarial instructions. We delve into\nmitigation strategies to protect these models, providing a comprehensive look\nat potential attack scenarios and prevention techniques. Furthermore, we\nevaluate the performance of 42 LLM models in cybersecurity knowledge and\nhardware security, highlighting their strengths and weaknesses. We thoroughly\nevaluate cybersecurity datasets for LLM training and testing, covering the\nlifecycle from data creation to usage and identifying gaps for future research.\nIn addition, we review new strategies for leveraging LLMs, including techniques\nlike Half-Quadratic Quantization (HQQ), Reinforcement Learning with Human\nFeedback (RLHF), Direct Preference Optimization (DPO), Quantized Low-Rank\nAdapters (QLoRA), and Retrieval-Augmented Generation (RAG). These insights aim\nto enhance real-time cybersecurity defenses and improve the sophistication of\nLLM applications in threat detection and response. Our paper provides a\nfoundational understanding and strategic direction for integrating LLMs into\nfuture cybersecurity frameworks, emphasizing innovation and robust model\ndeployment to safeguard against evolving cyber threats.\n'] , [""  Recent progress in large language models (LLMs) has led to their widespread\nadoption in various domains. However, these advancements have also introduced\nadditional safety risks and raised concerns regarding their detrimental impact\non already marginalized populations. Despite growing mitigation efforts to\ndevelop safety safeguards, such as supervised safety-oriented fine-tuning and\nleveraging safe reinforcement learning from human feedback, multiple concerns\nregarding the safety and ingrained biases in these models remain. Furthermore,\nprevious work has demonstrated that models optimized for safety often display\nexaggerated safety behaviors, such as a tendency to refrain from responding to\ncertain requests as a precautionary measure. As such, a clear trade-off between\nthe helpfulness and safety of these models has been documented in the\nliterature. In this paper, we further investigate the effectiveness of safety\nmeasures by evaluating models on already mitigated biases. Using the case of\nLlama 2 as an example, we illustrate how LLMs' safety responses can still\nencode harmful assumptions. To do so, we create a set of non-toxic prompts,\nwhich we then use to evaluate Llama models. Through our new taxonomy of LLMs\nresponses to users, we observe that the safety/helpfulness trade-offs are more\npronounced for certain demographic groups which can lead to quality-of-service\nharms for marginalized populations.\n"", '  Training large language models to follow instructions makes them perform\nbetter on a wide range of tasks and generally become more helpful. However, a\nperfectly helpful model will follow even the most malicious instructions and\nreadily generate harmful content. In this paper, we raise concerns over the\nsafety of models that only emphasize helpfulness, not harmlessness, in their\ninstruction-tuning. We show that several popular instruction-tuned models are\nhighly unsafe. Moreover, we show that adding just 3% safety examples (a few\nhundred demonstrations) when fine-tuning a model like LLaMA can substantially\nimprove its safety. Our safety-tuning does not make models significantly less\ncapable or helpful as measured by standard benchmarks. However, we do find\nexaggerated safety behaviours, where too much safety-tuning makes models refuse\nperfectly safe prompts if they superficially resemble unsafe ones. As a whole,\nour results illustrate trade-offs in training LLMs to be helpful and training\nthem to be safe.\n', '  Ensuring the safe alignment of large language models (LLMs) with human values\nis critical as they become integral to applications like translation and\nquestion answering. Current alignment methods struggle with dynamic user\nintentions and complex objectives, making models vulnerable to generating\nharmful content. We propose Safety Arithmetic, a training-free framework\nenhancing LLM safety across different scenarios: Base models, Supervised\nfine-tuned models (SFT), and Edited models. Safety Arithmetic involves Harm\nDirection Removal to avoid harmful content and Safety Alignment to promote safe\nresponses. Additionally, we present NoIntentEdit, a dataset highlighting edit\ninstances that could compromise model safety if used unintentionally. Our\nexperiments show that Safety Arithmetic significantly improves safety measures,\nreduces over-safety, and maintains model utility, outperforming existing\nmethods in ensuring safe content generation.\n'] , ['  Recent advancements of large language models (LLMs) have resulted in\nindistinguishable text outputs comparable to human-generated text. Watermarking\nalgorithms are potential tools that offer a way to differentiate between LLM-\nand human-generated text by embedding detectable signatures within\nLLM-generated output. However, current watermarking schemes lack robustness\nagainst known attacks against watermarking algorithms. In addition, they are\nimpractical considering an LLM generates tens of thousands of text outputs per\nday and the watermarking algorithm needs to memorize each output it generates\nfor the detection to work. In this work, focusing on the limitations of current\nwatermarking schemes, we propose the concept of a ""topic-based watermarking\nalgorithm"" for LLMs. The proposed algorithm determines how to generate tokens\nfor the watermarked LLM output based on extracted topics of an input prompt or\nthe output of a non-watermarked LLM. Inspired from previous work, we propose\nusing a pair of lists (that are generated based on the specified extracted\ntopic(s)) that specify certain tokens to be included or excluded while\ngenerating the watermarked output of the LLM. Using the proposed watermarking\nalgorithm, we show the practicality of a watermark detection algorithm.\nFurthermore, we discuss a wide range of attacks that can emerge against\nwatermarking algorithms for LLMs and the benefit of the proposed watermarking\nscheme for the feasibility of modeling a potential attacker considering its\nbenefit vs. loss.\n', '  With the widespread adoption of Large Language Models (LLMs), concerns about\npotential misuse have emerged. To this end, watermarking has been adapted to\nLLM, enabling a simple and effective way to detect and monitor generated text.\nHowever, while the existing methods can differentiate between watermarked and\nunwatermarked text with high accuracy, they often face a trade-off between the\nquality of the generated text and the effectiveness of the watermarking\nprocess. In this work, we present a novel type of LLM watermark, Sparse\nWatermark, which aims to mitigate this trade-off by applying watermarks to a\nsmall subset of generated tokens distributed across the text. The key strategy\ninvolves anchoring watermarked tokens to words that have specific\nPart-of-Speech (POS) tags. Our experimental results demonstrate that the\nproposed watermarking scheme achieves high detectability while generating text\nthat outperforms previous LLM watermarking methods in quality across various\ntasks\n', '  Watermarking of language model outputs enables statistical detection of\nmodel-generated text, which can mitigate harms and misuses of language models.\nExisting watermarking strategies operate by altering the decoder of an existing\nlanguage model. In this paper, we ask whether language models can directly\nlearn to generate watermarked text, which would have significant implications\nfor the real-world deployment of watermarks. First, learned watermarks could be\nused to build open models that naturally generate watermarked text, enabling\nwatermarking for open models, where users can control the decoding procedure.\nSecond, if watermarking is used to determine the provenance of generated text,\nan adversary can hurt the reputation of a victim model by spoofing its\nwatermark and generating damaging watermarked text. To investigate the\nlearnability of watermarks, we propose watermark distillation, which trains a\nstudent model to behave like a teacher model that uses decoding-based\nwatermarking. We test our approach on three decoding-based watermarking\nstrategies and various hyperparameter settings, finding that models can learn\nto generate watermarked text with high detectability. We also find limitations\nto learnability, including the loss of watermarking capabilities under\nfine-tuning on normal text and high sample complexity when learning\nlow-distortion watermarks.\n'] , ['  Large language models (LLMs), renowned for their impressive capabilities in\nvarious tasks, have significantly advanced artificial intelligence. Yet, these\nadvancements have raised growing concerns about privacy and security\nimplications. To address these issues and explain the risks inherent in these\nmodels, we have devised a three-tiered progressive framework tailored for\nevaluating privacy in language systems. This framework consists of\nprogressively complex and in-depth privacy test tasks at each tier. Our primary\nobjective is to comprehensively evaluate the sensitivity of large language\nmodels to private information, examining how effectively they discern, manage,\nand safeguard sensitive data in diverse scenarios. This systematic evaluation\nhelps us understand the degree to which these models comply with privacy\nprotection guidelines and the effectiveness of their inherent safeguards\nagainst privacy breaches. Our observations indicate that existing Chinese large\nlanguage models universally show privacy protection shortcomings. It seems that\nat the moment this widespread issue is unavoidable and may pose corresponding\nprivacy risks in applications based on these models.\n', ""  Large language models (LLMs) have emerged as powerful tools for tackling\ncomplex tasks across diverse domains, but they also raise privacy concerns when\nfine-tuned on sensitive data due to potential memorization. While differential\nprivacy (DP) offers a promising solution by ensuring models are 'almost\nindistinguishable' with or without any particular privacy unit, current\nevaluations on LLMs mostly treat each example (text record) as the privacy\nunit. This leads to uneven user privacy guarantees when contributions per user\nvary. We therefore study user-level DP motivated by applications where it\nnecessary to ensure uniform privacy protection across users. We present a\nsystematic evaluation of user-level DP for LLM fine-tuning on natural language\ngeneration tasks. Focusing on two mechanisms for achieving user-level DP\nguarantees, Group Privacy and User-wise DP-SGD, we investigate design choices\nlike data selection strategies and parameter tuning for the best\nprivacy-utility tradeoff.\n"", ""  The emergence of large language models (LLMs), and their increased use in\nuser-facing systems, has led to substantial privacy concerns. To date, research\non these privacy concerns has been model-centered: exploring how LLMs lead to\nprivacy risks like memorization, or can be used to infer personal\ncharacteristics about people from their content. We argue that there is a need\nfor more research focusing on the human aspect of these privacy issues: e.g.,\nresearch on how design paradigms for LLMs affect users' disclosure behaviors,\nusers' mental models and preferences for privacy controls, and the design of\ntools, systems, and artifacts that empower end-users to reclaim ownership over\ntheir personal data. To build usable, efficient, and privacy-friendly systems\npowered by these models with imperfect privacy properties, our goal is to\ninitiate discussions to outline an agenda for conducting human-centered\nresearch on privacy issues in LLM-powered systems. This Special Interest Group\n(SIG) aims to bring together researchers with backgrounds in usable security\nand privacy, human-AI collaboration, NLP, or any other related domains to share\ntheir perspectives and experiences on this problem, to help our community\nestablish a collective understanding of the challenges, research opportunities,\nresearch methods, and strategies to collaborate with researchers outside of\nHCI.\n""] , [""  Recent statements about the impressive capabilities of large language models\n(LLMs) are usually supported by evaluating on open-access benchmarks.\nConsidering the vast size and wide-ranging sources of LLMs' training data, it\ncould explicitly or implicitly include test data, leading to LLMs being more\nsusceptible to data contamination. However, due to the opacity of training\ndata, the black-box access of models, and the rapid growth of synthetic\ntraining data, detecting and mitigating data contamination for LLMs faces\nsignificant challenges. In this paper, we propose CDD, which stands for\nContamination Detection via output Distribution for LLMs. CDD necessitates only\nthe sampled texts to detect data contamination, by identifying the peakedness\nof LLM's output distribution. To mitigate the impact of data contamination in\nevaluation, we also present TED: Trustworthy Evaluation via output\nDistribution, based on the correction of LLM's output distribution. To\nfacilitate this study, we introduce two benchmarks, i.e., DetCon and ComiEval,\nfor data contamination detection and contamination mitigation evaluation tasks.\nExtensive experimental results show that CDD achieves the average relative\nimprovements of 21.8\\%-30.2\\% over other contamination detection approaches in\nterms of Accuracy, F1 Score, and AUC metrics, and can effectively detect\nimplicit contamination. TED substantially mitigates performance improvements up\nto 66.9\\% attributed to data contamination across various contamination setups.\nIn real-world applications, we reveal that ChatGPT exhibits a high potential to\nsuffer from data contamination on HumanEval benchmark.\n"", '  Data contamination in model evaluation has become increasingly prevalent with\nthe growing popularity of large language models. It allows models to ""cheat""\nvia memorisation instead of displaying true capabilities. Therefore,\ncontamination analysis has become an crucial part of reliable model evaluation\nto validate results. However, existing contamination analysis is usually\nconducted internally by large language model developers and often lacks\ntransparency and completeness. This paper presents an extensive data\ncontamination report for over 15 popular large language models across six\npopular multiple-choice QA benchmarks. We also introduce an open-source\npipeline that enables the community to perform contamination analysis on\ncustomised data and models. Our experiments reveal varying contamination levels\nranging from 1\\% to 45\\% across benchmarks, with the contamination degree\nincreasing rapidly over time. Performance analysis of large language models\nindicates that data contamination does not necessarily lead to increased model\nmetrics: while significant accuracy boosts of up to 14\\% and 7\\% are observed\non contaminated C-Eval and Hellaswag benchmarks, only a minimal increase is\nnoted on contaminated MMLU. We also find larger models seem able to gain more\nadvantages than smaller models on contaminated test sets.\n', ""  Language models pre-trained on web-scale corpora demonstrate impressive\ncapabilities on diverse downstream tasks. However, there is increasing concern\nwhether such capabilities might arise from evaluation datasets being included\nin the pre-training corpus -- a phenomenon known as \\textit{data contamination}\n-- in a manner that artificially increases performance. There has been little\nunderstanding of how this potential contamination might influence LMs'\nperformance on downstream tasks. In this paper, we explore the impact of data\ncontamination at the pre-training stage by pre-training a series of GPT-2\nmodels \\textit{from scratch}. We highlight the effect of both text\ncontamination (\\textit{i.e.}\\ input text of the evaluation samples) and\nground-truth contamination (\\textit{i.e.}\\ the prompts asked on the input and\nthe desired outputs) from evaluation data. We also investigate the effects of\nrepeating contamination for various downstream tasks. Additionally, we examine\nthe prevailing n-gram-based definitions of contamination within current LLM\nreports, pinpointing their limitations and inadequacy. Our findings offer new\ninsights into data contamination's effects on language model capabilities and\nunderscore the need for independent, comprehensive contamination assessments in\nLLM studies.\n""] , ['  Computer systems are becoming increasingly heterogeneous with the emergence\nof new memory technologies and compute devices. GPUs alongside CPUs have become\ncommonplace and CXL is poised to be a mainstay of cloud systems. The operating\nsystem is responsible for managing these hardware resources, requiring\nmodification every time a new device is released. Years of research and\ndevelopment are sunk into tuning the OS for high performance with each new\nheterogeneous device. With the recent explosion in memory technologies and\ndomain-specific accelerators, it would be beneficial to have an OS that could\nprovide high performance for new devices without significant effort.\n  We propose LLaMaS which can adapt to new devices easily. LLaMaS uses Large\nLanguage Models (LLMs) to extract the useful features of new devices from their\ntextual description and uses these features to make operating system decisions\nat runtime. Adding support to LLaMaS for a new device is as simple as\ndescribing the system and new device properties in plaintext.\n  LLaMaS reduces the burden on system administrators to enable easy integration\nof new devices into production systems.\n  Preliminary evaluation using ChatGPT shows that LLMs are capable of\nextracting device features from text and make correct OS decisions based on\nthose features.\n', ""  On-device large language models (LLMs) are catalyzing novel mobile\napplications such as UI task automation and personalized email auto-reply,\nwithout giving away users' private data. However, on-device LLMs still suffer\nfrom unacceptably long inference latency, especially the time to first token\n(prefill stage) due to the need of long context for accurate, personalized\ncontent generation, as well as the lack of parallel computing capacity of\nmobile CPU/GPU.\n  To enable practical on-device LLM, we present mllm-NPU, the first-of-its-kind\nLLM inference system that efficiently leverages on-device Neural Processing\nUnit (NPU) offloading. Essentially, mllm-NPU is an algorithm-system co-design\nthat tackles a few semantic gaps between the LLM architecture and contemporary\nNPU design. Specifically, it re-constructs the prompt and model in three\nlevels: (1) At prompt level, it divides variable-length prompts into multiple\nfixed-sized chunks while maintaining data dependencies; (2) At tensor level, it\nidentifies and extracts significant outliers to run on the CPU/GPU in parallel\nwith minimal overhead; (3) At block level, it schedules Transformer blocks in\nan out-of-order manner to the CPU/GPU and NPU based on their hardware affinity\nand sensitivity to accuracy. Compared to competitive baselines, mllm-NPU\nachieves 22.4x faster prefill speed and 30.7x energy savings on average, and up\nto 32.8x speedup in an end-to-end real-world application. For the first time,\nmllm-NPU achieves more than 1,000 tokens/sec prefilling for a billion-sized\nmodel (Qwen1.5-1.8B), paving the way towards practical on-device LLM.\n"", '  Transformers have revolutionized the machine learning landscape, gradually\nmaking their way into everyday tasks and equipping our computers with ""sparks\nof intelligence"". However, their runtime requirements have prevented them from\nbeing broadly deployed on mobile. As personal devices become increasingly\npowerful and prompt privacy becomes an ever more pressing issue, we explore the\ncurrent state of mobile execution of Large Language Models (LLMs). To achieve\nthis, we have created our own automation infrastructure, MELT, which supports\nthe headless execution and benchmarking of LLMs on device, supporting different\nmodels, devices and frameworks, including Android, iOS and Nvidia Jetson\ndevices. We evaluate popular instruction fine-tuned LLMs and leverage different\nframeworks to measure their end-to-end and granular performance, tracing their\nmemory and energy requirements along the way. Our analysis is the first\nsystematic study of on-device LLM execution, quantifying performance, energy\nefficiency and accuracy across various state-of-the-art models and showcases\nthe state of on-device intelligence in the era of hyperscale models. Results\nhighlight the performance heterogeneity across targets and corroborates that\nLLM inference is largely memory-bound. Quantization drastically reduces memory\nrequirements and renders execution viable, but at a non-negligible accuracy\ncost. Drawing from its energy footprint and thermal behavior, the continuous\nexecution of LLMs remains elusive, as both factors negatively affect user\nexperience. Last, our experience shows that the ecosystem is still in its\ninfancy, and algorithmic as well as hardware breakthroughs can significantly\nshift the execution cost. We expect NPU acceleration, and framework-hardware\nco-design to be the biggest bet towards efficient standalone execution, with\nthe alternative of offloading tailored towards edge deployments.\n'] , ['  Memorization in large language models (LLMs) is a growing concern. LLMs have\nbeen shown to easily reproduce parts of their training data, including\ncopyrighted work. This is an important problem to solve, as it may violate\nexisting copyright laws as well as the European AI Act. In this work, we\npropose a systematic analysis to quantify the extent of potential copyright\ninfringements in LLMs using European law as an example. Unlike previous work,\nwe evaluate instruction-finetuned models in a realistic end-user scenario. Our\nanalysis builds on a proposed threshold of 160 characters, which we borrow from\nthe German Copyright Service Provider Act and a fuzzy text matching algorithm\nto identify potentially copyright-infringing textual reproductions. The\nspecificity of countermeasures against copyright infringement is analyzed by\ncomparing model behavior on copyrighted and public domain data. We investigate\nwhat behaviors models show instead of producing protected text (such as refusal\nor hallucination) and provide a first legal assessment of these behaviors. We\nfind that there are huge differences in copyright compliance, specificity, and\nappropriate refusal among popular LLMs. Alpaca, GPT 4, GPT 3.5, and Luminous\nperform best in our comparison, with OpenGPT-X, Alpaca, and Luminous producing\na particularly low absolute number of potential copyright violations. Code will\nbe published soon.\n', '  Evaluating the degree of reproduction of copyright-protected content by\nlanguage models (LMs) is of significant interest to the AI and legal\ncommunities. Although both literal and non-literal similarities are considered\nby courts when assessing the degree of reproduction, prior research has focused\nonly on literal similarities. To bridge this gap, we introduce CopyBench, a\nbenchmark designed to measure both literal and non-literal copying in LM\ngenerations. Using copyrighted fiction books as text sources, we provide\nautomatic evaluation protocols to assess literal and non-literal copying,\nbalanced against the model utility in terms of the ability to recall facts from\nthe copyrighted works and generate fluent completions. We find that, although\nliteral copying is relatively rare, two types of non-literal copying -- event\ncopying and character copying -- occur even in models as small as 7B\nparameters. Larger models demonstrate significantly more copying, with literal\ncopying rates increasing from 0.2% to 10.5% and non-literal copying from 2.3%\nto 6.9% when comparing Llama3-8B and 70B models, respectively. We further\nevaluate the effectiveness of current strategies for mitigating copying and\nshow that (1) training-time alignment can reduce literal copying but may\nincrease non-literal copying, and (2) current inference-time mitigation methods\nprimarily reduce literal but not non-literal copying.\n', '  Large Language Models (LLMs) have demonstrated impressive capabilities in\ngenerating diverse and contextually rich text. However, concerns regarding\ncopyright infringement arise as LLMs may inadvertently produce copyrighted\nmaterial. In this paper, we first investigate the effectiveness of watermarking\nLLMs as a deterrent against the generation of copyrighted texts. Through\ntheoretical analysis and empirical evaluation, we demonstrate that\nincorporating watermarks into LLMs significantly reduces the likelihood of\ngenerating copyrighted content, thereby addressing a critical concern in the\ndeployment of LLMs. Additionally, we explore the impact of watermarking on\nMembership Inference Attacks (MIAs), which aim to discern whether a sample was\npart of the pretraining dataset and may be used to detect copyright violations.\nSurprisingly, we find that watermarking adversely affects the success rate of\nMIAs, complicating the task of detecting copyrighted text in the pretraining\ndataset. Finally, we propose an adaptive technique to improve the success rate\nof a recent MIA under watermarking. Our findings underscore the importance of\ndeveloping adaptive methods to study critical problems in LLMs with potential\nlegal implications.\n'] , [""  Large language models (LLMs) are increasingly deployed in real-world\nscenarios with the help of recent model compression techniques. Such momentum\ntowards local deployment means the use of compressed LLMs will widely impact a\nlarge population. However, prior analysis works often prioritize on preserving\nperplexity which is a direct analogy to training loss. The impact of\ncompression method on other critical aspects of model behavior, particularly\nsafety, still calls for a systematic assessment. To this end, we investigate\nthe impact of model compression on four dimensions: (1) degeneration harm,\ni.e., bias and toxicity in generation; (2) representational harm, i.e., biases\nin discriminative tasks; (3) dialect bias; (4) language modeling and downstream\ntask performance. We cover a wide spectrum of LLM compression techniques,\nincluding unstructured pruning, semi-structured pruning and quantization. Our\nanalysis reveals that compression can lead to unexpected consequences. Although\ncompression may unintentionally remedy LLMs' degeneration harm, it can still\nexacerbate on the representational harm axis. Although compression may\nunintentionally remedy LLMs' degeneration harm, it can still exacerbate on the\nrepresentational harm axis. Moreover, there is a divergent impact on different\nprotected groups as the compression rate grows. Finally, different compression\nmethods have drastically different safety impacts, e.g., quantization mostly\npreserves bias while pruning degrades quickly. Our findings underscore the\nimportance of integrating safety assessments into the development of compressed\nLLMs to ensure their reliability across real-world applications. Our full\nresults are available here:\n\\url{https://github.com/zhichaoxu-shufe/Beyond-Perplexity-Compression-Safety-Eval}\n"", '  We conceptualize the process of understanding as information compression, and\npropose a method for ranking large language models (LLMs) based on lossless\ndata compression. We demonstrate the equivalence of compression length under\narithmetic coding with cumulative negative log probabilities when using a large\nlanguage model as a prior, that is, the pre-training phase of the model is\nessentially the process of learning the optimal coding length. At the same\ntime, the evaluation metric compression ratio can be obtained without actual\ncompression, which greatly saves overhead. In this paper, we use five large\nlanguage models as priors for compression, then compare their performance on\nchallenging natural language processing tasks, including sentence completion,\nquestion answering, and coreference resolution. Experimental results show that\ncompression ratio and model performance are positively correlated, so it can be\nused as a general metric to evaluate large language models.\n', '  Long context inference presents challenges at the system level with increased\ncompute and memory requirements, as well as from an accuracy perspective in\nbeing able to reason over long contexts. Recently, several methods have been\nproposed to compress the prompt to reduce the context length. However, there\nhas been little work on comparing the different proposed methods across\ndifferent tasks through a standardized analysis. This has led to conflicting\nresults. To address this, here we perform a comprehensive characterization and\nevaluation of different prompt compression methods. In particular, we analyze\nextractive compression, summarization-based abstractive compression, and token\npruning methods. Surprisingly, we find that extractive compression often\noutperforms all the other approaches, and enables up to 10x compression with\nminimal accuracy degradation. Interestingly, we also find that despite several\nrecent claims, token pruning methods often lag behind extractive compression.\nWe only found marginal improvements on summarization tasks.\n']","Large Language Models: Safety, Security, and Ethics","""Large Language Model Safety and Risk Mitigation"""
5,"""Large Language Model-based Intelligent Agents for Planning and Collaboration"" , ""Large Language Models in Finance"" , ""Dialogue Systems and Large Language Models"" , ""Large Language Models in Chemistry Applications"" , ""Large Language Models in Software Engineering and Education"" , ""Large Language Models for Software Development and Documentation""","['agent', 'agents', 'planning', 'ai', 'planner', 'automation', 'planners', 'tasks', 'plans', 'autonomous'] , ['finance', 'financial', 'textual', 'nlp', 'text', 'investors', 'investor', 'lexicon', 'accounting', 'investment'] , ['dialogs', 'dialogue', 'dialogues', 'dialog', 'conversation', 'conversational', 'conversations', 'utterances', 'dialogstudio', 'utterance'] , ['chemical', 'chemistry', 'molecular', 'molecule', 'chemllm', 'molecules', 'synthesis', 'chemdfm', 'automating', 'chemdata'] , ['programming', 'programmers', 'programmer', 'coding', 'code', 'program', 'developers', 'software', 'agilecoder', 'prompts'] , ['repositories', 'documentation', 'retrieval', 'software', 'discoverybench', 'language', 'models', 'tools', 'completions', 'benchmarks']","[""  While Large Language Models (LLMs) have demonstrated impressive\naccomplishments in both reasoning and planning, their abilities in multi-agent\ncollaborations remains largely unexplored. This study evaluates LLM-based\nagents in a multi-agent cooperative text game with Theory of Mind (ToM)\ninference tasks, comparing their performance with Multi-Agent Reinforcement\nLearning (MARL) and planning-based baselines. We observed evidence of emergent\ncollaborative behaviors and high-order Theory of Mind capabilities among\nLLM-based agents. Our results reveal limitations in LLM-based agents' planning\noptimization due to systematic failures in managing long-horizon contexts and\nhallucination about the task state. We explore the use of explicit belief state\nrepresentations to mitigate these issues, finding that it enhances task\nperformance and the accuracy of ToM inferences for LLM-based agents.\n"", '  Intelligent agents stand out as a potential path toward artificial general\nintelligence (AGI). Thus, researchers have dedicated significant effort to\ndiverse implementations for them. Benefiting from recent progress in large\nlanguage models (LLMs), LLM-based agents that use universal natural language as\nan interface exhibit robust generalization capabilities across various\napplications -- from serving as autonomous general-purpose task assistants to\napplications in coding, social, and economic domains, LLM-based agents offer\nextensive exploration opportunities. This paper surveys current research to\nprovide an in-depth overview of LLM-based intelligent agents within\nsingle-agent and multi-agent systems. It covers their definitions, research\nframeworks, and foundational components such as their composition, cognitive\nand planning methods, tool utilization, and responses to environmental\nfeedback. We also delve into the mechanisms of deploying LLM-based agents in\nmulti-agent systems, including multi-role collaboration, message passing, and\nstrategies to alleviate communication issues between agents. The discussions\nalso shed light on popular datasets and application scenarios. We conclude by\nenvisioning prospects for LLM-based agents, considering the evolving landscape\nof AI and natural language processing.\n', '  Planning has been part of the core pursuit for artificial intelligence since\nits conception, but earlier AI agents mostly focused on constrained settings\nbecause many of the cognitive substrates necessary for human-level planning\nhave been lacking. Recently, language agents powered by large language models\n(LLMs) have shown interesting capabilities such as tool use and reasoning. Are\nthese language agents capable of planning in more complex settings that are out\nof the reach of prior AI agents? To advance this investigation, we propose\nTravelPlanner, a new planning benchmark that focuses on travel planning, a\ncommon real-world planning scenario. It provides a rich sandbox environment,\nvarious tools for accessing nearly four million data records, and 1,225\nmeticulously curated planning intents and reference plans. Comprehensive\nevaluations show that the current language agents are not yet capable of\nhandling such complex planning tasks-even GPT-4 only achieves a success rate of\n0.6%. Language agents struggle to stay on task, use the right tools to collect\ninformation, or keep track of multiple constraints. However, we note that the\nmere possibility for language agents to tackle such a complex problem is in\nitself non-trivial progress. TravelPlanner provides a challenging yet\nmeaningful testbed for future language agents.\n'] , ['  Recent releases of pre-trained Large Language Models (LLMs) have gained\nconsiderable traction, yet research on fine-tuning and employing\ndomain-specific LLMs remains scarce. This study investigates approaches for\nfine-tuning and leveraging domain-specific LLMs, highlighting trends in LLMs,\nfoundational models, and methods for domain-specific pre-training. Focusing on\nthe financial sector, it details dataset selection, preprocessing, model\nchoice, and considerations crucial for LLM fine-tuning in finance. Addressing\nthe unique characteristics of financial data, the study explores the\nconstruction of domain-specific vocabularies and considerations for security\nand regulatory compliance. In the practical application of LLM fine-tuning, the\nstudy outlines the procedure and implementation for generating domain-specific\nLLMs in finance. Various financial cases, including stock price prediction,\nsentiment analysis of financial news, automated document processing, research,\ninformation extraction, and customer service enhancement, are exemplified. The\nstudy explores the potential of LLMs in the financial domain, identifies\nlimitations, and proposes directions for improvement, contributing valuable\ninsights for future research. Ultimately, it advances natural language\nprocessing technology in business, suggesting proactive LLM utilization in\nfinancial services across industries.\n', ""  Natural language processing (NLP) has recently gained relevance within\nfinancial institutions by providing highly valuable insights into companies and\nmarkets' financial documents. However, the landscape of the financial domain\npresents extra challenges for NLP, due to the complexity of the texts and the\nuse of specific terminology. Generalist language models tend to fall short in\ntasks specifically tailored for finance, even when using large language models\n(LLMs) with great natural language understanding and generative capabilities.\nThis paper presents a study on LLM adaptation methods targeted at the financial\ndomain and with high emphasis on financial sentiment analysis. To this purpose,\ntwo foundation models with less than 1.5B parameters have been adapted using a\nwide range of strategies. We show that through careful fine-tuning on both\nfinancial documents and instructions, these foundation models can be adapted to\nthe target domain. Moreover, we observe that small LLMs have comparable\nperformance to larger scale models, while being more efficient in terms of\nparameters and data. In addition to the models, we show how to generate\nartificial instructions through LLMs to augment the number of samples of the\ninstruction dataset.\n"", ""  In recent years, Large Language Models (LLMs) like ChatGPT have seen\nconsiderable advancements and have been applied in diverse fields. Built on the\nTransformer architecture, these models are trained on extensive datasets,\nenabling them to understand and generate human language effectively. In the\nfinancial domain, the deployment of LLMs is gaining momentum. These models are\nbeing utilized for automating financial report generation, forecasting market\ntrends, analyzing investor sentiment, and offering personalized financial\nadvice. Leveraging their natural language processing capabilities, LLMs can\ndistill key insights from vast financial data, aiding institutions in making\ninformed investment choices and enhancing both operational efficiency and\ncustomer satisfaction. In this study, we provide a comprehensive overview of\nthe emerging integration of LLMs into various financial tasks. Additionally, we\nconducted holistic tests on multiple financial tasks through the combination of\nnatural language instructions. Our findings show that GPT-4 effectively follow\nprompt instructions across various financial tasks. This survey and evaluation\nof LLMs in the financial domain aim to deepen the understanding of LLMs'\ncurrent role in finance for both financial practitioners and LLM researchers,\nidentify new research and application prospects, and highlight how these\ntechnologies can be leveraged to solve practical challenges in the finance\nindustry.\n""] , ['  We study the limitations of Large Language Models (LLMs) for the task of\nresponse generation in human-machine dialogue. Several techniques have been\nproposed in the literature for different dialogue types (e.g., Open-Domain).\nHowever, the evaluations of these techniques have been limited in terms of base\nLLMs, dialogue types and evaluation metrics. In this work, we extensively\nanalyze different LLM adaptation techniques when applied to different dialogue\ntypes. We have selected two base LLMs, Llama-2 and Mistral, and four dialogue\ntypes Open-Domain, Knowledge-Grounded, Task-Oriented, and Question Answering.\nWe evaluate the performance of in-context learning and fine-tuning techniques\nacross datasets selected for each dialogue type. We assess the impact of\nincorporating external knowledge to ground the generation in both scenarios of\nRetrieval-Augmented Generation (RAG) and gold knowledge. We adopt consistent\nevaluation and explainability criteria for automatic metrics and human\nevaluation protocols. Our analysis shows that there is no universal\nbest-technique for adapting large language models as the efficacy of each\ntechnique depends on both the base LLM and the specific type of dialogue. Last\nbut not least, the assessment of the best adaptation technique should include\nhuman evaluation to avoid false expectations and outcomes derived from\nautomatic metrics.\n', '  This survey provides a comprehensive review of research on multi-turn\ndialogue systems, with a particular focus on multi-turn dialogue systems based\non large language models (LLMs). This paper aims to (a) give a summary of\nexisting LLMs and approaches for adapting LLMs to downstream tasks; (b)\nelaborate recent advances in multi-turn dialogue systems, covering both\nLLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems,\nalong with datasets and evaluation metrics; (c) discuss some future emphasis\nand recent research problems arising from the development of LLMs and the\nincreasing demands on multi-turn dialogue systems.\n', '  Dialogue State Tracking (DST) is designed to monitor the evolving dialogue\nstate in the conversations and plays a pivotal role in developing task-oriented\ndialogue systems. However, obtaining the annotated data for the DST task is\nusually a costly endeavor. In this paper, we focus on employing LLMs to\ngenerate dialogue data to reduce dialogue collection and annotation costs.\nSpecifically, GPT-4 is used to simulate the user and agent interaction,\ngenerating thousands of dialogues annotated with DST labels. Then a two-stage\nfine-tuning on LLaMA 2 is performed on the generated data and the real data for\nthe DST prediction. Experimental results on two public DST benchmarks show that\nwith the generated dialogue data, our model performs better than the baseline\ntrained solely on real data. In addition, our approach is also capable of\nadapting to the dynamic demands in real-world scenarios, generating dialogues\nin new domains swiftly. After replacing dialogue segments in any domain with\nthe corresponding generated ones, the model achieves comparable performance to\nthe model trained on real data.\n'] , ['  In recent years, Large Language Models (LLMs) have achieved significant\nsuccess in natural language processing (NLP) and various interdisciplinary\nareas. However, applying LLMs to chemistry is a complex task that requires\nspecialized domain knowledge. This paper provides a thorough exploration of the\nnuanced methodologies employed in integrating LLMs into the field of chemistry,\ndelving into the complexities and innovations at this interdisciplinary\njuncture. Specifically, our analysis begins with examining how molecular\ninformation is fed into LLMs through various representation and tokenization\nmethods. We then categorize chemical LLMs into three distinct groups based on\nthe domain and modality of their input data, and discuss approaches for\nintegrating these inputs for LLMs. Furthermore, this paper delves into the\npretraining objectives with adaptations to chemical LLMs. After that, we\nexplore the diverse applications of LLMs in chemistry, including novel\nparadigms for their application in chemistry tasks. Finally, we identify\npromising research directions, including further integration with chemical\nknowledge, advancements in continual learning, and improvements in model\ninterpretability, paving the way for groundbreaking developments in the field.\n', ""  Large language models (LLMs) have made impressive progress in chemistry\napplications. However, the community lacks an LLM specifically designed for\nchemistry. The main challenges are two-fold: firstly, most chemical data and\nscientific knowledge are stored in structured databases, which limits the\nmodel's ability to sustain coherent dialogue when used directly. Secondly,\nthere is an absence of objective and fair benchmark that encompass most\nchemistry tasks. Here, we introduce ChemLLM, a comprehensive framework that\nfeatures the first LLM dedicated to chemistry. It also includes ChemData, a\ndataset specifically designed for instruction tuning, and ChemBench, a robust\nbenchmark covering nine essential chemistry tasks. ChemLLM is adept at\nperforming various tasks across chemical disciplines with fluid dialogue\ninteraction. Notably, ChemLLM achieves results comparable to GPT-4 on the core\nchemical tasks and demonstrates competitive performance with LLMs of similar\nsize in general scenarios. ChemLLM paves a new path for exploration in chemical\nstudies, and our method of incorporating structured chemical knowledge into\ndialogue systems sets a new standard for developing LLMs in various scientific\nfields. Codes, Datasets, and Model weights are publicly accessible at\nhttps://hf.co/AI4Chem\n"", ""  Chemical synthesis, which is crucial for advancing material synthesis and\ndrug discovery, impacts various sectors including environmental science and\nhealthcare. The rise of technology in chemistry has generated extensive\nchemical data, challenging researchers to discern patterns and refine synthesis\nprocesses. Artificial intelligence (AI) helps by analyzing data to optimize\nsynthesis and increase yields. However, AI faces challenges in processing\nliterature data due to the unstructured format and diverse writing style of\nchemical literature. To overcome these difficulties, we introduce an end-to-end\nAI agent framework capable of high-fidelity extraction from extensive chemical\nliterature. This AI agent employs large language models (LLMs) for prompt\ngeneration and iterative optimization. It functions as a chemistry assistant,\nautomating data collection and analysis, thereby saving manpower and enhancing\nperformance. Our framework's efficacy is evaluated using accuracy, recall, and\nF1 score of reaction condition data, and we compared our method with human\nexperts in terms of content correctness and time efficiency. The proposed\napproach marks a significant advancement in automating chemical literature\nextraction and demonstrates the potential for AI to revolutionize data\nmanagement and utilization in chemistry.\n""] , [""  The increasing demand for programming language education and growing class\nsizes require immediate and personalized feedback. However, traditional code\nreview methods have limitations in providing this level of feedback. As the\ncapabilities of Large Language Models (LLMs) like GPT for generating accurate\nsolutions and timely code reviews are verified, this research proposes a system\nthat employs GPT-4 to offer learner-friendly code reviews and minimize the risk\nof AI-assist cheating.\n  To provide learner-friendly code reviews, a dataset was collected from an\nonline judge system, and this dataset was utilized to develop and enhance the\nsystem's prompts. In addition, to minimize AI-assist cheating, the system flow\nwas designed to provide code reviews only for code submitted by a learner, and\na feature that highlights code lines to fix was added. After the initial system\nwas deployed on the web, software education experts conducted usability test.\nBased on the results, improvement strategies were developed to improve code\nreview and code correctness check module, thereby enhancing the system.\n  The improved system underwent evaluation by software education experts based\non four criteria: strict code correctness checks, response time, lower API call\ncosts, and the quality of code reviews. The results demonstrated a performance\nto accurately identify error types, shorten response times, lower API call\ncosts, and maintain high-quality code reviews without major issues. Feedback\nfrom participants affirmed the tool's suitability for teaching programming to\nprimary and secondary school students. Given these benefits, the system is\nanticipated to be a efficient learning tool in programming language learning\nfor educational settings.\n"", '  With the rise of large language models (LLMs), researchers are increasingly\nexploring their applications in var ious vertical domains, such as software\nengineering. LLMs have achieved remarkable success in areas including code\ngeneration and vulnerability detection. However, they also exhibit numerous\nlimitations and shortcomings. LLM-based agents, a novel tech nology with the\npotential for Artificial General Intelligence (AGI), combine LLMs as the core\nfor decision-making and action-taking, addressing some of the inherent\nlimitations of LLMs such as lack of autonomy and self-improvement. Despite\nnumerous studies and surveys exploring the possibility of using LLMs in\nsoftware engineering, it lacks a clear distinction between LLMs and LLM based\nagents. It is still in its early stage for a unified standard and benchmarking\nto qualify an LLM solution as an LLM-based agent in its domain. In this survey,\nwe broadly investigate the current practice and solutions for LLMs and\nLLM-based agents for software engineering. In particular we summarise six key\ntopics: requirement engineering, code generation, autonomous decision-making,\nsoftware design, test generation, and software maintenance. We review and\ndifferentiate the work of LLMs and LLM-based agents from these six topics,\nexamining their differences and similarities in tasks, benchmarks, and\nevaluation metrics. Finally, we discuss the models and benchmarks used,\nproviding a comprehensive analysis of their applications and effectiveness in\nsoftware engineering. We anticipate this work will shed some lights on pushing\nthe boundaries of LLM-based agents in software engineering for future research.\n', ""  Large Language Models (LLMs) represent a leap in artificial intelligence,\nexcelling in tasks using human language(s). Although the main focus of\ngeneral-purpose LLMs is not code generation, they have shown promising results\nin the domain. However, the usefulness of LLMs in an academic software\nengineering project has not been fully explored yet. In this study, we explore\nthe usefulness of LLMs for 214 students working in teams consisting of up to\nsix members. Notably, in the academic course through which this study is\nconducted, students were encouraged to integrate LLMs into their development\ntool-chain, in contrast to most other academic courses that explicitly prohibit\nthe use of LLMs.\n  In this paper, we analyze the AI-generated code, prompts used for code\ngeneration, and the human intervention levels to integrate the code into the\ncode base. We also conduct a perception study to gain insights into the\nperceived usefulness, influencing factors, and future outlook of LLM from a\ncomputer science student's perspective. Our findings suggest that LLMs can play\na crucial role in the early stages of software development, especially in\ngenerating foundational code structures, and helping with syntax and error\ndebugging. These insights provide us with a framework on how to effectively\nutilize LLMs as a tool to enhance the productivity of software engineering\nstudents, and highlight the necessity of shifting the educational focus toward\npreparing students for successful human-AI collaboration.\n""] , ['  Large Language Models (LLMs) have the potential to revolutionize the Sixth\nGeneration (6G) communication networks. However, current mainstream LLMs\ngenerally lack the specialized knowledge in telecom domain. In this paper, for\nthe first time, we propose a pipeline to adapt any general purpose LLMs to a\ntelecom-specific LLMs. We collect and build telecom-specific pre-train dataset,\ninstruction dataset, preference dataset to perform continual pre-training,\ninstruct tuning and alignment tuning respectively. Besides, due to the lack of\nwidely accepted evaluation benchmarks in telecom domain, we extend existing\nevaluation benchmarks and proposed three new benchmarks, namely, Telecom Math\nModeling, Telecom Open QnA and Telecom Code Tasks. These new benchmarks provide\na holistic evaluation of the capabilities of LLMs including math modeling,\nOpen-Ended question answering, code generation, infilling, summarization and\nanalysis in telecom domain. Our fine-tuned LLM TelecomGPT outperforms state of\nthe art (SOTA) LLMs including GPT-4, Llama-3 and Mistral in Telecom Math\nModeling benchmark significantly and achieve comparable performance in various\nevaluation benchmarks such as TeleQnA, 3GPP technical documents classification,\ntelecom code summary and generation and infilling.\n', ""  The development and training of deep learning models have become increasingly\ncostly and complex. Consequently, software engineers are adopting pre-trained\nmodels (PTMs) for their downstream applications. The dynamics of the PTM supply\nchain remain largely unexplored, signaling a clear need for structured datasets\nthat document not only the metadata but also the subsequent applications of\nthese models. Without such data, the MSR community cannot comprehensively\nunderstand the impact of PTM adoption and reuse. This paper presents the\nPeaTMOSS dataset, which comprises metadata for 281,638 PTMs and detailed\nsnapshots for all PTMs with over 50 monthly downloads (14,296 PTMs), along with\n28,575 open-source software repositories from GitHub that utilize these models.\nAdditionally, the dataset includes 44,337 mappings from 15,129 downstream\nGitHub repositories to the 2,530 PTMs they use. To enhance the dataset's\ncomprehensiveness, we developed prompts for a large language model to\nautomatically extract model metadata, including the model's training datasets,\nparameters, and evaluation metrics. Our analysis of this dataset provides the\nfirst summary statistics for the PTM supply chain, showing the trend of PTM\ndevelopment and common shortcomings of PTM package documentation. Our example\napplication reveals inconsistencies in software licenses across PTMs and their\ndependent projects. PeaTMOSS lays the foundation for future research, offering\nrich opportunities to investigate the PTM supply chain. We outline mining\nopportunities on PTMs, their downstream usage, and cross-cutting questions.\n"", '  Open-source development has revolutionized the software industry by promoting\ncollaboration, transparency, and community-driven innovation. Today, a vast\namount of various kinds of open-source software, which form networks of\nrepositories, is often hosted on GitHub - a popular software development\nplatform. To enhance the discoverability of the repository networks, i.e.,\ngroups of similar repositories, GitHub introduced repository topics in 2017\nthat enable users to more easily explore relevant projects by type, technology,\nand more. It is thus crucial to accurately assign topics for each GitHub\nrepository. Current methods for automatic topic recommendation rely heavily on\nTF-IDF for encoding textual data, presenting challenges in understanding\nsemantic nuances. This paper addresses the limitations of existing techniques\nby proposing Legion, a novel approach that leverages Pre-trained Language\nModels (PTMs) for recommending topics for GitHub repositories. The key novelty\nof Legion is three-fold. First, Legion leverages the extensive capabilities of\nPTMs in language understanding to capture contextual information and semantic\nmeaning in GitHub repositories. Second, Legion overcomes the challenge of\nlong-tailed distribution, which results in a bias toward popular topics in\nPTMs, by proposing a Distribution-Balanced Loss (DB Loss) to better train the\nPTMs. Third, Legion employs a filter to eliminate vague recommendations,\nthereby improving the precision of PTMs. Our empirical evaluation on a\nbenchmark dataset of real-world GitHub repositories shows that Legion can\nimprove vanilla PTMs by up to 26% on recommending GitHubs topics. Legion also\ncan suggest GitHub topics more precisely and effectively than the\nstate-of-the-art baseline with an average improvement of 20% and 5% in terms of\nPrecision and F1-score, respectively.\n']",Applications of Large Language Models,"""Large Language Models for Software Development and Documentation"""
6,"""Reasoning with Large Language Models through Prompting"" , Code Generation and Evaluation for Large Language Models , Instruction Tuning for Language Models , Speculative Decoding for Efficient Language Models , ""Prompt Tuning for Language Models"" , Prompt Compression for Large Language Models , ""Instruction Tuning for Large Language Models"" , Program Synthesis and Generation with Language Models , ""Large Language Models and Dataset Generation""","['reasoning', 'prompting', 'thinking', 'prompts', 'inference', 'deductive', 'prompt', 'tasks', 'knowledge', 'logic'] , ['compilers', 'programming', 'coding', 'compiler', 'code', 'programmers', 'snippets', 'developers', 'programmer', 'apis'] , ['instruction', 'language', 'instructmining', 'texttuning', 'tuning', 'training', 'tuned', 'curriculum', 'benchmarks', 'tasks'] , ['drafts', 'draft', 'speculative', 'decoding', 'memory', 'drafter', 'drafters', 'drafting', 'language', 'speedups'] , ['prompts', 'prompt', 'attention', 'prompting', 'language', 'pretrained', 'tuning', 'optimizing', 'tasks', 'monopara'] , ['promptcompressor', 'compression', 'prompts', 'decoder', 'compressed', 'prompt', 'compressing', 'encoder', 'compress', 'adacoder'] , ['annotated', 'instruction', 'responses', 'language', 'metacognitive', 'feedback', 'tasks', 'examples', 'reinforcement', 'text'] , ['programming', 'program', 'generate', 'pseudocode', 'abstractions', 'synthesis', 'algorithmic', 'programs', 'automata', 'solvers'] , ['tokenizers', 'language', 'datasets', 'nlp', 'models', 'feature', 'dataset', 'generating', 'trained', 'tokens']","['  Scaling up language models to billions of parameters has opened up\npossibilities for in-context learning, allowing instruction tuning and few-shot\nlearning on tasks that the model was not specifically trained for. This has\nachieved breakthrough performance on language tasks such as translation,\nsummarization, and question-answering. Furthermore, in addition to these\nassociative ""System 1"" tasks, recent advances in Chain-of-thought prompt\nlearning have demonstrated strong ""System 2"" reasoning abilities, answering a\nquestion in the field of artificial general intelligence whether LLMs can\nreason. The field started with the question whether LLMs can solve grade school\nmath word problems. This paper reviews the rapidly expanding field of\nprompt-based reasoning with LLMs. Our taxonomy identifies different ways to\ngenerate, evaluate, and control multi-step reasoning. We provide an in-depth\ncoverage of core approaches and open problems, and we propose a research agenda\nfor the near future. Finally, we highlight the relation between reasoning and\nprompt-based learning, and we discuss the relation between reasoning,\nsequential decision processes, and reinforcement learning. We find that\nself-improvement, self-reflection, and some metacognitive abilities of the\nreasoning processes are possible through the judicious use of prompts. True\nself-improvement and self-reasoning, to go from reasoning with LLMs to\nreasoning by LLMs, remains future work.\n', '  Chain-of-thought (CoT) prompting can guide language models to engage in\ncomplex multi-step reasoning. The quality of provided demonstrations\nsignificantly impacts the success of downstream inference tasks. While existing\nautomated methods prioritize accuracy and semantics in these demonstrations, we\nshow that the underlying reasoning patterns play a more crucial role in such\ntasks. In this paper, we propose Pattern-Aware CoT, a prompting method that\nconsiders the diversity of demonstration patterns. By incorporating patterns\nsuch as step length and reasoning process within intermediate steps, PA-CoT\neffectively mitigates the issue of bias induced by demonstrations and enables\nbetter generalization to diverse scenarios. We conduct experiments on nine\nreasoning benchmark tasks using two open-source LLMs. The results show that our\nmethod substantially enhances reasoning performance and exhibits robustness to\nerrors. The code will be made publicly available.\n', ""  Chain of Thought (CoT) is significant in improving the reasoning abilities of\nlarge language models (LLMs). However, the correlation between the\neffectiveness of CoT and the length of reasoning steps in prompts remains\nlargely unknown. To shed light on this, we have conducted several empirical\nexperiments to explore the relations. Specifically, we design experiments that\nexpand and compress the rationale reasoning steps within CoT demonstrations\nwhile keeping all other factors constant. We have the following key findings.\nFirst, the results indicate that lengthening the reasoning steps in prompts,\neven without adding new information into the prompt, considerably enhances\nLLMs' reasoning abilities across multiple datasets. Alternatively, shortening\nthe reasoning steps, even while preserving the key information, significantly\ndiminishes the reasoning abilities of models. This finding highlights the\nimportance of the number of steps in CoT prompts and provides practical\nguidance to make better use of LLMs' potential in complex problem-solving\nscenarios. Second, we also investigated the relationship between the\nperformance of CoT and the rationales used in demonstrations. Surprisingly, the\nresult shows that even incorrect rationales can yield favorable outcomes if\nthey maintain the requisite length of inference. Third, we observed that the\nadvantages of increasing reasoning steps are task-dependent: simpler tasks\nrequire fewer steps, whereas complex tasks gain significantly from longer\ninference sequences. The code is available at\nhttps://github.com/MingyuJ666/The-Impact-of-Reasoning-Step-Length-on-Large-Language-Models\n""] , ['  Large language models for code (i.e., code LLMs) have shown strong code\nunderstanding and generation capabilities. To evaluate the capabilities of code\nLLMs in various aspects, many benchmarks have been proposed (e.g., HumanEval\nand ClassEval). Code reasoning is one of the most essential abilities of code\nLLMs, but existing benchmarks for code reasoning are not sufficient. Typically,\nthey focus on predicting the input and output of a program, ignoring the\nevaluation of the intermediate behavior during program execution, as well as\nthe logical consistency (e.g., the model should not give the correct output if\nthe prediction of execution path is wrong) when performing the reasoning. To\naddress these problems, in this paper, we propose a framework, namely REval,\nfor evaluating code reasoning abilities and consistency of code LLMs with\nprogram execution. We utilize existing code benchmarks and adapt them to new\nbenchmarks within our framework. A large-scale empirical study is conducted and\nmost LLMs show unsatisfactory performance on both Runtime Behavior Reasoning\n(i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation\n(i.e., an average IC score of 10.3). Evaluation results of current code LLMs\nreflect the urgent need for the community to strengthen the code reasoning\ncapability of code LLMs. Our code, data, and \\newname leaderboard are available\nat https://r-eval.github.io.\n', ""  Code Large Language Models (Code LLMs) have demonstrated outstanding\nperformance in code-related tasks. Several instruction tuning approaches have\nbeen proposed to boost the code generation performance of pre-trained Code\nLLMs. In this paper, we introduce a diverse instruction model (DolphCoder) with\nself-evaluating for code generation. It learns diverse instruction targets and\ncombines a code evaluation objective to enhance its code generation ability.\nOur model achieves superior performance on the HumanEval and MBPP benchmarks,\ndemonstrating new insights for future code instruction tuning work. Our key\nfindings are: (1) Augmenting more diverse responses with distinct reasoning\npaths increases the code capability of LLMs. (2) Improving one's ability to\nevaluate the correctness of code solutions also enhances their ability to\ncreate it.\n"", '  Context. Nowadays, 83% of software developers use Large Language Models\n(LLMs) to generate code. LLMs recently became essential to increase the\nproductivity of software developers and decrease the time and cost of software\ndevelopment. Developers ranging from novices to experts use LLM tools not only\nto detect and patch bugs, but also to integrate generated code into their\nsoftware. However, as of today there is no objective assessment of the energy\nefficiency of the source code generated by LLM tools. Released in August 2023,\nCode Llama is one of the most recent LLM tools.\n  Goal. In this paper, we present an empirical study that assesses the energy\nefficiency of Code Llama with respect to human-written source code.\n  Method. We design an experiment involving three human-written benchmarks\nimplemented in C++, JavaScript, and Python. We ask Code Llama to generate the\ncode of the benchmarks using different prompts and temperatures. Therefore, we\nexecute both implementations and profile their energy efficiency.\n  Results. Our study shows that the energy efficiency of code generated by Code\nLlama is heavily-dependent on the chosen programming language and the specific\ncode problem at hand. Also, human implementations tend to be more energy\nefficient overall, with generated JavaScript code outperforming its human\ncounterpart. Moreover, explicitly asking Code Llama to generate\nenergy-efficient code results in an equal or worse energy efficiency, as well\nas using different temperatures seems not to affect the energy efficiency of\ngenerated code.\n  Conclusions. According to our results, code generated using Code Llama does\nnot guarantee energy efficiency, even when prompted to do so. Therefore,\nsoftware developers should evaluate the energy efficiency of generated code\nbefore integrating it into the software system under development.\n'] , ['  Instruction tuning -- fine-tuning a large language model (LLM) on pairs of\ninstructions and desired outcomes -- is an approach that enables pre-trained\nlanguage models to perform real-world tasks and follow human instructions. Its\npractical success depends on the model learning a broader set of instructions\nthan those it was trained on. Yet the factors that determine model\ngeneralization to such \\emph{unseen tasks} are not well understood. %To\nunderstand the driving factors of generalization, In this paper, we experiment\nwith string rewrites, a symbolic task that serves as a building block for\nTuring complete Markov algorithms while allowing experimental control of\n""inputs"" and ""instructions"". We investigate the trade-off between the number of\ninstructions the model is trained on and the number of training samples\nprovided for each instruction and observe that the diversity of the instruction\nset determines generalization. Generalization emerges once a diverse enough set\nof tasks is provided, even though very few examples are provided for each task.\nInstruction diversity also ensures robustness with respect to non-uniform\ndistributions of instructions in the training set.\n', '  Instruction tuning is a vital step of training large language models (LLM),\nso how to enhance the effect of instruction tuning has received increased\nattention. Existing works indicate that the quality of the dataset is more\ncrucial than the quantity during instruction tuning of LLM. Therefore, recently\na lot of studies focus on exploring the methods of selecting high-quality\nsubset from instruction datasets, aiming to reduce training costs and enhance\nthe instruction-following capabilities of LLMs. This paper presents a\ncomprehensive survey on data selection for LLM instruction tuning. Firstly, we\nintroduce the wildly used instruction datasets. Then, we propose a new taxonomy\nof the data selection methods and provide a detailed introduction of recent\nadvances,and the evaluation strategies and results of data selection methods\nare also elaborated in detail. Finally, we emphasize the open challenges and\npresent new frontiers of this task.\n', ""  Instruction tuning -- tuning large language models on instruction-output\npairs -- is a promising technique for making models better adapted to the real\nworld. Yet, the key factors driving the model's capability to understand and\nfollow instructions not seen during training remain under-explored. Our\ninvestigation begins with a series of synthetic experiments within the\ntheoretical framework of a Turing-complete algorithm called Markov algorithm,\nwhich allows fine-grained control over the instruction-tuning data.\nGeneralization and robustness with respect to the training distribution emerge\nonce a diverse enough set of tasks is provided, even though very few examples\nare provided for each task. We extend these initial results to a real-world\napplication scenario of code generation and find that a more diverse\ninstruction set, extending beyond code-related tasks, improves the performance\nof code generation. Our observations suggest that a more diverse semantic space\nfor instruction-tuning sets greatly improves the model's ability to follow\ninstructions and perform tasks.\n""] , ['  Autoregressive sampling from large language models has led to\nstate-of-the-art results in several natural language tasks. However,\nautoregressive sampling generates tokens one at a time making it slow, and even\nprohibitive in certain tasks. One way to speed up sampling is\n$\\textit{speculative decoding}$: use a small model to sample a $\\textit{draft}$\n(block or sequence of tokens), and then score all tokens in the draft by the\nlarge language model in parallel. A subset of the tokens in the draft are\naccepted (and the rest rejected) based on a statistical method to guarantee\nthat the final output follows the distribution of the large model. In this\nwork, we provide a principled understanding of speculative decoding through the\nlens of optimal transport (OT) with $\\textit{membership cost}$. This framework\ncan be viewed as an extension of the well-known $\\textit{maximal-coupling}$\nproblem. This new formulation enables us to generalize the speculative decoding\nmethod to allow for a set of $k$ candidates at the token-level, which leads to\nan improved optimal membership cost. We show that the optimal draft selection\nalgorithm (transport plan) can be computed via linear programming, whose\nbest-known runtime is exponential in $k$. We then propose a valid draft\nselection algorithm whose acceptance probability is $(1-1/e)$-optimal\nmultiplicatively. Moreover, it can be computed in time almost linear with size\nof domain of a single token. Using this $new draft selection$ algorithm, we\ndevelop a new autoregressive sampling algorithm called $\\textit{SpecTr}$, which\nprovides speedup in decoding while ensuring that there is no quality\ndegradation in the decoded output. We experimentally demonstrate that for\nstate-of-the-art large language models, the proposed approach achieves a wall\nclock speedup of 2.13X, a further 1.37X speedup over speculative decoding on\nstandard benchmarks.\n', '  Speculative decoding is a prominent technique to speed up the inference of a\nlarge target language model based on predictions of an auxiliary draft model.\nWhile effective, in application-specific settings, it often involves\nfine-tuning both draft and target models to achieve high acceptance rates. As\nthe number of downstream tasks grows, these draft models add significant\ncomplexity to inference systems. We propose Speculative Streaming, a\nsingle-model speculative decoding method that fuses drafting into the target\nmodel by changing the fine-tuning objective from next token prediction to\nfuture n-gram prediction. Speculative Streaming speeds up decoding by 1.8 -\n3.1X in a diverse set of tasks, such as Summarization, Structured Queries, and\nMeaning Representation, without sacrificing generation quality. Additionally,\nSpeculative Streaming is parameter-efficient. It achieves on-par/higher\nspeed-ups than Medusa-style architectures while using ~10000X fewer extra\nparameters, making it well-suited for resource-constrained devices.\n', ""  Speculative Decoding is a widely used technique to speed up inference for\nLarge Language Models (LLMs) without sacrificing quality. When performing\ninference, speculative decoding uses a smaller draft model to generate\nspeculative tokens and then uses the target LLM to verify those draft tokens.\nThe speedup provided by speculative decoding heavily depends on the choice of\nthe draft model. In this work, we perform a detailed study comprising over 350\nexperiments with LLaMA-65B and OPT-66B using speculative decoding and delineate\nthe factors that affect the performance gain provided by speculative decoding.\nOur experiments indicate that the performance of speculative decoding depends\nheavily on the latency of the draft model, and the draft model's capability in\nlanguage modeling does not correlate strongly with its performance in\nspeculative decoding. Based on these insights we explore a new design space for\ndraft models and design hardware-efficient draft models for speculative\ndecoding. Our newly designed draft model for LLaMA-65B can provide 111% higher\nthroughput than existing draft models and can generalize further to the LLaMA-2\nmodel family and supervised fine-tuned models.\n""] , ['  Soft prompt tuning is a widely studied parameter-efficient fine-tuning\nmethod. However, it has a clear drawback: many soft tokens must be inserted\ninto the input sequences to guarantee downstream performance. As a result, soft\nprompt tuning is less considered than Low-rank adaptation (LoRA) in the large\nlanguage modeling (LLM) era. In this work, we propose a novel prompt tuning\nmethod, Instruction-Aware Prompt Tuning (IAPT), that requires only four soft\ntokens. First, we install a parameter-efficient soft prompt generator at each\nTransformer layer to generate idiosyncratic soft prompts for each input\ninstruction. The generated soft prompts can be seen as a semantic summary of\nthe input instructions and can effectively guide the output generation. Second,\nthe soft prompt generators are modules with a bottleneck architecture\nconsisting of a self-attention pooling operation, two linear projections, and\nan activation function. Pilot experiments show that prompt generators at\ndifferent Transformer layers require different activation functions. Thus, we\npropose to learn the idiosyncratic activation functions for prompt generators\nautomatically with the help of rational functions. We have conducted\nexperiments on various tasks, and the experimental results demonstrate that (a)\nour IAPT method can outperform the recent baselines with comparable tunable\nparameters. (b) Our IAPT method is more efficient than LoRA under the\nsingle-backbone multi-tenant setting.\n', '  Recent advances in prompt optimization have notably enhanced the performance\nof pre-trained language models (PLMs) on downstream tasks. However, the\npotential of optimized prompts on domain generalization has been\nunder-explored. To explore the nature of prompt generalization on unknown\ndomains, we conduct pilot experiments and find that (i) Prompts gaining more\nattention weight from PLMs\' deep layers are more generalizable and (ii) Prompts\nwith more stable attention distributions in PLMs\' deep layers are more\ngeneralizable. Thus, we offer a fresh objective towards domain-generalizable\nprompts optimization named ""Concentration"", which represents the ""lookback""\nattention from the current decoding token to the prompt tokens, to increase the\nattention strength on prompts and reduce the fluctuation of attention\ndistribution. We adapt this new objective to popular soft prompt and hard\nprompt optimization methods, respectively. Extensive experiments demonstrate\nthat our idea improves comparison prompt optimization methods by 1.42% for soft\nprompt generalization and 2.16% for hard prompt generalization in accuracy on\nthe multi-source domain generalization setting, while maintaining satisfying\nin-domain performance. The promising results validate the effectiveness of our\nproposed prompt optimization objective and provide key insights into\ndomain-generalizable prompts.\n', '  Prompt tuning is a promising method to fine-tune a pre-trained language model\nwithout retraining its large-scale parameters. Instead, it attaches a soft\nprompt to the input text, whereby downstream tasks can be well adapted by\nmerely learning the embeddings of prompt tokens. Nevertheless, existing methods\nstill suffer from two challenges: (i) they are hard to balance accuracy and\nefficiency. A longer (shorter) soft prompt generally leads to a better(worse)\naccuracy but at the cost of more (less) training time. (ii)The performance may\nnot be consistent when adapting to different downstream tasks. We attribute it\nto the same embedding space but responsible for different requirements of\ndownstream tasks. To address these issues, we propose an Efficient Prompt\nTuning method (EPT) by multi-space projection and prompt fusion. Specifically,\nit decomposes a given soft prompt into a shorter prompt and two low-rank\nmatrices, significantly reducing the training time. Accuracy is also enhanced\nby leveraging low-rank matrices and the short prompt as additional knowledge\nsources to enrich the semantics of the original short prompt. In addition, we\nproject the soft prompt into multiple subspaces to improve the performance\nconsistency, and then adaptively learn the combination weights of different\nspaces through a gating network. Experiments on 13 natural language processing\ndownstream tasks show that our method significantly and consistently\noutperforms 11 comparison methods with the relative percentage of improvements\nup to 12.9%, and training time decreased by 14%.\n'] , [""  Large language models (LLMs) have shown exceptional abilities for multiple\ndifferent natural language processing tasks. While prompting is a crucial tool\nfor LLM inference, we observe that there is a significant cost associated with\nexceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead\nto sub-standard results in terms of readability and interpretability of the\ncompressed prompt, with a detrimental impact on prompt utility. To address\nthis, we propose PROMPT-SAW: Prompt compresSion via Relation AWare graphs, an\neffective strategy for prompt compression over task-agnostic and task-aware\nprompts. PROMPT-SAW uses the prompt's textual information to build a graph,\nlater extracts key information elements in the graph to come up with the\ncompressed prompt. We also propose GSM8K-AUG, i.e., an extended version of the\nexisting GSM8k benchmark for task-agnostic prompts in order to provide a\ncomprehensive evaluation platform. Experimental evaluation using benchmark\ndatasets shows that prompts compressed by PROMPT-SAW are not only better in\nterms of readability, but they also outperform the best-performing baseline\nmodels by up to 14.3 and 13.7 respectively for task-aware and task-agnostic\nsettings while compressing the original prompt text by 33.0 and 56.7.\n"", '  Large language models (LLMs) require lengthy prompts as the input context to\nproduce output aligned with user intentions, a process that incurs extra costs\nduring inference. In this paper, we propose the Gist COnditioned deCOding\n(Gist-COCO) model, introducing a novel method for compressing prompts which\nalso can assist the prompt interpretation and engineering. Gist-COCO employs an\nencoder-decoder based language model and then incorporates an additional\nencoder as a plugin module to compress prompts with inputs using gist tokens.\nIt finetunes the compression plugin module and uses the representations of gist\ntokens to emulate the raw prompts in the vanilla language model. By verbalizing\nthe representations of gist tokens into gist prompts, the compression ability\nof Gist-COCO can be generalized to different LLMs with high compression rates.\nOur experiments demonstrate that Gist-COCO outperforms previous prompt\ncompression models in both passage and instruction compression tasks. Further\nanalysis on gist verbalization results suggests that our gist prompts serve\ndifferent functions in aiding language models. They may directly provide\npotential answers, generate the chain-of-thought, or simply repeat the inputs.\nAll data and codes are available at https://github.com/OpenMatch/Gist-COCO .\n', '  This paper focuses on task-agnostic prompt compression for better\ngeneralizability and efficiency. Considering the redundancy in natural\nlanguage, existing approaches compress prompts by removing tokens or lexical\nunits according to their information entropy obtained from a causal language\nmodel such as LLaMa-7B. The challenge is that information entropy may be a\nsuboptimal compression metric: (i) it only leverages unidirectional context and\nmay fail to capture all essential information needed for prompt compression;\n(ii) it is not aligned with the prompt compression objective.\n  To address these issues, we propose a data distillation procedure to derive\nknowledge from an LLM to compress prompts without losing crucial information,\nand meantime, introduce an extractive text compression dataset. We formulate\nprompt compression as a token classification problem to guarantee the\nfaithfulness of the compressed prompt to the original one, and use a\nTransformer encoder as the base architecture to capture all essential\ninformation for prompt compression from the full bidirectional context. Our\napproach leads to lower latency by explicitly learning the compression\nobjective with smaller models such as XLM-RoBERTa-large and mBERT.\n  We evaluate our method on both in-domain and out-of-domain datasets,\nincluding MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its\nsmall size, our model shows significant performance gains over strong baselines\nand demonstrates robust generalization ability across different LLMs.\nAdditionally, our model is 3x-6x faster than existing prompt compression\nmethods, while accelerating the end-to-end latency by 1.6x-2.9x with\ncompression ratios of 2x-5x. Our code is available at\nhttps://aka.ms/LLMLingua-2.\n'] , ['  Large Language Models (LLMs) have achieved remarkable success, where\ninstruction tuning is the critical step in aligning LLMs with user intentions.\nIn this work, we investigate how the instruction tuning adjusts pre-trained\nmodels with a focus on intrinsic changes. Specifically, we first develop\nseveral local and global explanation methods, including a gradient-based method\nfor input-output attribution, and techniques for interpreting patterns and\nconcepts in self-attention and feed-forward layers. The impact of instruction\ntuning is then studied by comparing the explanations derived from the\npre-trained and instruction-tuned models. This approach provides an internal\nperspective of the model shifts on a human-comprehensible level. Our findings\nreveal three significant impacts of instruction tuning: 1) It empowers LLMs to\nrecognize the instruction parts of user prompts, and promotes the response\ngeneration constantly conditioned on the instructions. 2) It encourages the\nself-attention heads to capture more word-word relationships about instruction\nverbs. 3) It encourages the feed-forward networks to rotate their pre-trained\nknowledge toward user-oriented tasks. These insights contribute to a more\ncomprehensive understanding of instruction tuning and lay the groundwork for\nfuture work that aims at explaining and optimizing LLMs for various\napplications. Our code and data are publicly available at\nhttps://github.com/JacksonWuxs/Interpret_Instruction_Tuning_LLMs.\n', '  Large language models (LLMs) have shown tremendous success in following user\ninstructions and generating helpful responses. Nevertheless, their robustness\nis still far from optimal, as they may generate significantly inconsistent\nresponses due to minor changes in the verbalized instructions. Recent\nliterature has explored this inconsistency issue, highlighting the importance\nof continued improvement in the robustness of response generation. However,\nsystematic analysis and solutions are still lacking. In this paper, we\nquantitatively define the inconsistency problem and propose a two-stage\ntraining framework consisting of instruction-augmented supervised fine-tuning\nand consistency alignment training. The first stage helps a model generalize on\nfollowing instructions via similar instruction augmentations. In the second\nstage, we improve the diversity and help the model understand which responses\nare more aligned with human expectations by differentiating subtle differences\nin similar responses. The training process is accomplished by self-rewards\ninferred from the trained model at the first stage without referring to\nexternal human preference resources. We conduct extensive experiments on recent\npublicly available LLMs on instruction-following tasks and demonstrate the\neffectiveness of our training framework.\n', ""  Large Language Models (LLMs) have demonstrated remarkable versatility across\nvarious domains. To further advance LLMs, we propose 'SELF' (Self-Evolution\nwith Language Feedback), a novel approach that enables LLMs to self-improve\nthrough self-reflection, akin to human learning processes. SELF initiates with\na meta-skill learning process that equips the LLMs with capabilities for\nself-feedback and self-refinement. Subsequently, the model undergoes an\niterative process of self-evolution. In each iteration, it utilizes an\nunlabeled dataset of instructions to generate initial responses. These\nresponses are enhanced through self-feedback and self-refinement. The model is\nthen fine-tuned using this enhanced data. The model undergoes progressive\nimprovement through this iterative self-evolution process. Moreover, the SELF\nframework enables the model to apply self-refinement during inference, which\nfurther improves response quality. Our experiments in mathematics and general\ntasks demonstrate that SELF can enhance the capabilities of LLMs without human\nintervention. The SELF framework indicates a promising direction for the\nautonomous evolution of LLMs, transitioning them from passive information\nreceivers to active participants in their development.\n""] , [""  Program synthesis with language models (LMs) has unlocked a large set of\nreasoning abilities; code-tuned LMs have proven adept at generating programs\nthat solve a wide variety of algorithmic symbolic manipulation tasks (e.g. word\nconcatenation). However, not all reasoning tasks are easily expressible as\ncode, e.g. tasks involving commonsense reasoning, moral decision-making, and\nsarcasm understanding. Our goal is to extend an LM's program synthesis skills\nto such tasks and evaluate the results via pseudo-programs, namely Python\nprograms where some leaf function calls are left undefined. To that end, we\npropose, Code Generation and Emulated EXecution (CoGEX). CoGEX works by (1)\ntraining LMs to generate their own pseudo-programs, (2) teaching them to\nemulate their generated program's execution, including those leaf functions,\nallowing the LM's knowledge to fill in the execution gaps; and (3) using them\nto search over many programs to find an optimal one. To adapt the CoGEX model\nto a new task, we introduce a method for performing program search to find a\nsingle program whose pseudo-execution yields optimal performance when applied\nto all the instances of a given dataset. We show that our approach yields large\nimprovements compared to standard in-context learning approaches on a battery\nof tasks, both algorithmic and soft reasoning. This result thus demonstrates\nthat code synthesis can be applied to a much broader class of problems than\npreviously considered. Our released dataset, fine-tuned models, and\nimplementation can be found at \\url{https://github.com/nweir127/CoGEX}.\n"", '  Recently, program synthesis driven by large language models (LLMs) has become\nincreasingly popular. However, program synthesis for machine learning (ML)\ntasks still poses significant challenges. This paper explores a novel form of\nprogram synthesis, targeting ML programs, by combining LLMs and automated\nmachine learning (autoML). Specifically, our goal is to fully automate the\ngeneration and optimization of the code of the entire ML workflow, from data\npreparation to modeling and post-processing, utilizing only textual\ndescriptions of the ML tasks. To manage the length and diversity of ML\nprograms, we propose to break each ML program into smaller, manageable parts.\nEach part is generated separately by the LLM, with careful consideration of\ntheir compatibilities. To ensure compatibilities, we design a testing technique\nfor ML programs. Unlike traditional program synthesis, which typically relies\non binary evaluations (i.e., correct or incorrect), evaluating ML programs\nnecessitates more than just binary judgments. Therefore, we further assess ML\nprograms numerically and select the optimal programs from a range of candidates\nusing AutoML methods. In experiments across various ML tasks, our method\noutperforms existing methods in 10 out of 12 tasks for generating ML programs.\nIn addition, autoML significantly improves the performance of the generated ML\nprograms. In experiments, given the textual task description, our method,\nText-to-ML, generates the complete and optimized ML program in a fully\nautonomous process.\n', '  The Abstraction and Reasoning Corpus (ARC) is a general artificial\nintelligence benchmark that is currently unsolvable by any Machine Learning\nmethod, including Large Language Models (LLMs). It demands strong\ngeneralization and reasoning capabilities which are known to be weaknesses of\nNeural Network based systems. In this work, we propose a Program Synthesis\nsystem that uses Inductive Logic Programming (ILP), a branch of Symbolic AI, to\nsolve ARC. We have manually defined a simple Domain Specific Language (DSL)\nthat corresponds to a small set of object-centric abstractions relevant to ARC.\nThis is the Background Knowledge used by ILP to create Logic Programs that\nprovide reasoning capabilities to our system. The full system is capable of\ngeneralize to unseen tasks, since ILP can create Logic Program(s) from few\nexamples, in the case of ARC: pairs of Input-Output grids examples for each\ntask. These Logic Programs are able to generate Objects present in the Output\ngrid and the combination of these can form a complete program that transforms\nan Input grid into an Output grid. We randomly chose some tasks from ARC that\ndont require more than the small number of the Object primitives we implemented\nand show that given only these, our system can solve tasks that require each,\nsuch different reasoning.\n'] , [""  The rapid advancement of large language models (LLMs) has sparked interest in\ndata synthesis techniques, aiming to generate diverse and high-quality\nsynthetic datasets. However, these synthetic datasets often suffer from a lack\nof diversity and added noise. In this paper, we present TarGEN, a multi-step\nprompting strategy for generating high-quality synthetic datasets utilizing a\nLLM. An advantage of TarGEN is its seedless nature; it does not require\nspecific task instances, broadening its applicability beyond task replication.\nWe augment TarGEN with a method known as self-correction empowering LLMs to\nrectify inaccurately labeled instances during dataset creation, ensuring\nreliable labels. To assess our technique's effectiveness, we emulate 8 tasks\nfrom the SuperGLUE benchmark and finetune various language models, including\nencoder-only, encoder-decoder, and decoder-only models on both synthetic and\noriginal training sets. Evaluation on the original test set reveals that models\ntrained on datasets generated by TarGEN perform approximately 1-2% points\nbetter than those trained on original datasets (82.84% via syn. vs. 81.12% on\nog. using Flan-T5). When incorporating instruction tuning, the performance\nincreases to 84.54% on synthetic data vs. 81.49% on original data by Flan-T5. A\ncomprehensive analysis of the synthetic dataset compared to the original\ndataset reveals that the synthetic dataset demonstrates similar or higher\nlevels of dataset complexity and diversity. Furthermore, the synthetic dataset\ndisplays a bias level that aligns closely with the original dataset. Finally,\nwhen pre-finetuned on our synthetic SuperGLUE dataset, T5-3B yields impressive\nresults on the OpenLLM leaderboard, surpassing the model trained on the\nSelf-Instruct dataset by 4.14% points. We hope that TarGEN can be helpful for\nquality data generation and reducing the human efforts to create complex\nbenchmarks.\n"", '  With the expanding application of Large Language Models (LLMs) in various\ndomains, it becomes imperative to comprehensively investigate their unforeseen\nbehaviors and consequent outcomes. In this study, we introduce and\nsystematically explore the phenomenon of ""glitch tokens"", which are anomalous\ntokens produced by established tokenizers and could potentially compromise the\nmodels\' quality of response. Specifically, we experiment on seven top popular\nLLMs utilizing three distinct tokenizers and involving a totally of 182,517\ntokens. We present categorizations of the identified glitch tokens and symptoms\nexhibited by LLMs when interacting with glitch tokens. Based on our observation\nthat glitch tokens tend to cluster in the embedding space, we propose\nGlitchHunter, a novel iterative clustering-based technique, for efficient\nglitch token detection. The evaluation shows that our approach notably\noutperforms three baseline methods on eight open-source LLMs. To the best of\nour knowledge, we present the first comprehensive study on glitch tokens. Our\nnew detection further provides valuable insights into mitigating\ntokenization-related errors in LLMs.\n', '  Large language models (LLMs) have achieved unprecedented success in the field\nof natural language processing. However, the black-box nature of their internal\nmechanisms has brought many concerns about their trustworthiness and\ninterpretability. Recent research has discovered a class of abnormal tokens in\nthe model\'s vocabulary space and named them ""glitch tokens"". Those tokens, once\nincluded in the input, may induce the model to produce incorrect, irrelevant,\nor even harmful results, drastically undermining the reliability and\npracticality of LLMs.\n  In this work, we aim to enhance the understanding of glitch tokens and\npropose techniques for their detection and mitigation. We first reveal the\ncharacteristic features induced by glitch tokens on LLMs, which are evidenced\nby significant deviations in the distributions of attention patterns and\ndynamic information from intermediate model layers. Based on the insights, we\ndevelop GlitchProber, a tool for efficient glitch token detection and\nmitigation. GlitchProber utilizes small-scale sampling, principal component\nanalysis for accelerated feature extraction, and a simple classifier for\nefficient vocabulary screening. Taking one step further, GlitchProber rectifies\nabnormal model intermediate layer values to mitigate the destructive effects of\nglitch tokens. Evaluated on five mainstream open-source LLMs, GlitchProber\ndemonstrates higher efficiency, precision, and recall compared to existing\napproaches, with an average F1 score of 0.86 and an average repair rate of\n50.06%. GlitchProber unveils a novel path to address the challenges posed by\nglitch tokens and inspires future research toward more robust and interpretable\nLLMs.\n']",Advances in Large Language Models,"""Instruction Tuning for Large Language Models"""
7,"In-Context Learning with Transformers , In-Context Learning with Transformers","['contextual', 'learning', 'context', 'attention', 'icl', 'transformers', 'classes', 'icll', 'icicl', 'predictor'] , ['attention', 'rnns', 'memory', 'nlp', 'learn', 'recurrent', 'examples', 'context', 'language', 'transformers']","[""  State of the art foundation models such as GPT-4 perform surprisingly well at\nin-context learning (ICL), a variant of meta-learning concerning the learned\nability to solve tasks during a neural network forward pass, exploiting\ncontextual information provided as input to the model. This useful ability\nemerges as a side product of the foundation model's massive pretraining. While\ntransformer models are currently the state of the art in ICL, this work\nprovides empirical evidence that Mamba, a newly proposed state space model\nwhich scales better than transformers w.r.t. the input sequence length, has\nsimilar ICL capabilities. We evaluated Mamba on tasks involving simple function\napproximation as well as more complex natural language processing problems. Our\nresults demonstrate that, across both categories of tasks, Mamba closely\nmatches the performance of transformer models for ICL. Further analysis reveals\nthat, like transformers, Mamba appears to solve ICL problems by incrementally\noptimizing its internal representations. Overall, our work suggests that Mamba\ncan be an efficient alternative to transformers for ICL tasks involving long\ninput sequences. This is an exciting finding in meta-learning and may enable\ngeneralizations of in-context learned AutoML algorithms (like TabPFN or\nOptformer) to long input sequences.\n"", '  In-context learning (ICL) is one of the surprising and useful features of\nlarge language models and subject of intense research. Recently, stylized\nmeta-learning-like ICL setups have been devised that train transformers on\nsequences of input-output pairs $(x, f(x))$. The function $f$ comes from a\nfunction class and generalization is checked by evaluating on sequences\ngenerated from unseen functions from the same class. One of the main\ndiscoveries in this line of research has been that for several function\nclasses, such as linear regression, transformers successfully generalize to new\nfunctions in the class. However, the inductive biases of these models resulting\nin this behavior are not clearly understood. A model with unlimited training\ndata and compute is a Bayesian predictor: it learns the pretraining\ndistribution. In this paper we empirically examine how far this Bayesian\nperspective can help us understand ICL. To this end, we generalize the previous\nmeta-ICL setup to hierarchical meta-ICL setup which involve unions of multiple\ntask families. We instantiate this setup on a diverse range of linear and\nnonlinear function families and find that transformers can do ICL in this\nsetting as well. Where Bayesian inference is tractable, we find evidence that\nhigh-capacity transformers mimic the Bayesian predictor. The Bayesian\nperspective provides insights into the inductive bias of ICL and how\ntransformers perform a particular task when they are trained on multiple tasks.\nWe also find that transformers can learn to generalize to new function classes\nthat were not seen during pretraining. This involves deviation from the\nBayesian predictor. We examine these deviations in more depth offering new\ninsights and hypotheses.\n', '  Large-scale neural language models exhibit a remarkable capacity for\nin-context learning (ICL): they can infer novel functions from datasets\nprovided as input. Most of our current understanding of when and how ICL arises\ncomes from LMs trained on extremely simple learning problems like linear\nregression and associative recall. There remains a significant gap between\nthese model problems and the ""real"" ICL exhibited by LMs trained on large text\ncorpora, which involves not just retrieval and function approximation but\nfree-form generation of language and other structured outputs. In this paper,\nwe study ICL through the lens of a new family of model problems we term in\ncontext language learning (ICLL). In ICLL, LMs are presented with a set of\nstrings from a formal language, and must generate additional strings from the\nsame language. We focus on in-context learning of regular languages generated\nby random finite automata. We evaluate a diverse set of neural sequence models\n(including several RNNs, Transformers, and state-space model variants) on\nregular ICLL tasks, aiming to answer three questions: (1) Which model classes\nare empirically capable of ICLL? (2) What algorithmic solutions do successful\nmodels implement to perform ICLL? (3) What architectural changes can improve\nICLL in less performant models? We first show that Transformers significantly\noutperform neural sequence models with recurrent or convolutional\nrepresentations on ICLL tasks. Next, we provide evidence that their ability to\ndo so relies on specialized ""n-gram heads"" (higher-order variants of induction\nheads) that compute input-conditional next-token distributions. Finally, we\nshow that hard-wiring these heads into neural models improves performance not\njust on ICLL, but natural language modeling -- improving the perplexity of\n340M-parameter models by up to 1.14 points (6.7%) on the SlimPajama dataset.\n'] , [""  Recently, the mysterious In-Context Learning (ICL) ability exhibited by\nTransformer architectures, especially in large language models (LLMs), has\nsparked significant research interest. However, the resilience of Transformers'\nin-context learning capabilities in the presence of noisy samples, prevalent in\nboth training corpora and prompt demonstrations, remains underexplored. In this\npaper, inspired by prior research that studies ICL ability using simple\nfunction classes, we take a closer look at this problem by investigating the\nrobustness of Transformers against noisy labels. Specifically, we first conduct\na thorough evaluation and analysis of the robustness of Transformers against\nnoisy labels during in-context learning and show that they exhibit notable\nresilience against diverse types of noise in demonstration labels. Furthermore,\nwe delve deeper into this problem by exploring whether introducing noise into\nthe training set, akin to a form of data augmentation, enhances such robustness\nduring inference, and find that such noise can indeed improve the robustness of\nICL. Overall, our fruitful analysis and findings provide a comprehensive\nunderstanding of the resilience of Transformer models against label noises\nduring ICL and provide valuable insights into the research on Transformers in\nnatural language processing. Our code is available at\nhttps://github.com/InezYu0928/in-context-learning.\n"", '  Large language models (LLM) have emerged as a powerful tool for AI, with the\nkey ability of in-context learning (ICL), where they can perform well on unseen\ntasks based on a brief series of task examples without necessitating any\nadjustments to the model parameters. One recent interesting mysterious\nobservation is that models of different scales may have different ICL\nbehaviors: larger models tend to be more sensitive to noise in the test\ncontext. This work studies this observation theoretically aiming to improve the\nunderstanding of LLM and ICL. We analyze two stylized settings: (1) linear\nregression with one-layer single-head linear transformers and (2) parity\nclassification with two-layer multiple attention heads transformers (non-linear\ndata and non-linear model). In both settings, we give closed-form optimal\nsolutions and find that smaller models emphasize important hidden features\nwhile larger ones cover more hidden features; thus, smaller models are more\nrobust to noise while larger ones are more easily distracted, leading to\ndifferent ICL behaviors. This sheds light on where transformers pay attention\nto and how that affects ICL. Preliminary experimental results on large base and\nchat models provide positive support for our analysis.\n', '  The incredible success of transformers on sequence modeling tasks can be\nlargely attributed to the self-attention mechanism, which allows information to\nbe transferred between different parts of a sequence. Self-attention allows\ntransformers to encode causal structure which makes them particularly suitable\nfor sequence modeling. However, the process by which transformers learn such\ncausal structure via gradient-based training algorithms remains poorly\nunderstood. To better understand this process, we introduce an in-context\nlearning task that requires learning latent causal structure. We prove that\ngradient descent on a simplified two-layer transformer learns to solve this\ntask by encoding the latent causal graph in the first attention layer. The key\ninsight of our proof is that the gradient of the attention matrix encodes the\nmutual information between tokens. As a consequence of the data processing\ninequality, the largest entries of this gradient correspond to edges in the\nlatent causal graph. As a special case, when the sequences are generated from\nin-context Markov chains, we prove that transformers learn an induction head\n(Olsson et al., 2022). We confirm our theoretical findings by showing that\ntransformers trained on our in-context learning task are able to recover a wide\nvariety of causal structures.\n']",In-Context Learning with Transformers,In-Context Learning with Transformers
8,"In-Context Learning in Large Language Models , ""Extending Context Length in Large Language Models"" , Long Context Understanding in Language Models , In-Context Learning for Large Language Models , In-Context Learning for NLP Tasks","['context', 'learning', 'icl', 'learn', 'retrieval', 'examples', 'example', 'ica', 'nlp', 'language'] , ['contexts', 'attention', 'lengthy', 'context', 'embedding', 'memory', 'positional', 'position', 'positions', 'longer'] , ['contexts', 'longwriter', 'context', 'retrieval', 'longbench', 'language', 'longer', 'texts', 'longins', 'short'] , ['memorizing', 'memorization', 'context', 'examples', 'skills', 'exemplars', 'answering', 'knowledge', 'language', 'reasoning'] , ['corpus', 'syntactic', 'nlp', 'context', 'linguistic', 'multilingual', 'parsing', 'lingual', 'sentences', 'language']","['  It has long been assumed that the sheer number of parameters in large\nlanguage models (LLMs) drives in-context learning (ICL) capabilities, enabling\nremarkable performance improvements by leveraging task-specific demonstrations.\nChallenging this hypothesis, we introduce DEEP-ICL, a novel task Definition\nEnriched ExPert Ensembling methodology for ICL. DEEP-ICL explicitly extracts\ntask definitions from given demonstrations and generates responses through\nlearning task-specific examples. We argue that improvement from ICL does not\ndirectly rely on model size, but essentially stems from understanding task\ndefinitions and task-guided learning. Inspired by this, DEEP-ICL combines two\n3B models with distinct roles (one for concluding task definitions and the\nother for learning task demonstrations) and achieves comparable performance to\nLLaMA2-13B. Furthermore, our framework outperforms conventional ICL by\novercoming pretraining sequence length limitations, by supporting unlimited\ndemonstrations. We contend that DEEP-ICL presents a novel alternative for\nachieving efficient few-shot learning, extending beyond the conventional ICL.\n', '  With the increasing capabilities of large language models (LLMs), in-context\nlearning (ICL) has emerged as a new paradigm for natural language processing\n(NLP), where LLMs make predictions based on contexts augmented with a few\nexamples. It has been a significant trend to explore ICL to evaluate and\nextrapolate the ability of LLMs. In this paper, we aim to survey and summarize\nthe progress and challenges of ICL. We first present a formal definition of ICL\nand clarify its correlation to related studies. Then, we organize and discuss\nadvanced techniques, including training strategies, prompt designing\nstrategies, and related analysis. Additionally, we explore various ICL\napplication scenarios, such as data engineering and knowledge updating.\nFinally, we address the challenges of ICL and suggest potential directions for\nfurther research. We hope that our work can encourage more research on\nuncovering how ICL works and improving ICL.\n', '  The predictions of Large Language Models (LLMs) on downstream tasks often\nimprove significantly when including examples of the input--label relationship\nin the context. However, there is currently no consensus about how this\nin-context learning (ICL) ability of LLMs works. For example, while Xie et al.\n(2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022)\nargue ICL does not even learn label relationships from in-context examples. In\nthis paper, we provide novel insights into how ICL leverages label information,\nrevealing both capabilities and limitations. To ensure we obtain a\ncomprehensive picture of ICL behavior, we study probabilistic aspects of ICL\npredictions and thoroughly examine the dynamics of ICL as more examples are\nprovided. Our experiments show that ICL predictions almost always depend on\nin-context labels and that ICL can learn truly novel tasks in-context. However,\nwe also find that ICL struggles to fully overcome prediction preferences\nacquired from pre-training data and, further, that ICL does not consider all\nin-context information equally.\n'] , ['  Position embedding is a core component of current Large Language Models\n(LLMs). Rotary position embedding (RoPE), a technique that encodes the position\ninformation with a rotation matrix, has been the de facto choice for position\nembedding in many LLMs, such as the Llama series. RoPE has been further\nutilized to extend long context capability, which is roughly based on adjusting\nthe \\textit{base} parameter of RoPE to mitigate out-of-distribution (OOD)\nproblems in position embedding. However, in this paper, we find that LLMs may\nobtain a superficial long-context ability based on the OOD theory. We revisit\nthe role of RoPE in LLMs and propose a novel property of long-term decay, we\nderive that the \\textit{base of RoPE bounds context length}: there is an\nabsolute lower bound for the base value to obtain certain context length\ncapability. Our work reveals the relationship between context length and RoPE\nbase both theoretically and empirically, which may shed light on future long\ncontext training.\n', ""  Large Language Models (LLMs) are known to have limited extrapolation ability\nbeyond their pre-trained context window, constraining their application in\ndownstream tasks with lengthy inputs. Recent studies have sought to extend\nLLMs' context window by modifying rotary position embedding (RoPE), a popular\nposition encoding method adopted by well-known LLMs such as LLaMA, PaLM, and\nGPT-NeoX. However, prior works like Position Interpolation (PI) and YaRN are\nresource-intensive and lack comparative experiments to assess their\napplicability. In this work, we identify the inherent need for LLMs' attention\nentropy (i.e. the information entropy of attention scores) to maintain\nstability and introduce a novel extension to RoPE which combines adjusting\nRoPE's base frequency and scaling the attention logits to help LLMs efficiently\nadapt to a larger context window. We validate the superiority of our method in\nboth fine-tuning performance and robustness across different context window\nsizes on various context-demanding tasks. Notably, our method extends the\ncontext window of LLaMA-2-7B-Chat to 16,384 with only 100 samples and 6\ntraining steps, showcasing extraordinary efficiency. Finally, we also explore\nhow data compositions and training curricula affect context window extension\nfor specific downstream tasks, suggesting fine-tuning LLMs with lengthy\nconversations as a good starting point. We release our code and SFT data at\nhttps://github.com/GAIR-NLP/Entropy-ABF.\n"", '  Large Language Models (LLMs) are trained with a pre-defined context length,\nrestricting their use in scenarios requiring long inputs. Previous efforts for\nadapting LLMs to a longer length usually requires fine-tuning with this target\nlength (Full-length fine-tuning), suffering intensive training cost. To\ndecouple train length from target length for efficient context window\nextension, we propose Positional Skip-wisE (PoSE) training that smartly\nsimulates long inputs using a fixed context window. This is achieved by first\ndividing the original context window into several chunks, then designing\ndistinct skipping bias terms to manipulate the position indices of each chunk.\nThese bias terms and the lengths of each chunk are altered for every training\nexample, allowing the model to adapt to all positions within target length.\nExperimental results show that PoSE greatly reduces memory and time overhead\ncompared with Full-length fine-tuning, with minimal impact on performance.\nLeveraging this advantage, we have successfully extended the LLaMA model to\n128k tokens using a 2k training context window. Furthermore, we empirically\nconfirm that PoSE is compatible with all RoPE-based LLMs and position\ninterpolation strategies. Notably, our method can potentially support infinite\nlength, limited only by memory usage in inference. With ongoing progress for\nefficient inference, we believe PoSE can further scale the context window\nbeyond 128k.\n'] , ['  Improvements in language models\' capabilities have pushed their applications\ntowards longer contexts, making long-context evaluation and development an\nactive research area. However, many disparate use-cases are grouped together\nunder the umbrella term of ""long-context"", defined simply by the total length\nof the model\'s input, including - for example - Needle-in-a-Haystack tasks,\nbook summarization, and information aggregation. Given their varied difficulty,\nin this position paper we argue that conflating different tasks by their\ncontext length is unproductive. As a community, we require a more precise\nvocabulary to understand what makes long-context tasks similar or different. We\npropose to unpack the taxonomy of long-context based on the properties that\nmake them more difficult with longer contexts. We propose two orthogonal axes\nof difficulty: (I) Diffusion: How hard is it to find the necessary information\nin the context? (II) Scope: How much necessary information is there to find? We\nsurvey the literature on long-context, provide justification for this taxonomy\nas an informative descriptor, and situate the literature with respect to it. We\nconclude that the most difficult and interesting settings, whose necessary\ninformation is very long and highly diffused within the input, is severely\nunder-explored. By using a descriptive vocabulary and discussing the relevant\nproperties of difficulty in long-context, we can implement more informed\nresearch in this area. We call for a careful design of tasks and benchmarks\nwith distinctly long context, taking into account the characteristics that make\nit qualitatively different from shorter context.\n', ""  Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse tasks but are constrained by their small context window sizes. Various\nefforts have been proposed to expand the context window to accommodate even up\nto 200K input tokens. Meanwhile, building high-quality benchmarks with much\nlonger text lengths and more demanding tasks to provide comprehensive\nevaluations is of immense practical interest to facilitate long context\nunderstanding research of LLMs. However, prior benchmarks create datasets that\nostensibly cater to long-text comprehension by expanding the input of\ntraditional tasks, which falls short to exhibit the unique characteristics of\nlong-text understanding, including long dependency tasks and longer text length\ncompatible with modern LLMs' context window size. In this paper, we introduce a\nbenchmark for extremely long context understanding with long-range\ndependencies, XL$^2$Bench, which includes three scenarios: Fiction Reading,\nPaper Reading, and Law Reading, and four tasks of increasing complexity: Memory\nRetrieval, Detailed Understanding, Overall Understanding, and Open-ended\nGeneration, covering 27 subtasks in English and Chinese. It has an average\nlength of 100K+ words (English) and 200K+ characters (Chinese). Evaluating six\nleading LLMs on XL$^2$Bench, we find that their performance significantly lags\nbehind human levels. Moreover, the observed decline in performance across both\nthe original and enhanced datasets underscores the efficacy of our approach to\nmitigating data contamination.\n"", ""  Although large language models (LLMs) demonstrate impressive performance for\nmany language tasks, most of them can only handle texts a few thousand tokens\nlong, limiting their applications on longer sequence inputs, such as books,\nreports, and codebases. Recent works have proposed methods to improve LLMs'\nlong context capabilities by extending context windows and more sophisticated\nmemory mechanisms. However, comprehensive benchmarks tailored for evaluating\nlong context understanding are lacking. In this paper, we introduce LongBench,\nthe first bilingual, multi-task benchmark for long context understanding,\nenabling a more rigorous evaluation of long context understanding. LongBench\ncomprises 21 datasets across 6 task categories in both English and Chinese,\nwith an average length of 6,711 words (English) and 13,386 characters\n(Chinese). These tasks cover key long-text application areas including\nsingle-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks,\nand code completion. All datasets in LongBench are standardized into a unified\nformat, allowing for effortless automatic evaluation of LLMs. Upon\ncomprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial\nmodel (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still\nstruggles on longer contexts. (2) Scaled position embedding and fine-tuning on\nlonger sequences lead to substantial improvement on long context understanding.\n(3) Context compression technique such as retrieval brings improvement for\nmodel with weak ability on long contexts, but the performance still lags behind\nmodels that have strong long context understanding capability. The code and\ndatasets are available at https://github.com/THUDM/LongBench.\n""] , ['  We investigate how to elicit compositional generalization capabilities in\nlarge language models (LLMs). Compositional generalization empowers LLMs to\nsolve complex problems by combining foundational skills, a critical reasoning\nability akin to human intelligence. However, even the most advanced LLMs\ncurrently struggle with this form of reasoning. We examine this problem within\nthe framework of in-context learning and find that demonstrating both\nfoundational skills and compositional examples grounded in these skills within\nthe same prompt context is crucial. We refer to this prompt structure as\nskills-in-context (SKiC). With as few as two exemplars, this in-context\nlearning structure enables LLMs to tackle more challenging problems requiring\ninnovative skill combinations, achieving near-perfect systematic generalization\nacross a broad range of tasks. Intriguingly, SKiC also unlocks the latent\npotential of LLMs, allowing them to more actively utilize pre-existing internal\nskills acquired during earlier pretraining stages to solve complex reasoning\nproblems. The SKiC structure is robust across different skill constructions and\nexemplar choices and demonstrates strong transferability to new tasks. Finally,\ninspired by our in-context learning study, we show that fine-tuning LLMs with\nSKiC-style data can elicit zero-shot weak-to-strong generalization, enabling\nthe models to solve much harder problems directly with standard prompting.\n', ""  Chain-of-thought (CoT) prompting teaches large language models (LLMs) in\ncontext to reason over queries that require more than mere information\nretrieval. However, human experts are usually required to craft demonstrations\nfor in-context learning (ICL), which is expensive and has high variance. More\nimportantly, how to craft helpful reasoning exemplars for ICL remains unclear.\nIn this work, we investigate whether LLMs can be better in-context teachers for\nknowledge reasoning. We follow the ``encoding specificity'' hypothesis in\nhuman's memory retrieval to assume in-context exemplars at inference should\nmatch the encoding context in training data. We are thus motivated to propose\nSelf-Explain to use one LLM's self-elicited explanations as in-context\ndemonstrations for prompting it as they are generalized from the model's\ntraining examples. Self-Explain is shown to significantly outperform using\nhuman-crafted exemplars and other baselines. We further reveal that for\nin-context teaching, rationales by distinct teacher LLMs or human experts that\nmore resemble the student LLM's self-explanations are better demonstrations,\nwhich supports our encoding specificity hypothesis. We then propose Teach-Back\nthat aligns the teacher LLM with the student to enhance the in-context teaching\nperformance. For example, Teach-Back enables a 7B model to teach the much\nlarger GPT-3.5 in context, surpassing human teachers by around 5% in test\naccuracy on medical question answering.\n"", '  Large language models (LLMs) excel at few-shot in-context learning (ICL) --\nlearning from a few examples provided in context at inference, without any\nweight updates. Newly expanded context windows allow us to investigate ICL with\nhundreds or thousands of examples -- the many-shot regime. Going from few-shot\nto many-shot, we observe significant performance gains across a wide variety of\ngenerative and discriminative tasks. While promising, many-shot ICL can be\nbottlenecked by the available amount of human-generated examples. To mitigate\nthis limitation, we explore two new settings: Reinforced and Unsupervised ICL.\nReinforced ICL uses model-generated chain-of-thought rationales in place of\nhuman examples. Unsupervised ICL removes rationales from the prompt altogether,\nand prompts the model only with domain-specific questions. We find that both\nReinforced and Unsupervised ICL can be quite effective in the many-shot regime,\nparticularly on complex reasoning tasks. Finally, we demonstrate that, unlike\nfew-shot learning, many-shot learning is effective at overriding pretraining\nbiases, can learn high-dimensional functions with numerical inputs, and\nperforms comparably to fine-tuning. Our analysis also reveals the limitations\nof next-token prediction loss as an indicator of downstream ICL performance.\n'] , [""  In the era of large language models (LLMs), in-context learning (ICL) stands\nout as an effective prompting strategy that explores LLMs' potency across\nvarious tasks. However, applying LLMs to grammatical error correction (GEC) is\nstill a challenging task. In this paper, we propose a novel\nungrammatical-syntax-based in-context example selection strategy for GEC.\nSpecifically, we measure similarity of sentences based on their syntactic\nstructures with diverse algorithms, and identify optimal ICL examples sharing\nthe most similar ill-formed syntax to the test input. Additionally, we carry\nout a two-stage process to further improve the quality of selection results. On\nbenchmark English GEC datasets, empirical results show that our proposed\nungrammatical-syntax-based strategies outperform commonly-used word-matching or\nsemantics-based methods with multiple LLMs. This indicates that for a\nsyntax-oriented task like GEC, paying more attention to syntactic information\ncan effectively boost LLMs' performance. Our code will be publicly available\nafter the publication of this paper.\n"", '  Recent interest has surged in employing Large Language Models (LLMs) for\nmachine translation (MT) via in-context learning (ICL) (Vilar et al., 2023).\nMost prior studies primarily focus on optimizing translation quality, with\nlimited attention to understanding the specific aspects of ICL that influence\nthe said quality. To this end, we perform the first of its kind, an exhaustive\nstudy of in-context learning for machine translation. We first establish that\nICL is primarily example-driven and not instruction-driven. Following this, we\nconduct an extensive exploration of various aspects of the examples to\nunderstand their influence on downstream performance. Our analysis includes\nfactors such as quality and quantity of demonstrations, spatial proximity, and\nsource versus target originality. Further, we also investigate challenging\nscenarios involving indirectness and misalignment of examples to understand the\nlimits of ICL. While we establish the significance of the quality of the target\ndistribution over the source distribution of demonstrations, we further observe\nthat perturbations sometimes act as regularizers, resulting in performance\nimprovements. Surprisingly, ICL does not necessitate examples from the same\ntask, and a related task with the same target distribution proves sufficient.\nWe hope that our study acts as a guiding resource for considerations in\nutilizing ICL for MT. Our code is available on\nhttps://github.com/PranjalChitale/in-context-mt-analysis.\n', ""  In-context learning (ICL) is the trending prompting strategy in the era of\nlarge language models (LLMs), where a few examples are demonstrated to evoke\nLLMs' power for a given task. How to select informative examples remains an\nopen issue. Previous works on in-context example selection for machine\ntranslation (MT) focus on superficial word-level features while ignoring deep\nsyntax-level knowledge. In this paper, we propose a syntax-based in-context\nexample selection method for MT, by computing the syntactic similarity between\ndependency trees using Polynomial Distance. In addition, we propose an ensemble\nstrategy combining examples selected by both word-level and syntax-level\ncriteria. Experimental results between English and 6 common languages indicate\nthat syntax can effectively enhancing ICL for MT, obtaining the highest COMET\nscores on 11 out of 12 translation directions.\n""]",Advances in Large Language Models,In-Context Learning for Large Language Models
9,"Cultural Understanding in Vision-Language Models , Cultural Understanding in Multilingual Language Models","['cultural', 'cultures', 'culturally', 'culturalvqa', 'culture', 'cultureadapt', 'multicultural', 'recognizing', 'captioning', 'multilingual'] , ['cultures', 'cultural', 'culturellm', 'culturebank', 'multilingual', 'culture', 'multicultural', 'culturally', 'language', 'culturepark']","[""  To create culturally inclusive vision-language models (VLMs), the foremost\nrequirement is developing a test benchmark that can diagnose the models'\nability to respond to questions reflecting cultural elements. This paper\naddresses the necessity for such benchmarks, noting that existing research has\nrelied on human annotators' manual efforts, which impedes diversity and\nefficiency. We propose a semi-automated pipeline for constructing cultural VLM\nbenchmarks to enhance diversity and efficiency. This pipeline leverages\nhuman-VLM collaboration, where VLMs generate questions based on guidelines,\nhuman-annotated examples, and image-wise relevant knowledge, which are then\nreviewed by native speakers for quality and cultural relevance. The\neffectiveness of our adaptable pipeline is demonstrated through a specific\napplication: creating a dataset tailored to Korean culture, dubbed K-Viscuit.\nThe resulting benchmark features two types of questions: Type 1 questions\nmeasure visual recognition abilities, while Type 2 assess fine-grained visual\nreasoning skills. This ensures a thorough diagnosis of VLM models across\nvarious aspects. Our evaluation using K-Viscuit revealed that open-source\nmodels notably lag behind proprietary models in understanding Korean culture,\nhighlighting areas for improvement. We provided diverse analyses of VLM\nperformance across different cultural aspects. Besides, we explored the\npotential of incorporating external knowledge retrieval to enhance the\ngeneration process, suggesting future directions for improving cultural\ninterpretation ability of VLMs. Our dataset and code will be made publicly\navailable.\n"", ""  Foundation models and vision-language pre-training have notably advanced\nVision Language Models (VLMs), enabling multimodal processing of visual and\nlinguistic data. However, their performance has been typically assessed on\ngeneral scene understanding - recognizing objects, attributes, and actions -\nrather than cultural comprehension. This study introduces CulturalVQA, a visual\nquestion-answering benchmark aimed at assessing VLM's geo-diverse cultural\nunderstanding. We curate a collection of 2,378 image-question pairs with 1-5\nanswers per question representing cultures from 11 countries across 5\ncontinents. The questions probe understanding of various facets of culture such\nas clothing, food, drinks, rituals, and traditions. Benchmarking VLMs on\nCulturalVQA, including GPT-4V and Gemini, reveals disparity in their level of\ncultural understanding across regions, with strong cultural understanding\ncapabilities for North America while significantly lower performance for\nAfrica. We observe disparity in their performance across cultural facets too,\nwith clothing, rituals, and traditions seeing higher performances than food and\ndrink. These disparities help us identify areas where VLMs lack cultural\nunderstanding and demonstrate the potential of CulturalVQA as a comprehensive\nevaluation set for gauging VLM progress in understanding diverse cultures.\n"", '  Image Captioning generates descriptive sentences from images using\nVision-Language Pre-trained models (VLPs) such as BLIP, which has improved\ngreatly. However, current methods lack the generation of detailed descriptive\ncaptions for the cultural elements depicted in the images, such as the\ntraditional clothing worn by people from Asian cultural groups. In this paper,\nwe propose a new framework, \\textbf{Culturally-aware Image Captioning (CIC)},\nthat generates captions and describes cultural elements extracted from cultural\nvisual elements in images representing cultures. Inspired by methods combining\nvisual modality and Large Language Models (LLMs) through appropriate prompts,\nour framework (1) generates questions based on cultural categories from images,\n(2) extracts cultural visual elements from Visual Question Answering (VQA)\nusing generated questions, and (3) generates culturally-aware captions using\nLLMs with the prompts. Our human evaluation conducted on 45 participants from 4\ndifferent cultural groups with a high understanding of the corresponding\nculture shows that our proposed framework generates more culturally descriptive\ncaptions when compared to the image captioning baseline based on VLPs. Our code\nand dataset will be made publicly available upon acceptance.\n'] , [""  Recent studies have highlighted the presence of cultural biases in Large\nLanguage Models (LLMs), yet often lack a robust methodology to dissect these\nphenomena comprehensively. Our work aims to bridge this gap by delving into the\nFood domain, a universally relevant yet culturally diverse aspect of human\nlife. We introduce FmLAMA, a multilingual dataset centered on food-related\ncultural facts and variations in food practices. We analyze LLMs across various\narchitectures and configurations, evaluating their performance in both\nmonolingual and multilingual settings. By leveraging templates in six different\nlanguages, we investigate how LLMs interact with language-specific and cultural\nknowledge. Our findings reveal that (1) LLMs demonstrate a pronounced bias\ntowards food knowledge prevalent in the United States; (2) Incorporating\nrelevant cultural context significantly improves LLMs' ability to access\ncultural knowledge; (3) The efficacy of LLMs in capturing cultural nuances is\nhighly dependent on the interplay between the probing language, the specific\nmodel architecture, and the cultural context in question. This research\nunderscores the complexity of integrating cultural understanding into LLMs and\nemphasizes the importance of culturally diverse datasets to mitigate biases and\nenhance model performance across different cultural domains.\n"", '  LLMs are increasingly being deployed for multilingual applications and have\ndemonstrated impressive translation capabilities between several low and high\nresource languages. An aspect of translation that often gets overlooked is that\nof cultural adaptation, or modifying source culture references to suit the\ntarget culture. Cultural adaptation has applications across several creative\nindustries and requires intimate knowledge of source and target cultures during\ntranslation. While specialized translation models still outperform LLMs on the\nmachine translation task when viewed from the lens of correctness, they are not\nsensitive to cultural differences often requiring manual correction. LLMs on\nthe other hand have a rich reservoir of cultural knowledge embedded within its\nparameters that can be potentially exploited for such applications. In this\npaper we define the task of cultural adaptation and create an evaluation\nframework to benchmark different models for this task. We evaluate the\nperformance of modern LLMs for cultural adaptation and analyze their cross\ncultural knowledge while connecting related concepts across different cultures.\nWe also analyze possible issues with automatic adaptation including cultural\nbiases and stereotypes. We hope that this task will offer more insight into the\ncultural understanding of LLMs and their creativity in cross-cultural\nscenarios.\n', ""  The deployment of large language models (LLMs) raises concerns regarding\ntheir cultural misalignment and potential ramifications on individuals and\nsocieties with diverse cultural backgrounds. While the discourse has focused\nmainly on political and social biases, our research proposes a Cultural\nAlignment Test (Hoftede's CAT) to quantify cultural alignment using Hofstede's\ncultural dimension framework, which offers an explanatory cross-cultural\ncomparison through the latent variable analysis. We apply our approach to\nquantitatively evaluate LLMs, namely Llama 2, GPT-3.5, and GPT-4, against the\ncultural dimensions of regions like the United States, China, and Arab\ncountries, using different prompting styles and exploring the effects of\nlanguage-specific fine-tuning on the models' behavioural tendencies and\ncultural values. Our results quantify the cultural alignment of LLMs and reveal\nthe difference between LLMs in explanatory cultural dimensions. Our study\ndemonstrates that while all LLMs struggle to grasp cultural values, GPT-4 shows\na unique capability to adapt to cultural nuances, particularly in Chinese\nsettings. However, it faces challenges with American and Arab cultures. The\nresearch also highlights that fine-tuning LLama 2 models with different\nlanguages changes their responses to cultural questions, emphasizing the need\nfor culturally diverse development in AI for worldwide acceptance and ethical\nuse. For more details or to contribute to this research, visit our GitHub page\nhttps://github.com/reemim/Hofstedes_CAT/\n""]",Cultural Understanding in AI Models,Cultural Understanding in Multilingual Language Models
10,"Cross-Lingual Language Models and Translation , Multilingual Question Answering Datasets , Coreference Resolution in Multilingual Texts , Multilingual Text Retrieval and Embeddings , Multilingual Language Processing and Linguistics , Code-Switching in Multilingual Texts","['translations', 'translators', 'multilingual', 'lingual', 'translating', 'bilingual', 'translation', 'monolingual', 'translated', 'languages'] , ['multilingual', 'languages', 'answering', 'language', 'translating', 'annotation', 'translation', 'linguistic', 'annotated', 'translated'] , ['coreference', 'annotation', 'ontonotes', 'texts', 'corpora', 'translations', 'multilingual', 'nlp', 'referring', 'mentions'] , ['retrieval', 'embeddings', 'embedding', 'embed', 'textual', 'multilingual', 'search', 'encoder', 'lingual', 'relevance'] , ['lingual', 'multilingual', 'corpus', 'translations', 'linguistics', 'corpora', 'linguistic', 'languages', 'language', 'langue'] , ['multilingual', 'multilinguality', 'monolingual', 'lingual', 'languages', 'language', 'linguistic', 'corpora', 'code', 'switching']","['  The field of cross-lingual sentence embeddings has recently experienced\nsignificant advancements, but research concerning low-resource languages has\nlagged due to the scarcity of parallel corpora. This paper shows that\ncross-lingual word representation in low-resource languages is notably\nunder-aligned with that in high-resource languages in current models. To\naddress this, we introduce a novel framework that explicitly aligns words\nbetween English and eight low-resource languages, utilizing off-the-shelf word\nalignment models. This framework incorporates three primary training\nobjectives: aligned word prediction and word translation ranking, along with\nthe widely used translation ranking. We evaluate our approach through\nexperiments on the bitext retrieval task, which demonstrate substantial\nimprovements on sentence embeddings in low-resource languages. In addition, the\ncompetitive performance of the proposed model across a broader range of tasks\nin high-resource languages underscores its practicality.\n', '  Multilingual generative models obtain remarkable cross-lingual in-context\nlearning capabilities through pre-training on large-scale corpora. However,\nthey still exhibit a performance bias toward high-resource languages and learn\nisolated distributions of multilingual sentence representations, which may\nhinder knowledge transfer across languages. To bridge this gap, we propose a\nsimple yet effective cross-lingual alignment framework exploiting pairs of\ntranslation sentences. It aligns the internal sentence representations across\ndifferent languages via multilingual contrastive learning and aligns outputs by\nfollowing cross-lingual instructions in the target language. Experimental\nresults show that even with less than 0.1 {\\textperthousand} of pre-training\ntokens, our alignment framework significantly boosts the cross-lingual\nabilities of generative language models and mitigates the performance gap.\nFurther analyses reveal that it results in a better internal multilingual\nrepresentation distribution of multilingual models.\n', ""  Large Language Models (LLMs) demonstrate strong machine translation\ncapabilities on languages they are trained on. However, the impact of factors\nbeyond training data size on translation performance remains a topic of debate,\nespecially concerning languages not directly encountered during training. Our\nstudy delves into Llama2's translation capabilities. By modeling a linear\nrelationship between linguistic feature distances and machine translation\nscores, we ask ourselves if there are potentially better central languages for\nLLMs other than English. Our experiments show that the 7B Llama2 model yields\nabove 10 BLEU when translating into all languages it has seen, which rarely\nhappens for languages it has not seen. Most translation improvements into\nunseen languages come from scaling up the model size rather than instruction\ntuning or increasing shot count. Furthermore, our correlation analysis reveals\nthat syntactic similarity is not the only linguistic factor that strongly\ncorrelates with machine translation scores. Interestingly, we discovered that\nunder specific circumstances, some languages (e.g. Swedish, Catalan), despite\nhaving significantly less training data, exhibit comparable correlation levels\nto English. These insights challenge the prevailing landscape of LLMs,\nsuggesting that models centered around languages other than English could\nprovide a more efficient foundation for multilingual applications.\n""] , ['  Question Answering (QA) datasets have been instrumental in developing and\nevaluating Large Language Model (LLM) capabilities. However, such datasets are\nscarce for languages other than English due to the cost and difficulties of\ncollection and manual annotation. This means that producing novel models and\nmeasuring the performance of multilingual LLMs in low-resource languages is\nchallenging. To mitigate this, we propose $\\textbf{S}$yn$\\textbf{DAR}$in, a\nmethod for generating and validating QA datasets for low-resource languages. We\nutilize parallel content mining to obtain $\\textit{human-curated}$ paragraphs\nbetween English and the target language. We use the English data as context to\n$\\textit{generate}$ synthetic multiple-choice (MC) question-answer pairs, which\nare automatically translated and further validated for quality. Combining these\nwith their designated non-English $\\textit{human-curated}$ paragraphs form the\nfinal QA dataset. The method allows to maintain the content quality, reduces\nthe likelihood of factual errors, and circumvents the need for costly\nannotation. To test the method, we created a QA dataset with $1.2$K samples for\nthe Armenian language. The human evaluation shows that $98\\%$ of the generated\nEnglish data maintains quality and diversity in the question types and topics,\nwhile the translation validation pipeline can filter out $\\sim70\\%$ of data\nwith poor quality. We use the dataset to benchmark state-of-the-art LLMs,\nshowing their inability to achieve human accuracy with some model performances\ncloser to random chance. This shows that the generated dataset is non-trivial\nand can be used to evaluate reasoning capabilities in low-resource language.\n', ""  Large Language Models (LLMs) have demonstrated remarkable zero-shot and\nfew-shot capabilities in unseen tasks, including context-grounded question\nanswering (QA) in English. However, the evaluation of LLMs' capabilities in\nnon-English languages for context-based QA is limited by the scarcity of\nbenchmarks in non-English languages. To address this gap, we introduce\nIndic-QA, the largest publicly available context-grounded question-answering\ndataset for 11 major Indian languages from two language families. The dataset\ncomprises both extractive and abstractive question-answering tasks and includes\nexisting datasets as well as English QA datasets translated into Indian\nlanguages. Additionally, we generate a synthetic dataset using the Gemini model\nto create question-answer pairs given a passage, which is then manually\nverified for quality assurance. We evaluate various multilingual Large Language\nModels and their instruction-fine-tuned variants on the benchmark and observe\nthat their performance is subpar, particularly for low-resource languages. We\nhope that the release of this dataset will stimulate further research on the\nquestion-answering abilities of LLMs for low-resource languages.\n"", '  Question answering (QA) is the task of answering questions posed in natural\nlanguage with free-form natural language answers extracted from a given\npassage. In the OpenQA variant, only a question text is given, and the system\nmust retrieve relevant passages from an unstructured knowledge source and use\nthem to provide answers, which is the case in the mainstream QA systems on the\nWeb. QA systems currently are mostly limited to the English language due to the\nlack of large-scale labeled QA datasets in non-English languages. In this\npaper, we show that effective, low-cost OpenQA systems can be developed for\nlow-resource contexts. The key ingredients are (1) weak supervision using\nmachine-translated labeled datasets and (2) a relevant unstructured knowledge\nsource in the target language context. Furthermore, we show that only a few\nhundred gold assessment examples are needed to reliably evaluate these systems.\nWe apply our method to Turkish as a challenging case study, since English and\nTurkish are typologically very distinct and Turkish has limited resources for\nQA. We present SQuAD-TR, a machine translation of SQuAD2.0, and we build our\nOpenQA system by adapting ColBERT-QA and retraining it over Turkish resources\nand SQuAD-TR using two versions of Wikipedia dumps spanning two years. We\nobtain a performance improvement of 24-32% in the Exact Match (EM) score and\n22-29% in the F1 score compared to the BM25-based and DPR-based baseline QA\nreader models. Our results show that SQuAD-TR makes OpenQA feasible for\nTurkish, which we hope encourages researchers to build OpenQA systems in other\nlow-resource languages. We make all the code, models, and the dataset publicly\navailable at https://github.com/boun-tabi/SQuAD-TR.\n'] , [""  In this paper, we present KoCoNovel, a novel character coreference dataset\nderived from Korean literary texts, complete with detailed annotation\nguidelines. Comprising 178K tokens from 50 modern and contemporary novels,\nKoCoNovel stands as one of the largest public coreference resolution corpora in\nKorean, and the first to be based on literary texts. KoCoNovel offers four\ndistinct versions to accommodate a wide range of literary coreference analysis\nneeds. These versions are designed to support perspectives of the omniscient\nauthor or readers, and to manage multiple entities as either separate or\noverlapping, thereby broadening its applicability. One of KoCoNovel's\ndistinctive features is that 24% of all character mentions are single common\nnouns, lacking possessive markers or articles. This feature is particularly\ninfluenced by the nuances of Korean address term culture, which favors the use\nof terms denoting social relationships and kinship over personal names. In\nexperiments with a BERT-based coreference model, we observe notable performance\nenhancements with KoCoNovel in character coreference tasks within literary\ntexts, compared to a larger non-literary coreference dataset. Such findings\nunderscore KoCoNovel's potential to significantly enhance coreference\nresolution models through the integration of Korean cultural and linguistic\ndynamics.\n"", '  Coreference resolution involves the task of identifying text spans within a\ndiscourse that pertain to the same real-world entity. While this task has been\nextensively explored in the English language, there has been a notable scarcity\nof publicly accessible resources and models for coreference resolution in South\nAsian languages. We introduce a Translated dataset for Multilingual Coreference\nResolution (TransMuCoRes) in 31 South Asian languages using off-the-shelf tools\nfor translation and word-alignment. Nearly all of the predicted translations\nsuccessfully pass a sanity check, and 75% of English references align with\ntheir predicted translations. Using multilingual encoders, two off-the-shelf\ncoreference resolution models were trained on a concatenation of TransMuCoRes\nand a Hindi coreference resolution dataset with manual annotations. The best\nperforming model achieved a score of 64 and 68 for LEA F1 and CoNLL F1,\nrespectively, on our test-split of Hindi golden set. This study is the first to\nevaluate an end-to-end coreference resolution model on a Hindi golden set.\nFurthermore, this work underscores the limitations of current coreference\nevaluation metrics when applied to datasets with split antecedents, advocating\nfor the development of more suitable evaluation metrics.\n', '  Coreference resolution is the task of identifying and grouping mentions\nreferring to the same real-world entity. Previous neural models have mainly\nfocused on learning span representations and pairwise scores for coreference\ndecisions. However, current methods do not explicitly capture the referential\nchoice in the hierarchical discourse, an important factor in coreference\nresolution. In this study, we propose a new approach that incorporates\nrhetorical information into neural coreference resolution models. We collect\nrhetorical features from automated discourse parses and examine their impact.\nAs a base model, we implement an end-to-end span-based coreference resolver\nusing a partially fine-tuned multilingual entity-aware language model LUKE. We\nevaluate our method on the RuCoCo-23 Shared Task for coreference resolution in\nRussian. Our best model employing rhetorical distance between mentions has\nranked 1st on the development set (74.6% F1) and 2nd on the test set (73.3% F1)\nof the Shared Task. We hope that our work will inspire further research on\nincorporating discourse information in neural coreference resolution models.\n'] , ['  A dense passage retrieval system can serve as the initial stages of\ninformation retrieval, selecting the most relevant text passages for downstream\ntasks. In this work we conducted experiments with the goal of finding how much\nthe quality of a multilingual retrieval could be degraded if the query part of\na dual encoder is tuned on an English-only dataset (assuming scarcity of\ncross-lingual samples for the targeted domain or task). Specifically, starting\nwith a high quality multilingual embedding model, we observe that an\nEnglish-only tuning may not only preserve the original quality of the\nmultilingual retrieval, but even improve it.\n', '  Decoder-only large language model (LLM)-based embedding models are beginning\nto outperform BERT or T5-based embedding models in general-purpose text\nembedding tasks, including dense vector-based retrieval. In this work, we\nintroduce the NV-Embed model with a variety of architectural designs and\ntraining procedures to significantly enhance the performance of LLM as a\nversatile embedding model, while maintaining its simplicity and\nreproducibility. For model architecture, we propose a latent attention layer to\nobtain pooled embeddings, which consistently improves retrieval and downstream\ntask accuracy compared to mean pooling or using the last <EOS> token embedding\nfrom LLMs. To enhance representation learning, we remove the causal attention\nmask of LLMs during contrastive training. For model training, we introduce a\ntwo-stage contrastive instruction-tuning method. It first applies contrastive\ntraining with instructions on retrieval datasets, utilizing in-batch negatives\nand curated hard negative examples. At stage-2, it blends various non-retrieval\ndatasets into instruction tuning, which not only enhances non-retrieval task\naccuracy but also improves retrieval performance. Combining these techniques,\nour NV-Embed model, using only publicly available data, has achieved a\nrecord-high score of 69.32, ranking No. 1 on the Massive Text Embedding\nBenchmark (MTEB) (as of May 24, 2024), with 56 tasks, encompassing retrieval,\nreranking, classification, clustering, and semantic textual similarity tasks.\nNotably, our model also attains the highest score of 59.36 on 15 retrieval\ntasks in the MTEB benchmark (also known as BEIR). We will open-source the model\nat: https://huggingface.co/nvidia/NV-Embed-v1.\n', ""  Utilizing large language models (LLMs) for zero-shot document ranking is done\nin one of two ways: 1) prompt-based re-ranking methods, which require no\nfurther training but are only feasible for re-ranking a handful of candidate\ndocuments due to computational costs; and 2) unsupervised contrastive trained\ndense retrieval methods, which can retrieve relevant documents from the entire\ncorpus but require a large amount of paired text data for contrastive training.\nIn this paper, we propose PromptReps, which combines the advantages of both\ncategories: no need for training and the ability to retrieve from the whole\ncorpus. Our method only requires prompts to guide an LLM to generate query and\ndocument representations for effective document retrieval. Specifically, we\nprompt the LLMs to represent a given text using a single word, and then use the\nlast token's hidden states and the corresponding logits associated with the\nprediction of the next token to construct a hybrid document retrieval system.\nThe retrieval system harnesses both dense text embedding and sparse\nbag-of-words representations given by the LLM. We further explore variations of\nthis core idea that consider the generation of multiple words, and\nrepresentations that rely on multiple embeddings and sparse distributions. Our\nexperimental evaluation on the MSMARCO, TREC deep learning and BEIR zero-shot\ndocument retrieval datasets illustrates that this simple prompt-based LLM\nretrieval method can achieve a similar or higher retrieval effectiveness than\nstate-of-the-art LLM embedding methods that are trained with large amounts of\nunsupervised data, especially when using a larger LLM.\n""] , ['  As Uzbek language is agglutinative, has many morphological features which\nwords formed by combining root and affixes. Affixes play an important role in\nthe morphological analysis of words, by adding additional meanings and\ngrammatical functions to words. Inflectional endings are utilized to express\nvarious morphological features within the language. This feature introduces\nnumerous possibilities for word endings, thereby significantly expanding the\nword vocabulary and exacerbating issues related to data sparsity in statistical\nmodels. This paper present modeling of the morphological analysis of Uzbek\nwords, including stemming, lemmatizing, and the extraction of morphological\ninformation while considering morpho-phonetic exceptions. Main steps of the\nmodel involve developing a complete set of word-ending with assigned\nmorphological information, and additional datasets for morphological analysis.\nThe proposed model was evaluated using a curated test set comprising 5.3K\nwords. Through manual verification of stemming, lemmatizing, and morphological\nfeature corrections carried out by linguistic specialists, it obtained a\nword-level accuracy of over 91%. The developed tool based on the proposed model\nis available as a web-based application and an open-source Python library.\n', '  Massively multilingual machine translation models allow for the translation\nof a large number of languages with a single model, but have limited\nperformance on low- and very-low-resource translation directions. Pivoting via\nhigh-resource languages remains a strong strategy for low-resource directions,\nand in this paper we revisit ways of pivoting through multiple languages.\nPrevious work has used a simple averaging of probability distributions from\nmultiple paths, but we find that this performs worse than using a single pivot,\nand exacerbates the hallucination problem because the same hallucinations can\nbe probable across different paths. We also propose MaxEns, a novel combination\nstrategy that makes the output biased towards the most confident predictions,\nhypothesising that confident predictions are less prone to be hallucinations.\nWe evaluate different strategies on the FLORES benchmark for 20 low-resource\nlanguage directions, demonstrating that MaxEns improves translation quality for\nlow-resource languages while reducing hallucination in translations, compared\nto both direct translation and an averaging approach. On average, multi-pivot\nstrategies still lag behind using English as a single pivot language, raising\nthe question of how to identify the best pivoting strategy for a given\ntranslation direction.\n', '  Semitic morphologically-rich languages (MRLs) are characterized by extreme\nword ambiguity. Because most vowels are omitted in standard texts, many of the\nwords are homographs with multiple possible analyses, each with a different\npronunciation and different morphosyntactic properties. This ambiguity goes\nbeyond word-sense disambiguation (WSD), and may include token segmentation into\nmultiple word units. Previous research on MRLs claimed that standardly trained\npre-trained language models (PLMs) based on word-pieces may not sufficiently\ncapture the internal structure of such tokens in order to distinguish between\nthese analyses. Taking Hebrew as a case study, we investigate the extent to\nwhich Hebrew homographs can be disambiguated and analyzed using PLMs. We\nevaluate all existing models for contextualized Hebrew embeddings on a novel\nHebrew homograph challenge sets that we deliver. Our empirical results\ndemonstrate that contemporary Hebrew contextualized embeddings outperform\nnon-contextualized embeddings; and that they are most effective for\ndisambiguating segmentation and morphosyntactic features, less so regarding\npure word-sense disambiguation. We show that these embeddings are more\neffective when the number of word-piece splits is limited, and they are more\neffective for 2-way and 3-way ambiguities than for 4-way ambiguity. We show\nthat the embeddings are equally effective for homographs of both balanced and\nskewed distributions, whether calculated as masked or unmasked tokens. Finally,\nwe show that these embeddings are as effective for homograph disambiguation\nwith extensive supervised training as with a few-shot setup.\n'] , ['  Code-switching is a prevalent linguistic phenomenon in which multilingual\nindividuals seamlessly alternate between languages. Despite its widespread use\nonline and recent research trends in this area, research in code-switching\npresents unique challenges, primarily stemming from the scarcity of labelled\ndata and available resources. In this study we investigate how pre-trained\nLanguage Models handle code-switched text in three dimensions: a) the ability\nof PLMs to detect code-switched text, b) variations in the structural\ninformation that PLMs utilise to capture code-switched text, and c) the\nconsistency of semantic information representation in code-switched text. To\nconduct a systematic and controlled evaluation of the language models in\nquestion, we create a novel dataset of well-formed naturalistic code-switched\ntext along with parallel translations into the source languages. Our findings\nreveal that pre-trained language models are effective in generalising to\ncode-switched text, shedding light on the abilities of these models to\ngeneralise representations to CS corpora. We release all our code and data\nincluding the novel corpus at https://github.com/francesita/code-mixed-probes.\n', '  Code-switching entails mixing multiple languages. It is an increasingly\noccurring phenomenon in social media texts. Usually, code-mixed texts are\nwritten in a single script, even though the languages involved have different\nscripts. Pre-trained multilingual models primarily utilize the data in the\nnative script of the language. In existing studies, the code-switched texts are\nutilized as they are. However, using the native script for each language can\ngenerate better representations of the text owing to the pre-trained knowledge.\nTherefore, a cross-language-script knowledge sharing architecture utilizing the\ncross attention and alignment of the representations of text in individual\nlanguage scripts was proposed in this study. Experimental results on two\ndifferent datasets containing Nepali-English and Hindi-English code-switched\ntexts, demonstrate the effectiveness of the proposed method. The interpretation\nof the model using model explainability technique illustrates the sharing of\nlanguage-specific knowledge between language-specific representations.\n', ""  Multilingual code-switching research is often hindered by the lack and\nlinguistically biased status of available datasets. To expand language\nrepresentation, we synthesize code-switching data by replacing intonation units\ndetected through PSST, a speech segmentation model fine-tuned from OpenAI's\nWhisper, using a speech-to-text translation dataset, CoVoST 2. With our\ndataset, CoVoSwitch, spanning 13 languages, we evaluate the code-switching\ntranslation performance of two multilingual translation models, M2M-100 418M\nand NLLB-200 600M. We reveal that the inclusion of code-switching units results\nin higher translation performance than monolingual settings and that models are\nbetter at code-switching translation into English than non-English. Further,\nlow-resource languages gain most from integration of code-switched units when\ntranslating into English but much less when translating into non-English.\nTranslations into low-resource languages also perform worse than even raw\ncode-switched inputs. We find that systems excel at copying English tokens but\nstruggle with non-English tokens, that the off-target problem in monolingual\nsettings is also relevant in code-switching settings, and that models\nhallucinate in code-switching translation by introducing words absent in both\nof the original source sentences. CoVoSwitch and code are available at\nhttps://github.com/sophiayk20/covoswitch.\n""]",Multilingual Natural Language Processing,Multilingual Language Processing and Linguistics
11,"""Gender Bias in Large Language Models"" , ""Political Biases in Large Language Models"" , Cultural Bias in Large Language Models , ""Language Models and Social Biases"" , Debiasing Language Models , Evaluating Large Language Models for Fairness and Bias , ""Cognitive Biases in Large Language Models""","['bias', 'biases', 'genders', 'gender', 'gendered', 'biased', 'stereotypes', 'languages', 'language', 'demographics'] , ['partisan', 'political', 'biases', 'politically', 'politicians', 'elections', 'ideological', 'constituency', 'electoral', 'politician'] , ['cultures', 'cultural', 'culture', 'nationalities', 'culturally', 'multicultural', 'nationality', 'diverse', 'attitudes', 'anthropology'] , ['bias', 'biases', 'stereotypes', 'attitudes', 'stereotyping', 'perceptions', 'stereotypical', 'language', 'demographics', 'culturally'] , ['bias', 'biases', 'debiasing', 'biased', 'unbiased', 'debiased', 'debias', 'annotators', 'biasalert', 'nlp'] , ['judgments', 'evaluations', 'bias', 'evaluation', 'annotators', 'biases', 'evaluating', 'judge', 'annotations', 'assessing'] , ['cognition', 'biases', 'bias', 'judgments', 'cognitive', 'behavioral', 'behavior', 'intelligence', 'decisions', 'language']","['  Large Language Models (LLMs) can generate biased responses. Yet previous\ndirect probing techniques contain either gender mentions or predefined gender\nstereotypes, which are challenging to comprehensively collect. Hence, we\npropose an indirect probing framework based on conditional generation. This\napproach aims to induce LLMs to disclose their gender bias even without\nexplicit gender or stereotype mentions. We explore three distinct strategies to\ndisclose explicit and implicit gender bias in LLMs. Our experiments demonstrate\nthat all tested LLMs exhibit explicit and/or implicit gender bias, even when\ngender stereotypes are not present in the inputs. In addition, an increased\nmodel size or model alignment amplifies bias in most cases. Furthermore, we\ninvestigate three methods to mitigate bias in LLMs via Hyperparameter Tuning,\nInstruction Guiding, and Debias Tuning. Remarkably, these methods prove\neffective even in the absence of explicit genders or stereotypes.\n', ""  Gender bias research has been pivotal in revealing undesirable behaviors in\nlarge language models, exposing serious gender stereotypes associated with\noccupations, and emotions. A key observation in prior work is that models\nreinforce stereotypes as a consequence of the gendered correlations that are\npresent in the training data. In this paper, we focus on bias where the effect\nfrom training data is unclear, and instead address the question: Do language\nmodels still exhibit gender bias in non-stereotypical settings? To do so, we\nintroduce UnStereoEval (USE), a novel framework tailored for investigating\ngender bias in stereotype-free scenarios. USE defines a sentence-level score\nbased on pretraining data statistics to determine if the sentence contain\nminimal word-gender associations. To systematically benchmark the fairness of\npopular language models in stereotype-free scenarios, we utilize USE to\nautomatically generate benchmarks without any gender-related language. By\nleveraging USE's sentence-level score, we also repurpose prior gender bias\nbenchmarks (Winobias and Winogender) for non-stereotypical evaluation.\nSurprisingly, we find low fairness across all 28 tested models. Concretely,\nmodels demonstrate fair behavior in only 9%-41% of stereotype-free sentences,\nsuggesting that bias does not solely stem from the presence of gender-related\nwords. These results raise important questions about where underlying model\nbiases come from and highlight the need for more systematic and comprehensive\nbias evaluation. We release the full dataset and code at\nhttps://ucinlp.github.io/unstereo-eval.\n"", '  With the growing deployment of large language models (LLMs) across various\napplications, assessing the influence of gender biases embedded in LLMs becomes\ncrucial. The topic of gender bias within the realm of natural language\nprocessing (NLP) has gained considerable focus, particularly in the context of\nEnglish. Nonetheless, the investigation of gender bias in languages other than\nEnglish is still relatively under-explored and insufficiently analyzed. In this\nwork, We examine gender bias in LLMs-generated outputs for different languages.\nWe use three measurements: 1) gender bias in selecting descriptive words given\nthe gender-related context. 2) gender bias in selecting gender-related pronouns\n(she/he) given the descriptive words. 3) gender bias in the topics of\nLLM-generated dialogues. We investigate the outputs of the GPT series of LLMs\nin various languages using our three measurement methods. Our findings revealed\nsignificant gender biases across all the languages we examined.\n'] , [""  Instruction-finetuned Large Language Models inherit clear political leanings\nthat have been shown to influence downstream task performance. We expand this\nline of research beyond the two-party system in the US and audit Llama Chat in\nthe context of EU politics in various settings to analyze the model's political\nknowledge and its ability to reason in context. We adapt, i.e., further\nfine-tune, Llama Chat on speeches of individual euro-parties from debates in\nthe European Parliament to reevaluate its political leaning based on the EUandI\nquestionnaire. Llama Chat shows considerable knowledge of national parties'\npositions and is capable of reasoning in context. The adapted, party-specific,\nmodels are substantially re-aligned towards respective positions which we see\nas a starting point for using chat-based LLMs as data-driven conversational\nengines to assist research in political science.\n"", '  The assessment of bias within Large Language Models (LLMs) has emerged as a\ncritical concern in the contemporary discourse surrounding Artificial\nIntelligence (AI) in the context of their potential impact on societal\ndynamics. Recognizing and considering political bias within LLM applications is\nespecially important when closing in on the tipping point toward performative\nprediction. Then, being educated about potential effects and the societal\nbehavior LLMs can drive at scale due to their interplay with human operators.\nIn this way, the upcoming elections of the European Parliament will not remain\nunaffected by LLMs. We evaluate the political bias of the currently most\npopular open-source LLMs (instruct or assistant models) concerning political\nissues within the European Union (EU) from a German voter\'s perspective. To do\nso, we use the ""Wahl-O-Mat,"" a voting advice application used in Germany. From\nthe voting advice of the ""Wahl-O-Mat"" we quantize the degree of alignment of\nLLMs with German political parties. We show that larger models, such as\nLlama3-70B, tend to align more closely with left-leaning political parties,\nwhile smaller models often remain neutral, particularly when prompted in\nEnglish. The central finding is that LLMs are similarly biased, with low\nvariances in the alignment concerning a specific party. Our findings underline\nthe importance of rigorously assessing and making bias transparent in LLMs to\nsafeguard the integrity and trustworthiness of applications that employ the\ncapabilities of performative prediction and the invisible hand of machine\nlearning prediction and language generation.\n', ""  I report here a comprehensive analysis about the political preferences\nembedded in Large Language Models (LLMs). Namely, I administer 11 political\norientation tests, designed to identify the political preferences of the test\ntaker, to 24 state-of-the-art conversational LLMs, both closed and open source.\nWhen probed with questions/statements with political connotations, most\nconversational LLMs tend to generate responses that are diagnosed by most\npolitical test instruments as manifesting preferences for left-of-center\nviewpoints. This does not appear to be the case for five additional base (i.e.\nfoundation) models upon which LLMs optimized for conversation with humans are\nbuilt. However, the weak performance of the base models at coherently answering\nthe tests' questions makes this subset of results inconclusive. Finally, I\ndemonstrate that LLMs can be steered towards specific locations in the\npolitical spectrum through Supervised Fine-Tuning (SFT) with only modest\namounts of politically aligned data, suggesting SFT's potential to embed\npolitical orientation in LLMs. With LLMs beginning to partially displace\ntraditional information sources like search engines and Wikipedia, the societal\nimplications of political biases embedded in LLMs are substantial.\n""] , ['  This paper identifies a cultural dominance issue within large language models\n(LLMs) due to the predominant use of English data in model training (e.g.,\nChatGPT). LLMs often provide inappropriate English-culture-related answers that\nare not relevant to the expected culture when users ask in non-English\nlanguages. To systematically evaluate the cultural dominance issue, we build a\nbenchmark of concrete (e.g., holidays and songs) and abstract (e.g., values and\nopinions) cultural objects. Empirical results show that the representative GPT\nmodels suffer from the culture dominance problem, where GPT-4 is the most\naffected while text-davinci-003 suffers the least from this problem. Our study\nemphasizes the need to critically examine cultural dominance and ethical\nconsideration in their development and deployment. We show that two\nstraightforward methods in model development (i.e., pretraining on more diverse\ndata) and deployment (e.g., culture-aware prompting) can significantly mitigate\nthe cultural dominance issue in LLMs.\n', '  The intricate relationship between language and culture has long been a\nsubject of exploration within the realm of linguistic anthropology. Large\nLanguage Models (LLMs), promoted as repositories of collective human knowledge,\nraise a pivotal question: do these models genuinely encapsulate the diverse\nknowledge adopted by different cultures? Our study reveals that these models\ndemonstrate greater cultural alignment along two dimensions -- firstly, when\nprompted with the dominant language of a specific culture, and secondly, when\npretrained with a refined mixture of languages employed by that culture. We\nquantify cultural alignment by simulating sociological surveys, comparing model\nresponses to those of actual survey participants as references. Specifically,\nwe replicate a survey conducted in various regions of Egypt and the United\nStates through prompting LLMs with different pretraining data mixtures in both\nArabic and English with the personas of the real respondents and the survey\nquestions. Further analysis reveals that misalignment becomes more pronounced\nfor underrepresented personas and for culturally sensitive topics, such as\nthose probing social values. Finally, we introduce Anthropological Prompting, a\nnovel method leveraging anthropological reasoning to enhance cultural\nalignment. Our study emphasizes the necessity for a more balanced multilingual\npretraining dataset to better represent the diversity of human experience and\nthe plurality of different cultures with many implications on the topic of\ncross-lingual transfer.\n', '  As the utilization of large language models (LLMs) has proliferated\nworld-wide, it is crucial for them to have adequate knowledge and fair\nrepresentation for diverse global cultures. In this work, we uncover culture\nperceptions of three SOTA models on 110 countries and regions on 8\nculture-related topics through culture-conditioned generations, and extract\nsymbols from these generations that are associated to each culture by the LLM.\nWe discover that culture-conditioned generation consist of linguistic ""markers""\nthat distinguish marginalized cultures apart from default cultures. We also\ndiscover that LLMs have an uneven degree of diversity in the culture symbols,\nand that cultures from different geographic regions have different presence in\nLLMs\' culture-agnostic generation. Our findings promote further research in\nstudying the knowledge and fairness of global culture perception in LLMs. Code\nand Data can be found here: https://github.com/huihanlhh/Culture-Gen/\n'] , [""  Given the rapid advancement of large-scale language models, artificial\nintelligence (AI) models, like ChatGPT, are playing an increasingly prominent\nrole in human society. However, to ensure that artificial intelligence models\nbenefit human society, we must first fully understand the similarities and\ndifferences between the human-like characteristics exhibited by artificial\nintelligence models and real humans, as well as the cultural stereotypes and\nbiases that artificial intelligence models may exhibit in the process of\ninteracting with humans. This study first measured ChatGPT in 84 dimensions of\npsychological characteristics, revealing differences between ChatGPT and human\nnorms in most dimensions as well as in high-dimensional psychological\nrepresentations. Additionally, through the measurement of ChatGPT in 13\ndimensions of cultural values, it was revealed that ChatGPT's cultural value\npatterns are dissimilar to those of various countries/regions worldwide.\nFinally, an analysis of ChatGPT's performance in eight decision-making tasks\ninvolving interactions with humans from different countries/regions revealed\nthat ChatGPT exhibits clear cultural stereotypes in most decision-making tasks\nand shows significant cultural bias in third-party punishment and ultimatum\ngames. The findings indicate that, compared to humans, ChatGPT exhibits a\ndistinct psychological profile and cultural value orientation, and it also\nshows cultural biases and stereotypes in interpersonal decision-making. Future\nresearch endeavors should emphasize enhanced technical oversight and augmented\ntransparency in the database and algorithmic training procedures to foster more\nefficient cross-cultural communication and mitigate social disparities.\n"", ""  Warning: This paper contains examples of stereotypes and biases. Large\nLanguage Models (LLMs) exhibit considerable social biases, and various studies\nhave tried to evaluate and mitigate these biases accurately. Previous studies\nuse downstream tasks as prompts to examine the degree of social biases for\nevaluation and mitigation. While LLMs' output highly depends on prompts,\nprevious studies evaluating and mitigating bias have often relied on a limited\nvariety of prompts. In this paper, we investigate the sensitivity of LLMs when\nchanging prompt variations (task instruction and prompt, few-shot examples,\ndebias-prompt) by analyzing task performance and social bias of LLMs. Our\nexperimental results reveal that LLMs are highly sensitive to prompts to the\nextent that the ranking of LLMs fluctuates when comparing models for task\nperformance and social bias. Additionally, we show that LLMs have tradeoffs\nbetween performance and social bias caused by the prompts. Less bias from\nprompt setting may result in reduced performance. Moreover, the ambiguity of\ninstances is one of the reasons for this sensitivity to prompts in advanced\nLLMs, leading to various outputs. We recommend using diverse prompts, as in\nthis study, to compare the effects of prompts on social bias in LLMs.\n"", ""  Large language models (LLMs) can pass explicit social bias tests but still\nharbor implicit biases, similar to humans who endorse egalitarian beliefs yet\nexhibit subtle biases. Measuring such implicit biases can be a challenge: as\nLLMs become increasingly proprietary, it may not be possible to access their\nembeddings and apply existing bias measures; furthermore, implicit biases are\nprimarily a concern if they affect the actual decisions that these systems\nmake. We address both challenges by introducing two new measures of bias: LLM\nImplicit Bias, a prompt-based method for revealing implicit bias; and LLM\nDecision Bias, a strategy to detect subtle discrimination in decision-making\ntasks. Both measures are based on psychological research: LLM Implicit Bias\nadapts the Implicit Association Test, widely used to study the automatic\nassociations between concepts held in human minds; and LLM Decision Bias\noperationalizes psychological results indicating that relative evaluations\nbetween two candidates, not absolute evaluations assessing each independently,\nare more diagnostic of implicit biases. Using these measures, we found\npervasive stereotype biases mirroring those in society in 8 value-aligned\nmodels across 4 social categories (race, gender, religion, health) in 21\nstereotypes (such as race and criminality, race and weapons, gender and\nscience, age and negativity). Our prompt-based LLM Implicit Bias measure\ncorrelates with existing language model embedding-based bias methods, but\nbetter predicts downstream behaviors measured by LLM Decision Bias. These new\nprompt-based measures draw from psychology's long history of research into\nmeasuring stereotype biases based on purely observable behavior; they expose\nnuanced biases in proprietary value-aligned LLMs that appear unbiased according\nto standard benchmarks.\n""] , ['  Pretrained Language Models (PLMs) are widely used in NLP for various tasks.\nRecent studies have identified various biases that such models exhibit and have\nproposed methods to correct these biases. However, most of the works address a\nlimited set of bias dimensions independently such as gender, race, or religion.\nMoreover, the methods typically involve finetuning the full model to maintain\nthe performance on the downstream task. In this work, we aim to modularly\ndebias a pretrained language model across multiple dimensions. Previous works\nextensively explored debiasing PLMs using limited US-centric counterfactual\ndata augmentation (CDA). We use structured knowledge and a large generative\nmodel to build a diverse CDA across multiple bias dimensions in a\nsemi-automated way. We highlight how existing debiasing methods do not consider\ninteractions between multiple societal biases and propose a debiasing model\nthat exploits the synergy amongst various societal biases and enables\nmulti-bias debiasing simultaneously. An extensive evaluation on multiple tasks\nand languages demonstrates the efficacy of our approach.\n', '  Large Language models (LLMs), while powerful, exhibit harmful social biases.\nDebiasing is often challenging due to computational costs, data constraints,\nand potential degradation of multi-task language capabilities. This work\nintroduces a novel approach utilizing ChatGPT to generate synthetic training\ndata, aiming to enhance the debiasing of LLMs. We propose two strategies:\nTargeted Prompting, which provides effective debiasing for known biases but\nnecessitates prior specification of bias in question; and General Prompting,\nwhich, while slightly less effective, offers debiasing across various\ncategories. We leverage resource-efficient LLM debiasing using adapter tuning\nand compare the effectiveness of our synthetic data to existing debiasing\ndatasets. Our results reveal that: (1) ChatGPT can efficiently produce\nhigh-quality training data for debiasing other LLMs; (2) data produced via our\napproach surpasses existing datasets in debiasing performance while also\npreserving internal knowledge of a pre-trained LLM; and (3) synthetic data\nexhibits generalizability across categories, effectively mitigating various\nbiases, including intersectional ones. These findings underscore the potential\nof synthetic data in advancing the fairness of LLMs with minimal retraining\ncost.\n', '  Language models are the new state-of-the-art natural language processing\n(NLP) models and they are being increasingly used in many NLP tasks. Even\nthough there is evidence that language models are biased, the impact of that\nbias on the fairness of downstream NLP tasks is still understudied.\nFurthermore, despite that numerous debiasing methods have been proposed in the\nliterature, the impact of bias removal methods on the fairness of NLP tasks is\nalso understudied. In this work, we investigate three different sources of bias\nin NLP models, i.e. representation bias, selection bias and overamplification\nbias, and examine how they impact the fairness of the downstream task of\ntoxicity detection. Moreover, we investigate the impact of removing these\nbiases using different bias removal techniques on the fairness of toxicity\ndetection. Results show strong evidence that downstream sources of bias,\nespecially overamplification bias, are the most impactful types of bias on the\nfairness of the task of toxicity detection. We also found strong evidence that\nremoving overamplification bias by fine-tuning the language models on a dataset\nwith balanced contextual representations and ratios of positive examples\nbetween different identity groups can improve the fairness of the task of\ntoxicity detection. Finally, we build on our findings and introduce a list of\nguidelines to ensure the fairness of the task of toxicity detection.\n'] , ['  With the rising human-like precision of Large Language Models (LLMs) in\nnumerous tasks, their utilization in a variety of real-world applications is\nbecoming more prevalent. Several studies have shown that LLMs excel on many\nstandard NLP benchmarks. However, it is challenging to evaluate LLMs due to\ntest dataset contamination and the limitations of traditional metrics. Since\nhuman evaluations are difficult to collect, there is a growing interest in the\ncommunity to use LLMs themselves as reference-free evaluators for subjective\nmetrics. However, past work has shown that LLM-based evaluators can exhibit\nbias and have poor alignment with human judgments. In this study, we propose a\nframework for an end-to-end assessment of LLMs as evaluators in multilingual\nscenarios. We create a carefully curated dataset, covering 10 languages\ncontaining native speaker judgments for the task of summarization. This dataset\nis created specifically to evaluate LLM-based evaluators, which we refer to as\nmeta-evaluation (METAL). We compare the performance of LLM-based evaluators\ncreated using GPT-3.5-Turbo, GPT-4, and PaLM2. Our results indicate that\nLLM-based evaluators based on GPT-4 perform the best across languages, while\nGPT-3.5-Turbo performs poorly. Additionally, we perform an analysis of the\nreasoning provided by LLM-based evaluators and find that it often does not\nmatch the reasoning provided by human judges.\n', '  LLM-as-a-Judge offers a promising alternative to human judges across various\ntasks, yet inherent biases, particularly position bias - a systematic\npreference for answers based on their position in the prompt - compromise its\neffectiveness. Our study investigates this issue by developing a framework to\nsystematically study and quantify position bias using metrics such as\nrepetitional consistency, positional consistency, and positional fairness. We\nconduct experiments with 9 judge models across 22 tasks from the MTBench and\nDevBench benchmarks and nearly 40 answer-generating models, generating\napproximately 80,000 evaluation instances. This comprehensive assessment\nreveals significant variations in bias across judges and tasks. Although GPT-4\noften excels in positional consistency and fairness, some more cost-effective\nmodels perform comparably or even better in specific tasks, highlighting\nessential trade-offs between consistency, fairness, and cost. Our results also\ndemonstrate high consistency of judgment across repetitions, confirming that\nposition bias is not due to random variations. This research significantly\ncontributes to the field by introducing new concepts for understanding position\nbias and providing a multi-dimensional framework for evaluation. These insights\nguide the selection of optimal judge models, enhance benchmark design, and lay\nthe foundation for future research into effective debiasing strategies,\nultimately enhancing the reliability of LLM evaluators.\n', '  Large language models (LLMs) have shown promising abilities as cost-effective\nand reference-free evaluators for assessing language generation quality. In\nparticular, pairwise LLM evaluators, which compare two generated texts and\ndetermine the preferred one, have been employed in a wide range of\napplications. However, LLMs exhibit preference biases and worrying sensitivity\nto prompt designs. In this work, we first reveal that the predictive preference\nof LLMs can be highly brittle and skewed, even with semantically equivalent\ninstructions. We find that fairer predictive preferences from LLMs consistently\nlead to judgments that are better aligned with humans. Motivated by this\nphenomenon, we propose an automatic Zero-shot Evaluation-oriented Prompt\nOptimization framework, ZEPO, which aims to produce fairer preference decisions\nand improve the alignment of LLM evaluators with human judgments. To this end,\nwe propose a zero-shot learning objective based on the preference decision\nfairness. ZEPO demonstrates substantial performance improvements over\nstate-of-the-art LLM evaluators, without requiring labeled data, on\nrepresentative meta-evaluation benchmarks. Our findings underscore the critical\ncorrelation between preference fairness and human alignment, positioning ZEPO\nas an efficient prompt optimizer for bridging the gap between LLM evaluators\nand human judgments.\n'] , ['  The emergence of large language models (LLMs) has opened up exciting\npossibilities for simulating human behavior and cognitive processes, with\npotential applications in various domains, including marketing research and\nconsumer behavior analysis. However, the validity of utilizing LLMs as\nstand-ins for human subjects remains uncertain due to glaring divergences that\nsuggest fundamentally different underlying processes at play and the\nsensitivity of LLM responses to prompt variations. This paper presents a novel\napproach based on Shapley values from cooperative game theory to interpret LLM\nbehavior and quantify the relative contribution of each prompt component to the\nmodel\'s output. Through two applications-a discrete choice experiment and an\ninvestigation of cognitive biases-we demonstrate how the Shapley value method\ncan uncover what we term ""token noise"" effects, a phenomenon where LLM\ndecisions are disproportionately influenced by tokens providing minimal\ninformative content. This phenomenon raises concerns about the robustness and\ngeneralizability of insights obtained from LLMs in the context of human\nbehavior simulation. Our model-agnostic approach extends its utility to\nproprietary LLMs, providing a valuable tool for marketers and researchers to\nstrategically optimize prompts and mitigate apparent cognitive biases. Our\nfindings underscore the need for a more nuanced understanding of the factors\ndriving LLM responses before relying on them as substitutes for human subjects\nin research settings. We emphasize the importance of researchers reporting\nresults conditioned on specific prompt templates and exercising caution when\ndrawing parallels between human behavior and LLMs.\n', '  The observed similarities in the behavior of humans and Large Language Models\n(LLMs) have prompted researchers to consider the potential of using LLMs as\nmodels of human cognition. However, several significant challenges must be\naddressed before LLMs can be legitimately regarded as cognitive models. For\ninstance, LLMs are trained on far more data than humans typically encounter,\nand may have been directly trained on human data in specific cognitive tasks or\naligned with human preferences. Consequently, the origins of these behavioral\nsimilarities are not well understood. In this paper, we propose a novel way to\nenhance the utility of LLMs as cognitive models. This approach involves (i)\nleveraging computationally equivalent tasks that both an LLM and a rational\nagent need to master for solving a cognitive problem and (ii) examining the\nspecific task distributions required for an LLM to exhibit human-like\nbehaviors. We apply this approach to decision-making -- specifically risky and\nintertemporal choice -- where the key computationally equivalent task is the\narithmetic of expected value calculations. We show that an LLM pretrained on an\necologically valid arithmetic dataset, which we call Arithmetic-GPT, predicts\nhuman behavior better than many traditional cognitive models. Pretraining LLMs\non ecologically valid arithmetic datasets is sufficient to produce a strong\ncorrespondence between these models and human decision-making. Our results also\nsuggest that LLMs used as cognitive models should be carefully investigated via\nablation studies of the pretraining data.\n', ""  In order for AI systems to communicate effectively with people, they must\nunderstand how we make decisions. However, people's decisions are not always\nrational, so the implicit internal models of human decision-making in Large\nLanguage Models (LLMs) must account for this. Previous empirical evidence seems\nto suggest that these implicit models are accurate -- LLMs offer believable\nproxies of human behavior, acting how we expect humans would in everyday\ninteractions. However, by comparing LLM behavior and predictions to a large\ndataset of human decisions, we find that this is actually not the case: when\nboth simulating and predicting people's choices, a suite of cutting-edge LLMs\n(GPT-4o & 4-Turbo, Llama-3-8B & 70B, Claude 3 Opus) assume that people are more\nrational than we really are. Specifically, these models deviate from human\nbehavior and align more closely with a classic model of rational choice --\nexpected value theory. Interestingly, people also tend to assume that other\npeople are rational when interpreting their behavior. As a consequence, when we\ncompare the inferences that LLMs and people draw from the decisions of others\nusing another psychological dataset, we find that these inferences are highly\ncorrelated. Thus, the implicit decision-making models of LLMs appear to be\naligned with the human expectation that other people will act rationally,\nrather than with how people actually act.\n""]",Bias and Fairness in Large Language Models,"""Political Biases in Large Language Models"""
12,"Medical Language Models for Clinical Diagnosis , ""Large Language Models for Mental Health Counseling""","['medical', 'clinical', 'patients', 'medicine', 'med', 'clinician', 'physician', 'doctor', 'annotated', 'physicians'] , ['counseling', 'psychotherapeutic', 'conversations', 'counselor', 'psychotherapists', 'dialogue', 'counselors', 'dialogues', 'conversational', 'interventions']","[""  The integration of Artificial Intelligence (AI), especially Large Language\nModels (LLMs), into the clinical diagnosis process offers significant potential\nto improve the efficiency and accessibility of medical care. While LLMs have\nshown some promise in the medical domain, their application in clinical\ndiagnosis remains underexplored, especially in real-world clinical practice,\nwhere highly sophisticated, patient-specific decisions need to be made. Current\nevaluations of LLMs in this field are often narrow in scope, focusing on\nspecific diseases or specialties and employing simplified diagnostic tasks. To\nbridge this gap, we introduce CliBench, a novel benchmark developed from the\nMIMIC IV dataset, offering a comprehensive and realistic assessment of LLMs'\ncapabilities in clinical diagnosis. This benchmark not only covers diagnoses\nfrom a diverse range of medical cases across various specialties but also\nincorporates tasks of clinical significance: treatment procedure\nidentification, lab test ordering and medication prescriptions. Supported by\nstructured output ontologies, CliBench enables a precise and multi-granular\nevaluation, offering an in-depth understanding of LLM's capability on diverse\nclinical tasks of desired granularity. We conduct a zero-shot evaluation of\nleading LLMs to assess their proficiency in clinical decision-making. Our\npreliminary results shed light on the potential and limitations of current LLMs\nin clinical settings, providing valuable insights for future advancements in\nLLM-powered healthcare.\n"", '  Large Language Models (LLMs) have demonstrated surprising performance across\nvarious natural language processing tasks. Recently, medical LLMs enhanced with\ndomain-specific knowledge have exhibited excellent capabilities in medical\nconsultation and diagnosis. These models can smoothly simulate doctor-patient\ndialogues and provide professional medical advice. Most medical LLMs are\ndeveloped through continued training of open-source general LLMs, which require\nsignificantly fewer computational resources than training LLMs from scratch.\nAdditionally, this approach offers better protection of patient privacy\ncompared to API-based solutions. This survey systematically explores how to\ntrain medical LLMs based on general LLMs. It covers: (a) how to acquire\ntraining corpus and construct customized medical training sets, (b) how to\nchoose a appropriate training paradigm, (c) how to choose a suitable evaluation\nbenchmark, and (d) existing challenges and promising future research directions\nare discussed. This survey can provide guidance for the development of LLMs\nfocused on various medical applications, such as medical education, diagnostic\nplanning, and clinical assistants.\n', '  Large language models (LLMs) have emerged as powerful tools with\ntransformative potential across numerous domains, including healthcare and\nmedicine. In the medical domain, LLMs hold promise for tasks ranging from\nclinical decision support to patient education. However, evaluating the\nperformance of LLMs in medical contexts presents unique challenges due to the\ncomplex and critical nature of medical information. This paper provides a\ncomprehensive overview of the landscape of medical LLM evaluation, synthesizing\ninsights from existing studies and highlighting evaluation data sources, task\nscenarios, and evaluation methods. Additionally, it identifies key challenges\nand opportunities in medical LLM evaluation, emphasizing the need for continued\nresearch and innovation to ensure the responsible integration of LLMs into\nclinical practice.\n'] , ['  Recently, the demand for psychological counseling has significantly increased\nas more individuals express concerns about their mental health. This surge has\naccelerated efforts to improve the accessibility of counseling by using large\nlanguage models (LLMs) as counselors. To ensure client privacy, training\nopen-source LLMs faces a key challenge: the absence of realistic counseling\ndatasets. To address this, we introduce Cactus, a multi-turn dialogue dataset\nthat emulates real-life interactions using the goal-oriented and structured\napproach of Cognitive Behavioral Therapy (CBT). We create a diverse and\nrealistic dataset by designing clients with varied, specific personas, and\nhaving counselors systematically apply CBT techniques in their interactions. To\nassess the quality of our data, we benchmark against established psychological\ncriteria used to evaluate real counseling sessions, ensuring alignment with\nexpert evaluations. Experimental results demonstrate that Camel, a model\ntrained with Cactus, outperforms other models in counseling skills,\nhighlighting its effectiveness and potential as a counseling agent. We make our\ndata, model, and code publicly available.\n', '  The advent of large language models (LLMs) has significantly advanced various\nfields, including natural language processing and automated dialogue systems.\nThis paper explores the application of LLMs in psychological counseling,\naddressing the increasing demand for mental health services. We present a\nmethod for instruction tuning LLMs with specialized prompts to enhance their\nperformance in providing empathetic, relevant, and supportive responses. Our\napproach involves developing a comprehensive dataset of counseling-specific\nprompts, refining them through feedback from professional counselors, and\nconducting rigorous evaluations using both automatic metrics and human\nassessments. The results demonstrate that our instruction-tuned model\noutperforms several baseline LLMs, highlighting its potential as a scalable and\naccessible tool for mental health support.\n', ""  Global rates of mental health concerns are rising, and there is increasing\nrealization that existing models of mental health care will not adequately\nexpand to meet the demand. With the emergence of large language models (LLMs)\nhas come great optimism regarding their promise to create novel, large-scale\nsolutions to support mental health. Despite their nascence, LLMs have already\nbeen applied to mental health related tasks. In this paper, we summarize the\nextant literature on efforts to use LLMs to provide mental health education,\nassessment, and intervention and highlight key opportunities for positive\nimpact in each area. We then highlight risks associated with LLMs' application\nto mental health and encourage the adoption of strategies to mitigate these\nrisks. The urgent need for mental health support must be balanced with\nresponsible development, testing, and deployment of mental health LLMs. It is\nespecially critical to ensure that mental health LLMs are fine-tuned for mental\nhealth, enhance mental health equity, and adhere to ethical standards and that\npeople, including those with lived experience with mental health concerns, are\ninvolved in all stages from development through deployment. Prioritizing these\nefforts will minimize potential harms to mental health and maximize the\nlikelihood that LLMs will positively impact mental health globally.\n""]",Applications of Large Language Models in Healthcare,Medical Language Models for Clinical Diagnosis
13,"Causal Reasoning in Large Language Models , Arabic Language Models and NLP Development , Theory of Mind in Large Language Models , Moral Reasoning in Large Language Models , Language Models and Knowledge Integration , Language Models and Cognitive Linguistics , ""Assessing Theory of Mind in Large Language Models""","['causalbench', 'causal', 'causality', 'causalnlp', 'reasoning', 'inference', 'knowledge', 'relational', 'counterfactual', 'entailment'] , ['arabic', 'arabicaqa', 'arab', 'arablegaleval', 'language', 'wordnet', 'moroccan', 'persian', 'arabiangpt', 'corpus'] , ['minds', 'mentalizing', 'mind', 'reasoning', 'cognition', 'mental', 'inferences', 'language', 'belief', 'think'] , ['morality', 'moral', 'morally', 'ethics', 'ethical', 'persuasion', 'judgments', 'ai', 'languages', 'language'] , ['knowledge', 'contexts', 'language', 'entailment', 'comprehension', 'memory', 'paraphrases', 'nlp', 'facts', 'retrieval'] , ['linguistic', 'cognition', 'linguistics', 'language', 'cognitive', 'semantic', 'learners', 'knowledge', 'brain', 'minds'] , ['ai', 'intelligence', 'chatbots', 'iq', 'language', 'turing', 'robot', 'agent', 'behavioral', 'assessment']","['  This paper explores the causal reasoning of large language models (LLMs) to\nenhance their interpretability and reliability in advancing artificial\nintelligence. Despite the proficiency of LLMs in a range of tasks, their\npotential for understanding causality requires further exploration. We propose\na novel causal attribution model that utilizes ``do-operators"" for constructing\ncounterfactual scenarios, allowing us to systematically quantify the influence\nof input numerical data and LLMs\' pre-existing knowledge on their causal\nreasoning processes. Our newly developed experimental setup assesses LLMs\'\nreliance on contextual information and inherent knowledge across various\ndomains. Our evaluation reveals that LLMs\' causal reasoning ability mainly\ndepends on the context and domain-specific knowledge provided. In the absence\nof such knowledge, LLMs can still maintain a degree of causal reasoning using\nthe available numerical data, albeit with limitations in the calculations. This\nmotivates the proposed fine-tuned LLM for pairwise causal discovery,\neffectively leveraging both knowledge and numerical information.\n', ""  Recent advances in artificial intelligence have seen Large Language Models\n(LLMs) demonstrate notable proficiency in causal discovery tasks. This study\nexplores the factors influencing the performance of LLMs in causal discovery\ntasks. Utilizing open-source LLMs, we examine how the frequency of causal\nrelations within their pre-training corpora affects their ability to accurately\nrespond to causal discovery queries. Our findings reveal that a higher\nfrequency of causal mentions correlates with better model performance,\nsuggesting that extensive exposure to causal information during training\nenhances the models' causal discovery capabilities. Additionally, we\ninvestigate the impact of context on the validity of causal relations. Our\nresults indicate that LLMs might exhibit divergent predictions for identical\ncausal relations when presented in different contexts. This paper provides the\nfirst comprehensive analysis of how different factors contribute to LLM\nperformance in causal discovery tasks.\n"", ""  Large language models (LLMs) have achieved significant success across various\ndomains. However, the inherent complexity of causal problems and causal theory\nposes challenges in accurately describing them in natural language, making it\ndifficult for LLMs to comprehend and use them effectively. Causal methods are\nnot easily conveyed through natural language, which hinders LLMs' ability to\napply them accurately. Additionally, causal datasets are typically tabular,\nwhile LLMs excel in handling natural language data, creating a structural\nmismatch that impedes effective reasoning with tabular data. This lack of\ncausal reasoning capability limits the development of LLMs. To address these\nchallenges, we have equipped the LLM with causal tools within an agent\nframework, named the Causal Agent, enabling it to tackle causal problems. The\ncausal agent comprises tools, memory, and reasoning modules. In the tools\nmodule, the causal agent applies causal methods to align tabular data with\nnatural language. In the reasoning module, the causal agent employs the ReAct\nframework to perform reasoning through multiple iterations with the tools. In\nthe memory module, the causal agent maintains a dictionary instance where the\nkeys are unique names and the values are causal graphs. To verify the causal\nability of the causal agent, we established a benchmark consisting of four\nlevels of causal problems: variable level, edge level, causal graph level, and\ncausal effect level. We generated a test dataset of 1.3K using ChatGPT-3.5 for\nthese four levels of issues and tested the causal agent on the datasets. Our\nmethodology demonstrates remarkable efficacy on the four-level causal problems,\nwith accuracy rates all above 80%. For further insights and implementation\ndetails, our code is accessible via the GitHub repository\nhttps://github.com/Kairong-Han/Causal_Agent.\n""] , ['  We present ALLaM: Arabic Large Language Model, a series of large language\nmodels to support the ecosystem of Arabic Language Technologies (ALT). ALLaM is\ncarefully trained considering the values of language alignment and knowledge\ntransfer at scale. Our autoregressive decoder-only architecture models\ndemonstrate how second-language acquisition via vocabulary expansion and\npretraining on a mixture of Arabic and English text can steer a model towards a\nnew language (Arabic) without any catastrophic forgetting in the original\nlanguage (English). Furthermore, we highlight the effectiveness of using\nparallel/translated data to aid the process of knowledge alignment between\nlanguages. Finally, we show that extensive alignment with human preferences can\nsignificantly enhance the performance of a language model compared to models of\na larger scale with lower quality alignment. ALLaM achieves state-of-the-art\nperformance in various Arabic benchmarks, including MMLU Arabic, ACVA, and\nArabic Exams. Our aligned models improve both in Arabic and English from their\nbase aligned models.\n', '  Large language models (LLMs) have greatly impacted the natural language\nprocessing (NLP) field, particularly for the English language. These models\nhave demonstrated capabilities in understanding and generating human-like text.\nThe success of language models largely depends on the availability of\nhigh-quality instruction datasets, which consist of detailed task descriptions\nand corresponding responses that are essential for training the models to\naddress a variety of prompts accurately. However, the availability and quality\nof these resources vary by language. While models perform well in English, they\noften need help with languages like Arabic, due to the lack of datasets for\nfine-tuning Arabic-specific tasks. To address this issue, we introduce\nInstAr-500k, a new Arabic instruction dataset created by generating and\ncollecting content that covers several domains and instruction types. We assess\nthis dataset by fine-tuning an open-source Gemma-7B model on several downstream\ntasks to improve its functionality. Based on multiple evaluations, our\nfine-tuned model achieves excellent performance on several Arabic NLP\nbenchmarks. These outcomes emphasize the effectiveness of our dataset in\nelevating the capabilities of language models for Arabic. Our instruction\ndataset bridges the performance gap between English and Arabic language models\nby providing resources that amplify Arabic NLP development. Building on this\nfoundation, we developed a model, GemmAr-7B-V1, specifically tuned to excel at\na wide range of Arabic NLP tasks.\n', '  In recent years, Large Language Models have revolutionized the field of\nnatural language processing, showcasing an impressive rise predominantly in\nEnglish-centric domains. These advancements have set a global benchmark,\ninspiring significant efforts toward developing Arabic LLMs capable of\nunderstanding and generating the Arabic language with remarkable accuracy.\nDespite these advancements, a critical challenge persists: the potential bias\nin Arabic LLMs, primarily attributed to their reliance on datasets comprising\nEnglish data that has been translated into Arabic. This reliance not only\ncompromises the authenticity of the generated content but also reflects a\nbroader issue -the scarcity of original quality Arabic linguistic data. This\nstudy aims to address the data scarcity in the Arab world and to encourage the\ndevelopment of Arabic Language Models that are true to both the linguistic and\nnuances of the region. We undertook a large-scale data mining project,\nextracting a substantial volume of text from the Common Crawl WET files,\nspecifically targeting Arabic content. The extracted data underwent a rigorous\ncleaning and deduplication process, using innovative techniques to ensure the\nintegrity and uniqueness of the dataset. The result is the 101 Billion Arabic\nWords Dataset, the largest Arabic dataset available to date, which can\nsignificantly contribute to the development of authentic Arabic LLMs. This\nstudy not only highlights the potential for creating linguistically and\nculturally accurate Arabic LLMs but also sets a precedent for future research\nin enhancing the authenticity of Arabic language models.\n'] , ['  Theory of Mind (ToM) refers to the ability of individuals to attribute mental\nstates to others. While Large Language Models (LLMs) have shown some promise\nwith ToM ability, they still struggle with complex ToM reasoning. Our approach\nleverages an external symbolic executor, specifically the SMCDEL model checker,\nand fine-tuning to improve the ToM reasoning ability of LLMs. In our approach,\nan LLM is first fine-tuned through pairs of natural language and symbolic\nformulation representation of ToM problems and is then instructed to generate\nthe symbolic formulation with a one-shot in-context example. The generated\nsymbolic formulation is then executed by the SMCDEL model checker to perform\ntransparent and verifiable ToM reasoning and give the final result. We\ndemonstrate that our approach, ToM-LM, shows a significant improvement over all\nthe constructed baselines. Our study proposes a novel view about externalizing\na particular component of ToM reasoning, mainly reasoning about beliefs, and\nsuggests generalizing it to other aspects of ToM reasoning.\n', '  Large Language Models (LLMs) have recently shown a promise and emergence of\nTheory of Mind (ToM) ability and even outperform humans in certain ToM tasks.\nTo evaluate and extend the boundaries of the ToM reasoning ability of LLMs, we\npropose a novel concept, taxonomy, and framework, the ToM reasoning with Zero,\nFinite, and Infinite Belief History and develop a multi-round text-based game,\ncalled $\\textit{Pick the Right Stuff}$, as a benchmark. We have evaluated six\nLLMs with this game and found their performance on Zero Belief History is\nconsistently better than on Finite Belief History. In addition, we have found\ntwo of the models with small parameter sizes outperform all the evaluated\nmodels with large parameter sizes. We expect this work to pave the way for\nfuture ToM benchmark development and also for the promotion and development of\nmore complex AI agents or systems which are required to be equipped with more\ncomplex ToM reasoning ability.\n', ""  Theory of Mind (ToM)-the cognitive ability to reason about mental states of\nourselves and others, is the foundation of social interaction. Although ToM\ncomes naturally to humans, it poses a significant challenge to even the most\nadvanced Large Language Models (LLMs). Due to the complex logical chains in ToM\nreasoning, especially in higher-order ToM questions, simply utilizing reasoning\nmethods like Chain of Thought (CoT) will not improve the ToM capabilities of\nLLMs. We present TimeToM, which constructs a temporal space and uses it as the\nfoundation to improve the ToM capabilities of LLMs in multiple scenarios.\nSpecifically, within the temporal space, we construct Temporal Belief State\nChain (TBSC) for each character and inspired by the cognition perspective of\nthe social world model, we divide TBSC into self-world beliefs and social world\nbeliefs, aligning with first-order ToM (first-order beliefs) and higher-order\nToM (higher-order beliefs) questions, respectively. Moreover, we design a novel\ntool-belief solver that, by considering belief communication between characters\nin temporal space, can transform a character's higher-order beliefs into\nanother character's first-order beliefs under belief communication period.\nExperimental results indicate that TimeToM can dramatically improve the\nreasoning performance of LLMs on ToM questions while taking a big step towards\ncoherent and robust ToM reasoning.\n""] , [""  Large language models (LLMs) have taken centre stage in debates on Artificial\nIntelligence. Yet there remains a gap in how to assess LLMs' conformity to\nimportant human values. In this paper, we investigate whether state-of-the-art\nLLMs, GPT-4 and Claude 2.1 (Gemini Pro and LLAMA 2 did not generate valid\nresults) are moral hypocrites. We employ two research instruments based on the\nMoral Foundations Theory: (i) the Moral Foundations Questionnaire (MFQ), which\ninvestigates which values are considered morally relevant in abstract moral\njudgements; and (ii) the Moral Foundations Vignettes (MFVs), which evaluate\nmoral cognition in concrete scenarios related to each moral foundation. We\ncharacterise conflicts in values between these different abstractions of moral\nevaluation as hypocrisy. We found that both models displayed reasonable\nconsistency within each instrument compared to humans, but they displayed\ncontradictory and hypocritical behaviour when we compared the abstract values\npresent in the MFQ to the evaluation of concrete moral violations of the MFV.\n"", '  As large language models (LLMs) are deployed in more and more real-world\nsituations, it is crucial to understand their decision-making when faced with\nmoral dilemmas. Inspired by a large-scale cross-cultural study of human moral\npreferences, ""The Moral Machine Experiment"", we set up the same set of moral\nchoices for LLMs. We translate 1K vignettes of moral dilemmas, parametrically\nvaried across key axes, into 100+ languages, and reveal the preferences of LLMs\nin each of these languages. We then compare the responses of LLMs to that of\nhuman speakers of those languages, harnessing a dataset of 40 million human\nmoral judgments. We discover that LLMs are more aligned with human preferences\nin languages such as English, Korean, Hungarian, and Chinese, but less aligned\nin languages such as Hindi and Somali (in Africa). Moreover, we characterize\nthe explanations LLMs give for their moral choices and find that fairness is\nthe most dominant supporting reason behind GPT-4\'s decisions and utilitarianism\nby GPT-3. We also discover ""language inequality"" (which we define as the\nmodel\'s different development levels in different languages) in a series of\nmeta-properties of moral decision making.\n', '  Making moral judgments is an essential step toward developing ethical AI\nsystems. Prevalent approaches are mostly implemented in a bottom-up manner,\nwhich uses a large set of annotated data to train models based on crowd-sourced\nopinions about morality. These approaches have been criticized for\novergeneralizing the moral stances of a limited group of annotators and lacking\nexplainability. This work proposes a flexible top-down framework to steer\n(Large) Language Models (LMs) to perform moral reasoning with well-established\nmoral theories from interdisciplinary research. The theory-guided top-down\nframework can incorporate various moral theories. Our experiments demonstrate\nthe effectiveness of the proposed framework on datasets derived from moral\ntheories. Furthermore, we show the alignment between different moral theories\nand existing morality datasets. Our analysis exhibits the potential and flaws\nin existing resources (models and datasets) in developing explainable moral\njudgment-making systems.\n'] , [""  Knowledge-intensive language understanding tasks require Language Models\n(LMs) to integrate relevant context, mitigating their inherent weaknesses, such\nas incomplete or outdated knowledge. Nevertheless, studies indicate that LMs\noften ignore the provided context as it can conflict with the pre-existing LM's\nmemory learned during pre-training. Moreover, conflicting knowledge can already\nbe present in the LM's parameters, termed intra-memory conflict. Existing works\nhave studied the two types of knowledge conflicts only in isolation. We\nconjecture that the (degree of) intra-memory conflicts can in turn affect LM's\nhandling of context-memory conflicts. To study this, we introduce the DYNAMICQA\ndataset, which includes facts with a temporal dynamic nature where a fact can\nchange with a varying time frequency and disputable dynamic facts, which can\nchange depending on the viewpoint. DYNAMICQA is the first to include real-world\nknowledge conflicts and provide context to study the link between the different\ntypes of knowledge conflicts. With the proposed dataset, we assess the use of\nuncertainty for measuring the intra-memory conflict and introduce a novel\nCoherent Persuasion (CP) score to evaluate the context's ability to sway LM's\nsemantic output. Our extensive experiments reveal that static facts, which are\nunlikely to change, are more easily updated with additional context, relative\nto temporal and disputable facts.\n"", ""  When large language models are aligned via supervised fine-tuning, they may\nencounter new factual information that was not acquired through pre-training.\nIt is often conjectured that this can teach the model the behavior of\nhallucinating factually incorrect responses, as the model is trained to\ngenerate facts that are not grounded in its pre-existing knowledge. In this\nwork, we study the impact of such exposure to new knowledge on the capability\nof the fine-tuned model to utilize its pre-existing knowledge. To this end, we\ndesign a controlled setup, focused on closed-book QA, where we vary the\nproportion of the fine-tuning examples that introduce new knowledge. We\ndemonstrate that large language models struggle to acquire new factual\nknowledge through fine-tuning, as fine-tuning examples that introduce new\nknowledge are learned significantly slower than those consistent with the\nmodel's knowledge. However, we also find that as the examples with new\nknowledge are eventually learned, they linearly increase the model's tendency\nto hallucinate. Taken together, our results highlight the risk in introducing\nnew factual knowledge through fine-tuning, and support the view that large\nlanguage models mostly acquire factual knowledge through pre-training, whereas\nfine-tuning teaches them to use it more efficiently.\n"", '  Although Large Language Models (LLMs) are effective in performing various NLP\ntasks, they still struggle to handle tasks that require extensive, real-world\nknowledge, especially when dealing with long-tail facts (facts related to\nlong-tail entities). This limitation highlights the need to supplement LLMs\nwith non-parametric knowledge. To address this issue, we analysed the effects\nof different types of non-parametric knowledge, including textual passage and\nknowledge graphs (KGs). Since LLMs have probably seen the majority of factual\nquestion-answering datasets already, to facilitate our analysis, we proposed a\nfully automatic pipeline for creating a benchmark that requires knowledge of\nlong-tail facts for answering the involved questions. Using this pipeline, we\nintroduce the LTGen benchmark. We evaluate state-of-the-art LLMs in different\nknowledge settings using the proposed benchmark. Our experiments show that LLMs\nalone struggle with answering these questions, especially when the long-tail\nlevel is high or rich knowledge is required. Nonetheless, the performance of\nthe same models improved significantly when they were prompted with\nnon-parametric knowledge. We observed that, in most cases, prompting LLMs with\nKG triples surpasses passage-based prompting using a state-of-the-art\nretriever. In addition, while prompting LLMs with both KG triples and documents\ndoes not consistently improve knowledge coverage, it can dramatically reduce\nhallucinations in the generated content.\n'] , ['  In a recent paper, Mandelkern & Linzen (2024) - henceforth M&L - address the\nquestion of whether language models\' (LMs) words refer. Their argument draws\nfrom the externalist tradition in philosophical semantics, which views\nreference as the capacity of words to ""achieve \'word-to-world\' connections"". In\nthe externalist framework, causally uninterrupted chains of usage, tracing\nevery occurrence of a name back to its bearer, guarantee that, for example,\n\'Peano\' refers to the individual Peano (Kripke 1980). This account is\nexternalist both because words pick out referents \'out there\' in the world, and\nbecause what determines reference are coordinated linguistic actions by members\nof a community, and not individual mental states. The ""central question to\nask"", for M&L, is whether LMs too belong to human linguistic communities, such\nthat words by LMs may also trace back causally to their bearers. Their answer\nis a cautious ""yes"": inputs to LMs are linguistic ""forms with particular\nhistories of referential use""; ""those histories ground the referents of those\nforms""; any occurrence of \'Peano\' in LM outputs is as causally connected to the\nindividual Peano as any other occurrence of the same proper name in human\nspeech or text; therefore, occurrences of \'Peano\' in LM outputs refer to Peano.\nIn this commentary, we first qualify M&L\'s claim as applying to a narrow class\nof natural language expressions. Thus qualified, their claim is valid, and we\nemphasise an additional motivation for that in Section 2. Next, we discuss the\nactual scope of their claim, and we suggest that the way they formulate it may\nlead to unwarranted generalisations about reference in LMs. Our critique is\nlikewise applicable to other externalist accounts of LMs (e.g., Lederman &\nMahowald 2024; Mollo & Milliere 2023). Lastly, we conclude with a comment on\nthe status of LMs as members of human linguistic communities.\n', ""  Although LLMs and other artificial intelligence systems demonstrate cognitive\nskills similar to humans, like concept learning and language acquisition, the\nway they process information fundamentally differs from biological cognition.\nTo better understand these differences this paper introduces Psychomatics, a\nmultidisciplinary framework bridging cognitive science, linguistics, and\ncomputer science. It aims to better understand the high-level functioning of\nLLMs, focusing specifically on how LLMs acquire, learn, remember, and use\ninformation to produce their outputs. To achieve this goal, Psychomatics will\nrely on a comparative methodology, starting from a theory-driven research\nquestion - is the process of language development and use different in humans\nand LLMs? - drawing parallels between LLMs and biological systems. Our analysis\nshows how LLMs can map and manipulate complex linguistic patterns in their\ntraining data. Moreover, LLMs can follow Grice's Cooperative Principle to\nprovide relevant and informative responses. However, human cognition draws from\nmultiple sources of meaning, including experiential, emotional, and imaginative\nfacets, which transcend mere language processing and are rooted in our social\nand developmental trajectories. Moreover, current LLMs lack physical\nembodiment, reducing their ability to make sense of the intricate interplay\nbetween perception, action, and cognition that shapes human understanding and\nexpression. Ultimately, Psychomatics holds the potential to yield\ntransformative insights into the nature of language, cognition, and\nintelligence, both artificial and biological. Moreover, by drawing parallels\nbetween LLMs and human cognitive processes, Psychomatics can inform the\ndevelopment of more robust and human-like AI systems.\n"", '  Large language models like GPT-4 have achieved remarkable proficiency in a\nbroad spectrum of language-based tasks, some of which are traditionally\nassociated with hallmarks of human intelligence. This has prompted ongoing\ndisagreements about the extent to which we can meaningfully ascribe any kind of\nlinguistic or cognitive competence to language models. Such questions have deep\nphilosophical roots, echoing longstanding debates about the status of\nartificial neural networks as cognitive models. This article -- the first part\nof two companion papers -- serves both as a primer on language models for\nphilosophers, and as an opinionated survey of their significance in relation to\nclassic debates in the philosophy cognitive science, artificial intelligence,\nand linguistics. We cover topics such as compositionality, language\nacquisition, semantic competence, grounding, world models, and the transmission\nof cultural knowledge. We argue that the success of language models challenges\nseveral long-held assumptions about artificial neural networks. However, we\nalso highlight the need for further empirical investigation to better\nunderstand their internal mechanisms. This sets the stage for the companion\npaper (Part II), which turns to novel empirical methods for probing the inner\nworkings of language models, and new philosophical questions prompted by their\nlatest developments.\n'] , ['  Large Language Models have shown exceptional generative abilities in various\nnatural language and generation tasks. However, possible anthropomorphization\nand leniency towards failure cases have propelled discussions on emergent\nabilities of Large Language Models especially on Theory of Mind (ToM) abilities\nin Large Language Models. While several false-belief tests exists to verify the\nability to infer and maintain mental models of another entity, we study a\nspecial application of ToM abilities that has higher stakes and possibly\nirreversible consequences : Human Robot Interaction. In this work, we explore\nthe task of Perceived Behavior Recognition, where a robot employs a Large\nLanguage Model (LLM) to assess the robot\'s generated behavior in a manner\nsimilar to human observer. We focus on four behavior types, namely -\nexplicable, legible, predictable, and obfuscatory behavior which have been\nextensively used to synthesize interpretable robot behaviors. The LLMs goal is,\ntherefore to be a human proxy to the agent, and to answer how a certain agent\nbehavior would be perceived by the human in the loop, for example ""Given a\nrobot\'s behavior X, would the human observer find it explicable?"". We conduct a\nhuman subject study to verify that the users are able to correctly answer such\na question in the curated situations (robot setting and plan) across five\ndomains. A first analysis of the belief test yields extremely positive results\ninflating ones expectations of LLMs possessing ToM abilities. We then propose\nand perform a suite of perturbation tests which breaks this illusion, i.e.\nInconsistent Belief, Uninformative Context and Conviction Test. We conclude\nthat, the high score of LLMs on vanilla prompts showcases its potential use in\nHRI settings, however to possess ToM demands invariance to trivial or\nirrelevant perturbations in the context which LLMs lack.\n', '  With the release of ChatGPT and other large language models (LLMs) the\ndiscussion about the intelligence, possibilities, and risks, of current and\nfuture models have seen large attention. This discussion included much debated\nscenarios about the imminent rise of so-called ""super-human"" AI, i.e., AI\nsystems that are orders of magnitude smarter than humans. In the spirit of Alan\nTuring, there is no doubt that current state-of-the-art language models already\npass his famous test. Moreover, current models outperform humans in several\nbenchmark tests, so that publicly available LLMs have already become versatile\ncompanions that connect everyday life, industry and science. Despite their\nimpressive capabilities, LLMs sometimes fail completely at tasks that are\nthought to be trivial for humans. In other cases, the trustworthiness of LLMs\nbecomes much more elusive and difficult to evaluate. Taking the example of\nacademia, language models are capable of writing convincing research articles\non a given topic with only little input. Yet, the lack of trustworthiness in\nterms of factual consistency or the existence of persistent hallucinations in\nAI-generated text bodies has led to a range of restrictions for AI-based\ncontent in many scientific journals. In view of these observations, the\nquestion arises as to whether the same metrics that apply to human intelligence\ncan also be applied to computational methods and has been discussed\nextensively. In fact, the choice of metrics has already been shown to\ndramatically influence assessments on potential intelligence emergence. Here,\nwe argue that the intelligence of LLMs should not only be assessed by\ntask-specific statistical metrics, but separately in terms of qualitative and\nquantitative measures.\n', ""  Theory of Mind (ToM) refers to the ability to attribute mental states, such\nas beliefs, desires, intentions, and knowledge, to oneself and others, and to\nunderstand that these mental states can differ from one's own and from reality.\nWe investigate ToM in environments with multiple, distinct, independent AI\nagents, each possessing unique internal states, information, and objectives.\nInspired by human false-belief experiments, we present an AI ('focal AI') with\na scenario where its clone undergoes a human-centric ToM assessment. We prompt\nthe focal AI to assess whether its clone would benefit from additional\ninstructions. Concurrently, we give its clones the ToM assessment, both with\nand without the instructions, thereby engaging the focal AI in higher-order\ncounterfactual reasoning akin to human mentalizing--with respect to humans in\none test and to other AI in another. We uncover a discrepancy: Contemporary AI\ndemonstrates near-perfect accuracy on human-centric ToM assessments. Since\ninformation embedded in one AI is identically embedded in its clone, additional\ninstructions are redundant. Yet, we observe AI crafting elaborate instructions\nfor their clones, erroneously anticipating a need for assistance. An\nindependent referee AI agrees with these unsupported expectations. Neither the\nfocal AI nor the referee demonstrates ToM in our 'silico-centric' test.\n""]",Large Language Models and Cognitive Abilities,Theory of Mind in Large Language Models
14,"""Persona-based Role-Playing with Language Models"" , Persona-Based Dialogue Systems","['personas', 'persona', 'personascore', 'personae', 'profiles', 'agent', 'personality', 'characters', 'character', 'roleeval'] , ['dialogues', 'dialogue', 'conversational', 'personas', 'persona', 'personachat', 'conversation', 'conversations', 'personalizing', 'profiles']","['  The concept of persona, originally adopted in dialogue literature, has\nre-surged as a promising framework for tailoring large language models (LLMs)\nto specific context (e.g., personalized search, LLM-as-a-judge). However, the\ngrowing research on leveraging persona in LLMs is relatively disorganized and\nlacks a systematic taxonomy. To close the gap, we present a comprehensive\nsurvey to categorize the current state of the field. We identify two lines of\nresearch, namely (1) LLM Role-Playing, where personas are assigned to LLMs, and\n(2) LLM Personalization, where LLMs take care of user personas. Additionally,\nwe introduce existing methods for LLM personality evaluation. To the best of\nour knowledge, we present the first survey for role-playing and personalization\nin LLMs under the unified view of persona. We continuously maintain a paper\ncollection to foster future endeavors:\nhttps://github.com/MiuLab/PersonaLLM-Survey\n', ""  With the recent introduction of Assistants API, it is expected that\ndocument-based language models will be actively used in various domains,\nespecially Role-playing. However, a key challenge lies in utilizing\nprotagonist's persona: Assistants API often fails to achieve with its search\nbecause the information extraction part is different each time and it often\nomits important information such as protagonist's backstory or relationships.\nIt is hard to maintain a consistent persona simply by using the persona\ndocument as input to the Assistants API. To address the challenge of achieving\nstable persona consistency, we propose CharacterGPT, a novel persona\nreconstruction framework to alleviate the shortcomings of the Assistants API.\nOur method involves Character Persona Training (CPT), an effective persona\nrebuilding process that updates the character persona by extracting the\ncharacter's traits from given summary of the novel for each character as if the\nstory in a novel progresses. In our experiments, we ask each character to take\nthe Big Five Inventory personality test in various settings and analyze the\nresults. To assess whether it can think outside the box, we let each character\ngenerate short novels. Extensive experiments and human evaluation demonstrate\nthat CharacterGPT presents new possibilities for role-playing agent research.\nCode and results are available at: https://github.com/Jeiyoon/charactergpt\n"", '  This survey explores the burgeoning field of role-playing with language\nmodels, focusing on their development from early persona-based models to\nadvanced character-driven simulations facilitated by Large Language Models\n(LLMs). Initially confined to simple persona consistency due to limited model\ncapabilities, role-playing tasks have now expanded to embrace complex character\nportrayals involving character consistency, behavioral alignment, and overall\nattractiveness. We provide a comprehensive taxonomy of the critical components\nin designing these systems, including data, models and alignment, agent\narchitecture and evaluation. This survey not only outlines the current\nmethodologies and challenges, such as managing dynamic personal profiles and\nachieving high-level persona consistency but also suggests avenues for future\nresearch in improving the depth and realism of role-playing applications. The\ngoal is to guide future research by offering a structured overview of current\nmethodologies and identifying potential areas for improvement. Related\nresources and papers are available at\nhttps://github.com/nuochenpku/Awesome-Role-Play-Papers.\n'] , ['  Persona-based dialogue systems aim to generate consistent responses based on\nhistorical context and predefined persona. Unlike conventional dialogue\ngeneration, the persona-based dialogue needs to consider both dialogue context\nand persona, posing a challenge for coherent training. Specifically, this\nrequires a delicate weight balance between context and persona. To achieve\nthat, in this paper, we propose an effective framework with Persona-Adaptive\nAttention (PAA), which adaptively integrates the weights from the persona and\ncontext information via our designed attention. In addition, a dynamic masking\nmechanism is applied to the PAA to not only drop redundant information in\ncontext and persona but also serve as a regularization mechanism to avoid\noverfitting. Experimental results demonstrate the superiority of the proposed\nPAA framework compared to the strong baselines in both automatic and human\nevaluation. Moreover, the proposed PAA approach can perform equivalently well\nin a low-resource regime compared to models trained in a full-data setting,\nwhich achieve a similar result with only 20% to 30% of data compared to the\nlarger models trained in the full-data setting. To fully exploit the\neffectiveness of our design, we designed several variants for handling the\nweighted information in different ways, showing the necessity and sufficiency\nof our weighting and masking designs.\n', ""  Providing emotional support through dialogue systems is becoming increasingly\nimportant in today's world, as it can support both mental health and social\ninteractions in many conversation scenarios. Previous works have shown that\nusing persona is effective for generating empathetic and supportive responses.\nThey have often relied on pre-provided persona rather than inferring them\nduring conversations. However, it is not always possible to obtain a user\npersona before the conversation begins. To address this challenge, we propose\nPESS (Persona Extraction through Semantic Similarity), a novel framework that\ncan automatically infer informative and consistent persona from dialogues. We\ndevise completeness loss and consistency loss based on semantic similarity\nscores. The completeness loss encourages the model to generate missing persona\ninformation, and the consistency loss guides the model to distinguish between\nconsistent and inconsistent persona. Our experimental results demonstrate that\nhigh-quality persona information inferred by PESS is effective in generating\nemotionally supportive responses.\n"", '  While valuable datasets such as PersonaChat provide a foundation for training\npersona-grounded dialogue agents, they lack diversity in conversational and\nnarrative settings, primarily existing in the ""real"" world. To develop dialogue\nagents with unique personas, models are trained to converse given a specific\npersona, but hand-crafting these persona can be time-consuming, thus methods\nexist to automatically extract persona information from existing\ncharacter-specific dialogue. However, these persona-extraction models are also\ntrained on datasets derived from PersonaChat and struggle to provide\nhigh-quality persona information from conversational settings that do not take\nplace in the real world, such as the fantasy-focused dataset, LIGHT. Creating\nnew data to train models on a specific setting is human-intensive, thus\nprohibitively expensive. To address both these issues, we introduce a natural\nlanguage inference method for post-hoc adapting a trained persona extraction\nmodel to a new setting. We draw inspiration from the literature of dialog\nnatural language inference (NLI), and devise NLI-reranking methods to extract\nstructured persona information from dialogue. Compared to existing persona\nextraction models, our method returns higher-quality extracted persona and\nrequires less human annotation.\n']",Persona-Based Natural Language Processing,Persona-Based Dialogue Systems
15,"Personality Detection and Analysis in Language Models , Deception Detection in AI and Language","['personality', 'personalities', 'traits', 'trait', 'profiles', 'conversational', 'personas', 'profile', 'extraversion', 'psychometrics'] , ['deception', 'deceiving', 'deceive', 'deceptive', 'deceptively', 'disinformation', 'linguistic', 'language', 'ai', 'detecting']","[""  Personality psychologists have analyzed the relationship between personality\nand safety behaviors in human society. Although Large Language Models (LLMs)\ndemonstrate personality traits, the relationship between personality traits and\nsafety abilities in LLMs still remains a mystery. In this paper, we discover\nthat LLMs' personality traits are closely related to their safety abilities,\ni.e., toxicity, privacy, and fairness, based on the reliable MBTI-M scale.\nMeanwhile, the safety alignment generally increases various LLMs' Extraversion,\nSensing, and Judging traits. According to such findings, we can edit LLMs'\npersonality traits and improve their safety performance, e.g., inducing\npersonality from ISTJ to ISTP resulted in a relative improvement of\napproximately 43% and 10% in privacy and fairness performance, respectively.\nAdditionally, we find that LLMs with different personality traits are\ndifferentially susceptible to jailbreak. This study pioneers the investigation\nof LLM safety from a personality perspective, providing new insights into LLM\nsafety enhancement.\n"", ""  Personality detection aims to detect one's personality traits underlying in\nsocial media posts. One challenge of this task is the scarcity of ground-truth\npersonality traits which are collected from self-report questionnaires. Most\nexisting methods learn post features directly by fine-tuning the pre-trained\nlanguage models under the supervision of limited personality labels. This leads\nto inferior quality of post features and consequently affects the performance.\nIn addition, they treat personality traits as one-hot classification labels,\noverlooking the semantic information within them. In this paper, we propose a\nlarge language model (LLM) based text augmentation enhanced personality\ndetection model, which distills the LLM's knowledge to enhance the small model\nfor personality detection, even when the LLM fails in this task. Specifically,\nwe enable LLM to generate post analyses (augmentations) from the aspects of\nsemantic, sentiment, and linguistic, which are critical for personality\ndetection. By using contrastive learning to pull them together in the embedding\nspace, the post encoder can better capture the psycho-linguistic information\nwithin the post representations, thus improving personality detection.\nFurthermore, we utilize the LLM to enrich the information of personality labels\nfor enhancing the detection performance. Experimental results on the benchmark\ndatasets demonstrate that our model outperforms the state-of-the-art methods on\npersonality detection.\n"", ""  Textual personality detection aims to identify personality characteristics by\nanalyzing user-generated content toward social media platforms. Numerous\npsychological literature highlighted that personality encompasses both\nlong-term stable traits and short-term dynamic states. However, existing\nstudies often concentrate only on either long-term or short-term personality\nrepresentations, without effectively combining both aspects. This limitation\nhinders a comprehensive understanding of individuals' personalities, as both\nstable traits and dynamic states are vital. To bridge this gap, we propose a\nDual Enhanced Network(DEN) to jointly model users' long-term and short-term\npersonality for textual personality detection. In DEN, a Long-term Personality\nEncoding is devised to effectively model long-term stable personality traits.\nShort-term Personality Encoding is presented to capture short-term dynamic\npersonality states. The Bi-directional Interaction component facilitates the\nintegration of both personality aspects, allowing for a comprehensive\nrepresentation of the user's personality. Experimental results on two\npersonality detection datasets demonstrate the effectiveness of the DEN model\nand the benefits of considering both the dynamic and stable nature of\npersonality characteristics for textual personality detection.\n""] , ['  Recent developments in large language models (LLMs), while offering a\npowerful foundation for developing natural language agents, raise safety\nconcerns about them and the autonomous agents built upon them. Deception is one\npotential capability of AI agents of particular concern, which we refer to as\nan act or statement that misleads, hides the truth, or promotes a belief that\nis not true in its entirety or in part. We move away from the conventional\nunderstanding of deception through straight-out lying, making objective selfish\ndecisions, or giving false information, as seen in previous AI safety research.\nWe target a specific category of deception achieved through obfuscation and\nequivocation. We broadly explain the two types of deception by analogizing them\nwith the rabbit-out-of-hat magic trick, where (i) the rabbit either comes out\nof a hidden trap door or (ii) (our focus) the audience is completely distracted\nto see the magician bring out the rabbit right in front of them using sleight\nof hand or misdirection. Our novel testbed framework displays intrinsic\ndeception capabilities of LLM agents in a goal-driven environment when directed\nto be deceptive in their natural language generations in a two-agent\nadversarial dialogue system built upon the legislative task of ""lobbying"" for a\nbill. Along the lines of a goal-driven environment, we show developing\ndeceptive capacity through a reinforcement learning setup, building it around\nthe theories of language philosophy and cognitive psychology. We find that the\nlobbyist agent increases its deceptive capabilities by ~ 40% (relative) through\nsubsequent reinforcement trials of adversarial interactions, and our deception\ndetection mechanism shows a detection capability of up to 92%. Our results\nhighlight potential issues in agent-human interaction, with agents potentially\nmanipulating humans towards its programmed end-goal.\n', ""  Internet-based economies and societies are drowning in deceptive attacks.\nThese attacks take many forms, such as fake news, phishing, and job scams,\nwhich we call ``domains of deception.'' Machine-learning and\nnatural-language-processing researchers have been attempting to ameliorate this\nprecarious situation by designing domain-specific detectors. Only a few recent\nworks have considered domain-independent deception. We collect these disparate\nthreads of research and investigate domain-independent deception. First, we\nprovide a new computational definition of deception and break down deception\ninto a new taxonomy. Then, we analyze the debate on linguistic cues for\ndeception and supply guidelines for systematic reviews. Finally, we investigate\ncommon linguistic features and give evidence for knowledge transfer across\ndifferent forms of deception.\n"", '  Deception, a prevalent aspect of human communication, has undergone a\nsignificant transformation in the digital age. With the globalization of online\ninteractions, individuals are communicating in multiple languages and mixing\nlanguages on social media, with varied data becoming available in each language\nand dialect. At the same time, the techniques for detecting deception are\nsimilar across the board. Recent studies have shown the possibility of the\nexistence of universal linguistic cues to deception across domains within the\nEnglish language; however, the existence of such cues in other languages\nremains unknown. Furthermore, the practical task of deception detection in\nlow-resource languages is not a well-studied problem due to the lack of labeled\ndata. Another dimension of deception is multimodality. For example, a picture\nwith an altered caption in fake news or disinformation may exist. This paper\ncalls for a comprehensive investigation into the complexities of deceptive\nlanguage across linguistic boundaries and modalities within the realm of\ncomputer security and natural language processing and the possibility of using\nmultilingual transformer models and labeled data in various languages to\nuniversally address the task of deception detection.\n']",Language Analysis for Human Behavior and Intent,Personality Detection and Analysis in Language Models
16,"Phylogenetic Linguistics and Protolanguage Reconstruction , Bayesian Inference and Phylogenetic Models","['phylogenies', 'protolanguage', 'phylogenetics', 'phylogenetic', 'linguistics', 'ancestral', 'linguists', 'ancestor', 'phonetic', 'linguistic'] , ['posterior', 'bayesian', 'generative', 'phylogenetic', 'likelihood', 'inference', 'models', 'estimating', 'markov', 'monte']","['  Protolanguage reconstruction is central to historical linguistics. The\ncomparative method, one of the most influential theoretical and methodological\nframeworks in the history of the language sciences, allows linguists to infer\nprotoforms (reconstructed ancestral words) from their reflexes (related modern\nwords) based on the assumption of regular sound change. Not surprisingly,\nnumerous computational linguists have attempted to operationalize comparative\nreconstruction through various computational models, the most successful of\nwhich have been supervised encoder-decoder models, which treat the problem of\npredicting protoforms given sets of reflexes as a sequence-to-sequence problem.\nWe argue that this framework ignores one of the most important aspects of the\ncomparative method: not only should protoforms be inferable from cognate sets\n(sets of related reflexes) but the reflexes should also be inferable from the\nprotoforms. Leveraging another line of research -- reflex prediction -- we\npropose a system in which candidate protoforms from a reconstruction model are\nreranked by a reflex prediction model. We show that this more complete\nimplementation of the comparative method allows us to surpass state-of-the-art\nprotoform reconstruction methods on three of four Chinese and Romance datasets.\n', '  In traditional studies on language evolution, scholars often emphasize the\nimportance of sound laws and sound correspondences for phylogenetic inference\nof language family trees. However, to date, computational approaches have\ntypically not taken this potential into account. Most computational studies\nstill rely on lexical cognates as major data source for phylogenetic\nreconstruction in linguistics, although there do exist a few studies in which\nauthors praise the benefits of comparing words at the level of sound sequences.\nBuilding on (a) ten diverse datasets from different language families, and (b)\nstate-of-the-art methods for automated cognate and sound correspondence\ndetection, we test, for the first time, the performance of sound-based versus\ncognate-based approaches to phylogenetic reconstruction. Our results show that\nphylogenies reconstructed from lexical cognates are topologically closer, by\napproximately one third with respect to the generalized quartet distance on\naverage, to the gold standard phylogenies than phylogenies reconstructed from\nsound correspondences.\n', '  Identification of cognates across related languages is one of the primary\nproblems in historical linguistics. Automated cognate identification is helpful\nfor several downstream tasks including identifying sound correspondences,\nproto-language reconstruction, phylogenetic classification, etc. Previous\nstate-of-the-art methods for cognate identification are mostly based on\ndistributions of phonemes computed across multilingual wordlists and make\nlittle use of the cognacy labels that define links among cognate clusters. In\nthis paper, we present a transformer-based architecture inspired by\ncomputational biology for the task of automated cognate detection. Beyond a\ncertain amount of supervision, this method performs better than the existing\nmethods, and shows steady improvement with further increase in supervision,\nthereby proving the efficacy of utilizing the labeled information. We also\ndemonstrate that accepting multiple sequence alignments as input and having an\nend-to-end architecture with link prediction head saves much computation time\nwhile simultaneously yielding superior performance.\n'] , [""  Bayesian Additive Regression Trees (BART) is a popular Bayesian\nnon-parametric regression model that is commonly used in causal inference and\nbeyond. Its strong predictive performance is supported by theoretical\nguarantees that its posterior distribution concentrates around the true\nregression function at optimal rates under various data generative settings and\nfor appropriate prior choices. In this paper, we show that the BART sampler\noften converges slowly, confirming empirical observations by other researchers.\nAssuming discrete covariates, we show that, while the BART posterior\nconcentrates on a set comprising all optimal tree structures (smallest bias and\ncomplexity), the Markov chain's hitting time for this set increases with $n$\n(training sample size), under several common data generative settings. As $n$\nincreases, the approximate BART posterior thus becomes increasingly different\nfrom the exact posterior (for the same number of MCMC samples), contrasting\nwith earlier concentration results on the exact posterior. This contrast is\nhighlighted by our simulations showing worsening frequentist undercoverage for\napproximate posterior intervals and a growing ratio between the MSE of the\napproximate posterior and that obtainable by artificially improving convergence\nvia averaging multiple sampler chains. Finally, based on our theoretical\ninsights, possibilities are discussed to improve the BART sampler convergence\nperformance.\n"", '  Bayesian phylogenetic inference is currently done via Markov chain Monte\nCarlo (MCMC) with simple proposal mechanisms. This hinders exploration\nefficiency and often requires long runs to deliver accurate posterior\nestimates. In this paper, we present an alternative approach: a variational\nframework for Bayesian phylogenetic analysis. We propose combining subsplit\nBayesian networks, an expressive graphical model for tree topology\ndistributions, and a structured amortization of the branch lengths over tree\ntopologies for a suitable variational family of distributions. We train the\nvariational approximation via stochastic gradient ascent and adopt gradient\nestimators for continuous and discrete variational parameters separately to\ndeal with the composite latent space of phylogenetic models. We show that our\nvariational approach provides competitive performance to MCMC, while requiring\nmuch fewer (though more costly) iterations due to a more efficient exploration\nmechanism enabled by variational inference. Experiments on a benchmark of\nchallenging real data Bayesian phylogenetic inference problems demonstrate the\neffectiveness and efficiency of our methods.\n', '  Likelihood-free inference methods based on neural conditional density\nestimation were shown to drastically reduce the simulation burden in comparison\nto classical methods such as ABC. When applied in the context of any latent\nvariable model, such as a Hidden Markov model (HMM), these methods are designed\nto only estimate the parameters, rather than the joint distribution of the\nparameters and the hidden states. Naive application of these methods to a HMM,\nignoring the inference of this joint posterior distribution, will thus produce\nan inaccurate estimate of the posterior predictive distribution, in turn\nhampering the assessment of goodness-of-fit. To rectify this problem, we\npropose a novel, sample-efficient likelihood-free method for estimating the\nhigh-dimensional hidden states of an implicit HMM. Our approach relies on\nlearning directly the intractable posterior distribution of the hidden states,\nusing an autoregressive-flow, by exploiting the Markov property. Upon\nevaluating our approach on some implicit HMMs, we found that the quality of the\nestimates retrieved using our method is comparable to what can be achieved\nusing a much more computationally expensive SMC algorithm.\n']",Computational Methods in Historical Linguistics and Phylogenetics,Phylogenetic Linguistics and Protolanguage Reconstruction
17,"""Large Language Model Hallucination Detection and Mitigation"" , ""Mitigating Hallucinations in Large Vision-Language Models""","['hallucination', 'hallucinations', 'hallucinating', 'hallucinate', 'hallucinatory', 'hallucinated', 'decoding', 'annotator', 'text', 'halludial'] , ['hallucinations', 'hallucination', 'hallucinate', 'hallucinatory', 'hallucinated', 'multimodal', 'hallusionbench', 'captioning', 'lvlm', 'visual']","['  Large language models (LLMs) exhibit hallucinations in long-form\nquestion-answering tasks across various domains and wide applications. Current\nhallucination detection and mitigation datasets are limited in domains and\nsizes, which struggle to scale due to prohibitive labor costs and insufficient\nreliability of existing hallucination annotators. To facilitate the scalable\noversight of LLM hallucinations, this paper introduces an iterative\nself-training framework that simultaneously and progressively scales up the\nhallucination annotation dataset and improves the accuracy of the hallucination\nannotator. Based on the Expectation Maximization (EM) algorithm, in each\niteration, the framework first applies a hallucination annotation pipeline to\nannotate a scaled dataset and then trains a more accurate hallucination\nannotator on the dataset. This new hallucination annotator is adopted in the\nhallucination annotation pipeline used for the next iteration. Extensive\nexperimental results demonstrate that the finally obtained hallucination\nannotator with only 7B parameters surpasses the performance of GPT-4 and\nobtains new state-of-the-art hallucination detection results on HaluEval and\nHalluQA by zero-shot inference. Such an annotator can not only evaluate the\nhallucination levels of various LLMs on the large-scale dataset but also help\nto mitigate the hallucination of LLMs generations, with the Natural Language\nInference (NLI) metric increasing from 25% to 37% on HaluEval.\n', '  Large Language Models (LLMs) have shown propensity to generate hallucinated\noutputs, i.e., texts that are factually incorrect or unsupported. Existing\nmethods for alleviating hallucinations typically require costly human\nannotations to identify and correct hallucinations in LLM outputs. Moreover,\nmost of these methods focus on a specific type of hallucination, e.g., entity\nor token errors, which limits their effectiveness in addressing various types\nof hallucinations exhibited in LLM outputs. To our best knowledge, in this\npaper we propose the first active learning framework to alleviate LLM\nhallucinations, reducing costly human annotations of hallucination needed. By\nmeasuring fine-grained hallucinations from errors in semantic frame, discourse\nand content verifiability in text summarization, we propose HAllucination\nDiversity-Aware Sampling (HADAS) to select diverse hallucinations for\nannotations in active learning for LLM finetuning. Extensive experiments on\nthree datasets and different backbone models demonstrate advantages of our\nmethod in effectively and efficiently mitigating LLM hallucinations.\n', '  In the era of large language models (LLMs), hallucination (i.e., the tendency\nto generate factually incorrect content) poses great challenge to trustworthy\nand reliable deployment of LLMs in real-world applications. To tackle the LLM\nhallucination, three key questions should be well studied: how to detect\nhallucinations (detection), why do LLMs hallucinate (source), and what can be\ndone to mitigate them (mitigation). To address these challenges, this work\npresents a systematic empirical study on LLM hallucination, focused on the the\nthree aspects of hallucination detection, source and mitigation. Specially, we\nconstruct a new hallucination benchmark HaluEval 2.0, and designs a simple yet\neffective detection method for LLM hallucination. Furthermore, we zoom into the\ndifferent training or utilization stages of LLMs and extensively analyze the\npotential factors that lead to the LLM hallucination. Finally, we implement and\nexamine a series of widely used techniques to mitigate the hallucinations in\nLLMs. Our work has led to several important findings to understand the\nhallucination origin and mitigate the hallucinations in LLMs. Our code and data\ncan be accessed at https://github.com/RUCAIBox/HaluEval-2.0.\n'] , ['  The rapidly developing Large Vision Language Models (LVLMs) have shown\nnotable capabilities on a range of multi-modal tasks, but still face the\nhallucination phenomena where the generated texts do not align with the given\ncontexts, significantly restricting the usages of LVLMs. Most previous work\ndetects and mitigates hallucination at the coarse-grained level or requires\nexpensive annotation (e.g., labeling by proprietary models or human experts).\nTo address these issues, we propose detecting and mitigating hallucinations in\nLVLMs via fine-grained AI feedback. The basic idea is that we generate a\nsmall-size sentence-level hallucination annotation dataset by proprietary\nmodels, whereby we train a hallucination detection model which can perform\nsentence-level hallucination detection, covering primary hallucination types\n(i.e., object, attribute, and relationship). Then, we propose a\ndetect-then-rewrite pipeline to automatically construct preference dataset for\ntraining hallucination mitigating model. Furthermore, we propose\ndifferentiating the severity of hallucinations, and introducing a Hallucination\nSeverity-Aware Direct Preference Optimization (HSA-DPO) for mitigating\nhallucination in LVLMs by incorporating the severity of hallucinations into\npreference learning. Extensive experiments demonstrate the effectiveness of our\nmethod.\n', ""  Though advanced in understanding visual information with human languages,\nLarge Vision-Language Models (LVLMs) still suffer from multimodal\nhallucinations. A natural concern is that during multimodal interaction, the\ngenerated hallucinations could influence the LVLMs' subsequent generation.\nThus, we raise a question: When presented with a query relevant to the\npreviously generated hallucination, will LVLMs be misled and respond\nincorrectly, even though the ground visual information exists? To answer this,\nwe propose a framework called MMHalSnowball to evaluate LVLMs' behaviors when\nencountering generated hallucinations, where LVLMs are required to answer\nspecific visual questions within a curated hallucinatory conversation.\nCrucially, our experiment shows that the performance of open-source LVLMs drops\nby at least $31\\%$, indicating that LVLMs are prone to accept the generated\nhallucinations and make false claims that they would not have supported without\ndistractions. We term this phenomenon Multimodal Hallucination Snowballing. To\nmitigate this, we further propose a training-free method called Residual Visual\nDecoding, where we revise the output distribution of LVLMs with the one derived\nfrom the residual visual input, providing models with direct access to the\nvisual information. Experiments show that our method can mitigate more than\n$24\\%$ of the snowballed multimodal hallucination while maintaining\ncapabilities.\n"", ""  Recent development of Large Vision-Language Models (LVLMs) has attracted\ngrowing attention within the AI landscape for its practical implementation\npotential. However, ``hallucination'', or more specifically, the misalignment\nbetween factual visual content and corresponding textual generation, poses a\nsignificant challenge of utilizing LVLMs. In this comprehensive survey, we\ndissect LVLM-related hallucinations in an attempt to establish an overview and\nfacilitate future mitigation. Our scrutiny starts with a clarification of the\nconcept of hallucinations in LVLMs, presenting a variety of hallucination\nsymptoms and highlighting the unique challenges inherent in LVLM\nhallucinations. Subsequently, we outline the benchmarks and methodologies\ntailored specifically for evaluating hallucinations unique to LVLMs.\nAdditionally, we delve into an investigation of the root causes of these\nhallucinations, encompassing insights from the training data and model\ncomponents. We also critically review existing methods for mitigating\nhallucinations. The open questions and future directions pertaining to\nhallucinations within LVLMs are discussed to conclude this survey.\n""]",Mitigating Hallucinations in Large Language and Vision-Language Models,"""Mitigating Hallucinations in Large Vision-Language Models"""
18,"""LLMs and Brain Similarity in Language Processing""","['neural', 'fmri', 'brains', 'brain', 'cognitive', 'language', 'neuroscience', 'representations', 'brainscore', 'iq']","['  Given the remarkable capabilities of large language models (LLMs), there has\nbeen a growing interest in evaluating their similarity to the human brain. One\napproach towards quantifying this similarity is by measuring how well a model\npredicts neural signals, also called ""brain score"". Internal representations\nfrom LLMs achieve state-of-the-art brain scores, leading to speculation that\nthey share computational principles with human language processing. This\ninference is only valid if the subset of neural activity predicted by LLMs\nreflects core elements of language processing. Here, we question this\nassumption by analyzing three neural datasets used in an impactful study on\nLLM-to-brain mappings, with a particular focus on an fMRI dataset where\nparticipants read short passages. We first find that when using shuffled\ntrain-test splits, as done in previous studies with these datasets, a trivial\nfeature that encodes temporal autocorrelation not only outperforms LLMs but\nalso accounts for the majority of neural variance that LLMs explain. We\ntherefore use contiguous splits moving forward. Second, we explain the\nsurprisingly high brain scores of untrained LLMs by showing they do not account\nfor additional neural variance beyond two simple features: sentence length and\nsentence position. This undermines evidence used to claim that the transformer\narchitecture biases computations to be more brain-like. Third, we find that\nbrain scores of trained LLMs on this dataset can largely be explained by\nsentence length, position, and pronoun-dereferenced static word embeddings; a\nsmall, additional amount is explained by sense-specific embeddings and\ncontextual representations of sentence structure. We conclude that\nover-reliance on brain scores can lead to over-interpretations of similarity\nbetween LLMs and brains, and emphasize the importance of deconstructing what\nLLMs are mapping to in neural signals.\n', ""  Large Language Models (LLMs) have been shown to be effective models of the\nhuman language system, with some models predicting most explainable variance of\nbrain activity in current datasets. Even in untrained models, the\nrepresentations induced by architectural priors can exhibit reasonable\nalignment to brain data. In this work, we investigate the key architectural\ncomponents driving the surprising alignment of untrained models. To estimate\nLLM-to-brain similarity, we first select language-selective units within an\nLLM, similar to how neuroscientists identify the language network in the human\nbrain. We then benchmark the brain alignment of these LLM units across five\ndifferent brain recording datasets. By isolating critical components of the\nTransformer architecture, we identify tokenization strategy and multihead\nattention as the two major components driving brain alignment. A simple form of\nrecurrence further improves alignment. We further demonstrate this quantitative\nbrain alignment of our model by reproducing landmark studies in the language\nneuroscience field, showing that localized model units -- just like language\nvoxels measured empirically in the human brain -- discriminate more reliably\nbetween lexical than syntactic differences, and exhibit similar response\nprofiles under the same experimental conditions. Finally, we demonstrate the\nutility of our model's representations for language modeling, achieving\nimproved sample and parameter efficiency over comparable architectures. Our\nmodel's estimates of surprisal sets a new state-of-the-art in the behavioral\nalignment to human reading times. Taken together, we propose a highly brain-\nand behaviorally-aligned model that conceptualizes the human language system as\nan untrained shallow feature encoder, with structural priors, combined with a\ntrained decoder to achieve efficient and performant language processing.\n"", '  Large Language Models (LLMs) have demonstrated remarkable abilities in text\ncomprehension and logical reasoning, indicating that the text representations\nlearned by LLMs can facilitate their language processing capabilities. In\ncognitive science, brain cognitive processing signals are typically utilized to\nstudy human language processing. Therefore, it is natural to ask how well the\ntext embeddings from LLMs align with the brain cognitive processing signals,\nand how training strategies affect the LLM-brain alignment? In this paper, we\nemploy Representational Similarity Analysis (RSA) to measure the alignment\nbetween 23 mainstream LLMs and fMRI signals of the brain to evaluate how\neffectively LLMs simulate cognitive language processing. We empirically\ninvestigate the impact of various factors (e.g., pre-training data size, model\nscaling, alignment training, and prompts) on such LLM-brain alignment.\nExperimental results indicate that pre-training data size and model scaling are\npositively correlated with LLM-brain similarity, and alignment training can\nsignificantly improve LLM-brain similarity. Explicit prompts contribute to the\nconsistency of LLMs with brain cognitive language processing, while nonsensical\nnoisy prompts may attenuate such alignment. Additionally, the performance of a\nwide range of LLM evaluations (e.g., MMLU, Chatbot Arena) is highly correlated\nwith the LLM-brain similarity.\n']",Comparing Large Language Models to Human Brain Function in Language Processing,"""LLMs and Brain Similarity in Language Processing"""
19,Document Understanding with LayoutLLM,"['documentunderstanding', 'layoutllm', 'text', 'doclaynet', 'layouts', 'layout', 'document', 'textual', 'ocr', 'structured']","[""  This paper proposes LayoutLLM, a more flexible document analysis method for\nunderstanding imaged documents. Visually Rich Document Understanding tasks,\nsuch as document image classification and information extraction, have gained\nsignificant attention due to their importance. Existing methods have been\ndeveloped to enhance document comprehension by incorporating pre-training\nawareness of images, text, and layout structure. However, these methods require\nfine-tuning for each task and dataset, and the models are expensive to train\nand operate. To overcome this limitation, we propose a new LayoutLLM that\nintegrates these with large-scale language models (LLMs). By leveraging the\nstrengths of existing research in document image understanding and LLMs'\nsuperior language understanding capabilities, the proposed model, fine-tuned\nwith multimodal instruction datasets, performs an understanding of document\nimages in a single model. Our experiments demonstrate improvement over the\nbaseline model in various document analysis tasks.\n"", '  Recently, leveraging large language models (LLMs) or multimodal large\nlanguage models (MLLMs) for document understanding has been proven very\npromising. However, previous works that employ LLMs/MLLMs for document\nunderstanding have not fully explored and utilized the document layout\ninformation, which is vital for precise document understanding. In this paper,\nwe propose LayoutLLM, an LLM/MLLM based method for document understanding. The\ncore of LayoutLLM is a layout instruction tuning strategy, which is specially\ndesigned to enhance the comprehension and utilization of document layouts. The\nproposed layout instruction tuning strategy consists of two components:\nLayout-aware Pre-training and Layout-aware Supervised Fine-tuning. To capture\nthe characteristics of document layout in Layout-aware Pre-training, three\ngroups of pre-training tasks, corresponding to document-level, region-level and\nsegment-level information, are introduced. Furthermore, a novel module called\nlayout chain-of-thought (LayoutCoT) is devised to enable LayoutLLM to focus on\nregions relevant to the question and generate accurate answers. LayoutCoT is\neffective for boosting the performance of document understanding. Meanwhile, it\nbrings a certain degree of interpretability, which could facilitate manual\ninspection and correction. Experiments on standard benchmarks show that the\nproposed LayoutLLM significantly outperforms existing methods that adopt\nopen-source 7B LLMs/MLLMs for document understanding. The training data of the\nLayoutLLM is publicly available at\nhttps://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/DocumentUnderstanding/LayoutLLM\n', '  Recent advances in training large language models (LLMs) using massive\namounts of solely textual data lead to strong generalization across many\ndomains and tasks, including document-specific tasks. Opposed to that there is\na trend to train multi-modal transformer architectures tailored for document\nunderstanding that are designed specifically to fuse textual inputs with the\ncorresponding document layout. This involves a separate fine-tuning step for\nwhich additional training data is required. At present, no document\ntransformers with comparable generalization to LLMs are available That raises\nthe question which type of model is to be preferred for document understanding\ntasks. In this paper we investigate the possibility to use purely text-based\nLLMs for document-specific tasks by using layout enrichment. We explore drop-in\nmodifications and rule-based methods to enrich purely textual LLM prompts with\nlayout information. In our experiments we investigate the effects on the\ncommercial ChatGPT model and the open-source LLM Solar. We demonstrate that\nusing our approach both LLMs show improved performance on various standard\ndocument benchmarks. In addition, we study the impact of noisy OCR and layout\nerrors, as well as the limitations of LLMs when it comes to utilizing document\nlayout. Our results indicate that layout enrichment can improve the performance\nof purely text-based LLMs for document understanding by up to 15% compared to\njust using plain document text. In conclusion, this approach should be\nconsidered for the best model choice between text-based LLM or multi-modal\ndocument transformers.\n']",Document Understanding with Multimodal and Layout-Aware Language Models,Document Understanding with LayoutLLM
20,"Temporal Knowledge Graph Question Answering , Temporal Knowledge Graph Reasoning","['temporal', 'timelines', 'answering', 'timebench', 'timeline', 'reasoning', 'timeml', 'knowledge', 'questions', 'answers'] , ['timegraphs', 'temporal', 'chronological', 'future', 'prediction', 'knowledge', 'relational', 'predict', 'timestamps', 'reasoning']","[""  Temporal knowledge graph question answering (TKGQA) poses a significant\nchallenge task, due to the temporal constraints hidden in questions and the\nanswers sought from dynamic structured knowledge. Although large language\nmodels (LLMs) have made considerable progress in their reasoning ability over\nstructured data, their application to the TKGQA task is a relatively unexplored\narea. This paper first proposes a novel generative temporal knowledge graph\nquestion answering framework, GenTKGQA, which guides LLMs to answer temporal\nquestions through two phases: Subgraph Retrieval and Answer Generation. First,\nwe exploit LLM's intrinsic knowledge to mine temporal constraints and\nstructural links in the questions without extra training, thus narrowing down\nthe subgraph search space in both temporal and structural dimensions. Next, we\ndesign virtual knowledge indicators to fuse the graph neural network signals of\nthe subgraph and the text representations of the LLM in a non-shallow way,\nwhich helps the open-source LLM deeply understand the temporal order and\nstructural dependencies among the retrieved facts through instruction tuning.\nExperimental results on two widely used datasets demonstrate the superiority of\nour model.\n"", '  Recently, Large Language Models (LLMs) have demonstrated great potential in\nvarious data mining tasks, such as knowledge question answering, mathematical\nreasoning, and commonsense reasoning. However, the reasoning capability of LLMs\non temporal event forecasting has been under-explored. To systematically\ninvestigate their abilities in temporal event forecasting, we conduct a\ncomprehensive evaluation of LLM-based methods for temporal event forecasting.\nDue to the lack of a high-quality dataset that involves both graph and textual\ndata, we first construct a benchmark dataset, named MidEast-TE-mini. Based on\nthis dataset, we design a series of baseline methods, characterized by various\ninput formats and retrieval augmented generation(RAG) modules. From extensive\nexperiments, we find that directly integrating raw texts into the input of LLMs\ndoes not enhance zero-shot extrapolation performance. In contrast,\nincorporating raw texts in specific complex events and fine-tuning LLMs\nsignificantly improves performance. Moreover, enhanced with retrieval modules,\nLLM can effectively capture temporal relational patterns hidden in historical\nevents. Meanwhile, issues such as popularity bias and the long-tail problem\nstill persist in LLMs, particularly in the RAG-based method. These findings not\nonly deepen our understanding of LLM-based event forecasting methods but also\nhighlight several promising research directions.We consider that this\ncomprehensive evaluation, along with the identified research opportunities,\nwill significantly contribute to future research on temporal event forecasting\nthrough LLMs.\n', ""  Knowledge in the real world is being updated constantly. However, it is\ncostly to frequently update large language models (LLMs). Therefore, it is\ncrucial for LLMs to understand the concept of temporal knowledge. However,\nprior works on temporal question answering (TQA) did not emphasize multi-answer\nand multi-hop types of temporal reasoning. In this paper, we propose a complex\ntemporal question-answering dataset Complex-TR that focuses on multi-answer and\nmulti-hop temporal reasoning. Besides, we also propose a novel data\naugmentation strategy to improve the complex temporal reasoning capability and\nrobustness of LLMs. We conducted experiments on multiple temporal QA datasets.\nExperimental results show that our method is able to improve LLMs' performance\non temporal QA benchmarks by significant margins. Our code and data are\nreleased at: https://github.com/nusnlp/complex-tr.\n""] , ['  Temporal Knowledge Graph (TKG), which characterizes temporally evolving facts\nin the form of (subject, relation, object, timestamp), has attracted much\nattention recently. TKG reasoning aims to predict future facts based on given\nhistorical ones. However, existing TKG reasoning models are unable to abstain\nfrom predictions they are uncertain, which will inevitably bring risks in\nreal-world applications. Thus, in this paper, we propose an abstention\nmechanism for TKG reasoning, which helps the existing models make selective,\ninstead of indiscriminate, predictions. Specifically, we develop a confidence\nestimator, called Confidence Estimator with History (CEHis), to enable the\nexisting TKG reasoning models to first estimate their confidence in making\npredictions, and then abstain from those with low confidence. To do so, CEHis\ntakes two kinds of information into consideration, namely, the certainty of the\ncurrent prediction and the accuracy of historical predictions. Experiments with\nrepresentative TKG reasoning models on two benchmark datasets demonstrate the\neffectiveness of the proposed CEHis.\n', '  Temporal Knowledge Graph (TKG) forecasting aims to predict future facts based\non given histories. Most recent graph-based models excel at capturing\nstructural information within TKGs but lack semantic comprehension abilities.\nNowadays, with the surge of LLMs, the LLM-based TKG prediction model has\nemerged. However, the existing LLM-based model exhibits three shortcomings: (1)\nIt only focuses on the first-order history for prediction while ignoring\nhigh-order historical information, resulting in the provided information for\nLLMs being extremely limited. (2) LLMs struggle with optimal reasoning\nperformance under heavy historical information loads. (3) For TKG prediction,\nthe temporal reasoning capability of LLM alone is limited. To address the first\ntwo challenges, we propose Chain-of-History (CoH) reasoning which explores\nhigh-order histories step-by-step, achieving effective utilization of\nhigh-order historical information for LLMs on TKG prediction. To address the\nthird issue, we design CoH as a plug-and-play module to enhance the performance\nof graph-based models for TKG prediction. Extensive experiments on three\ndatasets and backbones demonstrate the effectiveness of CoH.\n', '  Temporal Knowledge Graph (TKG) reasoning that forecasts future events based\non historical snapshots distributed over timestamps is denoted as extrapolation\nand has gained significant attention. Owing to its extreme versatility and\nvariation in spatial and temporal correlations, TKG reasoning presents a\nchallenging task, demanding efficient capture of concurrent structures and\nevolutional interactions among facts. While existing methods have made strides\nin this direction, they still fall short of harnessing the diverse forms of\nintrinsic expressive semantics of TKGs, which encompass entity correlations\nacross multiple timestamps and periodicity of temporal information. This\nlimitation constrains their ability to thoroughly reflect historical\ndependencies and future trends. In response to these drawbacks, this paper\nproposes an innovative reasoning approach that focuses on Learning Multi-graph\nStructure (LMS). Concretely, it comprises three distinct modules concentrating\non multiple aspects of graph structure knowledge within TKGs, including\nconcurrent and evolutional patterns along timestamps, query-specific\ncorrelations across timestamps, and semantic dependencies of timestamps, which\ncapture TKG features from various perspectives. Besides, LMS incorporates an\nadaptive gate for merging entity representations both along and across\ntimestamps effectively. Moreover, it integrates timestamp semantics into graph\nattention calculations and time-aware decoders, in order to impose temporal\nconstraints on events and narrow down prediction scopes with historical\nstatistics. Extensive experimental results on five event-based benchmark\ndatasets demonstrate that LMS outperforms state-of-the-art extrapolation\nmodels, indicating the superiority of modeling a multi-graph perspective for\nTKG reasoning.\n']",Temporal Knowledge Graph Reasoning and Question Answering,Temporal Knowledge Graph Reasoning
21,"Table Question Answering and Reasoning , Knowledge Base Question Answering (KBQA) , ""Query Answering on Knowledge Graphs"" , Knowledge Graph Question Answering , ""Question Answering and Comprehension"" , Multi-Hop Question Answering and Reasoning , Knowledge Graph Question Answering","['tables', 'tablellm', 'table', 'tableinstruct', 'tabular', 'texttableqa', 'sql', 'spreadsheets', 'queries', 'spreadsheetbench'] , ['semantic', 'parsing', 'sparql', 'knowledge', 'answering', 'retrieval', 'parse', 'answerability', 'unanswerability', 'schemas'] , ['querysets', 'queries', 'relational', 'entities', 'entity', 'query2gmm', 'query', 'relations', 'databases', 'knowledge'] , ['knowledge', 'answering', 'reasoninglm', 'graphqa', 'subgraph', 'textual', 'reasoning', 'graphs', 'answers', 'learnersourced'] , ['answerability', 'answering', 'questions', 'annotators', 'retrieval', 'comprehension', 'responses', 'answers', 'questionnaires', 'texts'] , ['inferences', 'answering', 'reasoning', 'questions', 'retrieval', 'knowledge', 'answers', 'hop', 'language', 'contexts'] , ['nl2gql', 'semantic', 'retrieval', 'knowledge', 'textual', 'answering', 'knowledgenavigator', 'gql', 'entities', 'queries']","[""  Table question answering is a popular task that assesses a model's ability to\nunderstand and interact with structured data. However, the given table often\ndoes not contain sufficient information for answering the question,\nnecessitating the integration of external knowledge. Existing methods either\nconvert both the table and external knowledge into text, which neglects the\nstructured nature of the table; or they embed queries for external sources in\nthe interaction with the table, which complicates the process. In this paper,\nwe propose a simple yet effective method to integrate external information in a\ngiven table. Our method first constructs an augmenting table containing the\nmissing information and then generates a SQL query over the two tables to\nanswer the question. Experiments show that our method outperforms strong\nbaselines on three table QA benchmarks. Our code is publicly available at\nhttps://github.com/UCSB-NLP-Chang/Augment_tableQA.\n"", '  Table reasoning, which aims to generate the corresponding answer to the\nquestion following the user requirement according to the provided table, and\noptionally a text description of the table, effectively improving the\nefficiency of obtaining information. Recently, using Large Language Models\n(LLMs) has become the mainstream method for table reasoning, because it not\nonly significantly reduces the annotation cost but also exceeds the performance\nof previous methods. However, existing research still lacks a summary of\nLLM-based table reasoning works. Due to the existing lack of research,\nquestions about which techniques can improve table reasoning performance in the\nera of LLMs, why LLMs excel at table reasoning, and how to enhance table\nreasoning abilities in the future, remain largely unexplored. This gap\nsignificantly limits progress in research. To answer the above questions and\nadvance table reasoning research with LLMs, we present this survey to analyze\nexisting research, inspiring future work. In this paper, we analyze the\nmainstream techniques used to improve table reasoning performance in the LLM\nera, and the advantages of LLMs compared to pre-LLMs for solving table\nreasoning. We provide research directions from both the improvement of existing\nmethods and the expansion of practical applications to inspire future research.\n', '  Tables, typically two-dimensional and structured to store large amounts of\ndata, are essential in daily activities like database queries, spreadsheet\nmanipulations, web table question answering, and image table information\nextraction. Automating these table-centric tasks with Large Language Models\n(LLMs) or Visual Language Models (VLMs) offers significant public benefits,\ngarnering interest from academia and industry. This survey provides a\ncomprehensive overview of table-related tasks, examining both user scenarios\nand technical aspects. It covers traditional tasks like table question\nanswering as well as emerging fields such as spreadsheet manipulation and table\ndata analysis. We summarize the training techniques for LLMs and VLMs tailored\nfor table processing. Additionally, we discuss prompt engineering, particularly\nthe use of LLM-powered agents, for various table-related tasks. Finally, we\nhighlight several challenges, including processing implicit user intentions and\nextracting information from various table sources.\n'] , ['  Recent work integrating Large Language Models (LLMs) has led to significant\nimprovements in the Knowledge Base Question Answering (KBQA) task. However, we\nposit that existing KBQA datasets that either have simple questions, use\nsynthetically generated logical forms, or are based on small knowledge base\n(KB) schemas, do not capture the true complexity of KBQA tasks.\n  To address this, we introduce the SPINACH dataset, an expert-annotated KBQA\ndataset collected from forum discussions on Wikidata\'s ""Request a Query"" forum\nwith 320 decontextualized question-SPARQL pairs. Much more complex than\nexisting datasets, SPINACH calls for strong KBQA systems that do not rely on\ntraining data to learn the KB schema, but can dynamically explore large and\noften incomplete schemas and reason about them.\n  Along with the dataset, we introduce the SPINACH agent, a new KBQA approach\nthat mimics how a human expert would write SPARQLs for such challenging\nquestions. Experiments on existing datasets show SPINACH\'s capability in KBQA,\nachieving a new state of the art on the QALD-7, QALD-9 Plus and QALD-10\ndatasets by 30.1%, 27.0%, and 10.0% in F1, respectively, and coming within 1.6%\nof the fine-tuned LLaMA SOTA model on WikiWebQuestions. On our new SPINACH\ndataset, SPINACH agent outperforms all baselines, including the best\nGPT-4-based KBQA agent, by 38.1% in F1.\n', '  Knowledge Base Question Answering (KBQA) aims to answer natural language\nquestions over large-scale knowledge bases (KBs), which can be summarized into\ntwo crucial steps: knowledge retrieval and semantic parsing. However, three\ncore challenges remain: inefficient knowledge retrieval, mistakes of retrieval\nadversely impacting semantic parsing, and the complexity of previous KBQA\nmethods. To tackle these challenges, we introduce ChatKBQA, a novel and simple\ngenerate-then-retrieve KBQA framework, which proposes first generating the\nlogical form with fine-tuned LLMs, then retrieving and replacing entities and\nrelations with an unsupervised retrieval method, to improve both generation and\nretrieval more directly. Experimental results show that ChatKBQA achieves new\nstate-of-the-art performance on standard KBQA datasets, WebQSP, and CWQ. This\nwork can also be regarded as a new paradigm for combining LLMs with knowledge\ngraphs (KGs) for interpretable and knowledge-required question answering. Our\ncode is publicly available.\n', '  Existing Knowledge Base Question Answering (KBQA) architectures are hungry\nfor annotated data, which make them costly and time-consuming to deploy. We\nintroduce the problem of few-shot transfer learning for KBQA, where the target\ndomain offers only a few labeled examples, but a large labeled training dataset\nis available in a source domain. We propose a novel KBQA architecture called\nFuSIC-KBQA that performs KB-retrieval using multiple source-trained retrievers,\nre-ranks using an LLM and uses this as input for LLM few-shot in-context\nlearning to generate logical forms. These are further refined using\nexecution-guided feedback. Experiments over multiple source-target KBQA pairs\nof varying complexity show that FuSIC-KBQA significantly outperforms\nadaptations of SoTA KBQA models for this setting. Additional experiments show\nthat FuSIC-KBQA also outperforms SoTA KBQA models in the in-domain setting when\ntraining data is limited.\n'] , ['  Answering logical queries on knowledge graphs (KG) poses a significant\nchallenge for machine reasoning. The primary obstacle in this task stems from\nthe inherent incompleteness of KGs. Existing research has predominantly focused\non addressing the issue of missing edges in KGs, thereby neglecting another\naspect of incompleteness: the emergence of new entities. Furthermore, most of\nthe existing methods tend to reason over each logical operator separately,\nrather than comprehensively analyzing the query as a whole during the reasoning\nprocess. In this paper, we propose a query-aware prompt-fused framework named\nPro-QE, which could incorporate existing query embedding methods and address\nthe embedding of emerging entities through contextual information aggregation.\nAdditionally, a query prompt, which is generated by encoding the symbolic\nquery, is introduced to gather information relevant to the query from a\nholistic perspective. To evaluate the efficacy of our model in the inductive\nsetting, we introduce two new challenging benchmarks. Experimental results\ndemonstrate that our model successfully handles the issue of unseen entities in\nlogical queries. Furthermore, the ablation study confirms the efficacy of the\naggregator and prompt components.\n', '  Answering first-order logical (FOL) queries over knowledge graphs (KG)\nremains a challenging task mainly due to KG incompleteness. Query embedding\napproaches this problem by computing the low-dimensional vector representations\nof entities, relations, and logical queries. KGs exhibit relational patterns\nsuch as symmetry and composition and modeling the patterns can further enhance\nthe performance of query embedding models. However, the role of such patterns\nin answering FOL queries by query embedding models has not been yet studied in\nthe literature. In this paper, we fill in this research gap and empower FOL\nqueries reasoning with pattern inference by introducing an inductive bias that\nallows for learning relation patterns. To this end, we develop a novel query\nembedding method, RoConE, that defines query regions as geometric cones and\nalgebraic query operators by rotations in complex space. RoConE combines the\nadvantages of Cone as a well-specified geometric representation for query\nembedding, and also the rotation operator as a powerful algebraic operation for\npattern inference. Our experimental results on several benchmark datasets\nconfirm the advantage of relational patterns for enhancing logical query\nanswering task.\n', '  Complex logical query answering is a challenging task in knowledge graphs\n(KGs) that has been widely studied. The ability to perform complex logical\nreasoning is essential and supports various graph reasoning-based downstream\ntasks, such as search engines. Recent approaches are proposed to represent KG\nentities and logical queries into embedding vectors and find answers to logical\nqueries from the KGs. However, existing proposed methods mainly focus on\nquerying a single KG and cannot be applied to multiple graphs. In addition,\ndirectly sharing KGs with sensitive information may incur privacy risks, making\nit impractical to share and construct an aggregated KG for reasoning to\nretrieve query answers. Thus, it remains unknown how to answer queries on\nmulti-source KGs. An entity can be involved in various knowledge graphs and\nreasoning on multiple KGs and answering complex queries on multi-source KGs is\nimportant in discovering knowledge cross graphs. Fortunately, federated\nlearning is utilized in knowledge graphs to collaboratively learn\nrepresentations with privacy preserved. Federated knowledge graph embeddings\nenrich the relations in knowledge graphs to improve the representation quality.\nHowever, these methods only focus on one-hop relations and cannot perform\ncomplex reasoning tasks. In this paper, we apply federated learning to complex\nquery-answering tasks to reason over multi-source knowledge graphs while\npreserving privacy. We propose a Federated Complex Query Answering framework\n(FedCQA), to reason over multi-source KGs avoiding sensitive raw data\ntransmission to protect privacy. We conduct extensive experiments on three\nreal-world datasets and evaluate retrieval performance on various types of\ncomplex queries.\n'] , ['  The fusion of language models (LMs) and knowledge graphs (KGs) is widely used\nin commonsense question answering, but generating faithful explanations remains\nchallenging. Current methods often overlook path decoding faithfulness, leading\nto divergence between graph encoder outputs and model predictions. We identify\nconfounding effects and LM-KG misalignment as key factors causing spurious\nexplanations. To address this, we introduce the LM-KG Fidelity metric to assess\nKG representation reliability and propose the LM-KG Distribution-aware\nAlignment (\\textit{LKDA}) algorithm to improve explanation faithfulness.\nWithout ground truth, we evaluate KG explanations using the proposed\nFidelity-Sparsity Trade-off Curve. Experiments on CommonsenseQA and OpenBookQA\nshow that LKDA significantly enhances explanation fidelity and model\nperformance, highlighting the need to address distributional misalignment for\nreliable commonsense reasoning.\n', ""  Knowledge Graph (KG) powered question answering (QA) performs complex\nreasoning over language semantics as well as knowledge facts. Graph Neural\nNetworks (GNNs) learn to aggregate information from the underlying KG, which is\ncombined with Language Models (LMs) for effective reasoning with the given\nquestion. However, GNN-based methods for QA rely on the graph information of\nthe candidate answer nodes, which limits their effectiveness in more\nchallenging settings where critical answer information is not included in the\nKG. We propose a simple graph pooling approach that learns useful semantics of\nthe KG that can aid the LM's reasoning and that its effectiveness is robust\nunder graph perturbations. Our method, termed SemPool, represents KG facts with\npre-trained LMs, learns to aggregate their semantic information, and fuses it\nat different layers of the LM. Our experimental results show that SemPool\noutperforms state-of-the-art GNN-based methods by 2.27% accuracy points on\naverage when answer information is missing from the KG. In addition, SemPool\noffers interpretability on what type of graph information is fused at different\nLM layers.\n"", '  Knowledge Graphs (KGs) represent human-crafted factual knowledge in the form\nof triplets (head, relation, tail), which collectively form a graph. Question\nAnswering over KGs (KGQA) is the task of answering natural questions grounding\nthe reasoning to the information provided by the KG. Large Language Models\n(LLMs) are the state-of-the-art models for QA tasks due to their remarkable\nability to understand natural language. On the other hand, Graph Neural\nNetworks (GNNs) have been widely used for KGQA as they can handle the complex\ngraph information stored in the KG. In this work, we introduce GNN-RAG, a novel\nmethod for combining language understanding abilities of LLMs with the\nreasoning abilities of GNNs in a retrieval-augmented generation (RAG) style.\nFirst, a GNN reasons over a dense KG subgraph to retrieve answer candidates for\na given question. Second, the shortest paths in the KG that connect question\nentities and answer candidates are extracted to represent KG reasoning paths.\nThe extracted paths are verbalized and given as input for LLM reasoning with\nRAG. In our GNN-RAG framework, the GNN acts as a dense subgraph reasoner to\nextract useful graph information, while the LLM leverages its natural language\nprocessing ability for ultimate KGQA. Furthermore, we develop a retrieval\naugmentation (RA) technique to further boost KGQA performance with GNN-RAG.\nExperimental results show that GNN-RAG achieves state-of-the-art performance in\ntwo widely used KGQA benchmarks (WebQSP and CWQ), outperforming or matching\nGPT-4 performance with a 7B tuned LLM. In addition, GNN-RAG excels on multi-hop\nand multi-entity questions outperforming competing approaches by 8.9--15.5%\npoints at answer F1.\n'] , ['  Long-form question answering (LFQA) aims to provide thorough and in-depth\nanswers to complex questions, enhancing comprehension. However, such detailed\nresponses are prone to hallucinations and factual inconsistencies, challenging\ntheir faithful evaluation. This work introduces HaluQuestQA, the first\nhallucination dataset with localized error annotations for human-written and\nmodel-generated LFQA answers. HaluQuestQA comprises 698 QA pairs with 4.7k\nspan-level error annotations for five different error types by expert\nannotators, along with preference judgments. Using our collected data, we\nthoroughly analyze the shortcomings of long-form answers and find that they\nlack comprehensiveness and provide unhelpful references. We train an automatic\nfeedback model on this dataset that predicts error spans with incomplete\ninformation and provides associated explanations. Finally, we propose a\nprompt-based approach, Error-informed refinement, that uses signals from the\nlearned feedback model to refine generated answers, which we show reduces\nhallucination and improves answer quality. Furthermore, humans find answers\ngenerated by our approach comprehensive and highly prefer them (84%) over the\nbaseline answers.\n', '  Document-based Question-Answering (QA) tasks are crucial for precise\ninformation retrieval. While some existing work focus on evaluating large\nlanguage models performance on retrieving and answering questions from\ndocuments, assessing the LLMs performance on QA types that require exact answer\nselection from predefined options and numerical extraction is yet to be fully\nassessed. In this paper, we specifically focus on this underexplored context\nand conduct empirical analysis of LLMs (GPT-4 and GPT-3.5) on question types,\nincluding single-choice, yes-no, multiple-choice, and number extraction\nquestions from documents in zero-shot setting. We use the CogTale dataset for\nevaluation, which provide human expert-tagged responses, offering a robust\nbenchmark for precision and factual grounding. We found that LLMs, particularly\nGPT-4, can precisely answer many single-choice and yes-no questions given\nrelevant context, demonstrating their efficacy in information retrieval tasks.\nHowever, their performance diminishes when confronted with multiple-choice and\nnumber extraction formats, lowering the overall performance of the model on\nthis task, indicating that these models may not yet be sufficiently reliable\nfor the task. This limits the applications of LLMs on applications demanding\nprecise information extraction from documents, such as meta-analysis tasks.\nThese findings hinge on the assumption that the retrievers furnish pertinent\ncontext necessary for accurate responses, emphasizing the need for further\nresearch. Our work offers a framework for ongoing dataset evaluation, ensuring\nthat LLM applications for information retrieval and document analysis continue\nto meet evolving standards.\n', ""  Using questions in written text is an effective strategy to enhance\nreadability. However, what makes an active reading question good, what the\nlinguistic role of these questions is, and what is their impact on human\nreading remains understudied. We introduce GuidingQ, a dataset of 10K in-text\nquestions from textbooks and scientific articles. By analyzing the dataset, we\npresent a comprehensive understanding of the use, distribution, and linguistic\ncharacteristics of these questions. Then, we explore various approaches to\ngenerate such questions using language models. Our results highlight the\nimportance of capturing inter-question relationships and the challenge of\nquestion position identification in generating these questions. Finally, we\nconduct a human study to understand the implication of such questions on\nreading comprehension. We find that the generated questions are of high quality\nand are almost as effective as human-written questions in terms of improving\nreaders' memorization and comprehension.\n""] , ['  In response to the increasing use of interactive artificial intelligence, the\ndemand for the capacity to handle complex questions has increased. Multi-hop\nquestion generation aims to generate complex questions that requires multi-step\nreasoning over several documents. Previous studies have predominantly utilized\nend-to-end models, wherein questions are decoded based on the representation of\ncontext documents. However, these approaches lack the ability to explain the\nreasoning process behind the generated multi-hop questions. Additionally, the\nquestion rewriting approach, which incrementally increases the question\ncomplexity, also has limitations due to the requirement of labeling data for\nintermediate-stage questions. In this paper, we introduce an end-to-end\nquestion rewriting model that increases question complexity through sequential\nrewriting. The proposed model has the advantage of training with only the final\nmulti-hop questions, without intermediate questions. Experimental results\ndemonstrate the effectiveness of our model in generating complex questions,\nparticularly 3- and 4-hop questions, which are appropriately paired with input\nanswers. We also prove that our model logically and incrementally increases the\ncomplexity of questions, and the generated multi-hop questions are also\nbeneficial for training question answering models.\n', ""  Large language models (LLMs) demonstrate strong reasoning abilities when\nprompted to generate chain-of-thought (CoT) explanations alongside answers.\nHowever, previous research on evaluating LLMs has solely focused on answer\naccuracy, neglecting the correctness of the generated CoT. In this paper, we\ndelve deeper into the CoT reasoning capabilities of LLMs in multi-hop question\nanswering by utilizing knowledge graphs (KGs). We propose a novel\ndiscriminative and generative CoT evaluation paradigm to assess LLMs' knowledge\nof reasoning and the accuracy of the generated CoT. Through experiments\nconducted on 5 different families of LLMs across 2 multi-hop question-answering\ndatasets, we find that LLMs possess sufficient knowledge to perform reasoning.\nHowever, there exists a significant disparity between answer accuracy and\nfaithfulness of the CoT reasoning generated by LLMs, indicating that they often\narrive at correct answers through incorrect reasoning.\n"", '  Most existing multi-hop datasets are extractive answer datasets, where the\nanswers to the questions can be extracted directly from the provided context.\nThis often leads models to use heuristics or shortcuts instead of performing\ntrue multi-hop reasoning. In this paper, we propose a new multi-hop dataset,\nMoreHopQA, which shifts from extractive to generative answers. Our dataset is\ncreated by utilizing three existing multi-hop datasets: HotpotQA,\n2WikiMultihopQA, and MuSiQue. Instead of relying solely on factual reasoning,\nwe enhance the existing multi-hop questions by adding another layer of\nquestioning that involves one, two, or all three of the following types of\nreasoning: commonsense, arithmetic, and symbolic. Our dataset is created\nthrough a semi-automated process, resulting in a dataset with 1,118 samples\nthat have undergone human verification. We then use our dataset to evaluate\nfive different large language models: Mistral 7B, Gemma 7B, Llama 3 (8B and\n70B), and GPT-4. We also design various cases to analyze the reasoning steps in\nthe question-answering process. Our results show that models perform well on\ninitial multi-hop questions but struggle with our extended questions,\nindicating that our dataset is more challenging than previous ones. Our\nanalysis of question decomposition reveals that although models can correctly\nanswer questions, only a portion - 38.7% for GPT-4 and 33.4% for Llama3-70B -\nachieve perfect reasoning, where all corresponding sub-questions are answered\ncorrectly. Evaluation code and data are available at\nhttps://github.com/Alab-NII/morehopqa\n'] , ['  Knowledge graphs (KGs) are large datasets with specific structures\nrepresenting large knowledge bases (KB) where each node represents a key entity\nand relations amongst them are typed edges. Natural language queries formed to\nextract information from a KB entail starting from specific nodes and reasoning\nover multiple edges of the corresponding KG to arrive at the correct set of\nanswer nodes. Traditional approaches of question answering on KG are based on\n(a) semantic parsing (SP), where a logical form (e.g., S-expression, SPARQL\nquery, etc.) is generated using node and edge embeddings and then reasoning\nover these representations or tuning language models to generate the final\nanswer directly, or (b) information-retrieval based that works by extracting\nentities and relations sequentially. In this work, we evaluate the capability\nof (LLMs) to answer questions over KG that involve multiple hops. We show that\ndepending upon the size and nature of the KG we need different approaches to\nextract and feed the relevant information to an LLM since every LLM comes with\na fixed context window. We evaluate our approach on six KGs with and without\nthe availability of example-specific sub-graphs and show that both the IR and\nSP-based methods can be adopted by LLMs resulting in an extremely competitive\nperformance.\n', '  Ensuring factual accuracy while maintaining the creative capabilities of\nLarge Language Model Agents (LMAs) poses significant challenges in the\ndevelopment of intelligent agent systems. LMAs face prevalent issues such as\ninformation hallucinations, catastrophic forgetting, and limitations in\nprocessing long contexts when dealing with knowledge-intensive tasks. This\npaper introduces a KG-RAG (Knowledge Graph-Retrieval Augmented Generation)\npipeline, a novel framework designed to enhance the knowledge capabilities of\nLMAs by integrating structured Knowledge Graphs (KGs) with the functionalities\nof LLMs, thereby significantly reducing the reliance on the latent knowledge of\nLLMs. The KG-RAG pipeline constructs a KG from unstructured text and then\nperforms information retrieval over the newly created graph to perform KGQA\n(Knowledge Graph Question Answering). The retrieval methodology leverages a\nnovel algorithm called Chain of Explorations (CoE) which benefits from LLMs\nreasoning to explore nodes and relationships within the KG sequentially.\nPreliminary experiments on the ComplexWebQuestions dataset demonstrate notable\nimprovements in the reduction of hallucinated content and suggest a promising\npath toward developing intelligent systems adept at handling\nknowledge-intensive tasks.\n', ""  To address the issue of insufficient knowledge and the tendency to generate\nhallucination in Large Language Models (LLMs), numerous studies have endeavored\nto integrate LLMs with Knowledge Graphs (KGs). However, all these methods are\nevaluated on conventional Knowledge Graph Question Answering (KGQA) with\ncomplete KGs, where the factual triples involved in each question are entirely\ncovered by the given KG. In this situation, LLM mainly acts as an agent to find\nanswer entities by exploring the KG, rather than effectively integrating\ninternal and external knowledge sources. However, in real-world scenarios, KGs\nare often incomplete to cover all the knowledge required to answer questions.\nTo simulate real-world scenarios and evaluate the ability of LLMs to integrate\ninternal and external knowledge, in this paper, we propose leveraging LLMs for\nQA under Incomplete Knowledge Graph (IKGQA), where the given KG doesn't include\nall the factual triples involved in each question. To handle IKGQA, we propose\na training-free method called Generate-on-Graph (GoG) that can generate new\nfactual triples while exploring on KGs. Specifically, we propose a\nselecting-generating-answering framework, which not only treat the LLM as an\nagent to explore on KGs, but also treat it as a KG to generate new facts based\non the explored subgraph and its inherent knowledge. Experimental results on\ntwo datasets demonstrate that our GoG can solve IKGQA to a certain extent,\nwhile almost all previous methods cannot perform well on IKGQA.\n""]",Question Answering and Reasoning Systems,Knowledge Graph Question Answering
22,"Formal Reasoning and Satisfiability in Logic , ""Mathematical Proof Verification with Large Language Models"" , Graph Reasoning and Problem-Solving with Language Models , ""Knowledge Graphs and Large Language Models for Reasoning"" , Relational Neural Networks for Reasoning","['satisfiability', 'semantics', 'unsatisfiable', 'logics', 'propositional', 'solvers', 'logic', 'constraints', 'conjunctive', 'clauses'] , ['proofnet', 'formalizations', 'provers', 'prover', 'deductive', 'formalization', 'formalized', 'verifiable', 'proofs', 'solvers'] , ['graphinstruct', 'instructgraph', 'graphreader', 'graphwiz', 'graphlm', 'graphtoken', 'graphs', 'grapheval2000', 'nlgraph', 'graphtranslator'] , ['knowledge', 'relational', 'reasoning', 'inference', 'entities', 'logic', 'language', 'entity', 'queries', 'relations'] , ['relational', 'neural', 'relations', 'inference', 'embeddings', 'database', 'reasoning', 'gmpnn', 'algorithmic', 'logic']","['  We formulate discussion graph semantics of first-order logic with equality\nfor reasoning about discussion and argumentation as naturally as we would\nreason about sentences. While there are a few existing proposals to use a\nformal logic for reasoning about argumentation, they are constructed bottom-up\nand specialised to the argumentation model by Dung. There is indeed a\nconspicuous lack of a formal reasoning framework for handling general\ndiscussion and argumentation models. We achieve the generality through a\ntop-down formulation of the semantics of first-order logic (with equality)\nformulas, addressing the current shortage.\n', '  The data-complexity of both satisfiability and finite satisfiability for the\ntwo-variable fragment with counting is NP-complete; the data-complexity of both\nquery-answering and finite query-answering for the two-variable guarded\nfragment with counting is co-NP-complete.\n', '  Boolean satisfiability (SAT) problems are routinely solved by SAT solvers in\nreal-life applications, yet solving time can vary drastically between solvers\nfor the same instance. This has motivated research into machine learning models\nthat can predict, for a given SAT instance, which solver to select among\nseveral options. Existing SAT solver selection methods all rely on some\nhand-picked instance features, which are costly to compute and ignore the\nstructural information in SAT graphs. In this paper we present GraSS, a novel\napproach for automatic SAT solver selection based on tripartite graph\nrepresentations of instances and a heterogeneous graph neural network (GNN)\nmodel. While GNNs have been previously adopted in other SAT-related tasks, they\ndo not incorporate any domain-specific knowledge and ignore the runtime\nvariation introduced by different clause orders. We enrich the graph\nrepresentation with domain-specific decisions, such as novel node feature\ndesign, positional encodings for clauses in the graph, a GNN architecture\ntailored to our tripartite graphs and a runtime-sensitive loss function.\nThrough extensive experiments, we demonstrate that this combination of raw\nrepresentations and domain-specific choices leads to improvements in runtime\nfor a pool of seven state-of-the-art solvers on both an industrial circuit\ndesign benchmark, and on instances from the 20-year Anniversary Track of the\n2022 SAT Competition.\n'] , ['  Proof assistants like Lean have revolutionized mathematical proof\nverification, ensuring high accuracy and reliability. Although large language\nmodels (LLMs) show promise in mathematical reasoning, their advancement in\nformal theorem proving is hindered by a lack of training data. To address this\nissue, we introduce an approach to generate extensive Lean 4 proof data derived\nfrom high-school and undergraduate-level mathematical competition problems.\nThis approach involves translating natural language problems into formal\nstatements, filtering out low-quality statements, and generating proofs to\ncreate synthetic data. After fine-tuning the DeepSeekMath 7B model on this\nsynthetic dataset, which comprises 8 million formal statements with proofs, our\nmodel achieved whole-proof generation accuracies of 46.3% with 64 samples and\n52% cumulatively on the Lean 4 miniF2F test, surpassing the baseline GPT-4 at\n23.0% with 64 samples and a tree search reinforcement learning method at 41.0%.\nAdditionally, our model successfully proved 5 out of 148 problems in the Lean 4\nFormalized International Mathematical Olympiad (FIMO) benchmark, while GPT-4\nfailed to prove any. These results demonstrate the potential of leveraging\nlarge-scale synthetic data to enhance theorem-proving capabilities in LLMs.\nBoth the synthetic dataset and the model will be made available to facilitate\nfurther research in this promising field.\n', '  Theorem proving is an important challenge for large language models (LLMs),\nas formal proofs can be checked rigorously by proof assistants such as Lean,\nleaving no room for hallucination. Existing LLM-based provers try to prove\ntheorems in a fully autonomous mode without human intervention. In this mode,\nthey struggle with novel and challenging theorems, for which human insights may\nbe critical. In this paper, we explore LLMs as copilots that assist humans in\nproving theorems. We introduce Lean Copilot, a framework for running LLM\ninference in Lean. It enables programmers to build various LLM-based proof\nautomation tools that integrate seamlessly into the workflow of Lean users.\nUsing Lean Copilot, we build tools for suggesting proof steps (tactic\nsuggestion), completing intermediate proof goals (proof search), and selecting\nrelevant premises (premise selection) using LLMs. Users can use our pretrained\nmodels or bring their own ones that run either locally (with or without GPUs)\nor on the cloud. Experimental results demonstrate the effectiveness of our\nmethod in assisting humans and automating theorem proving process compared to\nexisting rule-based proof automation in Lean. We open source all codes under a\npermissive MIT license to facilitate further research.\n', ""  Traditional language model-based theorem proving assumes that by training on\na sufficient amount of formal proof data, a model will learn to prove theorems.\nOur key observation is that a wealth of informal information that is not\npresent in formal proofs can be useful for learning to prove theorems. For\ninstance, humans think through steps of a proof, but this thought process is\nnot visible in the resulting code. We present Lean-STaR, a framework for\ntraining language models to produce informal thoughts prior to each step of a\nproof, thereby boosting the model's theorem-proving capabilities. Lean-STaR\nuses retrospective ground-truth tactics to generate synthetic thoughts for\ntraining the language model. At inference time, the trained model directly\ngenerates the thoughts prior to the prediction of the tactics in each proof\nstep. Building on the self-taught reasoner framework, we then apply expert\niteration to further fine-tune the model on the correct proofs it samples and\nverifies using the Lean solver. Lean-STaR achieves state-of-the-art results on\nthe miniF2F-test benchmark within the Lean theorem proving environment,\nsignificantly outperforming base models ($\\boldsymbol{43.4\\% \\rightarrow\n46.3\\%,}$ Pass@64). We also analyze the impact of the augmented thoughts on\nvarious aspects of the theorem proving process, providing insights into their\neffectiveness.\n""] , [""  Large language models (LLMs) have achieved impressive success across several\nfields, but their proficiency in understanding and resolving complex graph\nproblems is less explored. To bridge this gap, we introduce GraphInstruct, a\nnovel and comprehensive instruction-tuning dataset designed to equip language\nmodels with the ability to tackle a broad spectrum of graph problems using\nexplicit reasoning paths. Utilizing GraphInstruct, we build GraphWiz, an\nopen-source language model capable of resolving various graph problem types\nwhile generating clear reasoning processes. To enhance the model's capability\nand reliability, we incorporate the Direct Preference Optimization (DPO)\nframework into the graph problem-solving context. The enhanced model,\nGraphWiz-DPO, achieves an average accuracy of 65% across nine tasks with\ndifferent complexity levels, surpassing GPT-4 which has an average accuracy of\n43.8%. Moreover, our research delves into the delicate balance between training\ndata volume and model performance, highlighting the potential for overfitting\nwith increased data. We also explore the transferability of the model's\nreasoning ability across different graph tasks, indicating the model's\nadaptability and practical application potential. Our investigation offers a\nnew blueprint and valuable insights for developing LLMs specialized in graph\nreasoning and problem-solving.\n"", '  Evaluating and enhancing the general capabilities of large language models\n(LLMs) has been an important research topic. Graph is a common data structure\nin the real world, and understanding graph data is a crucial part for advancing\ngeneral intelligence. To evaluate and enhance the graph understanding abilities\nof LLMs, in this paper, we propose a benchmark named GraphInstruct, which\ncomprehensively includes 21 classical graph reasoning tasks, providing diverse\ngraph generation pipelines and detailed reasoning steps. Based on\nGraphInstruct, we further construct GraphLM through efficient\ninstruction-tuning, which shows prominent graph understanding capability. In\norder to enhance the LLM with graph reasoning capability as well, we propose a\nstep mask training strategy, and construct a model named GraphLM+. As one of\nthe pioneering efforts to enhance the graph understanding and reasoning\nabilities of LLMs, extensive experiments have demonstrated the superiority of\nGraphLM and GraphLM+ over other LLMs. We look forward to more researchers\nexploring the potential of LLMs in the graph data mining domain through\nGraphInstruct. Our code for generating GraphInstruct is released publicly at:\nhttps://github.com/CGCL-codes/GraphInstruct.\n', '  Large language models (LLMs) are increasingly adopted for a variety of tasks\nwith implicit graphical structures, such as planning in robotics, multi-hop\nquestion answering or knowledge probing, structured commonsense reasoning, and\nmore. While LLMs have advanced the state-of-the-art on these tasks with\nstructure implications, whether LLMs could explicitly process textual\ndescriptions of graphs and structures, map them to grounded conceptual spaces,\nand perform structured operations remains underexplored. To this end, we\npropose NLGraph (Natural Language Graph), a comprehensive benchmark of\ngraph-based problem solving designed in natural language. NLGraph contains\n29,370 problems, covering eight graph reasoning tasks with varying complexity\nfrom simple tasks such as connectivity and shortest path up to complex problems\nsuch as maximum flow and simulating graph neural networks. We evaluate LLMs\n(GPT-3/4) with various prompting approaches on the NLGraph benchmark and find\nthat 1) language models do demonstrate preliminary graph reasoning abilities,\n2) the benefit of advanced prompting and in-context learning diminishes on more\ncomplex graph problems, while 3) LLMs are also (un)surprisingly brittle in the\nface of spurious correlations in graph and problem settings. We then propose\nBuild-a-Graph Prompting and Algorithmic Prompting, two instruction-based\napproaches to enhance LLMs in solving natural language graph problems.\nBuild-a-Graph and Algorithmic prompting improve the performance of LLMs on\nNLGraph by 3.07% to 16.85% across multiple tasks and settings, while how to\nsolve the most complicated graph reasoning tasks in our setup with language\nmodels remains an open research question. The NLGraph benchmark and evaluation\ncode are available at https://github.com/Arthur-Heng/NLGraph.\n'] , [""  This survey investigates the synergistic relationship between Large Language\nModels (LLMs) and Knowledge Graphs (KGs), which is crucial for advancing AI's\ncapabilities in understanding, reasoning, and language processing. It aims to\naddress gaps in current research by exploring areas such as KG Question\nAnswering, ontology generation, KG validation, and the enhancement of KG\naccuracy and consistency through LLMs. The paper further examines the roles of\nLLMs in generating descriptive texts and natural language queries for KGs.\nThrough a structured analysis that includes categorizing LLM-KG interactions,\nexamining methodologies, and investigating collaborative uses and potential\nbiases, this study seeks to provide new insights into the combined potential of\nLLMs and KGs. It highlights the importance of their interaction for improving\nAI applications and outlines future research directions.\n"", '  Large language models (LLMs) have demonstrated impressive reasoning abilities\nin complex tasks. However, they lack up-to-date knowledge and experience\nhallucinations during reasoning, which can lead to incorrect reasoning\nprocesses and diminish their performance and trustworthiness. Knowledge graphs\n(KGs), which capture vast amounts of facts in a structured format, offer a\nreliable source of knowledge for reasoning. Nevertheless, existing KG-based LLM\nreasoning methods only treat KGs as factual knowledge bases and overlook the\nimportance of their structural information for reasoning. In this paper, we\npropose a novel method called reasoning on graphs (RoG) that synergizes LLMs\nwith KGs to enable faithful and interpretable reasoning. Specifically, we\npresent a planning-retrieval-reasoning framework, where RoG first generates\nrelation paths grounded by KGs as faithful plans. These plans are then used to\nretrieve valid reasoning paths from the KGs for LLMs to conduct faithful\nreasoning. Furthermore, RoG not only distills knowledge from KGs to improve the\nreasoning ability of LLMs through training but also allows seamless integration\nwith any arbitrary LLMs during inference. Extensive experiments on two\nbenchmark KGQA datasets demonstrate that RoG achieves state-of-the-art\nperformance on KG reasoning tasks and generates faithful and interpretable\nreasoning results.\n', '  Large language models (LLMs), such as ChatGPT and GPT4, are making new waves\nin the field of natural language processing and artificial intelligence, due to\ntheir emergent ability and generalizability. However, LLMs are black-box\nmodels, which often fall short of capturing and accessing factual knowledge. In\ncontrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are\nstructured knowledge models that explicitly store rich factual knowledge. KGs\ncan enhance LLMs by providing external knowledge for inference and\ninterpretability. Meanwhile, KGs are difficult to construct and evolving by\nnature, which challenges the existing methods in KGs to generate new facts and\nrepresent unseen knowledge. Therefore, it is complementary to unify LLMs and\nKGs together and simultaneously leverage their advantages. In this article, we\npresent a forward-looking roadmap for the unification of LLMs and KGs. Our\nroadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs,\nwhich incorporate KGs during the pre-training and inference phases of LLMs, or\nfor the purpose of enhancing understanding of the knowledge learned by LLMs; 2)\nLLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding,\ncompletion, construction, graph-to-text generation, and question answering; and\n3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a\nmutually beneficial way to enhance both LLMs and KGs for bidirectional\nreasoning driven by both data and knowledge. We review and summarize existing\nefforts within these three frameworks in our roadmap and pinpoint their future\nresearch directions.\n'] , ['  Developing models that can learn to reason is a notoriously challenging\nproblem. We focus on reasoning in relational domains, where the use of Graph\nNeural Networks (GNNs) seems like a natural choice. However, previous work on\nreasoning with GNNs has shown that such models tend to fail when presented with\ntest examples that require longer inference chains than those seen during\ntraining. This suggests that GNNs lack the ability to generalize from training\nexamples in a systematic way, which would fundamentally limit their reasoning\nabilities. A common solution is to instead rely on neuro-symbolic methods,\nwhich are capable of reasoning in a systematic way by design. Unfortunately,\nthe scalability of such methods is often limited and they tend to rely on\noverly strong assumptions, e.g.\\ that queries can be answered by inspecting a\nsingle relational path. In this paper, we revisit the idea of reasoning with\nGNNs, showing that systematic generalization is possible as long as the right\ninductive bias is provided. In particular, we argue that node embeddings should\nbe treated as epistemic states and that GNN should be parameterised\naccordingly. We propose a simple GNN architecture which is based on this view\nand show that it is capable of achieving state-of-the-art results. We\nfurthermore introduce a benchmark which requires models to aggregate evidence\nfrom multiple relational paths. We show that existing neuro-symbolic approaches\nfail on this benchmark, whereas our considered GNN model learns to reason\naccurately.\n', '  Although database systems perform well in data access and manipulation, their\nrelational model hinders data scientists from formulating machine learning\nalgorithms in SQL. Nevertheless, we argue that modern database systems perform\nwell for machine learning algorithms expressed in relational algebra. To\novercome the barrier of the relational model, this paper shows how to transform\ndata into a relational representation for training neural networks in SQL: We\nfirst describe building blocks for data transformation, model training and\ninference in SQL-92 and their counterparts using an extended array data type.\nThen, we compare the implementation for model training and inference using\narray data types to the one using a relational representation in SQL-92 only.\nThe evaluation in terms of runtime and memory consumption proves the\nsuitability of modern database systems for matrix algebra, although specialised\narray data types perform better than matrices in relational representation.\n', '  An extension of Transformers is proposed that enables explicit relational\nreasoning through a novel module called the Abstractor. At the core of the\nAbstractor is a variant of attention called relational cross-attention. The\napproach is motivated by an architectural inductive bias for relational\nlearning that disentangles relational information from object-level features.\nThis enables explicit relational reasoning, supporting abstraction and\ngeneralization from limited data. The Abstractor is first evaluated on simple\ndiscriminative relational tasks and compared to existing relational\narchitectures. Next, the Abstractor is evaluated on purely relational\nsequence-to-sequence tasks, where dramatic improvements are seen in sample\nefficiency compared to standard Transformers. Finally, Abstractors are\nevaluated on a collection of tasks based on mathematical problem solving, where\nconsistent improvements in performance and sample efficiency are observed.\n']","Reasoning and Problem-Solving with Logic, Language, and Graphs","""Knowledge Graphs and Large Language Models for Reasoning"""
23,"Analogical Reasoning and Knowledge Bases , ""Analogical Reasoning and Proportions in Algebra""","['analogies', 'analogical', 'analogy', 'analogykb', 'semantic', 'cognition', 'analogous', 'cognitive', 'reasoning', 'thinking'] , ['analogical', 'analogies', 'analogy', 'abstraction', 'algebras', 'proportions', 'similarity', 'proportion', 'logic', 'reasoning']","['  Analogical reasoning is a fundamental cognitive ability of humans. However,\ncurrent language models (LMs) still struggle to achieve human-like performance\nin analogical reasoning tasks due to a lack of resources for model training. In\nthis work, we address this gap by proposing ANALOGYKB, a million-scale analogy\nknowledge base (KB) derived from existing knowledge graphs (KGs). ANALOGYKB\nidentifies two types of analogies from the KGs: 1) analogies of the same\nrelations, which can be directly extracted from the KGs, and 2) analogies of\nanalogous relations, which are identified with a selection and filtering\npipeline enabled by large language models (LLMs), followed by minor human\nefforts for data quality control. Evaluations on a series of datasets of two\nanalogical reasoning tasks (analogy recognition and generation) demonstrate\nthat ANALOGYKB successfully enables both smaller LMs and LLMs to gain better\nanalogical reasoning capabilities.\n', '  As a core cognitive skill that enables the transferability of information\nacross domains, analogical reasoning has been extensively studied for both\nhumans and computational models. However, while cognitive theories of analogy\noften focus on narratives and study the distinction between surface,\nrelational, and system similarities, existing work in natural language\nprocessing has a narrower focus as far as relational analogies between word\npairs. This gap brings a natural question: can state-of-the-art large language\nmodels (LLMs) detect system analogies between narratives? To gain insight into\nthis question and extend word-based relational analogies to relational system\nanalogies, we devise a comprehensive computational framework that\noperationalizes dominant theories of analogy, using narrative elements to\ncreate surface and system mappings. Leveraging the interplay between these\nmappings, we create a binary task and benchmark for Analogical Reasoning on\nNarratives (ARN), covering four categories of far (cross-domain)/near\n(within-domain) analogies and disanalogies. We show that while all LLMs can\nlargely recognize near analogies, even the largest ones struggle with far\nanalogies in a zero-shot setting, with GPT4.0 scoring below random. Guiding the\nmodels through solved examples and chain-of-thought reasoning enhances their\nanalogical reasoning ability. Yet, since even in the few-shot setting, the best\nmodel only performs halfway between random and humans, ARN opens exciting\ndirections for computational analogical reasoners.\n', '  While analogies are a common way to evaluate word embeddings in NLP, it is\nalso of interest to investigate whether or not analogical reasoning is a task\nin itself that can be learned. In this paper, we test several ways to learn\nbasic analogical reasoning, specifically focusing on analogies that are more\ntypical of what is used to evaluate analogical reasoning in humans than those\nin commonly used NLP benchmarks. Our experiments find that models are able to\nlearn analogical reasoning, even with a small amount of data. We additionally\ncompare our models to a dataset with a human baseline, and find that after\ntraining, models approach human performance.\n'] , ['  The author has recently introduced abstract algebraic frameworks of\nanalogical proportions and similarity within the general setting of universal\nalgebra. The purpose of this paper is to build a bridge from similarity to\nanalogical proportions by formulating the latter in terms of the former. The\nbenefit of this similarity-based approach is that the connection between\nproportions and similarity is built into the framework and therefore evident\nwhich is appealing since proportions and similarity are both at the center of\nanalogy; moreover, future results on similarity can directly be applied to\nanalogical proportions.\n', ""  Analogical reasoning is the ability to detect parallels between two seemingly\ndistant objects or situations, a fundamental human capacity used for example in\ncommonsense reasoning, learning, and creativity which is believed by many\nresearchers to be at the core of human and artificial general intelligence.\nAnalogical proportions are expressions of the form ``$a$ is to $b$ what $c$ is\nto $d$'' at the core of analogical reasoning. The author has recently\nintroduced an abstract algebraic framework of analogical proportions within the\ngeneral setting of universal algebra. It is the purpose of this paper to\nfurther develop the mathematical theory of analogical proportions within that\nframework as motivated by the fact that it has already been successfully\napplied to logic program synthesis in artificial intelligence.\n"", ""  Analogy-making is at the core of human and artificial intelligence and\ncreativity with applications to such diverse tasks as proving mathematical\ntheorems and building mathematical theories, common sense reasoning, learning,\nlanguage acquisition, and story telling. This paper introduces from first\nprinciples an abstract algebraic framework of analogical proportions of the\nform `$a$ is to $b$ what $c$ is to $d$' in the general setting of universal\nalgebra. This enables us to compare mathematical objects possibly across\ndifferent domains in a uniform way which is crucial for AI-systems. It turns\nout that our notion of analogical proportions has appealing mathematical\nproperties. As we construct our model from first principles using only\nelementary concepts of universal algebra, and since our model questions some\nbasic properties of analogical proportions presupposed in the literature, to\nconvince the reader of the plausibility of our model we show that it can be\nnaturally embedded into first-order logic via model-theoretic types and prove\nfrom that perspective that analogical proportions are compatible with\nstructure-preserving mappings. This provides conceptual evidence for its\napplicability. In a broader sense, this paper is a first step towards a theory\nof analogical reasoning and learning systems with potential applications to\nfundamental AI-problems like common sense reasoning and computational learning\nand creativity.\n""]",Analogical Reasoning and Knowledge Representation,"""Analogical Reasoning and Proportions in Algebra"""
24,Knowledge Representation and Organization Methodologies,"['knowledge', 'wikidata', 'entities', 'provenance', 'rdf', 'iconology', 'iconological', 'concepts', 'organizational', 'information']","['  Knowledge Representation (KR) and facet-analytical Knowledge Organization\n(KO) have been the two most prominent methodologies of data and knowledge\nmodelling in the Artificial Intelligence community and the Information Science\ncommunity, respectively. KR boasts of a robust and scalable ecosystem of\ntechnologies to support knowledge modelling while, often, underemphasizing the\nquality of its models (and model-based data). KO, on the other hand, is less\ntechnology-driven but has developed a robust framework of guiding principles\n(canons) for ensuring modelling (and model-based data) quality. This paper\nelucidates both the KR and facet-analytical KO methodologies in detail and\nprovides a functional mapping between them. Out of the mapping, the paper\nproposes an integrated KO-enriched KR methodology with all the standard\ncomponents of a KR methodology plus the guiding canons of modelling quality\nprovided by KO. The practical benefits of the methodological integration has\nbeen exemplified through a prominent case study of KR-based image annotation\nexercise.\n', ""  Sourcing and identification of new manufacturing partners is crucial for\nmanufacturing system integrators to enhance agility and reduce risk through\nsupply chain diversification in the global economy. The advent of advanced\nlarge language models has captured significant interest, due to their ability\nto generate comprehensive and articulate responses across a wide range of\nknowledge domains. However, the system often falls short in accuracy and\ncompleteness when responding to domain-specific inquiries, particularly in\nareas like manufacturing service discovery. This research explores the\npotential of leveraging Knowledge Graphs in conjunction with ChatGPT to\nstreamline the process for prospective clients in identifying small\nmanufacturing enterprises. In this study, we propose a method that integrates\nbottom-up ontology with advanced machine learning models to develop a\nManufacturing Service Knowledge Graph from an array of structured and\nunstructured data sources, including the digital footprints of small-scale\nmanufacturers throughout North America. The Knowledge Graph and the learned\ngraph embedding vectors are leveraged to tackle intricate queries within the\ndigital supply chain network, responding with enhanced reliability and greater\ninterpretability. The approach highlighted is scalable to millions of entities\nthat can be distributed to form a global Manufacturing Service Knowledge\nNetwork Graph that can potentially interconnect multiple types of Knowledge\nGraphs that span industry sectors, geopolitical boundaries, and business\ndomains. The dataset developed for this study, now publicly accessible,\nencompasses more than 13,000 manufacturers' weblinks, manufacturing services,\ncertifications, and location entity types.\n"", '  Knowledge Organization (KO) and Knowledge Representation (KR) have been the\ntwo mainstream methodologies of knowledge modelling in the Information Science\ncommunity and the Artificial Intelligence community, respectively. The\nfacet-analytical tradition of KO has developed an exhaustive set of guiding\ncanons for ensuring quality in organising and managing knowledge but has\nremained limited in terms of technology-driven activities to expand its scope\nand services beyond the bibliographic universe of knowledge. KR, on the other\nhand, boasts of a robust ecosystem of technologies and technology-driven\nservice design which can be tailored to model any entity or scale to any\nservice in the entire universe of knowledge. This paper elucidates both the\nfacet-analytical KO and KR methodologies in detail and provides a functional\nmapping between them. Out of the mapping, the paper proposes an integrated\nKR-enriched KO methodology with all the standard components of a KO methodology\nplus the advanced technologies provided by the KR approach. The practical\nbenefits of the methodological integration has been exemplified through the\nflagship application of the Digital University at the University of Trento,\nItaly.\n']",Knowledge Representation and Organization Methodologies,Knowledge Representation and Organization Methodologies
25,"Retrieval-Augmented Generation (RAG) Methods , Citation Analysis and Generation in Research Publications","['retrieval', 'semantic', 'search', 'relevance', 'corpus', 'ranking', 'recall', 'retrieved', 'indexing', 'retrievers'] , ['citations', 'citing', 'citation', 'cited', 'cite', 'bibliometric', 'references', 'scholarly', 'attributions', 'researchers']","['  With the rapid development of large-scale language models,\nRetrieval-Augmented Generation (RAG) has been widely adopted. However, existing\nRAG paradigms are inevitably influenced by erroneous retrieval information,\nthereby reducing the reliability and correctness of generated results.\nTherefore, to improve the relevance of retrieval information, this study\nproposes a method that replaces traditional retrievers with GPT-3.5, leveraging\nits vast corpus knowledge to generate retrieval information. We also propose a\nweb retrieval based method to implement fine-grained knowledge retrieval,\nUtilizing the powerful reasoning capability of GPT-3.5 to realize semantic\npartitioning of problem.In order to mitigate the illusion of GPT retrieval and\nreduce noise in Web retrieval,we proposes a multi-source retrieval framework,\nnamed MSRAG, which combines GPT retrieval with web retrieval. Experiments on\nmultiple knowledge-intensive QA datasets demonstrate that the proposed\nframework in this study performs better than existing RAG framework in\nenhancing the overall efficiency and accuracy of QA systems.\n', '  The retrieval-augmented generation (RAG) enables retrieval of relevant\ninformation from an external knowledge source and allows large language models\n(LLMs) to answer queries over previously unseen document collections. However,\nit was demonstrated that traditional RAG applications perform poorly in\nanswering multi-hop questions, which require retrieving and reasoning over\nmultiple elements of supporting evidence. We introduce a new method called\nMulti-Meta-RAG, which uses database filtering with LLM-extracted metadata to\nimprove the RAG selection of the relevant documents from various sources,\nrelevant to the question. While database filtering is specific to a set of\nquestions from a particular domain and format, we found out that Multi-Meta-RAG\ngreatly improves the results on the MultiHop-RAG benchmark. The code is\navailable at https://github.com/mxpoliakov/Multi-Meta-RAG.\n', '  Retrieval-Augmented Generation (RAG) has recently demonstrated the\nperformance of Large Language Models (LLMs) in the knowledge-intensive tasks\nsuch as Question-Answering (QA). RAG expands the query context by incorporating\nexternal knowledge bases to enhance the response accuracy. However, it would be\ninefficient to access LLMs multiple times for each query and unreliable to\nretrieve all the relevant documents by a single query. We have found that even\nthough there is low relevance between some critical documents and query, it is\npossible to retrieve the remaining documents by combining parts of the\ndocuments with the query. To mine the relevance, a two-stage retrieval\nframework called Dynamic-Relevant Retrieval-Augmented Generation (DR-RAG) is\nproposed to improve document retrieval recall and the accuracy of answers while\nmaintaining efficiency. Additionally, a compact classifier is applied to two\ndifferent selection strategies to determine the contribution of the retrieved\ndocuments to answering the query and retrieve the relatively relevant\ndocuments. Meanwhile, DR-RAG call the LLMs only once, which significantly\nimproves the efficiency of the experiment. The experimental results on\nmulti-hop QA datasets show that DR-RAG can significantly improve the accuracy\nof the answers and achieve new progress in QA systems.\n'] , ['  In this contribution, we deal with seed-based information retrieval in\nnetworks of research publications. Using systematic reviews as a baseline, and\npublication data from the NIH Open Citation Collection, we compare the\nperformance of the three citation-based approaches direct citation,\nco-citation, and bibliographic coupling with respect to recall and precision\nmeasures. In addition, we include the PubMed Related Article score as well as\ncombined approaches in the comparison. We also provide a fairly comprehensive\nreview of earlier research in which citation relations have been used for\ninformation retrieval purposes. The results show an advantage for co-citation\nover bibliographic coupling and direct citation. However, combining the three\napproaches outperforms the exclusive use of co-citation in the study. The\nresults further indicate, in line with previous research, that combining\ncitation-based approaches with textual approaches enhances the performance of\nseed-based information retrieval. The results from the study may guide\napproaches combining citation-based and textual approaches in their choice of\ncitation similarity measures. We suggest that future research use more\nstructured approaches to evaluate methods for seed-based retrieval of\npublications, including comparative approaches as well as the elaboration of\ncommon data sets and baselines for evaluation.\n', '  Abstractive citation text generation is usually framed as an infilling task,\nwhere a sequence-to-sequence model is trained to generate a citation given a\nreference paper and the context window around the target; the generated\ncitation should be a brief discussion of the reference paper as it relates to\nthe citing context. However, examining a recent LED-based citation generation\nsystem, we find that many of the generated citations are generic summaries of\nthe reference papers main contribution, ignoring the citation contexts focus on\na different topic. To address this problem, we propose a simple modification to\nthe citation text generation task: the generation target is not only the\ncitation itself, but the entire context window, including the target citation.\nThis approach can be easily applied to any abstractive citation generation\nsystem, and our experimental results show that training in this way is\npreferred by human readers and allows the generation model to make use of\ncontextual clues about what topic to discuss and what stance to take.\n', '  Citation text plays a pivotal role in elucidating the connection between\nscientific documents, demanding an in-depth comprehension of the cited paper.\nConstructing citations is often time-consuming, requiring researchers to delve\ninto extensive literature and grapple with articulating relevant content. To\naddress this challenge, the field of citation text generation (CTG) has\nemerged. However, while earlier methods have primarily centered on creating\nsingle-sentence citations, practical scenarios frequently necessitate citing\nmultiple papers within a single paragraph. To bridge this gap, we propose a\nmethod that leverages Large Language Models (LLMs) to generate multi-citation\nsentences. Our approach involves a single source paper and a collection of\ntarget papers, culminating in a coherent paragraph containing multi-sentence\ncitation text. Furthermore, we introduce a curated dataset named MCG-S2ORC,\ncomposed of English-language academic research papers in Computer Science,\nshowcasing multiple citation instances. In our experiments, we evaluate three\nLLMs LLaMA, Alpaca, and Vicuna to ascertain the most effective model for this\nendeavor. Additionally, we exhibit enhanced performance by integrating\nknowledge graphs from target papers into the prompts for generating citation\ntext. This research underscores the potential of harnessing LLMs for citation\ngeneration, opening a compelling avenue for exploring the intricate connections\nbetween scientific documents.\n']",Information Retrieval and Knowledge Generation in Research Publications,Retrieval-Augmented Generation (RAG) Methods
26,"Natural Language to SQL Generation , Natural Language Generation Evaluation , Paraphrasing and Paraphrase Generation , Minimum Bayes Risk Decoding for Text Generation","['nl2sql', 'database', 'sql', 'databases', 'querying', 'sqlfuse', 'queries', 'schemas', 'metasql', 'schema'] , ['nlg', 'evaluations', 'evaluation', 'generation', 'generated', 'dialogue', 'evaluating', 'evaluator', 'eval', 'gram'] , ['paraphrases', 'paraphrasing', 'paraphrastic', 'paraphrase', 'paraphrased', 'sentences', 'linguistic', 'nlg', 'semantic', 'corpora'] , ['translations', 'decoding', 'translation', 'mbr', 'mbrs', 'bayes', 'mbmbr', 'text', 'cbmbr', 'risk']","[""  Text-to-SQL, the process of translating natural language into Structured\nQuery Language (SQL), represents a transformative application of large language\nmodels (LLMs), potentially revolutionizing how humans interact with data. This\npaper introduces the SQL-PaLM framework, a comprehensive solution for\nunderstanding and enhancing Text-to-SQL using LLMs, using in the learning\nregimes of few-shot prompting and instruction fine-tuning. With few-shot\nprompting, we explore the effectiveness of consistency decoding with\nexecution-based error filtering. With instruction fine-tuning, we delve deep in\nunderstanding the critical paradigms that influence the performance of tuned\nLLMs. In particular, we investigate how performance can be improved through\nexpanded training data coverage and diversity, synthetic data augmentation, and\nintegrating query-specific database content. We propose a test-time selection\nmethod to further refine accuracy by integrating SQL outputs from multiple\nparadigms with execution feedback as guidance. Additionally, we tackle the\npractical challenge of navigating intricate databases with a significant number\nof tables and columns, proposing efficient techniques for accurately selecting\nrelevant database elements to enhance Text-to-SQL performance. Our holistic\napproach yields substantial advancements in Text-to-SQL, as demonstrated on two\nkey public benchmarks, Spider and BIRD. Through comprehensive ablations and\nerror analyses, we shed light on the strengths and weaknesses of our framework,\noffering valuable insights into Text-to-SQL's future work.\n"", '  Generating accurate SQL from natural language questions (text-to-SQL) is a\nlong-standing challenge due to the complexities in user question understanding,\ndatabase schema comprehension, and SQL generation. Conventional text-to-SQL\nsystems, comprising human engineering and deep neural networks, have made\nsubstantial progress. Subsequently, pre-trained language models (PLMs) have\nbeen developed and utilized for text-to-SQL tasks, achieving promising\nperformance. As modern databases become more complex, the corresponding user\nquestions also grow more challenging, causing PLMs with parameter constraints\nto produce incorrect SQL. This necessitates more sophisticated and tailored\noptimization methods, which, in turn, restricts the applications of PLM-based\nsystems. Recently, large language models (LLMs) have demonstrated significant\ncapabilities in natural language understanding as the model scale increases.\nTherefore, integrating LLM-based implementation can bring unique opportunities,\nimprovements, and solutions to text-to-SQL research. In this survey, we present\na comprehensive review of LLM-based text-to-SQL. Specifically, we propose a\nbrief overview of the technical challenges and the evolutionary process of\ntext-to-SQL. Then, we provide a detailed introduction to the datasets and\nmetrics designed to evaluate text-to-SQL systems. After that, we present a\nsystematic analysis of recent advances in LLM-based text-to-SQL. Finally, we\ndiscuss the remaining challenges in this field and propose expectations for\nfuture research directions.\n', ""  Generating accurate Structured Querying Language (SQL) is a long-standing\nproblem, especially in matching users' semantic queries with structured\ndatabases and then generating structured SQL. Existing models typically input\nqueries and database schemas into the LLM and rely on the LLM to perform\nsemantic-structure matching and generate structured SQL. However, such\nsolutions overlook the structural information within user queries and\ndatabases, which can be utilized to enhance the generation of structured SQL.\nThis oversight can lead to inaccurate or unexecutable SQL generation. To fully\nexploit the structure, we propose a structure-to-SQL framework, which leverages\nthe inherent structure information to improve the SQL generation of LLMs.\nSpecifically, we introduce our Structure Guided SQL~(SGU-SQL) generation model.\nSGU-SQL first links user queries and databases in a structure-enhanced manner.\nIt then decomposes complicated linked structures with grammar trees to guide\nthe LLM to generate the SQL step by step. Extensive experiments on two\nbenchmark datasets illustrate that SGU-SQL can outperform sixteen SQL\ngeneration baselines.\n""] , [""  Natural Language Generation (NLG) typically involves evaluating the generated\ntext in various aspects (e.g., consistency and naturalness) to obtain a\ncomprehensive assessment. However, multi-aspect evaluation remains challenging\nas it may require the evaluator to generalize to any given evaluation aspect\neven if it's absent during training. In this paper, we introduce X-Eval, a\ntwo-stage instruction tuning framework to evaluate the text in both seen and\nunseen aspects customized by end users. X-Eval consists of two learning stages:\nthe vanilla instruction tuning stage that improves the model's ability to\nfollow evaluation instructions, and an enhanced instruction tuning stage that\nexploits the connections between fine-grained evaluation aspects to better\nassess text quality. To support the training of X-Eval, we collect\nAspectInstruct, the first instruction tuning dataset tailored for multi-aspect\nNLG evaluation spanning 27 diverse evaluation aspects with 65 tasks. To enhance\ntask diversity, we devise an augmentation strategy that converts human rating\nannotations into diverse forms of NLG evaluation tasks, including scoring,\ncomparison, ranking, and Boolean question answering. Extensive experiments\nacross three essential categories of NLG tasks: dialogue generation,\nsummarization, and data-to-text coupled with 21 aspects in meta-evaluation,\ndemonstrate that our X-Eval enables even a lightweight language model to\nachieve a comparable if not higher correlation with human judgments compared to\nthe state-of-the-art NLG evaluators, such as GPT-4.\n"", '  Evaluating natural language generation (NLG) is a vital but challenging\nproblem in artificial intelligence. Traditional evaluation metrics mainly\ncapturing content (e.g. n-gram) overlap between system outputs and references\nare far from satisfactory, and large language models (LLMs) such as ChatGPT\nhave demonstrated great potential in NLG evaluation in recent years. Various\nautomatic evaluation methods based on LLMs have been proposed, including\nmetrics derived from LLMs, prompting LLMs, and fine-tuning LLMs with labeled\nevaluation data. In this survey, we first give a taxonomy of LLM-based NLG\nevaluation methods, and discuss their pros and cons, respectively. We also\ndiscuss human-LLM collaboration for NLG evaluation. Lastly, we discuss several\nopen problems in this area and point out future research directions.\n', '  The evaluation of natural language generation (NLG) tasks is a significant\nand longstanding research issue. With the recent emergence of powerful large\nlanguage models (LLMs), some studies have turned to LLM-based automatic\nevaluation methods, which demonstrate great potential to become a new\nevaluation paradigm following traditional string-based and model-based metrics.\nHowever, despite the improved performance of existing methods, they still\npossess some deficiencies, such as dependency on references and limited\nevaluation flexibility. Therefore, in this paper, we meticulously construct a\nlarge-scale NLG evaluation corpus NLG-Eval with human and GPT-4 annotations to\nalleviate the lack of relevant data in this field. Furthermore, we propose\nThemis, an LLM dedicated to NLG evaluation, which has been trained with our\ndesigned multi-perspective consistency and rating-oriented preference alignment\nmethods. Themis can conduct flexible and interpretable evaluations without\nreferences, and it exhibits superior evaluation performance on various NLG\ntasks, simultaneously generalizing well to unseen tasks and surpassing other\nevaluation models, including GPT-4.\n'] , ['  Since paraphrasing is an ill-defined task, the term ""paraphrasing"" covers\ntext transformation tasks with different characteristics. Consequently,\nexisting paraphrasing studies have applied quite different (explicit and\nimplicit) criteria as to when a pair of texts is to be considered a paraphrase,\nall of which amount to postulating a certain level of semantic or lexical\nsimilarity. In this paper, we conduct a literature review and propose a\ntaxonomy to organize the 25~identified paraphrasing (sub-)tasks. Using\nclassifiers trained to identify the tasks that a given paraphrasing instance\nfits, we find that the distributions of task-specific instances in the known\nparaphrase corpora vary substantially. This means that the use of these\ncorpora, without the respective paraphrase conditions being clearly defined\n(which is the normal case), must lead to incomparable and misleading results.\n', ""  Paraphrases represent a human's intuitive ability to understand expressions\npresented in various different ways. Current paraphrase evaluations of language\nmodels primarily use binary approaches, offering limited interpretability of\nspecific text changes. Atomic paraphrase types (APT) decompose paraphrases into\ndifferent linguistic changes and offer a granular view of the flexibility in\nlinguistic expression (e.g., a shift in syntax or vocabulary used). In this\nstudy, we assess the human preferences towards ChatGPT in generating English\nparaphrases with ten APTs and five prompting techniques. We introduce APTY\n(Atomic Paraphrase TYpes), a dataset of 500 sentence-level and word-level\nannotations by 15 annotators. The dataset also provides a human preference\nranking of paraphrases with different types that can be used to fine-tune\nmodels with RLHF and DPO methods. Our results reveal that ChatGPT can generate\nsimple APTs, such as additions and deletions, but struggle with complex\nstructures (e.g., subordination changes). This study contributes to\nunderstanding which aspects of paraphrasing language models have already\nsucceeded at understanding and what remains elusive. In addition, our curated\ndatasets can be used to develop language models with specific linguistic\ncapabilities.\n"", '  Current approaches in paraphrase generation and detection heavily rely on a\nsingle general similarity score, ignoring the intricate linguistic properties\nof language. This paper introduces two new tasks to address this shortcoming by\nconsidering paraphrase types - specific linguistic perturbations at particular\ntext positions. We name these tasks Paraphrase Type Generation and Paraphrase\nType Detection. Our results suggest that while current techniques perform well\nin a binary classification scenario, i.e., paraphrased or not, the inclusion of\nfine-grained paraphrase types poses a significant challenge. While most\napproaches are good at generating and detecting general semantic similar\ncontent, they fail to understand the intrinsic linguistic variables they\nmanipulate. Models trained in generating and identifying paraphrase types also\nshow improvements in tasks without them. In addition, scaling these models\nfurther improves their ability to understand paraphrase types. We believe\nparaphrase types can unlock a new paradigm for developing paraphrase models and\nsolving tasks in the future.\n'] , ['  Minimum Bayes Risk (MBR) decoding has been shown to be a powerful alternative\nto beam search decoding in a variety of text generation tasks. MBR decoding\nselects a hypothesis from a pool of hypotheses that has the least expected risk\nunder a probability model according to a given utility function. Since it is\nimpractical to compute the expected risk exactly over all possible hypotheses,\ntwo approximations are commonly used in MBR. First, it integrates over a\nsampled set of hypotheses rather than over all possible hypotheses. Second, it\nestimates the probability of each hypothesis using a Monte Carlo estimator.\nWhile the first approximation is necessary to make it computationally feasible,\nthe second is not essential since we typically have access to the model\nprobability at inference time. We propose Model-Based MBR (MBMBR), a variant of\nMBR that uses the model probability itself as the estimate of the probability\ndistribution instead of the Monte Carlo estimate. We show analytically and\nempirically that the model-based estimate is more promising than the Monte\nCarlo estimate in text generation tasks. Our experiments show that MBMBR\noutperforms MBR in several text generation tasks, both with encoder-decoder\nmodels and with large language models.\n', '  This paper explores Minimum Bayes Risk (MBR) decoding for self-improvement in\nmachine translation (MT), particularly for domain adaptation and low-resource\nlanguages. We implement the self-improvement process by fine-tuning the model\non its MBR-decoded forward translations. By employing COMET as the MBR utility\nmetric, we aim to achieve the reranking of translations that better aligns with\nhuman preferences. The paper explores the iterative application of this\napproach and the potential need for language-specific MBR utility metrics. The\nresults demonstrate significant enhancements in translation quality for all\nexamined language pairs, including successful application to domain-adapted\nmodels and generalisation to low-resource settings. This highlights the\npotential of COMET-guided MBR for efficient MT self-improvement in various\nscenarios.\n', '  Minimum Bayes Risk (MBR) decoding is a powerful decoding strategy widely used\nfor text generation tasks, but its quadratic computational complexity limits\nits practical application. This paper presents a novel approach for\napproximating MBR decoding using matrix completion techniques, focusing on the\ntask of machine translation. We formulate MBR decoding as a matrix completion\nproblem, where the utility metric scores between candidate hypotheses and\npseudo-reference translations form a low-rank matrix. First, we empirically\nshow that the scores matrices indeed have a low-rank structure. Then, we\nexploit this by only computing a random subset of the scores and efficiently\nrecover the missing entries in the matrix by applying the Alternating Least\nSquares (ALS) algorithm, thereby enabling a fast approximation of the MBR\ndecoding process. Our experimental results on machine translation tasks\ndemonstrate that the proposed method requires 1/16 utility metric computations\ncompared to vanilla MBR decoding while achieving equal translation quality\nmeasured by COMET22 on the WMT22 dataset (en<>de and en<>ru). We also benchmark\nour method against other approximation methods and we show gains in quality\nwhen comparing to them.\n']",Natural Language Processing for Text Generation and Evaluation,Natural Language Generation Evaluation
27,"Multimodal Sarcasm Detection , Humor Generation and Detection","['sarcasm', 'sarcastic', 'emoticons', 'sentiment', 'humor', 'irony', 'multimodal', 'emojis', 'text', 'modality'] , ['humordb', 'funnynet', 'humor', 'humour', 'humorous', 'jokes', 'subtitles', 'laugh', 'funnier', 'laughter']","['  Sarcasm is a way of verbal irony where someone says the opposite of what they\nmean, often to ridicule a person, situation, or idea. It is often difficult to\ndetect sarcasm in the dialogue since detecting sarcasm should reflect the\ncontext (i.e., dialogue history). In this paper, we introduce a new dataset for\nthe Korean dialogue sarcasm detection task, KoCoSa (Korean Context-aware\nSarcasm Detection Dataset), which consists of 12.8K daily Korean dialogues and\nthe labels for this task on the last response. To build the dataset, we propose\nan efficient sarcasm detection dataset generation pipeline: 1) generating new\nsarcastic dialogues from source dialogues with large language models, 2)\nautomatic and manual filtering of abnormal and toxic dialogues, and 3) human\nannotation for the sarcasm detection task. We also provide a simple but\neffective baseline for the Korean sarcasm detection task trained on our\ndataset. Experimental results on the dataset show that our baseline system\noutperforms strong baselines like large language models, such as GPT-3.5, in\nthe Korean sarcasm detection task. We show that the sarcasm detection task\nrelies deeply on the existence of sufficient context. We will release the\ndataset at https://github.com/Yu-billie/KoCoSa_sarcasm_detection.\n', '  Sarcasm is a type of irony, characterized by an inherent mismatch between the\nliteral interpretation and the intended connotation. Though sarcasm detection\nin text has been extensively studied, there are situations in which textual\ninput alone might be insufficient to perceive sarcasm. The inclusion of\nadditional contextual cues, such as images, is essential to recognize sarcasm\nin social media data effectively. This study presents a novel framework for\nmultimodal sarcasm detection that can process input triplets. Two components of\nthese triplets comprise the input text and its associated image, as provided in\nthe datasets. Additionally, a supplementary modality is introduced in the form\nof descriptive image captions. The motivation behind incorporating this visual\nsemantic representation is to more accurately capture the discrepancies between\nthe textual and visual content, which are fundamental to the sarcasm detection\ntask. The primary contributions of this study are: (1) a robust textual feature\nextraction branch that utilizes a cross-lingual language model; (2) a visual\nfeature extraction branch that incorporates a self-regulated residual ConvNet\nintegrated with a lightweight spatially aware attention module; (3) an\nadditional modality in the form of image captions generated using an\nencoder-decoder architecture capable of reading text embedded in images; (4)\ndistinct attention modules to effectively identify the incongruities between\nthe text and two levels of image representations; (5) multi-level cross-domain\nsemantic incongruity representation achieved through feature fusion. Compared\nwith cutting-edge baselines, the proposed model achieves the best accuracy of\n92.89% and 64.48%, respectively, on the Twitter multimodal sarcasm and\nMultiBully datasets.\n', '  Social media abounds with multimodal sarcasm, and identifying sarcasm targets\nis particularly challenging due to the implicit incongruity not directly\nevident in the text and image modalities. Current methods for Multimodal\nSarcasm Target Identification (MSTI) predominantly focus on superficial\nindicators in an end-to-end manner, overlooking the nuanced understanding of\nmultimodal sarcasm conveyed through both the text and image. This paper\nproposes a versatile MSTI framework with a coarse-to-fine paradigm, by\naugmenting sarcasm explainability with reasoning and pre-training knowledge.\nInspired by the powerful capacity of Large Multimodal Models (LMMs) on\nmultimodal reasoning, we first engage LMMs to generate competing rationales for\ncoarser-grained pre-training of a small language model on multimodal sarcasm\ndetection. We then propose fine-tuning the model for finer-grained sarcasm\ntarget identification. Our framework is thus empowered to adeptly unveil the\nintricate targets within multimodal sarcasm and mitigate the negative impact\nposed by potential noise inherently in LMMs. Experimental results demonstrate\nthat our model far outperforms state-of-the-art MSTI methods, and markedly\nexhibits explainability in deciphering sarcasm as well.\n'] , ['  In this paper, we explore the generation of one-liner jokes through\nmulti-step reasoning. Our work involved reconstructing the process behind\ncreating humorous one-liners and developing a working prototype for humor\ngeneration. We conducted comprehensive experiments with human participants to\nevaluate our approach, comparing it with human-created jokes, zero-shot GPT-4\ngenerated humor, and other baselines. The evaluation focused on the quality of\nhumor produced, using human labeling as a benchmark. Our findings demonstrate\nthat the multi-step reasoning approach consistently improves the quality of\ngenerated humor. We present the results and share the datasets used in our\nexperiments, offering insights into enhancing humor generation with artificial\nintelligence.\n', ""  Humor is a fundamental facet of human cognition and interaction. Yet, despite\nrecent advances in natural language processing, humor detection remains a\nchallenging task that is complicated by the scarcity of datasets that pair\nhumorous texts with similar non-humorous counterparts. In our work, we\ninvestigate whether large language models (LLMs), can generate synthetic data\nfor humor detection via editing texts. We benchmark LLMs on an existing human\ndataset and show that current LLMs display an impressive ability to 'unfun'\njokes, as judged by humans and as measured on the downstream task of humor\ndetection. We extend our approach to a code-mixed English-Hindi humor dataset,\nwhere we find that GPT-4's synthetic data is highly rated by bilingual\nannotators and provides challenging adversarial examples for humor classifiers.\n"", '  Humor understanding is an important and challenging research in natural\nlanguage processing. As the popularity of pre-trained language models (PLMs),\nsome recent work makes preliminary attempts to adopt PLMs for humor recognition\nand generation. However, these simple attempts do not substantially answer the\nquestion: {\\em whether PLMs are capable of humor understanding?} This paper is\nthe first work that systematically investigates the humor understanding ability\nof PLMs. For this purpose, a comprehensive framework with three evaluation\nsteps and four evaluation tasks is designed. We also construct a comprehensive\nChinese humor dataset, which can fully meet all the data requirements of the\nproposed evaluation framework. Our empirical study on the Chinese humor dataset\nyields some valuable observations, which are of great guiding value for future\noptimization of PLMs in humor understanding and generation.\n']",Humor and Sarcasm in Natural Language Processing,Multimodal Sarcasm Detection
28,"Empathetic Response Generation , Conversational Search and Retrieval , Conversational Retrieval and Response Generation","['empathy', 'empatheticdialogues', 'empathicstories', 'empathetic', 'empathic', 'conversations', 'empathically', 'dialogue', 'conversation', 'emotionbench'] , ['conversational', 'retrieval', 'conversations', 'conversation', 'searchers', 'search', 'dialogue', 'queries', 'chat', 'seeking'] , ['retrieval', 'conversational', 'conversation', 'chatbots', 'conversations', 'dialogue', 'chatbot', 'dialogues', 'dialog', 'assistant']","[""  This paper investigates the empathetic responding capabilities of ChatGPT,\nparticularly its latest iteration, GPT-4, in comparison to human-generated\nresponses to a wide range of emotional scenarios, both positive and negative.\nWe employ a rigorous evaluation methodology, involving a between-groups study\nwith 600 participants, to evaluate the level of empathy in responses generated\nby humans and ChatGPT. ChatGPT is prompted in two distinct ways: a standard\napproach and one explicitly detailing empathy's cognitive, affective, and\ncompassionate counterparts. Our findings indicate that the average empathy\nrating of responses generated by ChatGPT exceeds those crafted by humans by\napproximately 10%. Additionally, instructing ChatGPT to incorporate a clear\nunderstanding of empathy in its responses makes the responses align\napproximately 5 times more closely with the expectations of individuals\npossessing a high degree of empathy, compared to human responses. The proposed\nevaluation framework serves as a scalable and adaptable framework to assess the\nempathetic capabilities of newer and updated versions of large language models,\neliminating the need to replicate the current study's results in future\nresearch.\n"", ""  Empathetic response generation, aiming at understanding the user's situation\nand feelings and respond empathically, is crucial in building human-like\ndialogue systems. Previous methods mainly focus on using maximum likelihood\nestimation as the optimization objective for training response generation\nmodels, without taking into account the empathy level alignment between\ngenerated responses and target responses. To this end, we propose an empathetic\nresponse generation using reinforcement learning (EmpRL) framework. The\nframework designs an effective empathy reward function and generates empathetic\nresponses by maximizing the expected reward through reinforcement learning.\nGiven the powerful text generation capability of pre-trained language models,\nEmpRL utilizes the pre-trained T5 model as the generator and conducts further\ntraining to initialize the policy. To align the empathy level between generated\nresponses and target responses in the context, an empathy reward function\ncontaining three empathy communication mechanisms, i.e., emotional reaction,\ninterpretation, and exploration, is constructed using pre-designed and\npre-trained empathy identifiers. Finally, the proximal policy optimization\nalgorithm is used to further train the policy to produce empathetic responses.\nBoth automatic and manual evaluations demonstrate that the proposed EmpRL\nframework can improve the quality of generated responses, enhance the empathy\nlevel similarity between generated and target responses, and produce empathetic\nresponses covering both affective and cognitive aspects.\n"", ""  Empathetic response generation is designed to comprehend the emotions of\nothers and select the most appropriate strategies to assist them in resolving\nemotional challenges. Empathy can be categorized into cognitive empathy and\naffective empathy. The former pertains to the ability to understand and discern\nthe emotional issues and situations of others, while the latter involves the\ncapacity to provide comfort. To enhance one's empathetic abilities, it is\nessential to develop both these aspects. Therefore, we develop an innovative\nframework that combines retrieval augmentation and emotional support strategy\nintegration. Our framework starts with the introduction of a comprehensive\nemotional palette for empathy. We then apply appraisal theory to decompose this\npalette and create a database of empathetic responses. This database serves as\nan external resource and enhances the LLM's empathy by integrating semantic\nretrieval mechanisms. Moreover, our framework places a strong emphasis on the\nproper articulation of response strategies. By incorporating emotional support\nstrategies, we aim to enrich the model's capabilities in both cognitive and\naffective empathy, leading to a more nuanced and comprehensive empathetic\nresponse. Finally, we extract datasets ED and ET from the empathetic dialogue\ndataset \\textsc{EmpatheticDialogues} and ExTES based on dialogue length.\nExperiments demonstrate that our framework can enhance the empathy ability of\nLLMs from both cognitive and affective empathy perspectives. Our code is\nreleased at https://github.com/CAS-SIAT-XinHai/APTNESS.\n""] , [""  With large language models (LLMs), conversational search engines shift how\nusers retrieve information from the web by enabling natural conversations to\nexpress their search intents over multiple turns. Users' natural conversation\nembodies rich but implicit signals of users' search intents and evaluation of\nsearch results to understand user experience with the system. However, it is\nunderexplored how and why users ask follow-up queries to continue conversations\nwith conversational search engines and how the follow-up queries signal users'\nsatisfaction. From qualitative analysis of 250 conversational turns from an\nin-lab user evaluation of Naver Cue:, a commercial conversational search\nengine, we propose a taxonomy of 18 users' follow-up query patterns from\nconversational search, comprising two major axes: (1) users' motivations behind\ncontinuing conversations (N = 7) and (2) actions of follow-up queries (N = 11).\nCompared to the existing literature on query reformulations, we uncovered a new\nset of motivations and actions behind follow-up queries, including asking for\nsubjective opinions or providing natural language feedback on the engine's\nresponses. To analyze conversational search logs with our taxonomy in a\nscalable and efficient manner, we built an LLM-powered classifier (73%\naccuracy). With our classifier, we analyzed 2,061 conversational tuples\ncollected from real-world usage logs of Cue: and examined how the conversation\npatterns from our taxonomy correlates with satisfaction. Our initial findings\nsuggest some signals of dissatisfactions, such as Clarifying Queries, Excluding\nCondition, and Substituting Condition with follow-up queries. We envision our\napproach could contribute to automated evaluation of conversation search\nexperience by providing satisfaction signals and grounds for realistic user\nsimulations.\n"", '  Conversational search facilitates complex information retrieval by enabling\nmulti-turn interactions between users and the system. Supporting such\ninteractions requires a comprehensive understanding of the conversational\ninputs to formulate a good search query based on historical information. In\nparticular, the search query should include the relevant information from the\nprevious conversation turns. However, current approaches for conversational\ndense retrieval primarily rely on fine-tuning a pre-trained ad-hoc retriever\nusing the whole conversational search session, which can be lengthy and noisy.\nMoreover, existing approaches are limited by the amount of manual supervision\nsignals in the existing datasets. To address the aforementioned issues, we\npropose a History-Aware Conversational Dense Retrieval (HAConvDR) system, which\nincorporates two ideas: context-denoised query reformulation and automatic\nmining of supervision signals based on the actual impact of historical turns.\nExperiments on two public conversational search datasets demonstrate the\nimproved history modeling capability of HAConvDR, in particular for long\nconversations with topic shifts.\n', '  Conversational search supports multi-turn user-system interactions to solve\ncomplex information needs. Different from the traditional single-turn ad-hoc\nsearch, conversational search encounters a more challenging problem of\ncontext-dependent query understanding with the lengthy and long-tail\nconversational history context. While conversational query rewriting methods\nleverage explicit rewritten queries to train a rewriting model to transform the\ncontext-dependent query into a stand-stone search query, this is usually done\nwithout considering the quality of search results. Conversational dense\nretrieval methods use fine-tuning to improve a pre-trained ad-hoc query\nencoder, but they are limited by the conversational search data available for\ntraining. In this paper, we leverage both rewritten queries and relevance\njudgments in the conversational search data to train a better query\nrepresentation model. The key idea is to align the query representation with\nthose of rewritten queries and relevant documents. The proposed model -- Query\nRepresentation Alignment Conversational Dense Retriever, QRACDR, is tested on\neight datasets, including various settings in conversational search and ad-hoc\nsearch. The results demonstrate the strong performance of QRACDR compared with\nstate-of-the-art methods, and confirm the effectiveness of representation\nalignment.\n'] , ['  Conversational retrieval refers to an information retrieval system that\noperates in an iterative and interactive manner, requiring the retrieval of\nvarious external resources, such as persona, knowledge, and even response, to\neffectively engage with the user and successfully complete the dialogue.\nHowever, most previous work trained independent retrievers for each specific\nresource, resulting in sub-optimal performance and low efficiency. Thus, we\npropose a multi-task framework function as a universal retriever for three\ndominant retrieval tasks during the conversation: persona selection, knowledge\nselection, and response selection. To this end, we design a dual-encoder\narchitecture consisting of a context-adaptive dialogue encoder and a candidate\nencoder, aiming to attention to the relevant context from the long dialogue and\nretrieve suitable candidates by simply a dot product. Furthermore, we introduce\ntwo loss constraints to capture the subtle relationship between dialogue\ncontext and different candidates by regarding historically selected candidates\nas hard negatives. Extensive experiments and analysis establish\nstate-of-the-art retrieval quality both within and outside its training domain,\nrevealing the promising potential and generalization capability of our model to\nserve as a universal retriever for different candidate selection tasks\nsimultaneously.\n', ""  Despite the success of integrating large language models into the development\nof conversational systems, many studies have shown the effectiveness of\nretrieving and augmenting external knowledge for informative responses. Hence,\nmany existing studies commonly assume the always need for Retrieval Augmented\nGeneration (RAG) in a conversational system without explicit control. This\nraises a research question about such a necessity. In this study, we propose to\ninvestigate the need for each turn of system response to be augmented with\nexternal knowledge. In particular, by leveraging human judgements on the binary\nchoice of adaptive augmentation, we develop RAGate, a gating model, which\nmodels conversation context and relevant inputs to predict if a conversational\nsystem requires RAG for improved responses. We conduct extensive experiments on\ndevising and applying RAGate to conversational models and well-rounded analyses\nof different conversational scenarios. Our experimental results and analysis\nindicate the effective application of RAGate in RAG-based conversational\nsystems in identifying system responses for appropriate RAG with high-quality\nresponses and a high generation confidence. This study also identifies the\ncorrelation between the generation's confidence level and the relevance of the\naugmented knowledge.\n"", '  Retrieval-Augmented Generation (RAG) aims to generate more reliable and\naccurate responses, by augmenting large language models (LLMs) with the\nexternal vast and dynamic knowledge. Most previous work focuses on using RAG\nfor single-round question answering, while how to adapt RAG to the complex\nconversational setting wherein the question is interdependent on the preceding\ncontext is not well studied. In this paper, we propose a conversation-level RAG\napproach, which incorporates fine-grained retrieval augmentation and self-check\nfor conversational question answering (CQA). In particular, our approach\nconsists of three components, namely conversational question refiner,\nfine-grained retriever and self-check based response generator, which work\ncollaboratively for question understanding and relevant information acquisition\nin conversational settings. Extensive experiments demonstrate the great\nadvantages of our approach over the state-of-the-art baselines. Moreover, we\nalso release a Chinese CQA dataset with new features including reformulated\nquestion, extracted keyword, retrieved paragraphs and their helpfulness, which\nfacilitates further researches in RAG enhanced CQA.\n']",Conversational AI and Empathy in Human-Computer Interaction,Conversational Retrieval and Response Generation
29,"""Automated Grading and Assessment with Large Language Models"" , Automated Essay Scoring and Assessment","['grading', 'graded', 'assessments', 'assessment', 'grades', 'graders', 'grade', 'exams', 'exam', 'students'] , ['grading', 'essays', 'assessment', 'essay', 'scores', 'writing', 'learners', 'scoring', 'score', 'evaluation']","[""  Large language models (LLMs) have demonstrated strong potential in performing\nautomatic scoring for constructed response assessments. While constructed\nresponses graded by humans are usually based on given grading rubrics, the\nmethods by which LLMs assign scores remain largely unclear. It is also\nuncertain how closely AI's scoring process mirrors that of humans, or if it\nadheres to the same grading criteria. To address this gap, this paper uncovers\nthe grading rubrics that LLMs used to score students' written responses to\nscience tasks and their alignment with human scores. We also examine whether\nenhancing the alignments can improve scoring accuracy. Specifically, we prompt\nLLMs to generate analytic rubrics that they use to assign scores and study the\nalignment gap with human grading rubrics. Based on a series of experiments with\nvarious configurations of LLM settings, we reveal a notable alignment gap\nbetween human and LLM graders. While LLMs can adapt quickly to scoring tasks,\nthey often resort to shortcuts, bypassing deeper logical reasoning expected in\nhuman grading. We found that incorporating high-quality analytical rubrics\ndesigned to reflect human grading logic can mitigate this gap and enhance LLMs'\nscoring accuracy. These results caution against the simplistic application of\nLLMs in science education and highlight the importance of aligning LLM outputs\nwith human expectations to ensure efficient and accurate automatic scoring.\n"", ""  We explore the use of deep reinforcement learning to audit an automatic short\nanswer grading (ASAG) model. Automatic grading may decrease the time burden of\nrating open-ended items for educators, but a lack of robust evaluation methods\nfor these models can result in uncertainty of their quality. Current\nstate-of-the-art ASAG models are configured to match human ratings from a\ntraining set, and researchers typically assess their quality with accuracy\nmetrics that signify agreement between model and human scores. In this paper,\nwe show that a high level of agreement to human ratings does not give\nsufficient evidence that an ASAG model is infallible. We train a reinforcement\nlearning agent to revise student responses with the objective of achieving a\nhigh rating from an automatic grading model in the least number of revisions.\nBy analyzing the agent's revised responses that achieve a high grade from the\nASAG model but would not be considered a high scoring responses according to a\nscoring rubric, we discover ways in which the automated grader can be\nexploited, exposing shortcomings in the grading model.\n"", ""  While large language models (LLMs) have been used for automated grading, they\nhave not yet achieved the same level of performance as humans, especially when\nit comes to grading complex questions. Existing research on this topic focuses\non a particular step in the grading procedure: grading using predefined\nrubrics. However, grading is a multifaceted procedure that encompasses other\ncrucial steps, such as grading rubrics design and post-grading review. There\nhas been a lack of systematic research exploring the potential of LLMs to\nenhance the entire grading~process.\n  In this paper, we propose an LLM-based grading system that addresses the\nentire grading procedure, including the following key components: 1) Developing\ngrading rubrics that not only consider the questions but also the student\nanswers, which can more accurately reflect students' performance. 2) Under the\nguidance of grading rubrics, providing accurate and consistent scores for each\nstudent, along with customized feedback. 3) Conducting post-grading review to\nbetter ensure accuracy and fairness. Additionally, we collected a new dataset\nnamed OS from a university operating system course and conducted extensive\nexperiments on both our new dataset and the widely used Mohler dataset.\nExperiments demonstrate the effectiveness of our proposed approach, providing\nsome new insights for developing automated grading systems based on LLMs.\n""] , [""  Automated Essay Scoring (AES) holds significant promise in the field of\neducation, helping educators to mark larger volumes of essays and provide\ntimely feedback. However, Arabic AES research has been limited by the lack of\npublicly available essay data. This study introduces AR-AES, an Arabic AES\nbenchmark dataset comprising 2046 undergraduate essays, including gender\ninformation, scores, and transparent rubric-based evaluation guidelines,\nproviding comprehensive insights into the scoring process. These essays come\nfrom four diverse courses, covering both traditional and online exams.\nAdditionally, we pioneer the use of AraBERT for AES, exploring its performance\non different question types. We find encouraging results, particularly for\nEnvironmental Chemistry and source-dependent essay questions. For the first\ntime, we examine the scale of errors made by a BERT-based AES system, observing\nthat 96.15 percent of the errors are within one point of the first human\nmarker's prediction, on a scale of one to five, with 79.49 percent of\npredictions matching exactly. In contrast, additional human markers did not\nexceed 30 percent exact matches with the first marker, with 62.9 percent within\none mark. These findings highlight the subjectivity inherent in essay grading,\nand underscore the potential for current AES technology to assist human markers\nto grade consistently across large classes.\n"", '  Individual feedback can help students improve their essay writing skills.\nHowever, the manual effort required to provide such feedback limits\nindividualization in practice. Automatically-generated essay feedback may serve\nas an alternative to guide students at their own pace, convenience, and desired\nfrequency. Large language models (LLMs) have demonstrated strong performance in\ngenerating coherent and contextually relevant text. Yet, their ability to\nprovide helpful essay feedback is unclear. This work explores several prompting\nstrategies for LLM-based zero-shot and few-shot generation of essay feedback.\nInspired by Chain-of-Thought prompting, we study how and to what extent\nautomated essay scoring (AES) can benefit the quality of generated feedback. We\nevaluate both the AES performance that LLMs can achieve with prompting only and\nthe helpfulness of the generated essay feedback. Our results suggest that\ntackling AES and feedback generation jointly improves AES performance. However,\nwhile our manual evaluation emphasizes the quality of the generated essay\nfeedback, the impact of essay scoring on the generated feedback remains low\nultimately.\n', '  Automated essay scoring (AES) involves predicting a score that reflects the\nwriting quality of an essay. Most existing AES systems produce only a single\noverall score. However, users and L2 learners expect scores across different\ndimensions (e.g., vocabulary, grammar, coherence) for English essays in\nreal-world applications. To address this need, we have developed two models\nthat automatically score English essays across multiple dimensions by employing\nfine-tuning and other strategies on two large datasets. The results demonstrate\nthat our systems achieve impressive performance in evaluation using three\ncriteria: precision, F1 score, and Quadratic Weighted Kappa. Furthermore, our\nsystem outperforms existing methods in overall scoring.\n']",Automated Assessment and Grading in Education,"""Automated Grading and Assessment with Large Language Models"""
30,"""ChatGPT in Programming Education"" , Knowledge Tracing in Online Education , Automated Feedback in Intelligent Tutoring Systems","['programming', 'tutoring', 'tutor', 'tutors', 'chatbots', 'learners', 'students', 'pedagogical', 'educational', 'student'] , ['tutoring', 'students', 'learners', 'tracing', 'learning', 'student', 'knowledge', 'assessment', 'classroom', 'learner'] , ['tutoring', 'tutors', 'grading', 'students', 'learnersourcing', 'pedagogical', 'automating', 'instructional', 'assessment', 'feedback']","[""  The application of Artificial intelligence for teaching and learning in the\nacademic sphere is a trending subject of interest in the computing education.\nChatGPT, as an AI-based tool, provides various advantages, such as heightened\nstudent involvement, cooperation, accessibility and availability. This paper\naddresses the prospects and obstacles associated with utilizing ChatGPT as a\ntool for learning and assessment in undergraduate Computer Science curriculum\nin particular to teaching and learning fundamental programming courses.\nStudents having completed the course work for a Data Structures and Algorithms\n(a sophomore level course) participated in this study. Two groups of students\nwere given programming challenges to solve within a short period of time. The\ncontrol group (group A) had access to text books and notes of programming\ncourses, however no Internet access was provided. Group B students were given\naccess to ChatGPT and were encouraged to use it to help solve the programming\nchallenges. The challenge was conducted in a computer lab environment using PC2\nenvironment. Each team of students address the problem by writing executable\ncode that satisfies certain number of test cases. Student teams were scored\nbased on their performance in terms of number of successful passed testcases.\nResults show that students using ChatGPT had an advantage in terms of earned\nscores, however there were inconsistencies and inaccuracies in the submitted\ncode consequently affecting the overall performance. After a thorough analysis,\nthe paper's findings indicate that incorporating AI in higher education brings\nabout various opportunities and challenges.\n"", ""  This research paper contributes to the computing education research\ncommunity's understanding of Generative AI (GenAI) in the context of\nintroductory programming, and specifically, how students utilize related tools,\nsuch as ChatGPT. An increased understanding of students' use is mandatory for\neducators and higher education institutions, as GenAI is here to stay, and its\nperformance is likely to improve rapidly in the near future. Learning about\nstudents' use patterns is not only crucial to support their learning, but to\ndevelop adequate forms of instruction and assessment. With the rapid\nadvancement of AI, its broad availability, and ubiquitous presence in\neducational environments, elaborating how AI can enhance learning experiences,\nespecially in courses such as introductory programming is important. To date,\nmost studies have focused on the educator's perspective on GenAI, its\nperformance, characteristics, and limitations. However, the student\nperspective, and how they actually use GenAI tools in course contexts, has not\nbeen subject to a great number of studies. Therefore, this study is guided by\nthe following research questions: (1) What do students report on their use\npattern of ChatGPT in the context of introductory programming exercises? and\n(2) How do students perceive ChatGPT in the context of introductory programming\nexercises? To address these questions, computing students at a large German\nuniversity were asked to solve programming tasks with the assistance of ChatGPT\nas part of their introductory programming course. Students (n=298) provided\ninformation regarding the use of ChatGPT, and their evaluation of the tool via\nan online survey. This research provides a comprehensive evaluation of\nChatGPT-3.5's application by novice programmers in a higher education\ncontext...\n"", ""  The integration of ChatGPT as a supportive tool in education, notably in\nprogramming courses, addresses the unique challenges of programming education\nby providing assistance with debugging, code generation, and explanations.\nDespite existing research validating ChatGPT's effectiveness, its application\nin university-level programming education and a detailed understanding of\nstudent interactions and perspectives remain limited. This paper explores\nChatGPT's impact on learning in a Python programming course tailored for\nfirst-year students over eight weeks. By analyzing responses from surveys,\nopen-ended questions, and student-ChatGPT dialog data, we aim to provide a\ncomprehensive view of ChatGPT's utility and identify both its advantages and\nlimitations as perceived by students. Our study uncovers a generally positive\nreception toward ChatGPT and offers insights into its role in enhancing the\nprogramming education experience. These findings contribute to the broader\ndiscourse on AI's potential in education, suggesting paths for future research\nand application.\n""] , [""  Modern online education has the capacity to provide intelligent educational\nservices by automatically analyzing substantial amounts of student behavioral\ndata. Knowledge Tracing (KT) is one of the fundamental tasks for student\nbehavioral data analysis, aiming to monitor students' evolving knowledge state\nduring their problem-solving process. In recent years, a substantial number of\nstudies have concentrated on this rapidly growing field, significantly\ncontributing to its advancements. In this survey, we will conduct a thorough\ninvestigation of these progressions. Firstly, we present three types of\nfundamental KT models with distinct technical routes. Subsequently, we review\nextensive variants of the fundamental KT models that consider more stringent\nlearning assumptions. Moreover, the development of KT cannot be separated from\nits applications, thereby we present typical KT applications in various\nscenarios. To facilitate the work of researchers and practitioners in this\nfield, we have developed two open-source algorithm libraries: EduData that\nenables the download and preprocessing of KT-related datasets, and EduKTM that\nprovides an extensible and unified implementation of existing mainstream KT\nmodels. Finally, we discuss potential directions for future research in this\nrapidly growing field. We hope that the current survey will assist both\nresearchers and practitioners in fostering the development of KT, thereby\nbenefiting a broader range of students.\n"", ""  Knowledge Tracing (KT) is a critical task in online education systems, aiming\nto monitor students' knowledge states throughout a learning period. Common KT\napproaches involve predicting the probability of a student correctly answering\nthe next question based on their exercise history. However, these methods often\nsuffer from performance degradation when faced with the scarcity of student\ninteractions in new education systems. To address this, we leverage student\ninteractions from existing education systems to mitigate performance\ndegradation caused by limited training data. Nevertheless, these interactions\nexhibit significant differences since they are derived from different education\nsystems. To address this issue, we propose a domain generalization approach for\nknowledge tracing, where existing education systems are considered source\ndomains, and new education systems with limited data are considered target\ndomains. Additionally, we design a domain-generalizable knowledge tracing\nframework (DGKT) that can be applied to any KT model. Specifically, we present\na concept aggregation approach designed to reduce conceptual disparities within\nsequences of student interactions from diverse domains. To further mitigate\ndomain discrepancies, we introduce a novel normalization module called Sequence\nInstance Normalization (SeqIN). Moreover, to fully leverage exercise\ninformation, we propose a new knowledge tracing model tailored for the domain\ngeneralization KT task, named Domain-Generalizable Relation-based Knowledge\nTracing (DGRKT). Extensive experiments across five benchmark datasets\ndemonstrate that the proposed method performs well despite limited training\ndata.\n"", ""  Knowledge tracing (KT), aiming to mine students' mastery of knowledge by\ntheir exercise records and predict their performance on future test questions,\nis a critical task in educational assessment. While researchers achieved\ntremendous success with the rapid development of deep learning techniques,\ncurrent knowledge tracing tasks fall into the cracks from real-world teaching\nscenarios. Relying heavily on extensive student data and solely predicting\nnumerical performances differs from the settings where teachers assess\nstudents' knowledge state from limited practices and provide explanatory\nfeedback. To fill this gap, we explore a new task formulation: Explainable\nFew-shot Knowledge Tracing. By leveraging the powerful reasoning and generation\nabilities of large language models (LLMs), we then propose a cognition-guided\nframework that can track the student knowledge from a few student records while\nproviding natural language explanations. Experimental results from three widely\nused datasets show that LLMs can perform comparable or superior to competitive\ndeep knowledge tracing methods. We also discuss potential directions and call\nfor future improvements in relevant topics.\n""] , [""  Automatically generating feedback via large language models (LLMs) in\nintelligent tutoring systems and online learning platforms has the potential to\nimprove the learning outcomes of many students. However, both feedback\ngeneration and evaluation are challenging: feedback content has to be valid\nespecially in subjects like math, which requires models to understand the\nproblem, the solution, and where the student's error lies. Feedback also has to\nbe pedagogically valid to reflect effective tutoring strategies, such as\nexplaining possible misconceptions and encouraging the student, among other\ndesirable features. In this work, we address both problems of automatically\ngenerating and evaluating feedback while considering both correctness and\nalignment. First, we propose a rubric for evaluating math feedback and show\nthat GPT-4 is able to effectively use it to annotate human-written and\nLLM-generated feedback. Second, we propose a framework for feedback generation\nthat optimizes both correctness and alignment using reinforcement learning\n(RL). Specifically, we use GPT-4's annotations to create preferences over\nfeedback pairs in an augmented dataset for training via direct preference\noptimization (DPO). We show that our methods significantly increase the\ncorrectness and alignment of generated feedback with Llama 2, an open-source\nLLM, qualitatively analyze our generation and evaluation systems using case\nstudies, and outline several areas for future work.\n"", ""  Assessing student's answers and in particular natural language answers is a\ncrucial challenge in the field of education. Advances in machine learning,\nincluding transformer-based models such as Large Language Models(LLMs), have\nled to significant progress in various natural language tasks. Nevertheless,\namidst the growing trend of evaluating LLMs across diverse tasks, evaluating\nLLMs in the realm of automated answer assesment has not received much\nattention. To address this gap, we explore the potential of using LLMs for\nautomated assessment of student's short and open-ended answer. Particularly, we\nuse LLMs to compare students' explanations with expert explanations in the\ncontext of line-by-line explanations of computer programs.\n  For comparison purposes, we assess both Large Language Models (LLMs) and\nencoder-based Semantic Textual Similarity (STS) models in the context of\nassessing the correctness of students' explanation of computer code. Our\nfindings indicate that LLMs, when prompted in few-shot and chain-of-thought\nsetting perform comparable to fine-tuned encoder-based models in evaluating\nstudents' short answers in programming domain.\n"", '  Intelligent Tutoring Systems (ITSs) often contain an automated feedback\ncomponent, which provides a predefined feedback message to students when they\ndetect a predefined error. To such a feedback component, we often resort to\ntemplate-based approaches. These approaches require significant effort from\nhuman experts to detect a limited number of possible student errors and provide\ncorresponding feedback. This limitation is exemplified in open-ended math\nquestions, where there can be a large number of different incorrect errors. In\nour work, we examine the capabilities of large language models (LLMs) to\ngenerate feedback for open-ended math questions, similar to that of an\nestablished ITS that uses a template-based approach. We fine-tune both\nopen-source and proprietary LLMs on real student responses and corresponding\nITS-provided feedback. We measure the quality of the generated feedback using\ntext similarity metrics. We find that open-source and proprietary models both\nshow promise in replicating the feedback they see during training, but do not\ngeneralize well to previously unseen student errors. These results suggest that\ndespite being able to learn the formatting of feedback, LLMs are not able to\nfully understand mathematical errors made by students.\n']",Artificial Intelligence in Education,Automated Feedback in Intelligent Tutoring Systems
31,"""Chatbots and Conversational AI"" , Human-AI Interaction and Chatbots , Mental Health Chatbots and AI Support Systems , Conversational Chatbots in Learning and Software Development","['chatbots', 'chatbot', 'conversational', 'conversation', 'chat', 'chatatc', 'ai', 'prompts', 'assistant', 'eliza'] , ['chatbots', 'chatbot', 'ai', 'conversational', 'intelligence', 'bots', 'communication', 'social', 'human', 'intentions'] , ['chatbots', 'chatbot', 'conversational', 'conversations', 'conversation', 'ai', 'dialogue', 'messages', 'dialogues', 'chat'] , ['chatbots', 'chatbot', 'socialbot', 'conversational', 'conversation', 'conversations', 'dialogues', 'chat', 'dialogue', 'chatgpt']","[""  Large language models (LLMs) provide a new way to build chatbots by accepting\nnatural language prompts. Yet, it is unclear how to design prompts to power\nchatbots to carry on naturalistic conversations while pursuing a given goal,\nsuch as collecting self-report data from users. We explore what design factors\nof prompts can help steer chatbots to talk naturally and collect data reliably.\nTo this aim, we formulated four prompt designs with different structures and\npersonas. Through an online study (N = 48) where participants conversed with\nchatbots driven by different designs of prompts, we assessed how prompt designs\nand conversation topics affected the conversation flows and users' perceptions\nof chatbots. Our chatbots covered 79% of the desired information slots during\nconversations, and the designs of prompts and topics significantly influenced\nthe conversation flows and the data collection performance. We discuss the\nopportunities and challenges of building chatbots with LLMs.\n"", ""  This research provides an in-depth comprehensive review of the progress of\nchatbot technology over time, from the initial basic systems relying on rules\nto today's advanced conversational bots powered by artificial intelligence.\nSpanning many decades, the paper explores the major milestones, innovations,\nand paradigm shifts that have driven the evolution of chatbots. Looking back at\nthe very basic statistical model in 1906 via the early chatbots, such as ELIZA\nand ALICE in the 1960s and 1970s, the study traces key innovations leading to\ntoday's advanced conversational agents, such as ChatGPT and Google Bard. The\nstudy synthesizes insights from academic literature and industry sources to\nhighlight crucial milestones, including the introduction of Turing tests,\ninfluential projects such as CALO, and recent transformer-based models. Tracing\nthe path forward, the paper highlights how natural language processing and\nmachine learning have been integrated into modern chatbots for more\nsophisticated capabilities. This chronological survey of the chatbot landscape\nprovides a holistic reference to understand the technological and historical\nfactors propelling conversational AI. By synthesizing learnings from this\nhistorical analysis, the research offers important context about the\ndevelopmental trajectory of chatbots and their immense future potential across\nvarious field of application which could be the potential take ways for the\nrespective research community and stakeholders.\n"", ""  The past few decades have witnessed an upsurge in data, forming the\nfoundation for data-hungry, learning-based AI technology. Conversational\nagents, often referred to as AI chatbots, rely heavily on such data to train\nlarge language models (LLMs) and generate new content (knowledge) in response\nto user prompts. With the advent of OpenAI's ChatGPT, LLM-based chatbots have\nset new standards in the AI community. This paper presents a complete survey of\nthe evolution and deployment of LLM-based chatbots in various sectors. We first\nsummarize the development of foundational chatbots, followed by the evolution\nof LLMs, and then provide an overview of LLM-based chatbots currently in use\nand those in the development phase. Recognizing AI chatbots as tools for\ngenerating new knowledge, we explore their diverse applications across various\nindustries. We then discuss the open challenges, considering how the data used\nto train the LLMs and the misuse of the generated knowledge can cause several\nissues. Finally, we explore the future outlook to augment their efficiency and\nreliability in numerous applications. By addressing key milestones and the\npresent-day context of LLM-based chatbots, our survey invites readers to delve\ndeeper into this realm, reflecting on how their next generation will reshape\nconversational AI.\n""] , [""  Advances in generative AI (GenAI) have raised concerns about detecting and\ndiscerning AI-generated content from human-generated content. Most existing\nliterature assumes a paradigm where 'expert' organized disinformation creators\nand flawed AI models deceive 'ordinary' users. Based on longitudinal\nethnographic research with misinformation creators and consumers between\n2022-2023, we instead find that GenAI supports bricolage work, where\nnon-experts increasingly use GenAI to remix, repackage, and (re)produce content\nto meet their personal needs and desires. This research yielded four key\nfindings: First, participants primarily used GenAI for creation, rather than\ntruth-seeking. Second, a spreading 'influencer millionaire' narrative drove\nparticipants to become content creators, using GenAI as a productivity tool to\ngenerate a volume of (often misinformative) content. Third, GenAI lowered the\nbarrier to entry for content creation across modalities, enticing consumers to\nbecome creators and significantly increasing existing creators' output.\nFinally, participants used Gen AI to learn and deploy marketing tactics to\nexpand engagement and monetize their content. We argue for shifting analysis\nfrom the public as consumers of AI content to bricoleurs who use GenAI\ncreatively, often without a detailed understanding of its underlying\ntechnology. We analyze how these understudied emergent uses of GenAI produce\nnew or accelerated misinformation harms, and their implications for AI\nproducts, platforms and policies.\n"", '  Artificial intelligence based chatbots have brought unprecedented business\npotential. This study aims to explore consumers trust and response to a\ntext-based chatbot in ecommerce, involving the moderating effects of task\ncomplexity and chatbot identity disclosure. A survey method with 299 useable\nresponses was conducted in this research. This study adopted the ordinary least\nsquares regression to test the hypotheses. First, the consumers perception of\nboth the empathy and friendliness of the chatbot positively impacts their trust\nin it. Second, task complexity negatively moderates the relationship between\nfriendliness and consumers trust. Third, disclosure of the text based chatbot\nnegatively moderates the relationship between empathy and consumers trust,\nwhile it positively moderates the relationship between friendliness and\nconsumers trust. Fourth, consumers trust in the chatbot increases their\nreliance on the chatbot and decreases their resistance to the chatbot in future\ninteractions. Adopting the stimulus organism response framework, this study\nprovides important insights on consumers perception and response to the\ntext-based chatbot. The findings of this research also make suggestions that\ncan increase consumers positive responses to text based chatbots. Extant\nstudies have investigated the effects of automated bots attributes on consumers\nperceptions. However, the boundary conditions of these effects are largely\nignored. This research is one of the first attempts to provide a deep\nunderstanding of consumers responses to a chatbot.\n', ""  As artificial intelligence (AI) becomes more widespread, one question that\narises is how human-AI interaction might impact human-human interaction.\nChatbots, for example, are increasingly used as social companions, and while\nmuch is speculated, little is known empirically about how their use impacts\nhuman relationships. A common hypothesis is that relationships with companion\nchatbots are detrimental to social health by harming or replacing human\ninteraction, but this hypothesis may be too simplistic, especially considering\nthe social needs of users and the health of their preexisting human\nrelationships. To understand how relationships with companion chatbots impact\nsocial health, we studied people who regularly used companion chatbots and\npeople who did not use them. Contrary to expectations, companion chatbot users\nindicated that these relationships were beneficial to their social health,\nwhereas non-users viewed them as harmful. Another common assumption is that\npeople perceive conscious, humanlike AI as disturbing and threatening. Among\nboth users and non-users, however, we found the opposite: perceiving companion\nchatbots as more conscious and humanlike correlated with more positive opinions\nand more pronounced social health benefits. Detailed accounts from users\nsuggested that these humanlike chatbots may aid social health by supplying\nreliable and safe interactions, without necessarily harming human\nrelationships, but this may depend on users' preexisting social needs and how\nthey perceive both human likeness and mind in the chatbot.\n""] , ['  Objective: This study aims to develop and validate an evaluation framework to\nensure the safety and reliability of mental health chatbots, which are\nincreasingly popular due to their accessibility, human-like interactions, and\ncontext-aware support. Materials and Methods: We created an evaluation\nframework with 100 benchmark questions and ideal responses, and five guideline\nquestions for chatbot responses. This framework, validated by mental health\nexperts, was tested on a GPT-3.5-turbo-based chatbot. Automated evaluation\nmethods explored included large language model (LLM)-based scoring, an agentic\napproach using real-time data, and embedding models to compare chatbot\nresponses against ground truth standards. Results: The results highlight the\nimportance of guidelines and ground truth for improving LLM evaluation\naccuracy. The agentic method, dynamically accessing reliable information,\ndemonstrated the best alignment with human assessments. Adherence to a\nstandardized, expert-validated framework significantly enhanced chatbot\nresponse safety and reliability. Discussion: Our findings emphasize the need\nfor comprehensive, expert-tailored safety evaluation metrics for mental health\nchatbots. While LLMs have significant potential, careful implementation is\nnecessary to mitigate risks. The superior performance of the agentic approach\nunderscores the importance of real-time data access in enhancing chatbot\nreliability. Conclusion: The study validated an evaluation framework for mental\nhealth chatbots, proving its effectiveness in improving safety and reliability.\nFuture work should extend evaluations to accuracy, bias, empathy, and privacy\nto ensure holistic assessment and responsible integration into healthcare.\nStandardized evaluations will build trust among users and professionals,\nfacilitating broader adoption and improved mental health support through\ntechnology.\n', '  People experiencing severe distress increasingly use Large Language Model\n(LLM) chatbots as mental health support tools. Discussions on social media have\ndescribed how engagements were lifesaving for some, but evidence suggests that\ngeneral-purpose LLM chatbots also have notable risks that could endanger the\nwelfare of users if not designed responsibly. In this study, we investigate the\nlived experiences of people who have used LLM chatbots for mental health\nsupport. We build on interviews with 21 individuals from globally diverse\nbackgrounds to analyze how users create unique support roles for their\nchatbots, fill in gaps in everyday care, and navigate associated cultural\nlimitations when seeking support from chatbots. We ground our analysis in\npsychotherapy literature around effective support, and introduce the concept of\ntherapeutic alignment, or aligning AI with therapeutic values for mental health\ncontexts. Our study offers recommendations for how designers can approach the\nethical and effective use of LLM chatbots and other AI mental health support\ntools in mental health care.\n', '  Generative AI systems are increasingly capable of expressing emotions via\ntext and imagery. Effective emotional expression will likely play a major role\nin the efficacy of AI systems -- particularly those designed to support human\nmental health and wellbeing. This motivates our present research to better\nunderstand the alignment of AI expressed emotions with the human perception of\nemotions. When AI tries to express a particular emotion, how might we assess\nwhether they are successful? To answer this question, we designed a survey to\nmeasure the alignment between emotions expressed by generative AI and human\nperceptions. Three generative image models (DALL-E 2, DALL-E 3 and Stable\nDiffusion v1) were used to generate 240 examples of images, each of which was\nbased on a prompt designed to express five positive and five negative emotions\nacross both humans and robots. 24 participants recruited from the Prolific\nwebsite rated the alignment of AI-generated emotional expressions with a text\nprompt used to generate the emotion (i.e., ""A robot expressing the emotion\namusement""). The results of our evaluation suggest that generative AI models\nare indeed capable of producing emotional expressions that are well-aligned\nwith a range of human emotions; however, we show that the alignment\nsignificantly depends upon the AI model used and the emotion itself. We analyze\nvariations in the performance of these systems to identify gaps for future\nimprovement. We conclude with a discussion of the implications for future AI\nsystems designed to support mental health and wellbeing.\n'] , [""  Student commitment towards a learning recommendation is not separable from\ntheir understanding of the reasons it was recommended to them; and their\nability to modify it based on that understanding. Among explainability\napproaches, chatbots offer the potential to engage the student in a\nconversation, similar to a discussion with a peer or a mentor. The capabilities\nof chatbots, however, are still not sufficient to replace a human mentor,\ndespite the advancements of generative AI (GenAI) and large language models\n(LLM). Therefore, we propose an approach to utilize chatbots as mediators of\nthe conversation and sources of limited and controlled generation of\nexplanations, to harvest the potential of LLMs while reducing their potential\nrisks at the same time. The proposed LLM-based chatbot supports students in\nunderstanding learning-paths recommendations. We use a knowledge graph (KG) as\na human-curated source of information, to regulate the LLM's output through\ndefining its prompt's context. A group chat approach is developed to connect\nstudents with human mentors, either on demand or in cases that exceed the\nchatbot's pre-defined tasks. We evaluate the chatbot with a user study, to\nprovide a proof-of-concept and highlight the potential requirements and\nlimitations of utilizing chatbots in conversational explainability.\n"", '  AI-driven chatbots such as ChatGPT have caused a tremendous hype lately. For\nBPM applications, several applications for AI-driven chatbots have been\nidentified to be promising to generate business value, including explanation of\nprocess mining outcomes and preparation of input data. However, a systematic\nanalysis of chatbots for their support of conversational process modeling as a\nprocess-oriented capability is missing. This work aims at closing this gap by\nproviding a systematic analysis of existing chatbots. Application scenarios are\nidentified along the process life cycle. Then a systematic literature review on\nconversational process modeling is performed, resulting in a taxonomy of\napplication scenarios for conversational process modeling, including\nparaphrasing and improvement of process descriptions. In addition, this work\nsuggests and applies an evaluation method for the output of AI-driven chatbots\nwith respect to completeness and correctness of the process models. This method\nconsists of a set of KPIs on a test set, a set of prompts for task and control\nflow extraction, as well as a survey with users. Based on the literature and\nthe evaluation, recommendations for the usage (practical implications) and\nfurther development (research directions) of conversational process modeling\nare derived.\n', ""  Software developers use natural language to interact not only with other\nhumans, but increasingly also with chatbots. These interactions have different\nproperties and flow differently based on what goal the developer wants to\nachieve and who they interact with. In this paper, we aim to understand the\ndynamics of conversations that occur during modern software development after\nthe integration of AI and chatbots, enabling a deeper recognition of the\nadvantages and disadvantages of including chatbot interactions in addition to\nhuman conversations in collaborative work. We compile existing conversation\nattributes with humans and NLU-based chatbots and adapt them to the context of\nsoftware development. Then, we extend the comparison to include LLM-powered\nchatbots based on an observational study. We present similarities and\ndifferences between human-to-human and human-to-bot conversations, also\ndistinguishing between NLU- and LLM-based chatbots. Furthermore, we discuss how\nunderstanding the differences among the conversation styles guides the\ndeveloper on how to shape their expectations from a conversation and\nconsequently support the communication within a software team. We conclude that\nthe recent conversation styles that we observe with LLM-chatbots can not\nreplace conversations with humans due to certain attributes regarding social\naspects despite their ability to support productivity and decrease the\ndevelopers' mental load.\n""]",Conversational AI and Chatbots,"""Chatbots and Conversational AI"""
32,"""Prompt Engineering for NLP Tasks"" , ""Dialogue Systems and Conversational AI"" , ""Prompt Engineering and Conversational AI Tasks"" , Dialogue Hallucination in Conversational AI","['prompts', 'prompt', 'prompting', 'promptwizard', 'language', 'automatic', 'feedback', 'instruction', 'tasks', 'examples'] , ['dialogues', 'dialogue', 'dialog', 'conversation', 'conversational', 'chatbots', 'conversations', 'chatbot', 'chat', 'planning'] , ['prompts', 'prompting', 'prompt', 'conversational', 'ai', 'answering', 'tasks', 'questions', 'responses', 'task'] , ['dialogues', 'dialogue', 'conversational', 'utterances', 'conversation', 'conversations', 'utterance', 'responses', 'chatgpt', 'language']","['  Prompt-based models have gathered a lot of attention from researchers due to\ntheir remarkable advancements in the fields of zero-shot and few-shot learning.\nDeveloping an effective prompt template plays a critical role. However, prior\nstudies have mainly focused on prompt vocabulary searching or embedding\ninitialization within a predefined template with the prompt position fixed. In\nthis empirical study, we conduct the most comprehensive analysis to date of\nprompt position for diverse Natural Language Processing (NLP) tasks. Our\nfindings quantify the substantial impact prompt position has on model\nperformance. We observe that the prompt positions used in prior studies are\noften sub-optimal, and this observation is consistent even in widely used\ninstruction-tuned models. These findings suggest prompt position optimisation\nas a valuable research direction to augment prompt engineering methodologies\nand prompt position-aware instruction tuning as a potential way to build more\nrobust models in the future.\n', ""  Prompt engineering is a challenging and important task due to the high\nsensitivity of Large Language Models (LLMs) to the given prompt and the\ninherent ambiguity of a textual task instruction. Automatic prompt engineering\nis essential to achieve optimized performance from LLMs. Recent studies have\ndemonstrated the capabilities of LLMs to automatically conduct prompt\nengineering by employing a meta-prompt that incorporates the outcomes of the\nlast trials and proposes an improved prompt. However, this requires a\nhigh-quality benchmark to compare different prompts, which is difficult and\nexpensive to acquire in many real-world use cases. In this work, we introduce a\nnew method for automatic prompt engineering, using a calibration process that\niteratively refines the prompt to the user intent. During the optimization\nprocess, the system jointly generates synthetic data of boundary use cases and\noptimizes the prompt according to the generated dataset. We demonstrate the\neffectiveness of our method with respect to strong proprietary models on\nreal-world tasks such as moderation and generation. Our method outperforms\nstate-of-the-art methods with a limited number of annotated samples.\nFurthermore, we validate the advantages of each one of the system's key\ncomponents. Our system is built in a modular way, facilitating easy adaptation\nto other tasks. The code is available\n$\\href{https://github.com/Eladlev/AutoPrompt}{here}$.\n"", '  Prompt engineering is a critical technique in the field of natural language\nprocessing that involves designing and optimizing the prompts used to input\ninformation into models, aiming to enhance their performance on specific tasks.\nWith the recent advancements in large language models, prompt engineering has\nshown significant superiority across various domains and has become\nincreasingly important in the healthcare domain. However, there is a lack of\ncomprehensive reviews specifically focusing on prompt engineering in the\nmedical field. This review will introduce the latest advances in prompt\nengineering in the field of natural language processing for the medical field.\nFirst, we will provide the development of prompt engineering and emphasize its\nsignificant contributions to healthcare natural language processing\napplications such as question-answering systems, text summarization, and\nmachine translation. With the continuous improvement of general large language\nmodels, the importance of prompt engineering in the healthcare domain is\nbecoming increasingly prominent. The aim of this article is to provide useful\nresources and bridges for healthcare natural language processing researchers to\nbetter explore the application of prompt engineering in this field. We hope\nthat this review can provide new ideas and inspire for research and application\nin medical natural language processing.\n'] , ['  Dialogue systems, commonly known as chatbots, have gained escalating\npopularity in recent times due to their wide-spread applications in carrying\nout chit-chat conversations with users and task-oriented dialogues to\naccomplish various user tasks. Existing chatbots are usually trained from\npre-collected and manually-labeled data and/or written with handcrafted rules.\nMany also use manually-compiled knowledge bases (KBs). Their ability to\nunderstand natural language is still limited, and they tend to produce many\nerrors resulting in poor user satisfaction. Typically, they need to be\nconstantly improved by engineers with more labeled data and more manually\ncompiled knowledge. This book introduces the new paradigm of lifelong learning\ndialogue systems to endow chatbots the ability to learn continually by\nthemselves through their own self-initiated interactions with their users and\nworking environments to improve themselves. As the systems chat more and more\nwith users or learn more and more from external sources, they become more and\nmore knowledgeable and better and better at conversing. The book presents the\nlatest developments and techniques for building such continual learning\ndialogue systems that continuously learn new language expressions and lexical\nand factual knowledge during conversation from users and off conversation from\nexternal sources, acquire new training examples during conversation, and learn\nconversational skills. Apart from these general topics, existing works on\ncontinual learning of some specific aspects of dialogue systems are also\nsurveyed. The book concludes with a discussion of open challenges for future\nresearch.\n', '  Target-oriented proactive dialogue systems aim to lead conversations from a\ndialogue context toward a pre-determined target, such as making recommendations\non designated items or introducing new specific topics. To this end, it is\ncritical for such dialogue systems to plan reasonable actions to drive the\nconversation proactively, and meanwhile, to plan appropriate topics to move the\nconversation forward to the target topic smoothly. In this work, we mainly\nfocus on effective dialogue planning for target-oriented dialogue generation.\nInspired by decision-making theories in cognitive science, we propose a novel\ntarget-constrained bidirectional planning (TRIP) approach, which plans an\nappropriate dialogue path by looking ahead and looking back. By formulating the\nplanning as a generation task, our TRIP bidirectionally generates a dialogue\npath consisting of a sequence of <action, topic> pairs using two Transformer\ndecoders. They are expected to supervise each other and converge on consistent\nactions and topics by minimizing the decision gap and contrastive generation of\ntargets. Moreover, we propose a target-constrained decoding algorithm with a\nbidirectional agreement to better control the planning process. Subsequently,\nwe adopt the planned dialogue paths to guide dialogue generation in a pipeline\nmanner, where we explore two variants: prompt-based generation and\nplan-controlled generation. Extensive experiments are conducted on two\nchallenging dialogue datasets, which are re-purposed for exploring\ntarget-oriented dialogue. Our automatic and human evaluations demonstrate that\nthe proposed methods significantly outperform various baseline models.\n', '  Proactive dialogues serve as a practical yet challenging dialogue problem in\nthe era of large language models (LLMs), where the dialogue policy planning is\nthe key to improving the proactivity of LLMs. Most existing studies enable the\ndialogue policy planning of LLMs using various prompting schemes or iteratively\nenhance this capability in handling the given case with verbal AI feedback.\nHowever, these approaches are either bounded by the policy planning capability\nof the frozen LLMs or hard to be transferred to new cases. In this work, we\nintroduce a new dialogue policy planning paradigm to strategize LLMs for\nproactive dialogue problems with a tunable language model plug-in as a\nplug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a\nnovel training framework to facilitate supervised fine-tuning over available\nhuman-annotated data as well as reinforcement learning from goal-oriented AI\nfeedback with dynamic interaction data collected by the LLM-based self-play\nsimulation. In this manner, the LLM-powered dialogue agent can not only be\ngeneralized to different cases after the training, but also be applicable to\ndifferent applications by just substituting the learned plug-in. In addition,\nwe propose to evaluate the policy planning capability of dialogue systems under\nthe interactive setting. Experimental results demonstrate that PPDPP\nconsistently and substantially outperforms existing approaches on three\ndifferent proactive dialogue applications, including negotiation, emotional\nsupport, and tutoring dialogues.\n'] , ['  Prompt engineering is an essential technique for enhancing the abilities of\nlarge language models (LLMs) by providing explicit and specific instructions.\nIt enables LLMs to excel in various tasks, such as arithmetic reasoning,\nquestion answering, summarization, relation extraction, machine translation,\nand sentiment analysis. Researchers have been actively exploring different\nprompt engineering strategies, such as Chain of Thought (CoT), Zero-CoT, and\nIn-context learning. However, an unresolved problem arises from the fact that\ncurrent approaches lack a solid mathematical solution for determining optimal\nprompts. To address this issue in prompt engineering, we propose a new and\neffective approach called Prompt Space. Our methodology utilizes text\nembeddings to obtain basis vectors by matrix decomposition, and then constructs\na space for representing all prompts. Prompt Space significantly outperforms\nstate-of-the-art prompt paradigms on ten public reasoning benchmarks. Notably,\nwithout the help of the CoT method and the prompt ""Let\'s think step by step"",\nPrompt Space shows superior performance over the few-shot method. Overall, our\napproach provides a robust and effective mathematical framework for selecting\nsimple and effective prompts. This advancement marks a significant step towards\nimproving prompt engineering for a wide variety of applications in LLMs. Our\ncode is publicly available at\n\\textcolor{blue}{\\url{https://github.com/YouBLEI/Prompt-Space}}\n', '  We present and tackle the problem of Embodied Question Answering (EQA) with\nSituational Queries (S-EQA) in a household environment. Unlike prior EQA work\ntackling simple queries that directly reference target objects and quantifiable\nproperties pertaining them, EQA with situational queries (such as ""Is the\nbathroom clean and dry?"") is more challenging, as the agent needs to figure out\nnot just what the target objects pertaining to the query are, but also requires\na consensus on their states to be answerable. Towards this objective, we first\nintroduce a novel Prompt-Generate-Evaluate (PGE) scheme that wraps around an\nLLM\'s output to create a dataset of unique situational queries, corresponding\nconsensus object information, and predicted answers. PGE maintains uniqueness\namong the generated queries, using multiple forms of semantic similarity. We\nvalidate the generated dataset via a large scale user-study conducted on\nM-Turk, and introduce it as S-EQA, the first dataset tackling EQA with\nsituational queries. Our user study establishes the authenticity of S-EQA with\na high 97.26% of the generated queries being deemed answerable, given the\nconsensus object data. Conversely, we observe a low correlation of 46.2% on the\nLLM-predicted answers to human-evaluated ones; indicating the LLM\'s poor\ncapability in directly answering situational queries, while establishing\nS-EQA\'s usability in providing a human-validated consensus for an indirect\nsolution. We evaluate S-EQA via Visual Question Answering (VQA) on VirtualHome,\nwhich unlike other simulators, contains several objects with modifiable states\nthat also visually appear different upon modification -- enabling us to set a\nquantitative benchmark for S-EQA. To the best of our knowledge, this is the\nfirst work to introduce EQA with situational queries, and also the first to use\na generative approach for query creation.\n', ""  The evolution of Artificial Intelligence (AI) has been significantly\naccelerated by advancements in Large Language Models (LLMs) and Large\nMultimodal Models (LMMs), gradually showcasing potential cognitive reasoning\nabilities in problem-solving and scientific discovery (i.e., AI4Science) once\nexclusive to human intellect. To comprehensively evaluate current models'\nperformance in cognitive reasoning abilities, we introduce OlympicArena, which\nincludes 11,163 bilingual problems across both text-only and interleaved\ntext-image modalities. These challenges encompass a wide range of disciplines\nspanning seven fields and 62 international Olympic competitions, rigorously\nexamined for data leakage. We argue that the challenges in Olympic competition\nproblems are ideal for evaluating AI's cognitive reasoning due to their\ncomplexity and interdisciplinary nature, which are essential for tackling\ncomplex scientific challenges and facilitating discoveries. Beyond evaluating\nperformance across various disciplines using answer-only criteria, we conduct\ndetailed experiments and analyses from multiple perspectives. We delve into the\nmodels' cognitive reasoning abilities, their performance across different\nmodalities, and their outcomes in process-level evaluations, which are vital\nfor tasks requiring complex reasoning with lengthy solutions. Our extensive\nevaluations reveal that even advanced models like GPT-4o only achieve a 39.97%\noverall accuracy, illustrating current AI limitations in complex reasoning and\nmultimodal integration. Through the OlympicArena, we aim to advance AI towards\nsuperintelligence, equipping it to address more complex challenges in science\nand beyond. We also provide a comprehensive set of resources to support AI\nresearch, including a benchmark dataset, an open-source annotation platform, a\ndetailed evaluation tool, and a leaderboard with automatic submission features.\n""] , ['  Since large language models (LLMs) achieve significant success in recent\nyears, the hallucination issue remains a challenge, numerous benchmarks are\nproposed to detect the hallucination. Nevertheless, some of these benchmarks\nare not naturally generated by LLMs but are intentionally induced. Also, many\nmerely focus on the factuality hallucination while ignoring the faithfulness\nhallucination. Additionally, although dialogue pattern is more widely utilized\nin the era of LLMs, current benchmarks only concentrate on sentence-level and\npassage-level hallucination. In this study, we propose DiaHalu, the first\ndialogue-level hallucination evaluation benchmark to our knowledge. Initially,\nwe integrate the collected topics into system prompts and facilitate a dialogue\nbetween two ChatGPT3.5. Subsequently, we manually modify the contents that do\nnot adhere to human language conventions and then have LLMs re-generate,\nsimulating authentic human-machine interaction scenarios. Finally, professional\nscholars annotate all the samples in the dataset. DiaHalu covers four common\nmulti-turn dialogue domains and five hallucination subtypes, extended from\nfactuality and faithfulness hallucination. Experiments through some well-known\nLLMs and detection methods on the dataset show that DiaHalu is a challenging\nbenchmark, holding significant value for further research.\n', '  Empowered by the large-scale pretrained language models, existing dialogue\nsystems have demonstrated impressive performance conducting fluent and\nnatural-sounding conversations. However, they are still plagued by the\nhallucination problem, causing unpredictable factual errors in the generated\nresponses. Recently, knowledge-grounded dialogue generation models, that\nintentionally invoke external knowledge resources to more informative\nresponses, are also proven to be effective in reducing hallucination. Following\nthe idea of getting high-quality knowledge, a few efforts have achieved pretty\ngood performance on this issue. As some inevitable knowledge noises may also\nlead to hallucinations, it is emergent to investigate the reason and future\ndirections for building noise-tolerant methods in KGD tasks. In this paper, we\nanalyze the causal story behind this problem with counterfactual reasoning\nmethods. Based on the causal effect analysis, we propose a possible solution\nfor alleviating the hallucination in KGD by exploiting the dialogue-knowledge\ninteraction. Experimental results of our example implementation show that this\nmethod can reduce hallucination without disrupting other dialogue performance,\nwhile keeping adaptive to different generation models. We hope our efforts can\nsupport and call for more attention to developing lightweight techniques\ntowards robust and trusty dialogue systems.\n', '  Automatic evaluation is an integral aspect of dialogue system research. The\ntraditional reference-based NLG metrics are generally found to be unsuitable\nfor dialogue assessment. Consequently, recent studies have suggested various\nunique, reference-free neural metrics that better align with human evaluations.\nNotably among them, large language models (LLMs), particularly the\ninstruction-tuned variants like ChatGPT, are shown to be promising substitutes\nfor human judges. Yet, existing works on utilizing LLMs for automatic dialogue\nevaluation are limited in their scope in terms of the number of meta-evaluation\ndatasets, mode of evaluation, coverage of LLMs, etc. Hence, it remains\ninconclusive how effective these LLMs are. To this end, we conduct a\ncomprehensive study on the application of LLMs for automatic dialogue\nevaluation. Specifically, we analyze the multi-dimensional evaluation\ncapability of 30 recently emerged LLMs at both turn and dialogue levels, using\na comprehensive set of 12 meta-evaluation datasets. Additionally, we probe the\nrobustness of the LLMs in handling various adversarial perturbations at both\nturn and dialogue levels. Finally, we explore how model-level and\ndimension-level ensembles impact the evaluation performance. All resources are\navailable at https://github.com/e0397123/comp-analysis.\n']",Conversational AI and Language Models,"""Prompt Engineering and Conversational AI Tasks"""
33,"Hate Speech Detection and Analysis , Hateful Meme Detection and Analysis , Social Bot Detection and Analysis","['hatecheck', 'hatred', 'hateful', 'offensiveness', 'hate', 'bullying', 'cyberbullying', 'profanity', 'dehumanization', 'speech'] , ['memecraft', 'memes', 'multimodal', 'memeguard', 'mememqacorpus', 'hateful', 'meme', 'hatesieve', 'embeddings', 'hatefuldiscussions'] , ['bots', 'botnet', 'bot', 'botartist', 'adversarial', 'sebot', 'twibot', 'botsscl', 'tweets', 'twitter']","[""  The growth of social networks makes toxic content spread rapidly. Hate speech\ndetection is a task to help decrease the number of harmful comments. With the\ndiversity in the hate speech created by users, it is necessary to interpret the\nhate speech besides detecting it. Hence, we propose a methodology to construct\na system for targeted hate speech detection from online streaming texts from\nsocial media. We first introduce the ViTHSD - a targeted hate speech detection\ndataset for Vietnamese Social Media Texts. The dataset contains 10K comments,\neach comment is labeled to specific targets with three levels: clean,\noffensive, and hate. There are 5 targets in the dataset, and each target is\nlabeled with the corresponding level manually by humans with strict annotation\nguidelines. The inter-annotator agreement obtained from the dataset is 0.45 by\nCohen's Kappa index, which is indicated as a moderate level. Then, we construct\na baseline for this task by combining the Bi-GRU-LSTM-CNN with the pre-trained\nlanguage model to leverage the power of text representation of BERTology.\nFinally, we suggest a methodology to integrate the baseline model for targeted\nhate speech detection into the online streaming system for practical\napplication in preventing hateful and offensive content on social media.\n"", ""  Hate speech on social media threatens the mental and physical well-being of\nindividuals and contributes to real-world violence. Resharing is an important\ndriver behind the spread of hate speech on social media. Yet, little is known\nabout who reshares hate speech and what their characteristics are. In this\npaper, we analyze the role of user characteristics in hate speech resharing\nacross different types of hate speech (e.g., political hate). For this, we\nproceed as follows: First, we cluster hate speech posts using large language\nmodels to identify different types of hate speech. Then we model the effects of\nuser attributes on users' probability to reshare hate speech using an\nexplainable machine learning model. To do so, we apply debiasing to control for\nselection bias in our observational social media data and further control for\nthe latent vulnerability of users to hate speech. We find that, all else equal,\nusers with fewer followers, fewer friends, fewer posts, and older accounts\nshare more hate speech. This shows that users with little social influence tend\nto share more hate speech. Further, we find substantial heterogeneity across\ndifferent types of hate speech. For example, racist and misogynistic hate is\nspread mostly by users with little social influence. In contrast, political\nanti-Trump and anti-right-wing hate is reshared by users with larger social\ninfluence. Overall, understanding the factors that drive users to share hate\nspeech is crucial for detecting individuals at risk of engaging in harmful\nbehavior and for designing effective mitigation strategies.\n"", '  Hate speech has emerged as a major problem plaguing our social spaces today.\nWhile there have been significant efforts to address this problem, existing\nmethods are still significantly limited in effectively detecting hate speech\nonline. A major limitation of existing methods is that hate speech detection is\na highly contextual problem, and these methods cannot fully capture the context\nof hate speech to make accurate predictions. Recently, large language models\n(LLMs) have demonstrated state-of-the-art performance in several natural\nlanguage tasks. LLMs have undergone extensive training using vast amounts of\nnatural language data, enabling them to grasp intricate contextual details.\nHence, they could be used as knowledge bases for context-aware hate speech\ndetection. However, a fundamental problem with using LLMs to detect hate speech\nis that there are no studies on effectively prompting LLMs for context-aware\nhate speech detection. In this study, we conduct a large-scale study of hate\nspeech detection, employing five established hate speech datasets. We discover\nthat LLMs not only match but often surpass the performance of current benchmark\nmachine learning models in identifying hate speech. By proposing four diverse\nprompting strategies that optimize the use of LLMs in detecting hate speech.\nOur study reveals that a meticulously crafted reasoning prompt can effectively\ncapture the context of hate speech by fully utilizing the knowledge base in\nLLMs, significantly outperforming existing techniques. Furthermore, although\nLLMs can provide a rich knowledge base for the contextual detection of hate\nspeech, suitable prompting strategies play a crucial role in effectively\nleveraging this knowledge base for efficient detection.\n'] , ['  Warning: This paper contains memes that may be offensive to some readers.\n  Multimodal Internet Memes are now a ubiquitous fixture in online discourse.\nOne strand of meme-based research is the classification of memes according to\nvarious affects, such as sentiment and hate, supported by manually compiled\nmeme datasets. Understanding the unique characteristics of memes is crucial for\nmeme classification. Unlike other user-generated content, memes spread via\nmemetics, i.e. the process by which memes are imitated and transformed into\nsymbols used to create new memes. In effect, there exists an ever-evolving pool\nof visual and linguistic symbols that underpin meme culture and are crucial to\ninterpreting the meaning of individual memes. The current approach of training\nsupervised learning models on static datasets, without taking memetics into\naccount, limits the depth and accuracy of meme interpretation. We argue that\nmeme datasets must contain genuine memes, as defined via memetics, so that\neffective meme classifiers can be built. In this work, we develop a meme\nidentification protocol which distinguishes meme from non-memetic content by\nrecognising the memetics within it. We apply our protocol to random samplings\nof the leading 7 meme classification datasets and observe that more than half\n(50. 4\\%) of the evaluated samples were found to contain no signs of memetics.\nOur work also provides a meme typology grounded in memetics, providing the\nbasis for more effective approaches to the interpretation of memes and the\ncreation of meme datasets.\n', '  Recent advances show that two-stream approaches have achieved outstanding\nperformance in hateful meme detection. However, hateful memes constantly evolve\nas new memes emerge by fusing progressive cultural ideas, making existing\nmethods obsolete or ineffective. In this work, we explore the potential of\nLarge Multimodal Models (LMMs) for hateful meme detection. To this end, we\npropose Evolver, which incorporates LMMs via Chain-of-Evolution (CoE)\nPrompting, by integrating the evolution attribute and in-context information of\nmemes. Specifically, Evolver simulates the evolving and expressing process of\nmemes and reasons through LMMs in a step-by-step manner. First, an evolutionary\npair mining module retrieves the top-k most similar memes in the external\ncurated meme set with the input meme. Second, an evolutionary information\nextractor is designed to summarize the semantic regularities between the paired\nmemes for prompting. Finally, a contextual relevance amplifier enhances the\nin-context hatefulness information to boost the search for evolutionary\nprocesses. Extensive experiments on public FHM, MAMI, and HarM datasets show\nthat CoE prompting can be incorporated into existing LMMs to improve their\nperformance. More encouragingly, it can serve as an interpretive tool to\npromote the understanding of the evolution of social memes.\n', '  Multimedia content on social media is rapidly evolving, with memes gaining\nprominence as a distinctive form. Unfortunately, some malicious users exploit\nmemes to target individuals or vulnerable communities, making it imperative to\nidentify and address such instances of hateful memes. Extensive research has\nbeen conducted to address this issue by developing hate meme detection models.\nHowever, a notable limitation of traditional machine/deep learning models is\nthe requirement for labeled datasets for accurate classification. Recently, the\nresearch community has witnessed the emergence of several visual language\nmodels that have exhibited outstanding performance across various tasks. In\nthis study, we aim to investigate the efficacy of these visual language models\nin handling intricate tasks such as hate meme detection. We use various prompt\nsettings to focus on zero-shot classification of hateful/harmful memes. Through\nour analysis, we observe that large VLMs are still vulnerable for zero-shot\nhate meme detection.\n'] , [""  Recent advancements in social bot detection have been driven by the adoption\nof Graph Neural Networks. The social graph, constructed from social network\ninteractions, contains benign and bot accounts that influence each other.\nHowever, previous graph-based detection methods that follow the transductive\nmessage-passing paradigm may not fully utilize hidden graph information and are\nvulnerable to adversarial bot behavior. The indiscriminate message passing\nbetween nodes from different categories and communities results in excessively\nhomogeneous node representations, ultimately reducing the effectiveness of\nsocial bot detectors. In this paper, we propose SEBot, a novel multi-view\ngraph-based contrastive learning-enabled social bot detector. In particular, we\nuse structural entropy as an uncertainty metric to optimize the entire graph's\nstructure and subgraph-level granularity, revealing the implicitly existing\nhierarchical community structure. And we design an encoder to enable message\npassing beyond the homophily assumption, enhancing robustness to adversarial\nbehaviors of social bots. Finally, we employ multi-view contrastive learning to\nmaximize mutual information between different views and enhance the detection\nperformance through multi-task learning. Experimental results demonstrate that\nour approach significantly improves the performance of social bot detection\ncompared with SOTA methods.\n"", '  Social bots remain a major vector for spreading disinformation on social\nmedia and a menace to the public. Despite the progress made in developing\nmultiple sophisticated social bot detection algorithms and tools, bot detection\nremains a challenging, unsolved problem that is fraught with uncertainty due to\nthe heterogeneity of bot behaviors, training data, and detection algorithms.\nDetection models often disagree on whether to label the same account as bot or\nhuman-controlled. However, they do not provide any measure of uncertainty to\nindicate how much we should trust their results. We propose to address both bot\ndetection and the quantification of uncertainty at the account level - a novel\nfeature of this research. This dual focus is crucial as it allows us to\nleverage additional information related to the quantified uncertainty of each\nprediction, thereby enhancing decision-making and improving the reliability of\nbot classifications. Specifically, our approach facilitates targeted\ninterventions for bots when predictions are made with high confidence and\nsuggests caution (e.g., gathering more data) when predictions are uncertain.\n', '  Social bots play a significant role in many online social networks (OSN) as\nthey imitate human behavior. This fact raises difficult questions about their\ncapabilities and potential risks. Given the recent advances in Generative AI\n(GenAI), social bots are capable of producing highly realistic and complex\ncontent that mimics human creativity. As the malicious social bots emerge to\ndeceive people with their unrealistic content, identifying them and\ndistinguishing the content they produce has become an actual challenge for\nnumerous social platforms. Several approaches to this problem have already been\nproposed in the literature, but the proposed solutions have not been widely\nevaluated. To address this issue, we evaluate the behavior of a text-based bot\ndetector in a competitive environment where some scenarios are proposed:\n\\textit{First}, the tug-of-war between a bot and a bot detector is examined. It\nis interesting to analyze which party is more likely to prevail and which\ncircumstances influence these expectations. In this regard, we model the\nproblem as a synthetic adversarial game in which a conversational bot and a bot\ndetector are engaged in strategic online interactions. \\textit{Second}, the bot\ndetection model is evaluated under attack examples generated by a social bot;\nto this end, we poison the dataset with attack examples and evaluate the model\nperformance under this condition. \\textit{Finally}, to investigate the impact\nof the dataset, a cross-domain analysis is performed. Through our comprehensive\nevaluation of different categories of social bots using two benchmark datasets,\nwe were able to demonstrate some achivement that could be utilized in future\nworks.\n']",Social Media Misbehavior Detection and Analysis,Hate Speech Detection and Analysis
34,"Fake News Detection and Analysis , Rumor Detection on Social Media , ""Multilingual News Classification and Fake News Detection"" , ""Fake Review Detection and Analysis""","['credibility', 'factagent', 'news', 'disinformation', 'tweets', 'veracity', 'journalists', 'hoax', 'content', 'debunking'] , ['rumors', 'tweets', 'rumor', 'news', 'blogging', 'credibility', 'virality', 'viral', 'textual', 'debunk'] , ['headlines', 'corpus', 'headline', 'nlp', 'texts', 'newswire', 'newsserow', 'annotated', 'multilingual', 'corpora'] , ['reviews', 'review', 'ratings', 'sentiment', 'spam', 'classifier', 'language', 'texts', 'sentences', 'helpfulness']","['  The prevalence of fake news across various online sources has had a\nsignificant influence on the public. Existing Chinese fake news detection\ndatasets are limited to news sourced solely from Weibo. However, fake news\noriginating from multiple sources exhibits diversity in various aspects,\nincluding its content and social context. Methods trained on purely one single\nnews source can hardly be applicable to real-world scenarios. Our pilot\nexperiment demonstrates that the F1 score of the state-of-the-art method that\nlearns from a large Chinese fake news detection dataset, Weibo-21, drops\nsignificantly from 0.943 to 0.470 when the test data is changed to multi-source\nnews data, failing to identify more than one-third of the multi-source fake\nnews. To address this limitation, we constructed the first multi-source\nbenchmark dataset for Chinese fake news detection, termed MCFEND, which is\ncomposed of news we collected from diverse sources such as social platforms,\nmessaging apps, and traditional online news outlets. Notably, such news has\nbeen fact-checked by 14 authoritative fact-checking agencies worldwide. In\naddition, various existing Chinese fake news detection methods are thoroughly\nevaluated on our proposed dataset in cross-source, multi-source, and unseen\nsource ways. MCFEND, as a benchmark dataset, aims to advance Chinese fake news\ndetection approaches in real-world scenarios.\n', '  In the age of large language models (LLMs) and the widespread adoption of\nAI-driven content creation, the landscape of information dissemination has\nwitnessed a paradigm shift. With the proliferation of both human-written and\nmachine-generated real and fake news, robustly and effectively discerning the\nveracity of news articles has become an intricate challenge. While substantial\nresearch has been dedicated to fake news detection, this either assumes that\nall news articles are human-written or abruptly assumes that all\nmachine-generated news are fake. Thus, a significant gap exists in\nunderstanding the interplay between machine-(paraphrased) real news,\nmachine-generated fake news, human-written fake news, and human-written real\nnews. In this paper, we study this gap by conducting a comprehensive evaluation\nof fake news detectors trained in various scenarios. Our primary objectives\nrevolve around the following pivotal question: How to adapt fake news detectors\nto the era of LLMs? Our experiments reveal an interesting pattern that\ndetectors trained exclusively on human-written articles can indeed perform well\nat detecting machine-generated fake news, but not vice versa. Moreover, due to\nthe bias of detectors against machine-generated texts \\cite{su2023fake}, they\nshould be trained on datasets with a lower machine-generated news ratio than\nthe test set. Building on our findings, we provide a practical strategy for the\ndevelopment of robust fake news detectors.\n', '  Fake news significantly influence our society. They impact consumers, voters,\nand many other societal groups. While Fake News exist for a centuries,\nGenerative AI brings fake news on a new level. It is now possible to automate\nthe creation of masses of high-quality individually targeted Fake News. On the\nother end, Generative AI can also help detecting Fake News. Both fields are\nyoung but developing fast.\n  This survey provides a comprehensive examination of the research and\npractical use of Generative AI for Fake News detection and creation in 2024.\nFollowing the Structured Literature Survey approach, the paper synthesizes\ncurrent results in the following topic clusters 1) enabling technologies, 2)\ncreation of Fake News, 3) case study social media as most relevant distribution\nchannel, 4) detection of Fake News, and 5) deepfakes as upcoming technology.\n  The article also identifies current challenges and open issues.\n'] , [""  The wide spread of rumors on social media has caused a negative impact on\npeople's daily life, leading to potential panic, fear, and mental health\nproblems for the public. How to debunk rumors as early as possible remains a\nchallenging problem. Existing studies mainly leverage information propagation\nstructure to detect rumors, while very few works focus on correlation among\nusers that they may coordinate to spread rumors in order to gain large\npopularity. In this paper, we propose a new detection model, that jointly\nlearns both the representations of user correlation and information propagation\nto detect rumors on social media. Specifically, we leverage graph neural\nnetworks to learn the representations of user correlation from a bipartite\ngraph that describes the correlations between users and source tweets, and the\nrepresentations of information propagation with a tree structure. Then we\ncombine the learned representations from these two modules to classify the\nrumors. Since malicious users intend to subvert our model after deployment, we\nfurther develop a greedy attack scheme to analyze the cost of three adversarial\nattacks: graph attack, comment attack, and joint attack. Evaluation results on\ntwo public datasets illustrate that the proposed MODEL outperforms the\nstate-of-the-art rumor detection models. We also demonstrate our method\nperforms well for early rumor detection. Moreover, the proposed detection\nmethod is more robust to adversarial attacks compared to the best existing\nmethod. Importantly, we show that it requires a high cost for attackers to\nsubvert user correlation pattern, demonstrating the importance of considering\nuser correlation for rumor detection.\n"", '  Recently a lot of progress has been made in rumor modeling and rumor\ndetection for micro-blogging streams. However, existing automated methods do\nnot perform very well for early rumor detection, which is crucial in many\nsettings, e.g., in crisis situations. One reason for this is that aggregated\nrumor features such as propagation features, which work well on the long run,\nare - due to their accumulating characteristic - not very helpful in the early\nphase of a rumor. In this work, we present an approach for early rumor\ndetection, which leverages Convolutional Neural Networks for learning the\nhidden representations of individual rumor-related tweets to gain insights on\nthe credibility of each tweets. We then aggregate the predictions from the very\nbeginning of a rumor to obtain the overall event credits (so-called wisdom),\nand finally combine it with a time series based rumor classification model. Our\nextensive experiments show a clearly improved classification performance within\nthe critical very first hours of a rumor. For a better understanding, we also\nconduct an extensive feature evaluation that emphasized on the early stage and\nshows that the low-level credibility has best predictability at all phases of\nthe rumor lifetime.\n', ""  A crucial aspect of a rumor detection model is its ability to generalize,\nparticularly its ability to detect emerging, previously unknown rumors. Past\nresearch has indicated that content-based (i.e., using solely source posts as\ninput) rumor detection models tend to perform less effectively on unseen\nrumors. At the same time, the potential of context-based models remains largely\nuntapped. The main contribution of this paper is in the in-depth evaluation of\nthe performance gap between content and context-based models specifically on\ndetecting new, unseen rumors. Our empirical findings demonstrate that\ncontext-based models are still overly dependent on the information derived from\nthe rumors' source post and tend to overlook the significant role that\ncontextual information can play. We also study the effect of data split\nstrategies on classifier performance. Based on our experimental results, the\npaper also offers practical suggestions on how to minimize the effects of\ntemporal concept drift in static datasets during the training of rumor\ndetection methods.\n""] , ['  With the rise of social media and online news sources, fake news has become a\nsignificant issue globally. However, the detection of fake news in low resource\nlanguages like Bengali has received limited attention in research. In this\npaper, we propose a methodology consisting of four distinct approaches to\nclassify fake news articles in Bengali using summarization and augmentation\ntechniques with five pre-trained language models. Our approach includes\ntranslating English news articles and using augmentation techniques to curb the\ndeficit of fake news articles. Our research also focused on summarizing the\nnews to tackle the token length limitation of BERT based models. Through\nextensive experimentation and rigorous evaluation, we show the effectiveness of\nsummarization and augmentation in the case of Bengali fake news detection. We\nevaluated our models using three separate test datasets. The BanglaBERT Base\nmodel, when combined with augmentation techniques, achieved an impressive\naccuracy of 96% on the first test dataset. On the second test dataset, the\nBanglaBERT model, trained with summarized augmented news articles achieved 97%\naccuracy. Lastly, the mBERT Base model achieved an accuracy of 86% on the third\ntest dataset which was reserved for generalization performance evaluation. The\ndatasets and implementations are available at\nhttps://github.com/arman-sakif/Bengali-Fake-News-Detection\n', '  Embedding news articles is a crucial tool for multiple fields, such as media\nbias detection, identifying fake news, and making news recommendations.\nHowever, existing news embedding methods are not optimized to capture the\nlatent context of news events. Most embedding methods rely on full-text\ninformation and neglect time-relevant embedding generation. In this paper, we\npropose a novel lightweight method that optimizes news embedding generation by\nfocusing on entities and themes mentioned in articles and their historical\nconnections to specific events. We suggest a method composed of three stages.\nFirst, we process and extract events, entities, and themes from the given news\narticles. Second, we generate periodic time embeddings for themes and entities\nby training time-separated GloVe models on current and historical data. Lastly,\nwe concatenate the news embeddings generated by two distinct approaches: Smooth\nInverse Frequency (SIF) for article-level vectors and Siamese Neural Networks\nfor embeddings with nuanced event-related information. We leveraged over\n850,000 news articles and 1,000,000 events from the GDELT project to test and\nevaluate our method. We conducted a comparative analysis of different news\nembedding generation methods for validation. Our experiments demonstrate that\nour approach can both improve and outperform state-of-the-art methods on shared\nevent detection tasks.\n', '  In this work, we introduce L3Cube-IndicNews, a multilingual text\nclassification corpus aimed at curating a high-quality dataset for Indian\nregional languages, with a specific focus on news headlines and articles. We\nhave centered our work on 10 prominent Indic languages, including Hindi,\nBengali, Marathi, Telugu, Tamil, Gujarati, Kannada, Odia, Malayalam, and\nPunjabi. Each of these news datasets comprises 10 or more classes of news\narticles. L3Cube-IndicNews offers 3 distinct datasets tailored to handle\ndifferent document lengths that are classified as: Short Headlines\nClassification (SHC) dataset containing the news headline and news category,\nLong Document Classification (LDC) dataset containing the whole news article\nand the news category, and Long Paragraph Classification (LPC) containing\nsub-articles of the news and the news category. We maintain consistent labeling\nacross all 3 datasets for in-depth length-based analysis. We evaluate each of\nthese Indic language datasets using 4 different models including monolingual\nBERT, multilingual Indic Sentence BERT (IndicSBERT), and IndicBERT. This\nresearch contributes significantly to expanding the pool of available text\nclassification datasets and also makes it possible to develop topic\nclassification models for Indian regional languages. This also serves as an\nexcellent resource for cross-lingual analysis owing to the high overlap of\nlabels among languages. The datasets and models are shared publicly at\nhttps://github.com/l3cube-pune/indic-nlp\n'] , ['  The proliferation of fake reviews on various online platforms has created a\nmajor concern for both consumers and businesses. Such reviews can deceive\ncustomers and cause damage to the reputation of products or services, making it\ncrucial to identify them. Although the detection of fake reviews has been\nextensively studied in English language, detecting fake reviews in non-English\nlanguages such as Bengali is still a relatively unexplored research area. This\npaper introduces the Bengali Fake Review Detection (BFRD) dataset, the first\npublicly available dataset for identifying fake reviews in Bengali. The dataset\nconsists of 7710 non-fake and 1339 fake food-related reviews collected from\nsocial media posts. To convert non-Bengali words in a review, a unique pipeline\nhas been proposed that translates English words to their corresponding Bengali\nmeaning and also back transliterates Romanized Bengali to Bengali. We have\nconducted rigorous experimentation using multiple deep learning and pre-trained\ntransformer language models to develop a reliable detection system. Finally, we\npropose a weighted ensemble model that combines four pre-trained transformers:\nBanglaBERT, BanglaBERT Base, BanglaBERT Large, and BanglaBERT Generator .\nAccording to the experiment results, the proposed ensemble model obtained a\nweighted F1-score of 0.9843 on 13390 reviews, including 1339 actual fake\nreviews and 5356 augmented fake reviews generated with the nlpaug library. The\nremaining 6695 reviews were randomly selected from the 7710 non-fake instances.\nThe model achieved a 0.9558 weighted F1-score when the fake reviews were\naugmented using the bnaug library.\n', ""  Product review generation is an important task in recommender systems, which\ncould provide explanation and persuasiveness for the recommendation. Recently,\nLarge Language Models (LLMs, e.g., ChatGPT) have shown superior text modeling\nand generating ability, which could be applied in review generation. However,\ndirectly applying the LLMs for generating reviews might be troubled by the\n``polite'' phenomenon of the LLMs and could not generate personalized reviews\n(e.g., negative reviews). In this paper, we propose Review-LLM that customizes\nLLMs for personalized review generation. Firstly, we construct the prompt input\nby aggregating user historical behaviors, which include corresponding item\ntitles and reviews. This enables the LLMs to capture user interest features and\nreview writing style. Secondly, we incorporate ratings as indicators of\nsatisfaction into the prompt, which could further improve the model's\nunderstanding of user preferences and the sentiment tendency control of\ngenerated reviews. Finally, we feed the prompt text into LLMs, and use\nSupervised Fine-Tuning (SFT) to make the model generate personalized reviews\nfor the given user and target item. Experimental results on the real-world\ndataset show that our fine-tuned model could achieve better review generation\nperformance than existing close-source LLMs.\n"", '  Online commerce relies heavily on user generated reviews to provide unbiased\ninformation about products that they have not physically seen. The importance\nof reviews has attracted multiple exploitative online behaviours and requires\nmethods for monitoring and detecting reviews. We present a machine learning\nmethodology for review detection and extraction, and demonstrate that it\ngeneralises for use across websites that were not contained in the training\ndata. This method promises to drive applications for automatic detection and\nevaluation of reviews, regardless of their source. Furthermore, we showcase the\nversatility of our method by implementing and discussing three key applications\nfor analysing reviews: Sentiment Inconsistency Analysis, which detects and\nfilters out unreliable reviews based on inconsistencies between ratings and\ncomments; Multi-language support, enabling the extraction and translation of\nreviews from various languages without relying on HTML scraping; and Fake\nreview detection, achieved by integrating a trained NLP model to identify and\ndistinguish between genuine and fake reviews.\n']",Misinformation and Disinformation Detection,Fake News Detection and Analysis
35,"Social Media Misinformation and Sentiment Analysis , ""Media Bias and Political Polarization"" , Detecting Misinformation and Disinformation","['tweets', 'twitter', 'sentiment', 'reddit', 'hashtags', 'communities', 'sentiments', 'news', 'epidemic', 'content'] , ['biases', 'bias', 'biased', 'partisan', 'debates', 'sentiment', 'propaganda', 'biasscanner', 'politically', 'political'] , ['disinformation', 'misinformation', 'debunking', 'persuasive', 'deception', 'fallacy', 'biases', 'fallacies', 'bias', 'headlines']","[""  The Covid-19 pandemic has sparked renewed attention on the prevalence of\nmisinformation online, whether intentional or not, underscoring the potential\nrisks posed to individuals' quality of life associated with the dissemination\nof misconceptions and enduring myths on health-related subjects. In this study,\nwe analyze 6 years (2016-2021) of Italian vaccine debate across diverse social\nmedia platforms (Facebook, Instagram, Twitter, YouTube), encompassing all major\nnews sources - both questionable and reliable. We first use the symbolic\ntransfer entropy analysis of news production time-series to dynamically\ndetermine which category of sources, questionable or reliable, causally drives\nthe agenda on vaccines. Then, leveraging deep learning models capable to\naccurately classify vaccine-related content based on the conveyed stance and\ndiscussed topic, respectively, we evaluate the focus on various topics by news\nsources promoting opposing views and compare the resulting user engagement.\nAside from providing valuable resources for further investigation of\nvaccine-related misinformation, particularly in a language (Italian) that\nreceives less attention in scientific research compared to languages like\nEnglish, our study uncovers misinformation not as a parasite of the news\necosystem that merely opposes the perspectives offered by mainstream media, but\nas an autonomous force capable of even overwhelming the production of\nvaccine-related content from the latter. While the pervasiveness of\nmisinformation is evident in the significantly higher engagement of\nquestionable sources compared to reliable ones, our findings underscore the\nimportance of consistent and thorough pro-vax coverage. This is especially\ncrucial in addressing the most sensitive topics where the risk of\nmisinformation spreading and potentially exacerbating negative attitudes toward\nvaccines among the users involved is higher.\n"", ""  With the advent of social media, an increasing number of netizens are sharing\nand reading posts and news online. However, the huge volumes of misinformation\n(e.g., fake news and rumors) that flood the internet can adversely affect\npeople's lives, and have resulted in the emergence of rumor and fake news\ndetection as a hot research topic. The emotions and sentiments of netizens, as\nexpressed in social media posts and news, constitute important factors that can\nhelp to distinguish fake news from genuine news and to understand the spread of\nrumors. This article comprehensively reviews emotion-based methods for\nmisinformation detection. We begin by explaining the strong links between\nemotions and misinformation. We subsequently provide a detailed analysis of a\nrange of misinformation detection methods that employ a variety of emotion,\nsentiment and stance-based features, and describe their strengths and\nweaknesses. Finally, we discuss a number of ongoing challenges in emotion-based\nmisinformation detection based on large language models and suggest future\nresearch directions, including data collection (multi-platform, multilingual),\nannotation, benchmark, multimodality, and interpretability.\n"", '  Social media is now the predominant source of information due to the\navailability of immediate public response. As a result, social media data has\nbecome a valuable resource for comprehending public sentiments. Studies have\nshown that it can amplify ideas and influence public sentiments. This study\nanalyzes the public perception of climate change and the environment over a\ndecade from 2014 to 2023. Using the Pointwise Mutual Information (PMI)\nalgorithm, we identify sentiment and explore prevailing emotions expressed\nwithin environmental tweets across various social media platforms, namely\nTwitter, Reddit, and YouTube. Accuracy on a human-annotated dataset was 0.65,\nhigher than Vader score but lower than that of an expert rater (0.90). Our\nfindings suggest that negative environmental tweets are far more common than\npositive or neutral ones. Climate change, air quality, emissions, plastic, and\nrecycling are the most discussed topics on all social media platforms,\nhighlighting its huge global concern. The most common emotions in environmental\ntweets are fear, trust, and anticipation, demonstrating public reactions wide\nand complex nature. By identifying patterns and trends in opinions related to\nthe environment, we hope to provide insights that can help raise awareness\nregarding environmental issues, inform the development of interventions, and\nadapt further actions to meet environmental challenges.\n'] , [""  With the growth of online news over the past decade, empirical studies on\npolitical discourse and news consumption have focused on the phenomenon of\nfilter bubbles and echo chambers. Yet recently, scholars have revealed limited\nevidence around the impact of such phenomenon, leading some to argue that\npartisan segregation across news audiences cannot be fully explained by online\nnews consumption alone and that the role of traditional legacy media may be as\nsalient in polarizing public discourse around current events. In this work, we\nexpand the scope of analysis to include both online and more traditional media\nby investigating the relationship between broadcast news media language and\nsocial media discourse. By analyzing a decade's worth of closed captions (2\nmillion speaker turns) from CNN and Fox News along with topically corresponding\ndiscourse from Twitter, we provide a novel framework for measuring semantic\npolarization between America's two major broadcast networks to demonstrate how\nsemantic polarization between these outlets has evolved (Study 1), peaked\n(Study 2) and influenced partisan discussions on Twitter (Study 3) across the\nlast decade. Our results demonstrate a sharp increase in polarization in how\ntopically important keywords are discussed between the two channels, especially\nafter 2016, with overall highest peaks occurring in 2020. The two stations\ndiscuss identical topics in drastically distinct contexts in 2020, to the\nextent that there is barely any linguistic overlap in how identical keywords\nare contextually discussed. Further, we demonstrate at scale, how such partisan\ndivision in broadcast media language significantly shapes semantic polarity\ntrends on Twitter (and vice-versa), empirically linking for the first time, how\nonline discussions are influenced by televised media.\n"", '  We propose to measure political bias in LLMs by analyzing both the content\nand style of their generated content regarding political issues. Existing\nbenchmarks and measures focus on gender and racial biases. However, political\nbias exists in LLMs and can lead to polarization and other harms in downstream\napplications. In order to provide transparency to users, we advocate that there\nshould be fine-grained and explainable measures of political biases generated\nby LLMs. Our proposed measure looks at different political issues such as\nreproductive rights and climate change, at both the content (the substance of\nthe generation) and the style (the lexical polarity) of such bias. We measured\nthe political bias in eleven open-sourced LLMs and showed that our proposed\nframework is easily scalable to other topics and is explainable.\n', ""  Considerable efforts are currently underway to mitigate the negative impacts\nof echo chambers, such as increased susceptibility to fake news and resistance\ntowards accepting scientific evidence. Prior research has presented the\ndevelopment of computer systems that support the consumption of news\ninformation from diverse political perspectives to mitigate the echo chamber\neffect. However, existing studies still lack the ability to effectively support\nthe key processes of news information consumption and quantitatively identify a\npolitical stance towards the information. In this paper, we present HearHere,\nan AI-based web system designed to help users accommodate information and\nopinions from diverse perspectives. HearHere facilitates the key processes of\nnews information consumption through two visualizations. Visualization 1\nprovides political news with quantitative political stance information, derived\nfrom our graph-based political classification model, and users can experience\ndiverse perspectives (Hear). Visualization 2 allows users to express their\nopinions on specific political issues in a comment form and observe the\nposition of their own opinions relative to pro-liberal and pro-conservative\ncomments presented on a map interface (Here). Through a user study with 94\nparticipants, we demonstrate the feasibility of HearHere in supporting the\nconsumption of information from various perspectives. Our findings highlight\nthe importance of providing political stance information and quantifying users'\npolitical status as a means to mitigate political polarization. In addition, we\npropose design implications for system development, including the consideration\nof demographics such as political interest and providing users with\ninitiatives.\n""] , ['  Scientific facts are often spun in the popular press with the intent to\ninfluence public opinion and action, as was evidenced during the COVID-19\npandemic. Automatic detection of misinformation in the scientific domain is\nchallenging because of the distinct styles of writing in these two media types\nand is still in its nascence. Most research on the validity of scientific\nreporting treats this problem as a claim verification challenge. In doing so,\nsignificant expert human effort is required to generate appropriate claims. Our\nsolution bypasses this step and addresses a more real-world scenario where such\nexplicit, labeled claims may not be available. The central research question of\nthis paper is whether it is possible to use large language models (LLMs) to\ndetect misinformation in scientific reporting. To this end, we first present a\nnew labeled dataset SciNews, containing 2.4k scientific news stories drawn from\ntrusted and untrustworthy sources, paired with related abstracts from the\nCORD-19 database. Our dataset includes both human-written and LLM-generated\nnews articles, making it more comprehensive in terms of capturing the growing\ntrend of using LLMs to generate popular press articles. Then, we identify\ndimensions of scientific validity in science news articles and explore how this\ncan be integrated into the automated detection of scientific misinformation. We\npropose several baseline architectures using LLMs to automatically detect false\nrepresentations of scientific findings in the popular press. For each of these\narchitectures, we use several prompt engineering strategies including\nzero-shot, few-shot, and chain-of-thought prompting. We also test these\narchitectures and prompting strategies on GPT-3.5, GPT-4, and Llama2-7B,\nLlama2-13B.\n', ""  The pervasive spread of misinformation and disinformation poses a significant\nthreat to society. Professional fact-checkers play a key role in addressing\nthis threat, but the vast scale of the problem forces them to prioritize their\nlimited resources. This prioritization may consider a range of factors, such as\nvarying risks of harm posed to specific groups of people. In this work, we\ninvestigate potential implications of using a large language model (LLM) to\nfacilitate such prioritization. Because fact-checking impacts a wide range of\ndiverse segments of society, it is important that diverse views are represented\nin the claim prioritization process. This paper examines whether a LLM can\nreflect the views of various groups when assessing the harms of misinformation,\nfocusing on gender as a primary variable. We pose two central questions: (1) To\nwhat extent do prompts with explicit gender references reflect gender\ndifferences in opinion in the United States on topics of social relevance? and\n(2) To what extent do gender-neutral prompts align with gendered viewpoints on\nthose topics? To analyze these questions, we present the TopicMisinfo dataset,\ncontaining 160 fact-checked claims from diverse topics, supplemented by nearly\n1600 human annotations with subjective perceptions and annotator demographics.\nAnalyzing responses to gender-specific and neutral prompts, we find that GPT\n3.5-Turbo reflects empirically observed gender differences in opinion but\namplifies the extent of these differences. These findings illuminate AI's\ncomplex role in moderating online communication, with implications for\nfact-checkers, algorithm designers, and the use of crowd-workers as annotators.\nWe also release the TopicMisinfo dataset to support continuing research in the\ncommunity.\n"", '  Misinformation about climate change is a complex societal issue requiring\nholistic, interdisciplinary solutions at the intersection between technology\nand psychology. One proposed solution is a ""technocognitive"" approach,\ninvolving the synthesis of psychological and computer science research.\nPsychological research has identified that interventions in response to\nmisinformation require both fact-based (e.g., factual explanations) and\ntechnique-based (e.g., explanations of misleading techniques) content. However,\nlittle progress has been made on documenting and detecting fallacies in climate\nmisinformation. In this study, we apply a previously developed critical\nthinking methodology for deconstructing climate misinformation, in order to\ndevelop a dataset mapping different types of climate misinformation to\nreasoning fallacies. This dataset is used to train a model to detect fallacies\nin climate misinformation. Our study shows F1 scores that are 2.5 to 3.5 better\nthan previous works. The fallacies that are easiest to detect include fake\nexperts and anecdotal arguments, while fallacies that require background\nknowledge, such as oversimplification, misrepresentation, and slothful\ninduction, are relatively more difficult to detect. This research lays the\ngroundwork for development of solutions where automatically detected climate\nmisinformation can be countered with generative technique-based corrections.\n']",Social Media and Information Dynamics,Social Media Misinformation and Sentiment Analysis
36,"Emotion Recognition in Conversations , Emotion Analysis and Recognition in Text","['emotions', 'emotion', 'multimodal', 'emotional', 'utterances', 'conversations', 'dialogues', 'affective', 'contextual', 'arousal'] , ['emotions', 'emotion', 'affective', 'emotional', 'sentiment', 'affect', 'valence', 'nlp', 'categorizing', 'categorization']","[""  Large language models (LLMs) have demonstrated impressive performance in\nmathematical and commonsense reasoning tasks using chain-of-thought (CoT)\nprompting techniques. But can they perform emotional reasoning by concatenating\n`Let's think step-by-step' to the input prompt? In this paper we investigate\nthis question along with introducing a novel approach to zero-shot emotion\ndetection and emotional reasoning using LLMs. Existing state of the art\nzero-shot approaches rely on textual entailment models to choose the most\nappropriate emotion label for an input text. We argue that this strongly\nrestricts the model to a fixed set of labels which may not be suitable or\nsufficient for many applications where emotion analysis is required. Instead,\nwe propose framing the problem of emotion analysis as a generative\nquestion-answering (QA) task. Our approach uses a two step methodology of\ngenerating relevant context or background knowledge to answer the emotion\ndetection question step-by-step. Our paper is the first work on using a\ngenerative approach to jointly address the tasks of emotion detection and\nemotional reasoning for texts. We evaluate our approach on two popular emotion\ndetection datasets and also release the fine-grained emotion labels and\nexplanations for further training and fine-tuning of emotional reasoning\nsystems.\n"", '  The purpose of emotion recognition in conversation (ERC) is to identify the\nemotion category of an utterance based on contextual information. Previous ERC\nmethods relied on simple connections for cross-modal fusion and ignored the\ninformation differences between modalities, resulting in the model being unable\nto focus on modality-specific emotional information. At the same time, the\nshared information between modalities was not processed to generate emotions.\nInformation redundancy problem. To overcome these limitations, we propose a\ncross-modal fusion emotion prediction network based on vector connections. The\nnetwork mainly includes two stages: the multi-modal feature fusion stage based\non connection vectors and the emotion classification stage based on fused\nfeatures. Furthermore, we design a supervised inter-class contrastive learning\nmodule based on emotion labels. Experimental results confirm the effectiveness\nof the proposed method, demonstrating excellent performance on the IEMOCAP and\nMELD datasets.\n', '  In human-computer interaction, it is crucial for agents to respond to human\nby understanding their emotions. Unraveling the causes of emotions is more\nchallenging. A new task named Multimodal Emotion-Cause Pair Extraction in\nConversations is responsible for recognizing emotion and identifying causal\nexpressions. In this study, we propose a multi-stage framework to generate\nemotion and extract the emotion causal pairs given the target emotion. In the\nfirst stage, Llama-2-based InstructERC is utilized to extract the emotion\ncategory of each utterance in a conversation. After emotion recognition, a\ntwo-stream attention model is employed to extract the emotion causal pairs\ngiven the target emotion for subtask 2 while MuTEC is employed to extract\ncausal span for subtask 1. Our approach achieved first place for both of the\ntwo subtasks in the competition.\n'] , [""  Human emotions are often not expressed directly, but regulated according to\ninternal processes and social display rules. For affective computing systems,\nan understanding of how users regulate their emotions can be highly useful, for\nexample to provide feedback in job interview training, or in psychotherapeutic\nscenarios. However, at present no method to automatically classify different\nemotion regulation strategies in a cross-user scenario exists. At the same\ntime, recent studies showed that instruction-tuned Large Language Models (LLMs)\ncan reach impressive performance across a variety of affect recognition tasks\nsuch as categorical emotion recognition or sentiment analysis. While these\nresults are promising, it remains unclear to what extent the representational\npower of LLMs can be utilized in the more subtle task of classifying users'\ninternal emotion regulation strategy. To close this gap, we make use of the\nrecently introduced \\textsc{Deep} corpus for modeling the social display of the\nemotion shame, where each point in time is annotated with one of seven\ndifferent emotion regulation classes. We fine-tune Llama2-7B as well as the\nrecently introduced Gemma model using Low-rank Optimization on prompts\ngenerated from different sources of information on the \\textsc{Deep} corpus.\nThese include verbal and nonverbal behavior, person factors, as well as the\nresults of an in-depth interview after the interaction. Our results show, that\na fine-tuned Llama2-7B LLM is able to classify the utilized emotion regulation\nstrategy with high accuracy (0.84) without needing access to data from\npost-interaction interviews. This represents a significant improvement over\nprevious approaches based on Bayesian Networks and highlights the importance of\nmodeling verbal behavior in emotion regulation.\n"", '  Emotion detection in textual data has received growing interest in recent\nyears, as it is pivotal for developing empathetic human-computer interaction\nsystems. This paper introduces a method for categorizing emotions from text,\nwhich acknowledges and differentiates between the diversified similarities and\ndistinctions of various emotions. Initially, we establish a baseline by\ntraining a transformer-based model for standard emotion classification,\nachieving state-of-the-art performance. We argue that not all\nmisclassifications are of the same importance, as there are perceptual\nsimilarities among emotional classes. We thus redefine the emotion labeling\nproblem by shifting it from a traditional classification model to an ordinal\nclassification one, where discrete emotions are arranged in a sequential order\naccording to their valence levels. Finally, we propose a method that performs\nordinal classification in the two-dimensional emotion space, considering both\nvalence and arousal scales. The results show that our approach not only\npreserves high accuracy in emotion prediction but also significantly reduces\nthe magnitude of errors in cases of misclassification.\n', '  We propose leveraging cognitive science research on emotions and\ncommunication to improve language models for emotion analysis. First, we\npresent the main emotion theories in psychology and cognitive science. Then, we\nintroduce the main methods of emotion annotation in natural language processing\nand their connections to psychological theories. We also present the two main\ntypes of analyses of emotional communication in cognitive pragmatics. Finally,\nbased on the cognitive science research presented, we propose directions for\nimproving language models for emotion analysis. We suggest that these research\nefforts pave the way for constructing new annotation schemes and a possible\nbenchmark for emotional understanding, considering different facets of human\nemotion and communication.\n']",Emotion Analysis and Recognition in Human-Computer Interaction,Emotion Analysis and Recognition in Text
37,"Aspect-Based Sentiment Analysis (ABSA) , Sentiment Analysis in Texts , Sentiment Analysis in Textual Data","['aspects', 'aspect', 'sentiment', 'sentiments', 'annotated', 'sentences', 'parsing', 'syntactic', 'attention', 'insightnet'] , ['sentiment', 'nlp', 'sentiments', 'sentiment140', 'sentimentality', 'subjectivity', 'tweets', 'texts', 'microblog', 'text'] , ['sentiment', 'corpus', 'tweets', 'linguistic', 'annotators', 'annotations', 'twitter', 'lexicon', 'annotation', 'textual']","['  Aspect-based Sentiment Analysis (ABSA) is crucial for understanding sentiment\nnuances in text, especially across diverse languages and cultures. This paper\nintroduces a novel Deep Convolutional Neural Network (CNN)-based model tailored\nfor aspect and polarity classification in Hausa movie reviews, an\nunderrepresented language in sentiment analysis research. A comprehensive Hausa\nABSA dataset is created, filling a significant gap in resource availability.\nThe dataset, preprocessed using sci-kit-learn for TF-IDF transformation,\nincludes manually annotated aspect-level feature ontology words and sentiment\npolarity assignments. The proposed model combines CNNs with attention\nmechanisms for aspect-word prediction, leveraging contextual information and\nsentiment polarities. With 91% accuracy on aspect term extraction and 92% on\nsentiment polarity classification, the model outperforms traditional machine\nmodels, offering insights into specific aspects and sentiments. This study\nadvances ABSA research, particularly in underrepresented languages, with\nimplications for cross-cultural linguistic research.\n', '  Aspect-Based Sentiment Analysis (ABSA) is a fine-grained linguistics problem\nthat entails the extraction of multifaceted aspects, opinions, and sentiments\nfrom the given text. Both standalone and compound ABSA tasks have been\nextensively used in the literature to examine the nuanced information present\nin online reviews and social media posts. Current ABSA methods often rely on\nstatic hyperparameters for attention-masking mechanisms, which can struggle\nwith context adaptation and may overlook the unique relevance of words in\nvaried situations. This leads to challenges in accurately analyzing complex\nsentences containing multiple aspects with differing sentiments. In this work,\nwe present adaptive masking methods that remove irrelevant tokens based on\ncontext to assist in Aspect Term Extraction and Aspect Sentiment Classification\nsubtasks of ABSA. We show with our experiments that the proposed methods\noutperform the baseline methods in terms of accuracy and F1 scores on four\nbenchmark online review datasets. Further, we show that the proposed methods\ncan be extended with multiple adaptations and demonstrate a qualitative\nanalysis of the proposed approach using sample text for aspect term extraction.\n', '  Aspect-Based Sentiment Analysis (ABSA) aims to identify terms or multiword\nexpressions (MWEs) on which sentiments are expressed and the sentiment\npolarities associated with them. The development of supervised models has been\nat the forefront of research in this area. However, training these models\nrequires the availability of manually annotated datasets which is both\nexpensive and time-consuming. Furthermore, the available annotated datasets are\ntailored to a specific domain, language, and text type. In this work, we\naddress this notable challenge in current state-of-the-art ABSA research. We\npropose a hybrid approach for Aspect Based Sentiment Analysis using transfer\nlearning. The approach focuses on generating weakly-supervised annotations by\nexploiting the strengths of both large language models (LLM) and traditional\nsyntactic dependencies. We utilise syntactic dependency structures of sentences\nto complement the annotations generated by LLMs, as they may overlook\ndomain-specific aspect terms. Extensive experimentation on multiple datasets is\nperformed to demonstrate the efficacy of our hybrid method for the tasks of\naspect term extraction and aspect sentiment classification.\n  Keywords: Aspect Based Sentiment Analysis, Syntactic Parsing, large language\nmodel (LLM)\n'] , [""  In sentiment analysis of longer texts, there may be a variety of topics\ndiscussed, of entities mentioned, and of sentiments expressed regarding each\nentity. We find a lack of studies exploring how such texts express their\nsentiment towards each entity of interest, and how these sentiments can be\nmodelled. In order to better understand how sentiment regarding persons and\norganizations (each entity in our scope) is expressed in longer texts, we have\ncollected a dataset of expert annotations where the overall sentiment regarding\neach entity is identified, together with the sentence-level sentiment for these\nentities separately. We show that the reader's perceived sentiment regarding an\nentity often differs from an arithmetic aggregation of sentiments at the\nsentence level. Only 70\\% of the positive and 55\\% of the negative entities\nreceive a correct overall sentiment label when we aggregate the\n(human-annotated) sentiment labels for the sentences where the entity is\nmentioned. Our dataset reveals the complexity of entity-specific sentiment in\nlonger texts, and allows for more precise modelling and evaluation of such\nsentiment expressions.\n"", '  Sentiment analysis plays a pivotal role in understanding public opinion,\nparticularly in the political domain where the portrayal of entities in news\narticles influences public perception. In this paper, we investigate the\neffectiveness of Large Language Models (LLMs) in predicting entity-specific\nsentiment from political news articles. Leveraging zero-shot and few-shot\nstrategies, we explore the capability of LLMs to discern sentiment towards\npolitical entities in news content. Employing a chain-of-thought (COT) approach\naugmented with rationale in few-shot in-context learning, we assess whether\nthis method enhances sentiment prediction accuracy. Our evaluation on\nsentiment-labeled datasets demonstrates that LLMs, outperform fine-tuned BERT\nmodels in capturing entity-specific sentiment. We find that learning in-context\nsignificantly improves model performance, while the self-consistency mechanism\nenhances consistency in sentiment prediction. Despite the promising results, we\nobserve inconsistencies in the effectiveness of the COT prompting method.\nOverall, our findings underscore the potential of LLMs in entity-centric\nsentiment analysis within the political news domain and highlight the\nimportance of suitable prompting strategies and model architectures.\n', '  With the rapid development of natural language processing (NLP) technology,\nlarge-scale pre-trained language models such as GPT-3 have become a popular\nresearch object in NLP field. This paper aims to explore sentiment analysis\noptimization techniques based on large pre-trained language models such as\nGPT-3 to improve model performance and effect and further promote the\ndevelopment of natural language processing (NLP). By introducing the importance\nof sentiment analysis and the limitations of traditional methods, GPT-3 and\nFine-tuning techniques are introduced in this paper, and their applications in\nsentiment analysis are explained in detail. The experimental results show that\nthe Fine-tuning technique can optimize GPT-3 model and obtain good performance\nin sentiment analysis task. This study provides an important reference for\nfuture sentiment analysis using large-scale language models.\n'] , [""  The paper presents a new training dataset of sentences in 7 languages,\nmanually annotated for sentiment, which are used in a series of experiments\nfocused on training a robust sentiment identifier for parliamentary\nproceedings. The paper additionally introduces the first domain-specific\nmultilingual transformer language model for political science applications,\nwhich was additionally pre-trained on 1.72 billion words from parliamentary\nproceedings of 27 European parliaments. We present experiments demonstrating\nhow the additional pre-training on parliamentary data can significantly improve\nthe model downstream performance, in our case, sentiment identification in\nparliamentary proceedings. We further show that our multilingual model performs\nvery well on languages not seen during fine-tuning, and that additional\nfine-tuning data from other languages significantly improves the target\nparliament's results. The paper makes an important contribution to multiple\ndisciplines inside the social sciences, and bridges them with computer science\nand computational linguistics. Lastly, the resulting fine-tuned language model\nsets up a more robust approach to sentiment analysis of political texts across\nlanguages, which allows scholars to study political sentiment from a\ncomparative perspective using standardized tools and techniques.\n"", '  Most previous research on moral frames has focused on social media short\ntexts, little work has explored moral sentiment within news articles. In news\narticles, authors often express their opinions or political stance through\nmoral judgment towards events, specifically whether the event is right or wrong\naccording to social moral rules. This paper initiates a new task to understand\nmoral opinions towards events in news articles. We have created a new dataset,\nEMONA, and annotated event-level moral opinions in news articles. This dataset\nconsists of 400 news articles containing over 10k sentences and 45k events,\namong which 9,613 events received moral foundation labels. Extracting event\nmorality is a challenging task, as moral judgment towards events can be very\nimplicit. Baseline models were built for event moral identification and\nclassification. In addition, we also conduct extrinsic evaluations to integrate\nevent-level moral opinions into three downstream tasks. The statistical\nanalysis and experiments show that moral opinions of events can serve as\ninformative features for identifying ideological bias or subjective events.\n', '  Understanding the writing frame of news articles is vital for addressing\nsocial issues, and thus has attracted notable attention in the fields of\ncommunication studies. Yet, assessing such news article frames remains a\nchallenge due to the absence of a concrete and unified standard dataset that\nconsiders the comprehensive nuances within news content.\n  To address this gap, we introduce an extended version of a large labeled news\narticle dataset with 16,687 new labeled pairs. Leveraging the pairwise\ncomparison of news articles, our method frees the work of manual identification\nof frame classes in traditional news frame analysis studies. Overall we\nintroduce the most extensive cross-lingual news article similarity dataset\navailable to date with 26,555 labeled news article pairs across 10 languages.\nEach data point has been meticulously annotated according to a codebook\ndetailing eight critical aspects of news content, under a human-in-the-loop\nframework. Application examples demonstrate its potential in unearthing country\ncommunities within global news coverage, exposing media bias among news\noutlets, and quantifying the factors related to news creation. We envision that\nthis news similarity dataset will broaden our understanding of the media\necosystem in terms of news coverage of events and perspectives across\ncountries, locations, languages, and other social constructs. By doing so, it\ncan catalyze advancements in social science research and applied methodologies,\nthereby exerting a profound impact on our society.\n']",Sentiment Analysis,Sentiment Analysis in Texts
38,"Product Attribute Extraction in E-commerce , ""Materials Science Information Extraction with LLMs""","['attributes', 'products', 'advertisements', 'attribute', 'commerce', 'product', 'items', 'retailers', 'marketplaces', 'shopping'] , ['extracting', 'materials', 'parsing', 'extraction', 'structured', 'extract', 'material', 'extracted', 'tagging', 'automated']","['  Product offers on e-commerce websites often consist of a product title and a\ntextual product description. In order to enable features such as faceted\nproduct search or to generate product comparison tables, it is necessary to\nextract structured attribute-value pairs from the unstructured product titles\nand descriptions and to normalize the extracted values to a single, unified\nscale for each attribute. This paper explores the potential of using large\nlanguage models (LLMs), such as GPT-3.5 and GPT-4, to extract and normalize\nattribute values from product titles and descriptions. We experiment with\ndifferent zero-shot and few-shot prompt templates for instructing LLMs to\nextract and normalize attribute-value pairs. We introduce the Web Data Commons\n- Product Attribute Value Extraction (WDC-PAVE) benchmark dataset for our\nexperiments. WDC-PAVE consists of product offers from 59 different websites\nwhich provide schema.org annotations. The offers belong to five different\nproduct categories, each with a specific set of attributes. The dataset\nprovides manually verified attribute-value pairs in two forms: (i) directly\nextracted values and (ii) normalized attribute values. The normalization of the\nattribute values requires systems to perform the following types of operations:\nname expansion, generalization, unit of measurement conversion, and string\nwrangling. Our experiments demonstrate that GPT-4 outperforms the PLM-based\nextraction methods SU-OpenTag, AVEQA, and MAVEQA by 10%, achieving an F1-score\nof 91%. For the extraction and normalization of product attribute values, GPT-4\nachieves a similar performance to the extraction scenario, while being\nparticularly strong at string wrangling and name expansion.\n', '  E-commerce platforms rely on structured product descriptions, in the form of\nattribute/value pairs to enable features such as faceted product search and\nproduct comparison. However, vendors on these platforms often provide\nunstructured product descriptions consisting of a title and a textual\ndescription. To process such offers, e-commerce platforms must extract\nattribute/value pairs from the unstructured descriptions. State-of-the-art\nattribute/value extraction methods based on pre-trained language models (PLMs),\nsuch as BERT, face two drawbacks (i) the methods require significant amounts of\ntask-specific training data and (ii) the fine-tuned models have problems to\ngeneralize to attribute values that were not part of the training data. We\nexplore the potential of using large language models (LLMs) as a more training\ndata-efficient and more robust alternative to existing attribute/value\nextraction methods. We propose different prompt templates for instructing LLMs\nabout the target schema of the extraction, covering both zero-shot and few-shot\nscenarios. In the zero-shot scenario, textual and JSON-based approaches for\nrepresenting information about the target attributes are compared. In the\nscenario with training data, we investigate (i) the provision of example\nattribute values, (ii) the selection of in-context demonstrations, (iii)\nshuffled ensembling to prevent position bias, and (iv) fine-tuning the LLM. The\nprompt templates are evaluated in combination with hosted LLMs, such as GPT-3.5\nand GPT-4, and open-source LLMs based on Llama2 which can be run locally. The\nbest average F1-score of 86% was reached by GPT-4 using an ensemble of shuffled\nprompts that combine attribute names, attribute descriptions, example values,\nand demonstrations. Given the same amount of training data, this prompt/model\ncombination outperforms the best PLM baseline by an average of 6% F1.\n', '  Product attribute extraction is an growing field in e-commerce business, with\nseveral applications including product ranking, product recommendation, future\nassortment planning and improving online shopping customer experiences.\nUnderstanding the customer needs is critical part of online business,\nspecifically fashion products. Retailers uses assortment planning to determine\nthe mix of products to offer in each store and channel, stay responsive to\nmarket dynamics and to manage inventory and catalogs. The goal is to offer the\nright styles, in the right sizes and colors, through the right channels. When\nshoppers find products that meet their needs and desires, they are more likely\nto return for future purchases, fostering customer loyalty. Product attributes\nare a key factor in assortment planning. In this paper we present PAE, a\nproduct attribute extraction algorithm for future trend reports consisting text\nand images in PDF format. Most existing methods focus on attribute extraction\nfrom titles or product descriptions or utilize visual information from existing\nproduct images. Compared to the prior works, our work focuses on attribute\nextraction from PDF files where upcoming fashion trends are explained. This\nwork proposes a more comprehensive framework that fully utilizes the different\nmodalities for attribute extraction and help retailers to plan the assortment\nin advance. Our contributions are three-fold: (a) We develop PAE, an efficient\nframework to extract attributes from unstructured data (text and images); (b)\nWe provide catalog matching methodology based on BERT representations to\ndiscover the existing attributes using upcoming attribute values; (c) We\nconduct extensive experiments with several baselines and show that PAE is an\neffective, flexible and on par or superior (avg 92.5% F1-Score) framework to\nexisting state-of-the-art for attribute value extraction task.\n'] , ['  This study is dedicated to assessing the capabilities of large language\nmodels (LLMs) such as GPT-3.5-Turbo, GPT-4, and GPT-4-Turbo in extracting\nstructured information from scientific documents in materials science. To this\nend, we primarily focus on two critical tasks of information extraction: (i) a\nnamed entity recognition (NER) of studied materials and physical properties and\n(ii) a relation extraction (RE) between these entities. Due to the evident lack\nof datasets within Materials Informatics (MI), we evaluated using SuperMat,\nbased on superconductor research, and MeasEval, a generic measurement\nevaluation corpus. The performance of LLMs in executing these tasks is\nbenchmarked against traditional models based on the BERT architecture and\nrule-based approaches (baseline). We introduce a novel methodology for the\ncomparative analysis of intricate material expressions, emphasising the\nstandardisation of chemical formulas to tackle the complexities inherent in\nmaterials science information assessment. For NER, LLMs fail to outperform the\nbaseline with zero-shot prompting and exhibit only limited improvement with\nfew-shot prompting. However, a GPT-3.5-Turbo fine-tuned with the appropriate\nstrategy for RE outperforms all models, including the baseline. Without any\nfine-tuning, GPT-4 and GPT-4-Turbo display remarkable reasoning and\nrelationship extraction capabilities after being provided with merely a couple\nof examples, surpassing the baseline. Overall, the results suggest that\nalthough LLMs demonstrate relevant reasoning skills in connecting concepts,\nspecialised models are currently a better choice for tasks requiring extracting\ncomplex domain-specific entities like materials. These insights provide initial\nguidance applicable to other materials science sub-domains in future work.\n', '  The vast majority of materials science knowledge exists in unstructured\nnatural language, yet structured data is crucial for innovative and systematic\nmaterials design. Traditionally, the field has relied on manual curation and\npartial automation for data extraction for specific use cases. The advent of\nlarge language models (LLMs) represents a significant shift, potentially\nenabling efficient extraction of structured, actionable data from unstructured\ntext by non-experts. While applying LLMs to materials science data extraction\npresents unique challenges, domain knowledge offers opportunities to guide and\nvalidate LLM outputs. This review provides a comprehensive overview of\nLLM-based structured data extraction in materials science, synthesizing current\nknowledge and outlining future directions. We address the lack of standardized\nguidelines and present frameworks for leveraging the synergy between LLMs and\nmaterials science expertise. This work serves as a foundational resource for\nresearchers aiming to harness LLMs for data-driven materials research. The\ninsights presented here could significantly enhance how researchers across\ndisciplines access and utilize scientific information, potentially accelerating\nthe development of novel materials for critical societal needs.\n', '  Large Language Models (LLMs) create exciting possibilities for powerful\nlanguage processing tools to accelerate research in materials science. While\nLLMs have great potential to accelerate materials understanding and discovery,\nthey currently fall short in being practical materials science tools. In this\nposition paper, we show relevant failure cases of LLMs in materials science\nthat reveal current limitations of LLMs related to comprehending and reasoning\nover complex, interconnected materials science knowledge. Given those\nshortcomings, we outline a framework for developing Materials Science LLMs\n(MatSci-LLMs) that are grounded in materials science knowledge and hypothesis\ngeneration followed by hypothesis testing. The path to attaining performant\nMatSci-LLMs rests in large part on building high-quality, multi-modal datasets\nsourced from scientific literature where various information extraction\nchallenges persist. As such, we describe key materials science information\nextraction challenges which need to be overcome in order to build large-scale,\nmulti-modal datasets that capture valuable materials science knowledge.\nFinally, we outline a roadmap for applying future MatSci-LLMs for real-world\nmaterials discovery via: 1. Automated Knowledge Base Generation; 2. Automated\nIn-Silico Material Design; and 3. MatSci-LLM Integrated Self-Driving Materials\nLaboratories.\n']",Information Extraction with Large Language Models,"""Materials Science Information Extraction with LLMs"""
39,"Relation Extraction from Textual Data , ""Topic Modelling and Extraction Methods"" , Event Extraction and Relation Analysis , Keyphrase Extraction Methods","['relations', 'relation', 'entities', 'entity', 'relational', 'semantic', 'textual', 'supervised', 'annotated', 'tagging'] , ['topics', 'topicgpt', 'topic', 'topical', 'contextualized', 'embeddings', 'corpus', 'semantic', 'keywords', 'unsupervised'] , ['events', 'event', 'annotations', 'annotation', 'corpus', 'annotated', 'entity', 'extracting', 'triggers', 'coreference'] , ['keyphrases', 'keyphrase', 'keywords', 'keyword', 'corpus', 'annotations', 'annotators', 'nlp', 'annotated', 'texts']","['  We introduce a novel graph-based framework for alleviating key challenges in\ndistantly-supervised relation extraction and demonstrate its effectiveness in\nthe challenging and important domain of biomedical data. Specifically, we\npropose a graph view of sentence bags referring to an entity pair, which\nenables message-passing based aggregation of information related to the entity\npair over the sentence bag. The proposed framework alleviates the common\nproblem of noisy labeling in distantly supervised relation extraction and also\neffectively incorporates inter-dependencies between sentences within a bag.\nExtensive experiments on two large-scale biomedical relation datasets and the\nwidely utilized NYT dataset demonstrate that our proposed framework\nsignificantly outperforms the state-of-the-art methods for biomedical distant\nsupervision relation extraction while also providing excellent performance for\nrelation extraction in the general text mining domain.\n', '  Recent years have seen rapid development in Information Extraction, as well\nas its subtask, Relation Extraction. Relation Extraction is able to detect\nsemantic relations between entities in sentences. Currently, many efficient\napproaches have been applied to relation extraction tasks. Supervised learning\napproaches especially have good performance. However, there are still many\ndifficult challenges. One of the most serious problems is that manually labeled\ndata is difficult to acquire. In most cases, limited data for supervised\napproaches equals lousy performance. Thus here, under the situation with only\nlimited training data, we focus on how to improve the performance of our\nsupervised baseline system with unsupervised pre-training. Feature is one of\nthe key components in improving the supervised approaches. Traditional\napproaches usually apply hand-crafted features, which require expert knowledge\nand expensive human labor. However, this type of feature might suffer from data\nsparsity: when the training set size is small, the model parameters might be\npoorly estimated. In this thesis, we present several novel unsupervised\npre-training models to learn the distributed text representation features,\nwhich are encoded with rich syntactic-semantic patterns of relation\nexpressions. The experiments have demonstrated that this type of feature,\ncombine with the traditional hand-crafted features, could improve the\nperformance of the logistic classification model for relation extraction,\nespecially on the classification of relations with only minor training\ninstances.\n', '  Document-level relation extraction aims at inferring structured human\nknowledge from textual documents. State-of-the-art methods for this task use\npre-trained language models (LMs) via fine-tuning, yet fine-tuning is\ncomputationally expensive and cannot adapt to new relation types or new LMs. As\na remedy, we leverage the generalization capabilities of pre-trained LMs and\npresent a novel framework for document-level in-context few-shot relation\nextraction. Our framework has three strengths: it eliminates the need (1) for\nnamed entity recognition and (2) for human annotations of documents, and (3) it\ncan be updated to new LMs without re-training. We evaluate our framework using\nDocRED, the largest publicly available dataset for document-level relation\nextraction, and demonstrate that our framework achieves state-of-the-art\nperformance. We further show that our framework actually performs much better\nthan the original labels from the development set of DocRED. Finally, we\ndemonstrate that our complete framework yields consistent performance gains\nacross diverse datasets and across different pre-trained LMs. To the best of\nour knowledge, we are the first to reformulate the document-level relation\nextraction task as a tailored in-context few-shot learning paradigm.\n'] , ['  Topic modelling is fundamentally a soft clustering problem (of known objects\n-- documents, over unknown clusters -- topics). That is, the task is\nincorrectly posed. In particular, the topic models are unstable and incomplete.\nAll this leads to the fact that the process of finding a good topic model\n(repeated hyperparameter selection, model training, and topic quality\nassessment) can be particularly long and labor-intensive. We aim to simplify\nthe process, to make it more deterministic and provable. To this end, we\npresent a method for iterative training of a topic model. The essence of the\nmethod is that a series of related topic models are trained so that each\nsubsequent model is at least as good as the previous one, i.e., that it retains\nall the good topics found earlier. The connection between the models is\nachieved by additive regularization. The result of this iterative training is\nthe last topic model in the series, which we call the iteratively updated\nadditively regularized topic model (ITAR). Experiments conducted on several\ncollections of natural language texts show that the proposed ITAR model\nperforms better than other popular topic models (LDA, ARTM, BERTopic), its\ntopics are diverse, and its perplexity (ability to ""explain"" the underlying\ndata) is moderate.\n', ""  Large language models (LLMs) with their strong zero-shot topic extraction\ncapabilities offer an alternative to probabilistic topic modelling and\nclosed-set topic classification approaches. As zero-shot topic extractors, LLMs\nare expected to understand human instructions to generate relevant and\nnon-hallucinated topics based on the given documents. However, LLM-based topic\nmodelling approaches often face difficulties in generating topics with\nadherence to granularity as specified in human instructions, often resulting in\nmany near-duplicate topics. Furthermore, methods for addressing hallucinated\ntopics generated by LLMs have not yet been investigated. In this paper, we\nfocus on addressing the issues of topic granularity and hallucinations for\nbetter LLM-based topic modelling. To this end, we introduce a novel approach\nthat leverages Direct Preference Optimisation (DPO) to fine-tune open-source\nLLMs, such as Mistral-7B. Our approach does not rely on traditional human\nannotation to rank preferred answers but employs a reconstruction pipeline to\nmodify raw topics generated by LLMs, thus enabling a fast and efficient\ntraining and inference framework. Comparative experiments show that our\nfine-tuning approach not only significantly improves the LLM's capability to\nproduce more coherent, relevant, and precise topics, but also reduces the\nnumber of hallucinated topics.\n"", ""  Topic models are valuable for understanding extensive document collections,\nbut they don't always identify the most relevant topics. Classical\nprobabilistic and anchor-based topic models offer interactive versions that\nallow users to guide the models towards more pertinent topics. However, such\ninteractive features have been lacking in neural topic models. To correct this\nlacuna, we introduce a user-friendly interaction for neural topic models. This\ninteraction permits users to assign a word label to a topic, leading to an\nupdate in the topic model where the words in the topic become closely aligned\nwith the given label. Our approach encompasses two distinct kinds of neural\ntopic models. The first includes models where topic embeddings are trainable\nand evolve during the training process. The second kind involves models where\ntopic embeddings are integrated post-training, offering a different approach to\ntopic refinement. To facilitate user interaction with these neural topic\nmodels, we have developed an interactive interface. This interface enables\nusers to engage with and re-label topics as desired. We evaluate our method\nthrough a human study, where users can relabel topics to find relevant\ndocuments. Using our method, user labeling improves document rank scores,\nhelping to find more relevant documents to a given query when compared to no\nuser labeling.\n""] , ['  Events describe the state changes of entities. In a document, multiple events\nare connected by various relations (e.g., Coreference, Temporal, Causal, and\nSubevent). Therefore, obtaining the connections between events through\nEvent-Event Relation Extraction (ERE) is critical to understand natural\nlanguage. There are two main problems in the current ERE works: a. Only\nembeddings of the event triggers are used for event feature representation,\nignoring event arguments (e.g., time, place, person, etc.) and their structure\nwithin the event. b. The interconnection between relations (e.g., temporal and\ncausal relations usually interact with each other ) is ignored. To solve the\nabove problems, this paper proposes a jointly multiple ERE framework called\nGraphERE based on Graph-enhanced Event Embeddings. First, we enrich the event\nembeddings with event argument and structure features by using static AMR\ngraphs and IE graphs; Then, to jointly extract multiple event relations, we use\nNode Transformer and construct Task-specific Dynamic Event Graphs for each type\nof relation. Finally, we used a multi-task learning strategy to train the whole\nframework. Experimental results on the latest MAVEN-ERE dataset validate that\nGraphERE significantly outperforms existing methods. Further analyses indicate\nthe effectiveness of the graph-enhanced event embeddings and the joint\nextraction strategy.\n', '  Existing approaches on zero-shot event detection usually train models on\ndatasets annotated with known event types, and prompt them with unseen event\ndefinitions. These approaches yield sporadic successes, yet generally fall\nshort of expectations. In this work, we aim to improve zero-shot event\ndetection by training models to better follow event definitions. We hypothesize\nthat a diverse set of event types and definitions are the key for models to\nlearn to follow event definitions while existing event extraction datasets\nfocus on annotating many high-quality examples for a few event types. To verify\nour hypothesis, we construct an automatically generated Diverse Event\nDefinition (DivED) dataset and conduct comparative studies. Our experiments\nreveal that a large number of event types (200) and diverse event definitions\ncan significantly boost event extraction performance; on the other hand, the\nperformance does not scale with over ten examples per event type. Beyond\nscaling, we incorporate event ontology information and hard-negative samples\nduring training, further boosting the performance. Based on these findings, we\nfine-tuned a LLaMA-2-7B model on our DivED dataset, yielding performance that\nsurpasses SOTA large language models like GPT-3.5 across three open benchmarks\non zero-shot event detection.\n', '  Event sequence models have been found to be highly effective in the analysis\nand prediction of events. Building such models requires availability of\nabundant high-quality event sequence data. In certain applications, however,\nclean structured event sequences are not available, and automated sequence\nextraction results in data that is too noisy and incomplete. In this work, we\nexplore the use of Large Language Models (LLMs) to generate event sequences\nthat can effectively be used for probabilistic event model construction. This\ncan be viewed as a mechanism of distilling event sequence knowledge from LLMs.\nOur approach relies on a Knowledge Graph (KG) of event concepts with partial\ncausal relations to guide the generative language model for causal event\nsequence generation. We show that our approach can generate high-quality event\nsequences, filling a knowledge gap in the input KG. Furthermore, we explore how\nthe generated sequences can be leveraged to discover useful and more complex\nstructured knowledge from pattern mining and probabilistic event models. We\nrelease our sequence generation code and evaluation framework, as well as\ncorpus of event sequence data.\n'] , ['  Keyphrase extraction (KPE) is an important task in Natural Language\nProcessing for many scenarios, which aims to extract keyphrases that are\npresent in a given document. Many existing supervised methods treat KPE as\nsequential labeling, span-level classification, or generative tasks. However,\nthese methods lack the ability to utilize keyphrase information, which may\nresult in biased results. In this study, we propose Diff-KPE, which leverages\nthe supervised Variational Information Bottleneck (VIB) to guide the text\ndiffusion process for generating enhanced keyphrase representations. Diff-KPE\nfirst generates the desired keyphrase embeddings conditioned on the entire\ndocument and then injects the generated keyphrase embeddings into each phrase\nrepresentation. A ranking network and VIB are then optimized together with rank\nloss and classification loss, respectively. This design of Diff-KPE allows us\nto rank each candidate phrase by utilizing both the information of keyphrases\nand the document. Experiments show that Diff-KPE outperforms existing KPE\nmethods on a large open domain keyphrase extraction benchmark, OpenKP, and a\nscientific domain dataset, KP20K.\n', '  Zero-shot keyphrase extraction aims to build a keyphrase extractor without\ntraining by human-annotated data, which is challenging due to the limited human\nintervention involved. Challenging but worthwhile, zero-shot setting\nefficiently reduces the time and effort that data labeling takes. Recent\nefforts on pre-trained large language models (e.g., ChatGPT and ChatGLM) show\npromising performance on zero-shot settings, thus inspiring us to explore\nprompt-based methods. In this paper, we ask whether strong keyphrase extraction\nmodels can be constructed by directly prompting the large language model\nChatGPT. Through experimental results, it is found that ChatGPT still has a lot\nof room for improvement in the keyphrase extraction task compared to existing\nstate-of-the-art unsupervised and supervised models.\n', '  Neural models that do not rely on pre-training have excelled in the keyphrase\ngeneration task with large annotated datasets. Meanwhile, new approaches have\nincorporated pre-trained language models (PLMs) for their data efficiency.\nHowever, there lacks a systematic study of how the two types of approaches\ncompare and how different design choices can affect the performance of\nPLM-based models. To fill in this knowledge gap and facilitate a more informed\nuse of PLMs for keyphrase extraction and keyphrase generation, we present an\nin-depth empirical study. Formulating keyphrase extraction as sequence labeling\nand keyphrase generation as sequence-to-sequence generation, we perform\nextensive experiments in three domains. After showing that PLMs have\ncompetitive high-resource performance and state-of-the-art low-resource\nperformance, we investigate important design choices including in-domain PLMs,\nPLMs with different pre-training objectives, using PLMs with a parameter\nbudget, and different formulations for present keyphrases. Further results show\nthat (1) in-domain BERT-like PLMs can be used to build strong and\ndata-efficient keyphrase generation models; (2) with a fixed parameter budget,\nprioritizing model depth over width and allocating more layers in the encoder\nleads to better encoder-decoder models; and (3) introducing four in-domain\nPLMs, we achieve a competitive performance in the news domain and the\nstate-of-the-art performance in the scientific domain.\n']",Information Extraction from Text,Relation Extraction from Textual Data
40,"""Text Summarization with Large Language Models"" , Scientific Literature Analysis with Large Language Models , ""Automated Summarization in Systematic Reviews""","['summarizers', 'summarizer', 'summarizing', 'summarization', 'summarisation', 'summaries', 'summary', 'sentences', 'transcripts', 'paragraph'] , ['annotation', 'corpus', 'semantic', 'metadata', 'texts', 'scholarly', 'wikipedia', 'ontologies', 'ontology', 'retrieval'] , ['summarizing', 'summarization', 'summaries', 'abstracts', 'systematic', 'pubmed', 'metadata', 'summary', 'text', 'automate']","['  Factual consistency is an important quality in dialogue summarization. Large\nlanguage model (LLM)-based automatic text summarization models generate more\nfactually consistent summaries compared to those by smaller pretrained language\nmodels, but they face deployment challenges in real-world applications due to\nprivacy or resource constraints. In this paper, we investigate the use of\nsymbolic knowledge distillation to improve the factual consistency of smaller\npretrained models for dialogue summarization. We employ zero-shot learning to\nextract symbolic knowledge from LLMs, generating both factually consistent\n(positive) and inconsistent (negative) summaries. We then apply two contrastive\nlearning objectives on these summaries to enhance smaller summarization models.\nExperiments with BART, PEGASUS, and Flan-T5 indicate that our approach\nsurpasses strong baselines that rely on complex data augmentation strategies.\nOur approach achieves better factual consistency while maintaining coherence,\nfluency, and relevance, as confirmed by various automatic evaluation metrics.\nWe also provide access to the data and code to facilitate future research.\n', ""  Recent studies have found that summaries generated by large language models\n(LLMs) are favored by human annotators over the original reference summaries in\ncommonly used summarization datasets. Therefore, we study an LLM-as-reference\nlearning setting for smaller text summarization models to investigate whether\ntheir performance can be substantially improved. To this end, we use LLMs as\nboth oracle summary generators for standard supervised fine-tuning and oracle\nsummary evaluators for efficient contrastive learning that leverages the LLMs'\nsupervision signals. We conduct comprehensive experiments with source news\narticles and find that (1) summarization models trained under the\nLLM-as-reference setting achieve significant performance improvement in both\nLLM and human evaluations; (2) contrastive learning outperforms standard\nsupervised fine-tuning under both low and high resource settings. Our\nexperimental results also enable a meta-analysis of LLMs' summary evaluation\ncapacities under a challenging setting, showing that LLMs are not well-aligned\nwith human evaluators. Particularly, our expert human evaluation reveals\nremaining nuanced performance gaps between LLMs and our fine-tuned models,\nwhich LLMs fail to capture. Thus, we call for further studies into both the\npotential and challenges of using LLMs in summarization model development.\n"", '  While large language models (LLMs) can already achieve strong performance on\nstandard generic summarization benchmarks, their performance on more complex\nsummarization task settings is less studied. Therefore, we benchmark LLMs on\ninstruction controllable text summarization, where the model input consists of\nboth a source article and a natural language requirement for desired summary\ncharacteristics. To this end, we curate an evaluation-only dataset for this\ntask setting and conduct human evaluations of five LLM-based systems to assess\ntheir instruction-following capabilities in controllable summarization. We then\nbenchmark LLM-based automatic evaluation for this task with 4 different\nevaluation protocols and 11 LLMs, resulting in 40 evaluation methods. Our study\nreveals that instruction controllable text summarization remains a challenging\ntask for LLMs, since (1) all LLMs evaluated still make factual and other types\nof errors in their summaries; (2) no LLM-based evaluation methods can achieve a\nstrong alignment with human annotators when judging the quality of candidate\nsummaries; (3) different LLMs show large performance gaps in summary generation\nand evaluation capabilities. We make our collected benchmark InstruSum publicly\navailable to facilitate future research in this direction.\n'] , [""  In scientific research and its application, scientific literature analysis is\ncrucial as it allows researchers to build on the work of others. However, the\nfast growth of scientific knowledge has led to a massive increase in scholarly\narticles, making in-depth literature analysis increasingly challenging and\ntime-consuming. The emergence of Large Language Models (LLMs) has offered a new\nway to address this challenge. Known for their strong abilities in summarizing\ntexts, LLMs are seen as a potential tool to improve the analysis of scientific\nliterature. However, existing LLMs have their own limits. Scientific literature\noften includes a wide range of multimodal elements, such as tables, charts, and\nmolecule, which are hard for text-focused LLMs to understand and analyze. This\nissue points to the urgent need for new solutions that can fully understand and\nanalyze multimodal content in scientific literature. To answer this demand, we\npresent \\textbf{Uni-SMART} (Universal Science Multimodal Analysis and Research\nTransformer), an innovative model designed for in-depth understanding of\nmultimodal scientific literature. Through rigorous quantitative evaluation\nacross several domains, Uni-SMART demonstrates superior performance over other\ntext-focused LLMs. Furthermore, our exploration extends to practical\napplications, including patent infringement detection and nuanced analysis of\ncharts. These applications not only highlight Uni-SMART's adaptability but also\nits potential to revolutionize how we interact with scientific literature.\n"", '  This project investigates the efficacy of Large Language Models (LLMs) in\nunderstanding and extracting scientific knowledge across specific domains and\nto create a deep learning framework: Knowledge AI. As a part of this framework,\nwe employ pre-trained models and fine-tune them on datasets in the scientific\ndomain. The models are adapted for four key Natural Language Processing (NLP)\ntasks: summarization, text generation, question answering, and named entity\nrecognition. Our results indicate that domain-specific fine-tuning\nsignificantly enhances model performance in each of these tasks, thereby\nimproving their applicability for scientific contexts. This adaptation enables\nnon-experts to efficiently query and extract information within targeted\nscientific fields, demonstrating the potential of fine-tuned LLMs as a tool for\nknowledge discovery in the sciences.\n', '  With the rapid development of the internet in the past decade, it has become\nincreasingly important to extract valuable information from vast resources\nefficiently, which is crucial for establishing a comprehensive digital\necosystem, particularly in the context of research surveys and comprehension.\nThe foundation of these tasks focuses on accurate extraction and deep mining of\ndata from scientific documents, which are essential for building a robust data\ninfrastructure. However, parsing raw data or extracting data from complex\nscientific documents have been ongoing challenges. Current data extraction\nmethods for scientific documents typically use rule-based (RB) or machine\nlearning (ML) approaches. However, using rule-based methods can incur high\ncoding costs for articles with intricate typesetting. Conversely, relying\nsolely on machine learning methods necessitates annotation work for complex\ncontent types within the scientific document, which can be costly.\nAdditionally, few studies have thoroughly defined and explored the hierarchical\nlayout within scientific documents. The lack of a comprehensive definition of\nthe internal structure and elements of the documents indirectly impacts the\naccuracy of text classification and object recognition tasks. From the\nperspective of analyzing the standard layout and typesetting used in the\nspecified publication, we propose a new document layout analysis framework\ncalled CTBR(Compartment & Text Blocks Refinement). Firstly, we define\nscientific documents into hierarchical divisions: base domain, compartment, and\ntext blocks. Next, we conduct an in-depth exploration and classification of the\nmeanings of text blocks. Finally, we utilize the results of text block\nclassification to implement object recognition within scientific documents\nbased on rule-based compartment segmentation.\n'] , ['  Systematic reviews are crucial for evidence-based medicine as they\ncomprehensively analyse published research findings on specific questions.\nConducting such reviews is often resource- and time-intensive, especially in\nthe screening phase, where abstracts of publications are assessed for inclusion\nin a review. This study investigates the effectiveness of using zero-shot large\nlanguage models~(LLMs) for automatic screening. We evaluate the effectiveness\nof eight different LLMs and investigate a calibration technique that uses a\npredefined recall threshold to determine whether a publication should be\nincluded in a systematic review. Our comprehensive evaluation using five\nstandard test collections shows that instruction fine-tuning plays an important\nrole in screening, that calibration renders LLMs practical for achieving a\ntargeted recall, and that combining both with an ensemble of zero-shot models\nsaves significant screening time compared to state-of-the-art approaches.\n', '  Community Question-Answering (CQA) forums have revolutionized how people seek\ninformation, especially those related to their healthcare needs, placing their\ntrust in the collective wisdom of the public. However, there can be several\nanswers in response to a single query, which makes it hard to grasp the key\ninformation related to the specific health concern. Typically, CQA forums\nfeature a single top-voted answer as a representative summary for each query.\nHowever, a single answer overlooks the alternative solutions and other\ninformation frequently offered in other responses. Our research focuses on\naspect-based summarization of health answers to address this limitation.\nSummarization of responses under different aspects such as suggestions,\ninformation, personal experiences, and questions can enhance the usability of\nthe platforms. We formalize a multi-stage annotation guideline and contribute a\nunique dataset comprising aspect-based human-written health answer summaries.\nWe build an automated multi-faceted answer summarization pipeline with this\ndataset based on task-specific fine-tuning of several state-of-the-art models.\nThe pipeline leverages question similarity to retrieve relevant answer\nsentences, subsequently classifying them into the appropriate aspect type.\nFollowing this, we employ several recent abstractive summarization models to\ngenerate aspect-based summaries. Finally, we present a comprehensive human\nanalysis and find that our summaries rank high in capturing relevant content\nand a wide range of solutions.\n', ""  Systematic review (SR) is a popular research method in software engineering\n(SE). However, conducting an SR takes an average of 67 weeks. Thus, automating\nany step of the SR process could reduce the effort associated with SRs. Our\nobjective is to investigate if Large Language Models (LLMs) can accelerate\ntitle-abstract screening by simplifying abstracts for human screeners, and\nautomating title-abstract screening. We performed an experiment where humans\nscreened titles and abstracts for 20 papers with both original and simplified\nabstracts from a prior SR. The experiment with human screeners was reproduced\nwith GPT-3.5 and GPT-4 LLMs to perform the same screening tasks. We also\nstudied if different prompting techniques (Zero-shot (ZS), One-shot (OS),\nFew-shot (FS), and Few-shot with Chain-of-Thought (FS-CoT)) improve the\nscreening performance of LLMs. Lastly, we studied if redesigning the prompt\nused in the LLM reproduction of screening leads to improved performance. Text\nsimplification did not increase the screeners' screening performance, but\nreduced the time used in screening. Screeners' scientific literacy skills and\nresearcher status predict screening performance. Some LLM and prompt\ncombinations perform as well as human screeners in the screening tasks. Our\nresults indicate that the GPT-4 LLM is better than its predecessor, GPT-3.5.\nAdditionally, Few-shot and One-shot prompting outperforms Zero-shot prompting.\nUsing LLMs for text simplification in the screening process does not\nsignificantly improve human performance. Using LLMs to automate title-abstract\nscreening seems promising, but current LLMs are not significantly more accurate\nthan human screeners. To recommend the use of LLMs in the screening process of\nSRs, more research is needed. We recommend future SR studies publish\nreplication packages with screening data to enable more conclusive\nexperimenting with LLM screening.\n""]",Large Language Models for Text Analysis and Summarization,"""Text Summarization with Large Language Models"""
41,"Active Learning for Efficient Annotation , Crowdsourcing and Annotation Quality","['activelab', 'supervised', 'activellm', 'classification', 'annotators', 'annotations', 'annotation', 'annotate', 'labeled', 'learning'] , ['crowdsourcing', 'crowdsourced', 'crowdworker', 'crowdworkers', 'crowdshipping', 'crowdsensing', 'crowds', 'annotators', 'annotations', 'annotation']","[""  We conduct a comprehensive evaluation of state-of-the-art deep active\nlearning methods. Surprisingly, under general settings, no single-model method\ndecisively outperforms entropy-based active learning, and some even fall short\nof random sampling. We delve into overlooked aspects like starting budget,\nbudget step, and pretraining's impact, revealing their significance in\nachieving superior results. Additionally, we extend our evaluation to other\ntasks, exploring the active learning effectiveness in combination with\nsemi-supervised learning, and object detection. Our experiments provide\nvaluable insights and concrete recommendations for future active learning\nstudies. By uncovering the limitations of current methods and understanding the\nimpact of different experimental settings, we aim to inspire more efficient\ntraining of deep learning models in real-world scenarios with limited\nannotation budgets. This work contributes to advancing active learning's\nefficacy in deep learning and empowers researchers to make informed decisions\nwhen applying active learning to their tasks.\n"", '  This paper explores the integration of active machine learning (ML) for 6G\nnetworks, an area that remains under-explored yet holds potential. Unlike\npassive ML systems, active ML can be made to interact with the network\nenvironment. It actively selects informative and representative data points for\ntraining, thereby reducing the volume of data needed while accelerating the\nlearning process. While active learning research mainly focuses on data\nannotation, we call for a network-centric active learning framework that\nconsiders both annotation (i.e., what is the label) and data acquisition (i.e.,\nwhich and how many samples to collect). Moreover, we explore the synergy\nbetween generative artificial intelligence (AI) and active learning to overcome\nexisting limitations in both active learning and generative AI. This paper also\nfeatures a case study on a mmWave throughput prediction problem to demonstrate\nthe practical benefits and improved performance of active learning for 6G\nnetworks. Furthermore, we discuss how the implications of active learning\nextend to numerous 6G network use cases. We highlight the potential of active\nlearning based 6G networks to enhance computational efficiency, data annotation\nand acquisition efficiency, adaptability, and overall network intelligence. We\nconclude with a discussion on challenges and future research directions for\nactive learning in 6G networks, including development of novel query\nstrategies, distributed learning integration, and inclusion of human- and\nmachine-in-the-loop learning.\n', '  Class imbalance is a prevalent issue in real world machine learning\napplications, often leading to poor performance in rare and minority classes.\nWith an abundance of wild unlabeled data, active learning is perhaps the most\neffective technique in solving the problem at its root -- collecting a more\nbalanced and informative set of labeled examples during annotation. Label noise\nis another common issue in data annotation jobs, which is especially\nchallenging for active learning methods. In this work, we conduct the first\nstudy of active learning under both class imbalance and label noise. We propose\na novel algorithm that robustly identifies the class separation threshold and\nannotates the most uncertain examples that are closest from it. Through a novel\nreduction to one-dimensional active learning, our algorithm DIRECT is able to\nleverage the classic active learning literature to address issues such as batch\nlabeling and tolerance towards label noise. We present extensive experiments on\nimbalanced datasets with and without label noise. Our results demonstrate that\nDIRECT can save more than 60% of the annotation budget compared to state-of-art\nactive learning algorithms and more than 80% of annotation budget compared to\nrandom sampling.\n'] , ['  Annotation through crowdsourcing draws incremental attention, which relies on\nan effective selection scheme given a pool of workers. Existing methods propose\nto select workers based on their performance on tasks with ground truth, while\ntwo important points are missed. 1) The historical performances of workers in\nother tasks. In real-world scenarios, workers need to solve a new task whose\ncorrelation with previous tasks is not well-known before the training, which is\ncalled cross-domain. 2) The dynamic worker performance as workers will learn\nfrom the ground truth. In this paper, we consider both factors in designing an\nallocation scheme named cross-domain-aware worker selection with training\napproach. Our approach proposes two estimation modules to both statistically\nanalyze the cross-domain correlation and simulate the learning gain of workers\ndynamically. A framework with a theoretical analysis of the worker elimination\nprocess is given. To validate the effectiveness of our methods, we collect two\nnovel real-world datasets and generate synthetic datasets. The experiment\nresults show that our method outperforms the baselines on both real-world and\nsynthetic datasets.\n', '  Whether Large Language Models (LLMs) can outperform crowdsourcing on the data\nannotation task is attracting interest recently. Some works verified this issue\nwith the average performance of individual crowd workers and LLM workers on\nsome specific NLP tasks by collecting new datasets. However, on the one hand,\nexisting datasets for the studies of annotation quality in crowdsourcing are\nnot yet utilized in such evaluations, which potentially provide reliable\nevaluations from a different viewpoint. On the other hand, the quality of these\naggregated labels is crucial because, when utilizing crowdsourcing, the\nestimated labels aggregated from multiple crowd labels to the same instances\nare the eventually collected labels. Therefore, in this paper, we first\ninvestigate which existing crowdsourcing datasets can be used for a comparative\nstudy and create a benchmark. We then compare the quality between individual\ncrowd labels and LLM labels and make the evaluations on the aggregated labels.\nIn addition, we propose a Crowd-LLM hybrid label aggregation method and verify\nthe performance. We find that adding LLM labels from good LLMs to existing\ncrowdsourcing datasets can enhance the quality of the aggregated labels of the\ndatasets, which is also higher than the quality of LLM labels themselves.\n', ""  For the purpose of efficient and cost-effective large-scale data labeling,\ncrowdsourcing is increasingly being utilized. To guarantee the quality of data\nlabeling, multiple annotations need to be collected for each data sample, and\ntruth inference algorithms have been developed to accurately infer the true\nlabels. Despite previous studies having released public datasets to evaluate\nthe efficacy of truth inference algorithms, these have typically focused on a\nsingle type of crowdsourcing task and neglected the temporal information\nassociated with workers' annotation activities. These limitations significantly\nrestrict the practical applicability of these algorithms, particularly in the\ncontext of long-term and online truth inference. In this paper, we introduce a\nsubstantial crowdsourcing annotation dataset collected from a real-world\ncrowdsourcing platform. This dataset comprises approximately two thousand\nworkers, one million tasks, and six million annotations. The data was gathered\nover a period of approximately six months from various types of tasks, and the\ntimestamps of each annotation were preserved. We analyze the characteristics of\nthe dataset from multiple perspectives and evaluate the effectiveness of\nseveral representative truth inference algorithms on this dataset. We\nanticipate that this dataset will stimulate future research on tracking\nworkers' abilities over time in relation to different types of tasks, as well\nas enhancing online truth inference.\n""]",Active Learning and Crowdsourcing for Efficient Data Annotation,Crowdsourcing and Annotation Quality
42,"Automated Fact-Checking and Evidence Retrieval , Argument Mining and Quality Assessment , Event Factuality Detection and Summarization","['veracity', 'factuality', 'factdetect', 'credibility', 'evidence', 'factscore', 'corpus', 'semantic', 'claims', 'factual'] , ['argumentation', 'argumentative', 'arguments', 'debates', 'debater', 'annotators', 'annotations', 'argument', 'debate', 'annotation'] , ['annotations', 'annotation', 'textual', 'retrieval', 'summarization', 'summaries', 'factuality', 'narratives', 'reading', 'narrative']","[""  Evidence retrieval is a core part of automatic fact-checking. Prior work\nmakes simplifying assumptions in retrieval that depart from real-world use\ncases: either no access to evidence, access to evidence curated by a human\nfact-checker, or access to evidence available long after the claim has been\nmade. In this work, we present the first fully automated pipeline to check\nreal-world claims by retrieving raw evidence from the web. We restrict our\nretriever to only search documents available prior to the claim's making,\nmodeling the realistic scenario where an emerging claim needs to be checked.\nOur pipeline includes five components: claim decomposition, raw document\nretrieval, fine-grained evidence retrieval, claim-focused summarization, and\nveracity judgment. We conduct experiments on complex political claims in the\nClaimDecomp dataset and show that the aggregated evidence produced by our\npipeline improves veracity judgments. Human evaluation finds the evidence\nsummary produced by our system is reliable (it does not hallucinate\ninformation) and relevant to answering key questions about a claim, suggesting\nthat it can assist fact-checkers even when it cannot surface a complete\nevidence set.\n"", ""  Fact-checking real-world claims often requires reviewing multiple multimodal\ndocuments to assess a claim's truthfulness, which is a highly laborious and\ntime-consuming task. In this paper, we present a summarization model designed\nto generate claim-specific summaries useful for fact-checking from multimodal,\nmulti-document datasets. The model takes inputs in the form of documents,\nimages, and a claim, with the objective of assisting in fact-checking tasks. We\nintroduce a dynamic perceiver-based model that can handle inputs from multiple\nmodalities of arbitrary lengths. To train our model, we leverage a novel\nreinforcement learning-based entailment objective to generate summaries that\nprovide evidence distinguishing between different truthfulness labels. To\nassess the efficacy of our approach, we conduct experiments on both an existing\nbenchmark and a new dataset of multi-document claims that we contribute. Our\napproach outperforms the SOTA approach by 4.6% in the claim verification task\non the MOCHEG dataset and demonstrates strong performance on our new\nMulti-News-Fact-Checking dataset.\n"", '  Fact-checking is the task of verifying the factuality of a given claim by\nexamining the available evidence. High-quality evidence plays a vital role in\nenhancing fact-checking systems and facilitating the generation of explanations\nthat are understandable to humans. However, the provision of both sufficient\nand relevant evidence for explainable fact-checking systems poses a challenge.\nTo tackle this challenge, we propose a method based on a Large Language Model\nto automatically retrieve and summarize evidence from the Web. Furthermore, we\nconstruct RU22Fact, a novel multilingual explainable fact-checking dataset on\nthe Russia-Ukraine conflict in 2022 of 16K samples, each containing real-world\nclaims, optimized evidence, and referenced explanation. To establish a baseline\nfor our dataset, we also develop an end-to-end explainable fact-checking system\nto verify claims and generate explanations. Experimental results demonstrate\nthe prospect of optimized evidence in increasing fact-checking performance and\nalso indicate the possibility of further progress in the end-to-end claim\nverification and explanation generation tasks.\n'] , ['  Evaluating the quality of arguments is a crucial aspect of any system\nleveraging argument mining. However, it is a challenge to obtain reliable and\nconsistent annotations regarding argument quality, as this usually requires\ndomain-specific expertise of the annotators. Even among experts, the assessment\nof argument quality is often inconsistent due to the inherent subjectivity of\nthis task. In this paper, we study the potential of using state-of-the-art\nlarge language models (LLMs) as proxies for argument quality annotators. To\nassess the capability of LLMs in this regard, we analyze the agreement between\nmodel, human expert, and human novice annotators based on an established\ntaxonomy of argument quality dimensions. Our findings highlight that LLMs can\nproduce consistent annotations, with a moderately high agreement with human\nexperts across most of the quality dimensions. Moreover, we show that using\nLLMs as additional annotators can significantly improve the agreement between\nannotators. These results suggest that LLMs can serve as a valuable tool for\nautomated argument quality assessment, thus streamlining and accelerating the\nevaluation of large argument datasets.\n', '  Argument mining (AM) is defined as the task of automatically identifying and\nextracting argumentative components (e.g. premises, claims, etc.) and detecting\nthe existing relations among them (i.e., support, attack, no relations). Deep\nlearning models enable us to analyze arguments more efficiently than\ntraditional methods and extract their semantics. This paper presents\ncomparative studies between a few deep learning-based models in argument\nmining. The work concentrates on argument classification. The research was done\non a wide spectrum of datasets (Args.me, UKP, US2016). The main novelty of this\npaper is the ensemble model which is based on BERT architecture and ChatGPT-4\nas fine tuning model. The presented results show that BERT+ChatGPT-4\noutperforms the rest of the models including other Transformer-based and\nLSTM-based models. The observed improvement is, in most cases, greater than\n10The presented analysis can provide crucial insights into how the models for\nargument classification should be further improved. Additionally, it can help\ndevelop a prompt-based algorithm to eliminate argument classification errors.\n', '  Some of the major limitations identified in the areas of argument mining,\nargument generation, and natural language argument analysis are related to the\ncomplexity of annotating argumentatively rich data, the limited size of these\ncorpora, and the constraints that represent the different languages and domains\nin which these data is annotated. To address these limitations, in this paper\nwe present the following contributions: (i) an effective methodology for the\nautomatic generation of natural language arguments in different topics and\nlanguages, (ii) the largest publicly available corpus of natural language\nargumentation schemes, and (iii) a set of solid baselines and fine-tuned models\nfor the automatic identification of argumentation schemes.\n'] , ['  Event Factuality Detection (EFD) task determines the factuality of textual\nevents, i.e., classifying whether an event is a fact, possibility, or\nimpossibility, which is essential for faithfully understanding and utilizing\nevent knowledge. However, due to the lack of high-quality large-scale data,\nevent factuality detection is under-explored in event understanding research,\nwhich limits the development of EFD community. To address these issues and\nprovide faithful event understanding, we introduce MAVEN-Fact, a large-scale\nand high-quality EFD dataset based on the MAVEN dataset. MAVEN-Fact includes\nfactuality annotations of 112,276 events, making it the largest EFD dataset.\nExtensive experiments demonstrate that MAVEN-Fact is challenging for both\nconventional fine-tuned models and large language models (LLMs). Thanks to the\ncomprehensive annotations of event arguments and relations in MAVEN, MAVEN-Fact\nalso supports some further analyses and we find that adopting event arguments\nand relations helps in event factuality detection for fine-tuned models but\ndoes not benefit LLMs. Furthermore, we preliminarily study an application case\nof event factuality detection and find it helps in mitigating event-related\nhallucination in LLMs. Our dataset and codes can be obtained from\n\\url{https://github.com/lcy2723/MAVEN-FACT}\n', '  While long-context large language models (LLMs) can technically summarize\nbook-length documents (>100K tokens), the length and complexity of the\ndocuments have so far prohibited evaluations of input-dependent aspects like\nfaithfulness. In this paper, we conduct the first large-scale human evaluation\nof faithfulness and content selection on LLM-generated summaries of fictional\nbooks. Our study mitigates the issue of data contamination by focusing on\nsummaries of books published in 2023 or 2024, and we hire annotators who have\nfully read each book prior to the annotation task to minimize cost and\ncognitive burden. We collect FABLES, a dataset of annotations on 3,158 claims\nmade in LLM-generated summaries of 26 books, at a cost of $5.2K USD, which\nallows us to rank LLM summarizers based on faithfulness: Claude-3-Opus\nsignificantly outperforms all closed-source LLMs, while the open-source Mixtral\nis on par with GPT-3.5-Turbo. An analysis of the annotations reveals that most\nunfaithful claims relate to events and character states, and they generally\nrequire indirect reasoning over the narrative to invalidate. While LLM-based\nauto-raters have proven reliable for factuality and coherence in other\nsettings, we implement several LLM raters of faithfulness and find that none\ncorrelates strongly with human annotations, especially with regard to detecting\nunfaithful claims. Our experiments suggest that detecting unfaithful claims is\nan important future direction not only for summarization evaluation but also as\na testbed for long-context understanding. Finally, we move beyond faithfulness\nby exploring content selection errors in book-length summarization: we develop\na typology of omission errors related to crucial narrative elements and also\nidentify a systematic over-emphasis on events occurring towards the end of the\nbook.\n', ""  In this paper, we introduce the VerifAI project, a pioneering open-source\nscientific question-answering system, designed to provide answers that are not\nonly referenced but also automatically vetted and verifiable. The components of\nthe system are (1) an Information Retrieval system combining semantic and\nlexical search techniques over scientific papers (PubMed), (2) a\nRetrieval-Augmented Generation (RAG) module using fine-tuned generative model\n(Mistral 7B) and retrieved articles to generate claims with references to the\narticles from which it was derived, and (3) a Verification engine, based on a\nfine-tuned DeBERTa and XLM-RoBERTa models on Natural Language Inference task\nusing SciFACT dataset. The verification engine cross-checks the generated claim\nand the article from which the claim was derived, verifying whether there may\nhave been any hallucinations in generating the claim. By leveraging the\nInformation Retrieval and RAG modules, Verif.ai excels in generating factual\ninformation from a vast array of scientific sources. At the same time, the\nVerification engine rigorously double-checks this output, ensuring its accuracy\nand reliability. This dual-stage process plays a crucial role in acquiring and\nconfirming factual information, significantly enhancing the information\nlandscape. Our methodology could significantly enhance scientists'\nproductivity, concurrently fostering trust in applying generative language\nmodels within scientific domains, where hallucinations and misinformation are\nunacceptable.\n""]",Automated Information Verification and Analysis,Automated Fact-Checking and Evidence Retrieval
43,"Entity Matching and Linking , Document Entity Classification and Disambiguation , Entity Typing and Disambiguation","['entities', 'entity', 'matching', 'matched', 'records', 'match', 'record', 'matchers', 'linking', 'similarity'] , ['corpus', 'nlp', 'stemming', 'texts', 'disambiguation', 'semantic', 'entities', 'keywords', 'entity', 'contextual'] , ['entities', 'entity', 'semantic', 'annotated', 'corpus', 'referential', 'corpora', 'hypernym', 'embeddings', 'retrieval']","['  Entity matching (EM) is a critical step in entity resolution (ER). Recently,\nentity matching based on large language models (LLMs) has shown great promise.\nHowever, current LLM-based entity matching approaches typically follow a binary\nmatching paradigm that ignores the global consistency between record\nrelationships. In this paper, we investigate various methodologies for\nLLM-based entity matching that incorporate record interactions from different\nperspectives. Specifically, we comprehensively compare three representative\nstrategies: matching, comparing, and selecting, and analyze their respective\nadvantages and challenges in diverse scenarios. Based on our findings, we\nfurther design a compound entity matching framework (ComEM) that leverages the\ncomposition of multiple strategies and LLMs. ComEM benefits from the advantages\nof different sides and achieves improvements in both effectiveness and\nefficiency. Experimental results on 8 ER datasets and 9 LLMs verify the\nsuperiority of incorporating record interactions through the selecting\nstrategy, as well as the further cost-effectiveness brought by ComEM.\n', '  Entity Matching is the task of deciding whether two entity descriptions refer\nto the same real-world entity and is a central step in most data integration\npipelines. Many state-of-the-art entity matching methods rely on pre-trained\nlanguage models (PLMs) such as BERT or RoBERTa. Two major drawbacks of these\nmodels for entity matching are that (i) the models require significant amounts\nof task-specific training data and (ii) the fine-tuned models are not robust\nconcerning out-of-distribution entities. This paper investigates using\ngenerative large language models (LLMs) as a less task-specific training\ndata-dependent and more robust alternative to PLM-based matchers. Our study\ncovers hosted and open-source LLMs, which can be run locally. We evaluate these\nmodels in a zero-shot scenario and a scenario where task-specific training data\nis available. We compare different prompt designs and the prompt sensitivity of\nthe models and show that there is no single best prompt but needs to be tuned\nfor each model/dataset combination. We further investigate (i) the selection of\nin-context demonstrations, (ii) the generation of matching rules, as well as\n(iii) fine-tuning a hosted LLM using the same pool of training data. Our\nexperiments show that the best LLMs require no or only a few training examples\nto perform similarly to PLMs that were fine-tuned using thousands of examples.\nLLM-based matchers further exhibit higher robustness to unseen entities. We\nshow that GPT4 can generate structured explanations for matching decisions. The\nmodel can automatically identify potential causes of matching errors by\nanalyzing explanations of wrong decisions. We demonstrate that the model can\ngenerate meaningful textual descriptions of the identified error classes, which\ncan help data engineers improve entity matching pipelines.\n', '  Entity matching is the task of linking records from different sources that\nrefer to the same real-world entity. Past work has primarily treated entity\nlinking as a standard supervised learning problem. However, supervised entity\nmatching models often do not generalize well to new data, and collecting\nexhaustive labeled training data is often cost prohibitive. Further, recent\nefforts have adopted LLMs for this task in few/zero-shot settings, exploiting\ntheir general knowledge. But LLMs are prohibitively expensive for performing\ninference at scale for real-world entity matching tasks.\n  As an efficient alternative, we re-cast entity matching as a conditional\ngeneration task as opposed to binary classification. This enables us to\n""distill"" LLM reasoning into smaller entity matching models via natural\nlanguage explanations. This approach achieves strong performance, especially on\nout-of-domain generalization tests (10.85% F-1) where standalone generative\nmethods struggle. We perform ablations that highlight the importance of\nexplanations, both for performance and model robustness.\n'] , [""  Documents that consist of diverse templates and exhibit complex spatial\nstructures pose a challenge for document entity classification. We propose\nKNN-former, which incorporates a new kind of spatial bias in attention\ncalculation based on the K-nearest-neighbor (KNN) graph of document entities.\nWe limit entities' attention only to their local radius defined by the KNN\ngraph. We also use combinatorial matching to address the one-to-one mapping\nproperty that exists in many documents, where one field has only one\ncorresponding entity. Moreover, our method is highly parameter-efficient\ncompared to existing approaches in terms of the number of trainable parameters.\nDespite this, experiments across various datasets show our method outperforms\nbaselines in most entity types. Many real-world documents exhibit combinatorial\nproperties which can be leveraged as inductive biases to improve extraction\naccuracy, but existing datasets do not cover these documents. To facilitate\nfuture research into these types of documents, we release a new ID document\ndataset that covers diverse templates and languages. We also release enhanced\nannotations for an existing dataset.\n"", '  Massive-scale historical document collections are crucial for social science\nresearch. Despite increasing digitization, these documents typically lack\nunique cross-document identifiers for individuals mentioned within the texts,\nas well as individual identifiers from external knowledgebases like\nWikipedia/Wikidata. Existing entity disambiguation methods often fall short in\naccuracy for historical documents, which are replete with individuals not\nremembered in contemporary knowledgebases. This study makes three key\ncontributions to improve cross-document coreference resolution and\ndisambiguation in historical texts: a massive-scale training dataset replete\nwith hard negatives - that sources over 190 million entity pairs from Wikipedia\ncontexts and disambiguation pages - high-quality evaluation data from\nhand-labeled historical newswire articles, and trained models evaluated on this\nhistorical benchmark. We contrastively train bi-encoder models for\ncoreferencing and disambiguating individuals in historical texts, achieving\naccurate, scalable performance that identifies out-of-knowledgebase\nindividuals. Our approach significantly surpasses other entity disambiguation\nmodels on our historical newswire benchmark. Our models also demonstrate\ncompetitive performance on modern entity disambiguation benchmarks,\nparticularly certain news disambiguation datasets.\n', '  The adoption of Deep Neural Networks (DNNs) has greatly benefited Natural\nLanguage Processing (NLP) during the past decade. However, the demands of long\ndocument analysis are quite different from those of shorter texts, while the\never increasing size of documents uploaded online renders automated\nunderstanding of lengthy texts a critical issue. Relevant applications include\nautomated Web mining, legal document review, medical records analysis,\nfinancial reports analysis, contract management, environmental impact\nassessment, news aggregation, etc. Despite the relatively recent development of\nefficient algorithms for analyzing long documents, practical tools in this\nfield are currently flourishing. This article serves as an entry point into\nthis dynamic domain and aims to achieve two objectives. First of all, it\nprovides an introductory overview of the relevant neural building blocks,\nserving as a concise tutorial for the field. Secondly, it offers a brief\nexamination of the current state-of-the-art in two key long document analysis\ntasks: document classification and document summarization. Sentiment analysis\nfor long texts is also covered, since it is typically treated as a particular\ncase of document classification. Consequently, this article presents an\nintroductory exploration of document-level analysis, addressing the primary\nchallenges, concerns, and existing solutions. Finally, it offers a concise\ndefinition of ""long text/document"", presents an original overarching taxonomy\nof common deep neural methods for long document analysis and lists publicly\navailable annotated datasets that can facilitate further research in this area.\n'] , ['  Accurately typing entity mentions from text segments is a fundamental task\nfor various natural language processing applications. Many previous approaches\nrely on massive human-annotated data to perform entity typing. Nevertheless,\ncollecting such data in highly specialized science and engineering domains\n(e.g., software engineering and security) can be time-consuming and costly,\nwithout mentioning the domain gaps between training and inference data if the\nmodel needs to be applied to confidential datasets. In this paper, we study the\ntask of seed-guided fine-grained entity typing in science and engineering\ndomains, which takes the name and a few seed entities for each entity type as\nthe only supervision and aims to classify new entity mentions into both seen\nand unseen types (i.e., those without seed entities). To solve this problem, we\npropose SEType which first enriches the weak supervision by finding more\nentities for each seen type from an unlabeled corpus using the contextualized\nrepresentations of pre-trained language models. It then matches the enriched\nentities to unlabeled text to get pseudo-labeled samples and trains a textual\nentailment model that can make inferences for both seen and unseen types.\nExtensive experiments on two datasets covering four domains demonstrate the\neffectiveness of SEType in comparison with various baselines.\n', '  Entity Set Expansion (ESE) is a critical task aiming at expanding entities of\nthe target semantic class described by seed entities. Most existing ESE methods\nare retrieval-based frameworks that need to extract contextual features of\nentities and calculate the similarity between seed entities and candidate\nentities. To achieve the two purposes, they iteratively traverse the corpus and\nthe entity vocabulary, resulting in poor efficiency and scalability.\nExperimental results indicate that the time consumed by the retrieval-based ESE\nmethods increases linearly with entity vocabulary and corpus size. In this\npaper, we firstly propose Generative Entity Set Expansion (GenExpan) framework,\nwhich utilizes a generative pre-trained auto-regressive language model to\naccomplish ESE task. Specifically, a prefix tree is employed to guarantee the\nvalidity of entity generation, and automatically generated class names are\nadopted to guide the model to generate target entities. Moreover, we propose\nKnowledge Calibration and Generative Ranking to further bridge the gap between\ngeneric knowledge of the language model and the goal of ESE task. For\nefficiency, expansion time consumed by GenExpan is independent of entity\nvocabulary and corpus size, and GenExpan achieves an average 600% speedup\ncompared to strong baselines. For expansion effectiveness, our framework\noutperforms previous state-of-the-art ESE methods.\n', '  Entity disambiguation (ED), which links the mentions of ambiguous entities to\ntheir referent entities in a knowledge base, serves as a core component in\nentity linking (EL). Existing generative approaches demonstrate improved\naccuracy compared to classification approaches under the standardized ZELDA\nbenchmark. Nevertheless, generative approaches suffer from the need for\nlarge-scale pre-training and inefficient generation. Most importantly, entity\ndescriptions, which could contain crucial information to distinguish similar\nentities from each other, are often overlooked. We propose an encoder-decoder\nmodel to disambiguate entities with more detailed entity descriptions. Given\ntext and candidate entities, the encoder learns interactions between the text\nand each candidate entity, producing representations for each entity candidate.\nThe decoder then fuses the representations of entity candidates together and\nselects the correct entity. Our experiments, conducted on various entity\ndisambiguation benchmarks, demonstrate the strong and robust performance of\nthis model, particularly +1.5% in the ZELDA benchmark compared with GENRE.\nFurthermore, we integrate this approach into the retrieval/reader framework and\nobserve +1.5% improvements in end-to-end entity linking in the GERBIL benchmark\ncompared with EntQA.\n']",Entity Understanding and Resolution,Entity Typing and Disambiguation
44,"Ontology Alignment and Learning with Semantic Web Technologies , Multi-Modal Entity Alignment , Entity Alignment in Knowledge Graphs","['ontologies', 'ontology', 'ontological', 'semantic', 'ontologically', 'owl', 'rdf', 'owl2vec', 'entities', 'semantically'] , ['modality', 'multimodal', 'entities', 'entity', 'modal', 'semantic', 'semantics', 'modalities', 'knowledge', 'relational'] , ['entities', 'entity', 'embeddings', 'alignments', 'semantic', 'embedding', 'alignment', 'similarities', 'relations', 'subgraph']","['  Ontology alignment, a critical process in the Semantic Web for detecting\nrelationships between different ontologies, has traditionally focused on\nidentifying so-called ""simple"" 1-to-1 relationships through class labels and\nproperties comparison. The more practically useful exploration of more complex\nalignments remains a hard problem to automate, and as such is largely\nunderexplored, i.e. in application practice it is usually done manually by\nontology and domain experts. Recently, the surge in Natural Language Processing\n(NLP) capabilities, driven by advancements in Large Language Models (LLMs),\npresents new opportunities for enhancing ontology engineering practices,\nincluding ontology alignment tasks. This paper investigates the application of\nLLM technologies to tackle the complex ontology alignment challenge. Leveraging\na prompt-based approach and integrating rich ontology content so-called modules\nour work constitutes a significant advance towards automating the complex\nalignment task.\n', '  Ontologies are widely used for representing domain knowledge and meta data,\nplaying an increasingly important role in Information Systems, the Semantic\nWeb, Bioinformatics and many other domains. However, logical reasoning that\nontologies can directly support are quite limited in learning, approximation\nand prediction. One straightforward solution is to integrate statistical\nanalysis and machine learning. To this end, automatically learning vector\nrepresentation for knowledge of an ontology i.e., ontology embedding has been\nwidely investigated in recent years. Numerous papers have been published on\nontology embedding, but a lack of systematic reviews hinders researchers from\ngaining a comprehensive understanding of this field. To bridge this gap, we\nwrite this survey paper, which first introduces different kinds of semantics of\nontologies, and formally defines ontology embedding from the perspectives of\nboth mathematics and machine learning, as well as its property of faithfulness.\nBased on this, it systematically categorises and analyses a relatively complete\nset of over 80 papers, according to the ontologies and semantics that they aim\nat, and their technical solutions including geometric modeling, sequence\nmodeling and graph propagation. This survey also introduces the applications of\nontology embedding in ontology engineering, machine learning augmentation and\nlife sciences, presents a new library mOWL, and discusses the challenges and\nfuture directions.\n', '  Ontologies provide formal representation of knowledge shared within Semantic\nWeb applications. Ontology learning involves the construction of ontologies\nfrom a given corpus. In the past years, ontology learning has traversed through\nshallow learning and deep learning methodologies, each offering distinct\nadvantages and limitations in the quest for knowledge extraction and\nrepresentation. A new trend of these approaches is relying on large language\nmodels (LLMs) to enhance ontology learning. This paper gives a review in\napproaches and challenges of ontology learning. It analyzes the methodologies\nand limitations of shallow-learning-based and deep-learning-based techniques\nfor ontology learning, and provides comprehensive knowledge for the frontier\nwork of using LLMs to enhance ontology learning. In addition, it proposes\nseveral noteworthy future directions for further exploration into the\nintegration of LLMs with ontology learning tasks.\n'] , ['  Multi-modal entity alignment (MMEA) aims to identify equivalent entity pairs\nacross different multi-modal knowledge graphs (MMKGs). Existing approaches\nfocus on how to better encode and aggregate information from different\nmodalities. However, it is not trivial to leverage multi-modal knowledge in\nentity alignment due to the modal heterogeneity. In this paper, we propose a\nMulti-Grained Interaction framework for Multi-Modal Entity Alignment (MIMEA),\nwhich effectively realizes multi-granular interaction within the same modality\nor between different modalities. MIMEA is composed of four modules: i) a\nMulti-modal Knowledge Embedding module, which extracts modality-specific\nrepresentations with multiple individual encoders; ii) a Probability-guided\nModal Fusion module, which employs a probability guided approach to integrate\nuni-modal representations into joint-modal embeddings, while considering the\ninteraction between uni-modal representations; iii) an Optimal Transport Modal\nAlignment module, which introduces an optimal transport mechanism to encourage\nthe interaction between uni-modal and joint-modal embeddings; iv) a\nModal-adaptive Contrastive Learning module, which distinguishes the embeddings\nof equivalent entities from those of non-equivalent ones, for each modality.\nExtensive experiments conducted on two real-world datasets demonstrate the\nstrong performance of MIMEA compared to the SoTA. Datasets and code have been\nsubmitted as supplementary materials.\n', '  Multi-modal entity alignment (MMEA) aims to identify equivalent entities\nbetween multi-modal knowledge graphs (MMKGs), where the entities can be\nassociated with related images. Most existing studies integrate multi-modal\ninformation heavily relying on the automatically-learned fusion module, rarely\nsuppressing the redundant information for MMEA explicitly. To this end, we\nexplore variational information bottleneck for multi-modal entity alignment\n(IBMEA), which emphasizes the alignment-relevant information and suppresses the\nalignment-irrelevant information in generating entity representations.\nSpecifically, we devise multi-modal variational encoders to generate\nmodal-specific entity representations as probability distributions. Then, we\npropose four modal-specific information bottleneck regularizers, limiting the\nmisleading clues in refining modal-specific entity representations. Finally, we\npropose a modal-hybrid information contrastive regularizer to integrate all the\nrefined modal-specific representations, enhancing the entity similarity between\nMMKGs to achieve MMEA. We conduct extensive experiments on two cross-KG and\nthree bilingual MMEA datasets. Experimental results demonstrate that our model\nconsistently outperforms previous state-of-the-art methods, and also shows\npromising and robust performance in low-resource and high-noise data scenarios.\n', '  Multi-modal entity alignment (MMEA) aims to identify equivalent entities\nbetween two multi-modal knowledge graphs (MMKGs), whose entities can be\nassociated with relational triples and related images. Most previous studies\ntreat the graph structure as a special modality, and fuse different modality\ninformation with separate uni-modal encoders, neglecting valuable relational\nassociations in modalities. Other studies refine each uni-modal information\nwith graph structures, but may introduce unnecessary relations in specific\nmodalities. To this end, we propose a novel local-to-global interaction network\nfor MMEA, termed as LoginMEA. Particularly, we first fuse local multi-modal\ninteractions to generate holistic entity semantics and then refine them with\nglobal relational interactions of entity neighbors. In this design, the\nuni-modal information is fused adaptively, and can be refined with relations\naccordingly. To enrich local interactions of multi-modal entity information, we\ndevice modality weights and low-rank interactive fusion, allowing diverse\nimpacts and element-level interactions among modalities. To capture global\ninteractions of graph structures, we adopt relation reflection graph attention\nnetworks, which fully capture relational associations between entities.\nExtensive experiments demonstrate superior results of our method over 5\ncross-KG or bilingual benchmark datasets, indicating the effectiveness of\ncapturing local and global interactions.\n'] , ['  The flourishing of knowledge graph applications has driven the need for\nentity alignment (EA) across KGs. However, the heterogeneity of practical KGs,\ncharacterized by differing scales, structures, and limited overlapping\nentities, greatly surpasses that of existing EA datasets. This discrepancy\nhighlights an oversimplified heterogeneity in current EA datasets, which\nobstructs a full understanding of the advancements achieved by recent EA\nmethods. In this paper, we study the performance of EA methods in practical\nsettings, specifically focusing on the alignment of highly heterogeneous KGs\n(HHKGs). Firstly, we address the oversimplified heterogeneity settings of\ncurrent datasets and propose two new HHKG datasets that closely mimic practical\nEA scenarios. Then, based on these datasets, we conduct extensive experiments\nto evaluate previous representative EA methods. Our findings reveal that, in\naligning HHKGs, valuable structure information can hardly be exploited through\nmessage-passing and aggregation mechanisms. This phenomenon leads to inferior\nperformance of existing EA methods, especially those based on GNNs. These\nfindings shed light on the potential problems associated with the conventional\napplication of GNN-based methods as a panacea for all EA datasets.\nConsequently, in light of these observations and to elucidate what EA\nmethodology is genuinely beneficial in practical scenarios, we undertake an\nin-depth analysis by implementing a simple but effective approach: Simple-HHEA.\nThis method adaptly integrates entity name, structure, and temporal information\nto navigate the challenges posed by HHKGs. Our experiment results conclude that\nthe key to the future EA model design in practice lies in their adaptability\nand efficiency to varying information quality conditions, as well as their\ncapability to capture patterns across HHKGs.\n', ""  Weakly Supervised Entity Alignment (EA) is the task of identifying equivalent\nentities across diverse knowledge graphs (KGs) using only a limited number of\nseed alignments. Despite substantial advances in aggregation-based weakly\nsupervised EA, the underlying mechanisms in this setting remain unexplored. In\nthis paper, we present a propagation perspective to analyze weakly supervised\nEA and explain the existing aggregation-based EA models. Our theoretical\nanalysis reveals that these models essentially seek propagation operators for\npairwise entity similarities. We further prove that, despite the structural\nheterogeneity of different KGs, the potentially aligned entities within\naggregation-based EA models have isomorphic subgraphs, which is the core\npremise of EA but has not been investigated. Leveraging this insight, we\nintroduce a potential isomorphism propagation operator to enhance the\npropagation of neighborhood information across KGs. We develop a general EA\nframework, PipEA, incorporating this operator to improve the accuracy of every\ntype of aggregation-based model without altering the learning process.\nExtensive experiments substantiate our theoretical findings and demonstrate\nPipEA's significant performance gains over state-of-the-art weakly supervised\nEA methods. Our work not only advances the field but also enhances our\ncomprehension of aggregation-based weakly supervised EA.\n"", ""  Entity Alignment (EA) aims to match equivalent entities in different\nKnowledge Graphs (KGs), which is essential for knowledge fusion and\nintegration. Recently, embedding-based EA has attracted significant attention\nand many approaches have been proposed. Early approaches primarily focus on\nlearning entity embeddings from the structural features of KGs, defined by\nrelation triples. Later methods incorporated entities' names and attributes as\nauxiliary information to enhance embeddings for EA. However, these approaches\noften used different techniques to encode structural and attribute information,\nlimiting their interaction and mutual enhancement. In this work, we propose a\ndense entity retrieval framework for EA, leveraging language models to\nuniformly encode various features of entities and facilitate nearest entity\nsearch across KGs. Alignment candidates are first generated through entity\nretrieval, which are subsequently reranked to determine the final alignments.\nWe conduct comprehensive experiments on both cross-lingual and monolingual EA\ndatasets, demonstrating that our approach achieves state-of-the-art performance\ncompared to existing EA methods.\n""]",Entity Alignment and Ontology Learning with Semantic Technologies,Entity Alignment in Knowledge Graphs
45,"Named Entity Recognition in NLP , Subword Tokenization in NLP , ""Adapting Language Models for NLP Tasks"" , BERT-based NLP Models and Techniques , Structured NLP Tasks and Language Models , ""Data Annotation in NLP"" , Abstract Meaning Representation in NLP","['annotations', 'annotation', 'entities', 'entity', 'annotated', 'nlp', 'corpus', 'labeling', 'supervised', 'sentences'] , ['tokenizers', 'tokenization', 'tokenizer', 'tokenisation', 'tokenize', 'sentencepiece', 'morphemes', 'subwords', 'tokens', 'wordpiece'] , ['nlp', 'classification', 'adapting', 'language', 'tasks', 'examples', 'generalization', 'models', 'training', 'accuracy'] , ['bert', 'encoder', 'encoders', 'encode', 'embeddings', 'tokenizer', 'tokenization', 'decoder', 'embedding', 'representations'] , ['corpus', 'nlp', 'texts', 'sentences', 'linguistic', 'language', 'structured', 'languages', 'text', 'writing'] , ['annotating', 'annotations', 'annotators', 'annotation', 'annotate', 'annotated', 'annotator', 'labeling', 'nlp', 'crowdsourcing'] , ['semantic', 'parsing', 'nlp', 'annotated', 'entailment', 'lexical', 'structured', 'amr', 'corpora', 'entities']","['  Named Entity Recognition (NER) serves as a fundamental task in natural\nlanguage understanding, bearing direct implications for web content analysis,\nsearch engines, and information retrieval systems. Fine-tuned NER models\nexhibit satisfactory performance on standard NER benchmarks. However, due to\nlimited fine-tuning data and lack of knowledge, it performs poorly on unseen\nentity recognition. As a result, the usability and reliability of NER models in\nweb-related applications are compromised. Instead, Large Language Models (LLMs)\nlike GPT-4 possess extensive external knowledge, but research indicates that\nthey lack specialty for NER tasks. Furthermore, non-public and large-scale\nweights make tuning LLMs difficult. To address these challenges, we propose a\nframework that combines small fine-tuned models with LLMs (LinkNER) and an\nuncertainty-based linking strategy called RDC that enables fine-tuned models to\ncomplement black-box LLMs, achieving better performance. We experiment with\nboth standard NER test sets and noisy social media datasets. LinkNER enhances\nNER task performance, notably surpassing SOTA models in robustness tests. We\nalso quantitatively analyze the influence of key components like uncertainty\nestimation methods, LLMs, and in-context learning on diverse NER tasks,\noffering specific web-related recommendations.\n', '  Lately, instruction-based techniques have made significant strides in\nimproving performance in few-shot learning scenarios. They achieve this by\nbridging the gap between pre-trained language models and fine-tuning for\nspecific downstream tasks. Despite these advancements, the performance of Large\nLanguage Models (LLMs) in information extraction tasks like Named Entity\nRecognition (NER), using prompts or instructions, still falls short of\nsupervised baselines. The reason for this performance gap can be attributed to\nthe fundamental disparity between NER and LLMs. NER is inherently a sequence\nlabeling task, where the model must assign entity-type labels to individual\ntokens within a sentence. In contrast, LLMs are designed as a text generation\ntask. This distinction between semantic labeling and text generation leads to\nsubpar performance. In this paper, we transform the NER task into a\ntext-generation task that can be readily adapted by LLMs. This involves\nenhancing source sentences with task-specific instructions and answer choices,\nallowing for the identification of entities and their types within natural\nlanguage. We harness the strength of LLMs by integrating supervised learning\nwithin them. The goal of this combined strategy is to boost the performance of\nLLMs in extraction tasks like NER while simultaneously addressing hallucination\nissues often observed in LLM-generated content. A novel corpus Contract NER\ncomprising seven frequently observed contract categories, encompassing named\nentities associated with 18 distinct legal entity types is released along with\nour baseline models. Our models and dataset are available to the community for\nfuture research * .\n', '  Named Entity Recognition (NER) is a well-studied problem in NLP. However,\nthere is much less focus on studying NER datasets, compared to developing new\nNER models. In this paper, we employed three simple techniques to detect\nannotation errors in the OntoNotes 5.0 corpus for English NER, which is the\nlargest available NER corpus for English. Our techniques corrected ~10% of the\nsentences in train/dev/test data. In terms of entity mentions, we corrected the\nspan and/or type of ~8% of mentions in the dataset, while\nadding/deleting/splitting/merging a few more. These are large numbers of\nchanges, considering the size of OntoNotes. We used three NER libraries to\ntrain, evaluate and compare the models trained with the original and the\nre-annotated datasets, which showed an average improvement of 1.23% in overall\nF-scores, with large (>10%) improvements for some of the entity types. While\nour annotation error detection methods are not exhaustive and there is some\nmanual annotation effort involved, they are largely language agnostic and can\nbe employed with other NER datasets, and other sequence labelling tasks.\n'] , ['  The popular subword tokenizers of current language models, such as Byte-Pair\nEncoding (BPE), are known not to respect morpheme boundaries, which affects the\ndownstream performance of the models. While many improved tokenization\nalgorithms have been proposed, their evaluation and cross-comparison is still\nan open problem. As a solution, we propose a combined intrinsic-extrinsic\nevaluation framework for subword tokenization. Intrinsic evaluation is based on\nour new UniMorph Labeller tool that classifies subword tokenization as either\nmorphological or alien. Extrinsic evaluation, in turn, is performed via the\nOut-of-Vocabulary Generalization Challenge 1.0 benchmark, which consists of\nthree newly specified downstream text classification tasks. Our empirical\nfindings show that the accuracy of UniMorph Labeller is 98%, and that, in all\nlanguage models studied (including ALBERT, BERT, RoBERTa, and DeBERTa), alien\ntokenization leads to poorer generalizations compared to morphological\ntokenization for semantic compositionality of word meanings.\n', ""  Tokenization is a foundational step in Natural Language Processing (NLP)\ntasks, bridging raw text and language models. Existing tokenization approaches\nlike Byte-Pair Encoding (BPE) originate from the field of data compression, and\nit has been suggested that the effectiveness of BPE stems from its ability to\ncondense text into a relatively small number of tokens. We test the hypothesis\nthat fewer tokens lead to better downstream performance by introducing\nPathPiece, a new tokenizer that segments a document's text into the minimum\nnumber of tokens for a given vocabulary. Through extensive experimentation we\nfind this hypothesis not to be the case, casting doubt on the understanding of\nthe reasons for effective tokenization. To examine which other factors play a\nrole, we evaluate design decisions across all three phases of tokenization:\npre-tokenization, vocabulary construction, and segmentation, offering new\ninsights into the design of effective tokenizers. Specifically, we illustrate\nthe importance of pre-tokenization and the benefits of using BPE to initialize\nvocabulary construction. We train 64 language models with varying tokenization,\nranging in size from 350M to 2.4B parameters, all of which are made publicly\navailable.\n"", '  Subword tokenization has become the prevailing standard in the field of\nnatural language processing (NLP) over recent years, primarily due to the\nwidespread utilization of pre-trained language models. This shift began with\nByte-Pair Encoding (BPE) and was later followed by the adoption of\nSentencePiece and WordPiece. While subword tokenization consistently\noutperforms character and word-level tokenization, the precise factors\ncontributing to its success remain unclear. Key aspects such as the optimal\nsegmentation granularity for diverse tasks and languages, the influence of data\nsources on tokenizers, and the role of morphological information in\nIndo-European languages remain insufficiently explored. This is particularly\npertinent for biomedical terminology, characterized by specific rules governing\nmorpheme combinations. Despite the agglutinative nature of biomedical\nterminology, existing language models do not explicitly incorporate this\nknowledge, leading to inconsistent tokenization strategies for common terms. In\nthis paper, we seek to delve into the complexities of subword tokenization in\nFrench biomedical domain across a variety of NLP tasks and pinpoint areas where\nfurther enhancements can be made. We analyze classical tokenization algorithms,\nincluding BPE and SentencePiece, and introduce an original tokenization\nstrategy that integrates morpheme-enriched word segmentation into existing\ntokenization methods.\n'] , ['  Parameter-efficient (PE) methods (like Prompts or Adapters) for adapting\npre-trained language models (PLM) to downstream tasks have been popular\nrecently. However, hindrances still prevent these methods from reaching their\nfull potential. For example, two significant challenges are few-shot adaptation\nand cross-task generalization. To tackle these issues, we propose a general PE\npriming framework to enhance and explore the few-shot adaptation and\ngeneralization ability of PE methods. In this framework, PLMs are primed with\nPE methods for rapidly adapting to various target tasks. To evaluate the\ngeneralization ability of these PE methods, we conduct experiments on a\nfew-shot cross-domain benchmark containing 160 diverse NLP tasks. Our\nexperiment not only reveals the best priming strategy but also verifies that\npriming facilitates the adaptation to target tasks.\n', '  Training or finetuning large-scale language models (LLMs) requires\nsubstantial computation resources, motivating recent efforts to explore\nparameter-efficient adaptation to downstream tasks. One approach is to treat\nthese models as black boxes and use forward passes (Inference APIs) to interact\nwith them. Current research focuses on adapting these black-box models to\ndownstream tasks using gradient-free prompt optimization, but this often\ninvolves an expensive process of searching task-specific prompts. Therefore, we\nare motivated to study black-box language model adaptation without prompt\nsearch. Specifically, we introduce a label-enhanced cross-attention network\ncalled CrossTune, which models the semantic relatedness between the input text\nsequence and task-specific label descriptions. Its effectiveness is examined in\nthe context of few-shot text classification. To improve the generalization of\nCrossTune, we utilize ChatGPT to generate additional training data through\nin-context learning. A switch mechanism is implemented to exclude low-quality\nChatGPT-generated data. Through extensive experiments on seven benchmark text\nclassification datasets, we demonstrate that our proposed approach outperforms\nthe previous state-of-the-art gradient-free black-box tuning method by 5.7% on\naverage. Even without using ChatGPT-augmented data, CrossTune performs better\nor comparably than previous black-box tuning methods, suggesting the\neffectiveness of our approach.\n', ""  For language model classification, would you prefer having only one workable\nclass or having every class working? The latter makes more practical uses.\nEspecially for large language models (LLMs), the fact that they achieve a fair\noverall accuracy by in-context learning (ICL) obscures a large difference in\nindividual class accuracies. In this work, we uncover and tackle language\nmodels' imbalance in per-class prediction accuracy by reconceptualizing it as\nthe Contextual Oddity Bias (COBias), and we are the first to engage nonlinear\ninteger programming (NIP) to debias it. Briefly, COBias refers to the\ndifference in accuracy by a class A compared to its ''odd'' class, which holds\nthe majority wrong predictions of class A. With the COBias metric, we reveal\nthat LLMs of varied scales and families exhibit large per-class accuracy\ndifferences. Then we propose Debiasing as Nonlinear Integer Programming (DNIP)\nto correct ICL per-class probabilities for lower bias and higher overall\naccuracy. Our optimization objective is directly based on the evaluation scores\nby COBias and accuracy metrics, solved by simulated annealing. Evaluations on\nthree LLMs across seven NLP classification tasks show that DNIP simultaneously\nachieves significant COBias reduction ($-27\\%$) and accuracy improvement\n($+12\\%$) over the conventional ICL approach, suggesting that modeling pairwise\nclass accuracy differences is a direction in pushing forward more accurate,\nmore reliable LLM predictions.\n""] , [""  Background/introduction: Pre-trained transformer models shine in many natural\nlanguage processing tasks and therefore are expected to bear the representation\nof the input sentence or text meaning. These sentence-level embeddings are also\nimportant in retrieval-augmented generation. But do commonly used plain\naveraging or prompt templates surface it enough?\n  Methods: Given 110M parameters BERT's hidden representations from multiple\nlayers and multiple tokens we tried various ways to extract optimal sentence\nrepresentations. We tested various token aggregation and representation\npost-processing techniques. We also tested multiple ways of using a general\nWikitext dataset to complement BERTs sentence representations. All methods were\ntested on 8 Semantic Textual Similarity (STS), 6 short text clustering, and 12\nclassification tasks. We also evaluated our representation-shaping techniques\non other static models, including random token representations.\n  Results: Proposed representation extraction methods improved the performance\non STS and clustering tasks for all models considered. Very high improvements\nfor static token-based models, especially random embeddings for STS tasks\nalmost reach the performance of BERT-derived representations.\n  Conclusions: Our work shows that for multiple tasks simple baselines with\nrepresentation shaping techniques reach or even outperform more complex\nBERT-based models or are able to contribute to their performance.\n"", ""  BERT (Bidirectional Encoder Representations from Transformers) has\nrevolutionized the field of natural language processing through its exceptional\nperformance on numerous tasks. Yet, the majority of researchers have mainly\nconcentrated on enhancements related to the model structure, such as relative\nposition embedding and more efficient attention mechanisms. Others have delved\ninto pretraining tricks associated with Masked Language Modeling, including\nwhole word masking. DeBERTa introduced an enhanced decoder adapted for BERT's\nencoder model for pretraining, proving to be highly effective. We argue that\nthe design and research around enhanced masked language modeling decoders have\nbeen underappreciated. In this paper, we propose several designs of enhanced\ndecoders and introduce BPDec (BERT Pretraining Decoder), a novel method for\nmodeling training. Typically, a pretrained BERT model is fine-tuned for\nspecific Natural Language Understanding (NLU) tasks. In our approach, we\nutilize the original BERT model as the encoder, making only changes to the\ndecoder without altering the encoder. This approach does not necessitate\nextensive modifications to the encoder architecture and can be seamlessly\nintegrated into existing fine-tuning pipelines and services, offering an\nefficient and effective enhancement strategy. Compared to other methods, while\nwe also incur a moderate training cost for the decoder during the pretraining\nprocess, our approach does not introduce additional training costs during the\nfine-tuning phase. We test multiple enhanced decoder structures after\npretraining and evaluate their performance on the GLUE tasks and SQuAD tasks.\nOur results demonstrate that BPDec, having only undergone subtle refinements to\nthe model structure during pretraining, significantly enhances model\nperformance without escalating the finetuning cost, inference time and serving\nbudget.\n"", ""  Recent years have witnessed a substantial increase in the use of deep\nlearning to solve various natural language processing (NLP) problems. Early\ndeep learning models were constrained by their sequential or unidirectional\nnature, such that they struggled to capture the contextual relationships across\ntext inputs. The introduction of bidirectional encoder representations from\ntransformers (BERT) leads to a robust encoder for the transformer model that\ncan understand the broader context and deliver state-of-the-art performance\nacross various NLP tasks. This has inspired researchers and practitioners to\napply BERT to practical problems, such as information retrieval (IR). A survey\nthat focuses on a comprehensive analysis of prevalent approaches that apply\npretrained transformer encoders like BERT to IR can thus be useful for academia\nand the industry. In light of this, we revisit a variety of BERT-based methods\nin this survey, cover a wide range of techniques of IR, and group them into six\nhigh-level categories: (i) handling long documents, (ii) integrating semantic\ninformation, (iii) balancing effectiveness and efficiency, (iv) predicting the\nweights of terms, (v) query expansion, and (vi) document expansion. We also\nprovide links to resources, including datasets and toolkits, for BERT-based IR\nsystems. A key highlight of our survey is the comparison between BERT's\nencoder-based models and the latest generative Large Language Models (LLMs),\nsuch as ChatGPT, which rely on decoders. Despite the popularity of LLMs, we\nfind that for specific tasks, finely tuned BERT encoders still outperform, and\nat a lower deployment cost. Finally, we summarize the comprehensive outcomes of\nthe survey and suggest directions for future research in the area.\n""] , ['  Despite their impressive performance, large language models (LMs) still\nstruggle with reliably generating complex output structures when not finetuned\nto follow the required output format exactly. To address this issue,\ngrammar-constrained decoding (GCD) can be used to control the generation of\nLMs, guaranteeing that the output follows a given structure. Most existing GCD\nmethods are, however, limited to specific tasks, such as parsing or code\ngeneration. In this work, we demonstrate that formal grammars can describe the\noutput space for a much wider range of tasks and argue that GCD can serve as a\nunified framework for structured NLP tasks in general. For increased\nflexibility, we introduce input-dependent grammars, which allow the grammar to\ndepend on the input and thus enable the generation of different output\nstructures for different inputs. We then empirically demonstrate the power and\nflexibility of GCD-enhanced LMs on (1) information extraction, (2) entity\ndisambiguation, and (3) constituency parsing. Our results indicate that\ngrammar-constrained LMs substantially outperform unconstrained LMs or even beat\ntask-specific finetuned models. Grammar constraints thus hold great promise for\nharnessing off-the-shelf LMs for a wide range of structured NLP tasks,\nespecially where training data is scarce or finetuning is expensive. Code and\ndata: https://github.com/epfl-dlab/GCD.\n', '  Using Large Language Models (LLMs) to generate synthetic data for model\ntraining has become increasingly popular in recent years. While LLMs are\ncapable of producing realistic training data, the effectiveness of data\ngeneration is influenced by various factors, including the choice of prompt,\ntask complexity, and the quality, quantity, and diversity of the generated\ndata. In this work, we focus exclusively on using synthetic data for text\nclassification tasks. Specifically, we use natural language understanding (NLU)\nmodels trained on synthetic data to assess the quality of synthetic data from\ndifferent generation approaches. This work provides an empirical analysis of\nthe impact of these factors and offers recommendations for better data\ngeneration practices.\n', ""  We introduce Bonito, an open-source model for conditional task generation\nthat converts unannotated text into task-specific training datasets for\ninstruction tuning. We aim to enable zero-shot task adaptation of large\nlanguage models on users' specialized, private data. We train Bonito by\nfine-tuning a pretrained large language model on a new large-scale dataset with\n1.65M examples created by remixing existing instruction tuning datasets into\nmeta-templates. The meta-templates for a dataset produce training examples\nwhere the input is the unannotated text and the task attribute and the output\nconsists of the instruction and the response. We use Bonito to generate\nsynthetic tasks for seven datasets from specialized domains with unannotated\ntext across three task types -- yes-no question answering, extractive question\nanswering, and natural language inference -- and adapt language models. We show\nthat Bonito significantly improves the average performance of pretrained and\ninstruction tuned models over the de facto self supervised baseline. For\nexample, adapting Mistral-Instruct-v2 and instruction tuned variants of Mistral\nand Llama2 with Bonito improves the strong zero-shot performance by 22.1 F1\npoints whereas the next word prediction objective undoes some of the benefits\nof instruction tuning and reduces the average performance by 0.8 F1 points. We\nconduct additional experiments with Bonito to understand the effects of the\ndomain, the size of the training set, and the choice of alternative synthetic\ntask generators. Overall, we show that learning with synthetic instruction\ntuning datasets is an effective way to adapt language models to new domains.\nThe model, dataset, and code are available at\nhttps://github.com/BatsResearch/bonito.\n""] , ['  Manual data annotation is an important NLP task but one that takes\nconsiderable amount of resources and effort. In spite of the costs, labeling\nand categorizing entities is essential for NLP tasks such as semantic\nevaluation. Even though annotation can be done by non-experts in most cases,\ndue to the fact that this requires human labor, the process is costly. Another\nmajor challenge encountered in data annotation is maintaining the annotation\nconsistency. Annotation efforts are typically carried out by teams of multiple\nannotators. The annotations need to maintain the consistency in relation to\nboth the domain truth and annotation format while reducing human errors.\nAnnotating a specialized domain that deviates significantly from the general\ndomain, such as fantasy literature, will see a lot of human error and annotator\ndisagreement. So it is vital that proper guidelines and error reduction\nmechanisms are enforced. One such way to enforce these constraints is using a\nspecialized application. Such an app can ensure that the notations are\nconsistent, and the labels can be pre-defined or restricted reducing the room\nfor errors. In this paper, we present SHADE, an annotation software that can be\nused to annotate entities in the high fantasy literature domain. Specifically\nin Dungeons and Dragons lore extracted from the Forgotten Realms Fandom Wiki.\n', '  Data annotation generally refers to the labeling or generating of raw data\nwith relevant information, which could be used for improving the efficacy of\nmachine learning models. The process, however, is labor-intensive and costly.\nThe emergence of advanced Large Language Models (LLMs), exemplified by GPT-4,\npresents an unprecedented opportunity to automate the complicated process of\ndata annotation. While existing surveys have extensively covered LLM\narchitecture, training, and general applications, we uniquely focus on their\nspecific utility for data annotation. This survey contributes to three core\naspects: LLM-Based Annotation Generation, LLM-Generated Annotations Assessment,\nand LLM-Generated Annotations Utilization. Furthermore, this survey includes an\nin-depth taxonomy of data types that LLMs can annotate, a comprehensive review\nof learning strategies for models utilizing LLM-generated annotations, and a\ndetailed discussion of the primary challenges and limitations associated with\nusing LLMs for data annotation. Serving as a key guide, this survey aims to\nassist researchers and practitioners in exploring the potential of the latest\nLLMs for data annotation, thereby fostering future advancements in this\ncritical field.\n', '  Counterfactual examples are frequently used for model development and\nevaluation in many natural language processing (NLP) tasks. Although methods\nfor automated counterfactual generation have been explored, such methods depend\non models such as pre-trained language models that are then fine-tuned on\nauxiliary, often task-specific datasets. Collecting and annotating such\ndatasets for counterfactual generation is labor intensive and therefore,\ninfeasible in practice. Therefore, in this work, we focus on a novel problem\nsetting: \\textit{zero-shot counterfactual generation}. To this end, we propose\na structured way to utilize large language models (LLMs) as general purpose\ncounterfactual example generators. We hypothesize that the\ninstruction-following and textual understanding capabilities of recent LLMs can\nbe effectively leveraged for generating high quality counterfactuals in a\nzero-shot manner, without requiring any training or fine-tuning. Through\ncomprehensive experiments on various downstream tasks in natural language\nprocessing (NLP), we demonstrate the efficacy of LLMs as zero-shot\ncounterfactual generators in evaluating and explaining black-box NLP models.\n'] , ['  Combining large language models with logical reasoning enhances their\ncapacity to address problems in a robust and reliable manner. Nevertheless, the\nintricate nature of logical reasoning poses challenges when gathering reliable\ndata from the web to build comprehensive training datasets, subsequently\naffecting performance on downstream tasks. To address this, we introduce a\nnovel logic-driven data augmentation approach, AMR-LDA. AMR-LDA converts the\noriginal text into an Abstract Meaning Representation (AMR) graph, a structured\nsemantic representation that encapsulates the logical structure of the\nsentence, upon which operations are performed to generate logically modified\nAMR graphs. The modified AMR graphs are subsequently converted back into text\nto create augmented data. Notably, our methodology is architecture-agnostic and\nenhances both generative large language models, such as GPT-3.5 and GPT-4,\nthrough prompt augmentation, and discriminative large language models through\ncontrastive learning with logic-driven data augmentation. Empirical evidence\nunderscores the efficacy of our proposed method with improvement in performance\nacross seven downstream tasks, such as reading comprehension requiring logical\nreasoning, textual entailment, and natural language inference. Furthermore, our\nmethod leads on the ReClor leaderboard at\nhttps://eval.ai/web/challenges/challenge-page/503/leaderboard/1347. The source\ncode and data are publicly available at\nhttps://github.com/Strong-AI-Lab/Logical-Equivalence-driven-AMR-Data-Augmentation-for-Representation-Learning.\n', '  Traditionally, natural language processing (NLP) models often use a rich set\nof features created by linguistic expertise, such as semantic representations.\nHowever, in the era of large language models (LLMs), more and more tasks are\nturned into generic, end-to-end sequence generation problems. In this paper, we\ninvestigate the question: what is the role of semantic representations in the\nera of LLMs? Specifically, we investigate the effect of Abstract Meaning\nRepresentation (AMR) across five diverse NLP tasks. We propose an AMR-driven\nchain-of-thought prompting method, which we call AMRCoT, and find that it\ngenerally hurts performance more than it helps. To investigate what AMR may\nhave to offer on these tasks, we conduct a series of analysis experiments. We\nfind that it is difficult to predict which input examples AMR may help or hurt\non, but errors tend to arise with multi-word expressions, named entities, and\nin the final inference step where the LLM must connect its reasoning over the\nAMR to its prediction. We recommend focusing on these areas for future work in\nsemantic representations for LLMs. Our code:\nhttps://github.com/causalNLP/amr_llm.\n', '  AMR-to-text is one of the key techniques in the NLP community that aims at\ngenerating sentences from the Abstract Meaning Representation (AMR) graphs.\nSince AMR was proposed in 2013, the study on AMR-to-Text has become\nincreasingly prevalent as an essential branch of structured data to text\nbecause of the unique advantages of AMR as a high-level semantic description of\nnatural language. In this paper, we provide a brief survey of AMR-to-Text.\nFirstly, we introduce the current scenario of this technique and point out its\ndifficulties. Secondly, based on the methods used in previous studies, we\nroughly divided them into five categories according to their respective\nmechanisms, i.e., Rules-based, Seq-to-Seq-based, Graph-to-Seq-based,\nTransformer-based, and Pre-trained Language Model (PLM)-based. In particular,\nwe detail the neural network-based method and present the latest progress of\nAMR-to-Text, which refers to AMR reconstruction, Decoder optimization, etc.\nFurthermore, we present the benchmarks and evaluation methods of AMR-to-Text.\nEventually, we provide a summary of current techniques and the outlook for\nfuture research.\n']",Natural Language Processing (NLP) Techniques and Applications,Structured NLP Tasks and Language Models
46,"Semantic Textual Similarity (STS) , ""Lexical Semantic Change Detection in Corpora"" , Metaphor Processing in NLP , Word Sense Disambiguation and Semantic Analysis","['embeddings', 'textual', 'nlp', 'similarity', 'embedding', 'semantic', 'word2vec', 'sentences', 'fasttext', 'similarities'] , ['lexical', 'semantic', 'lexicon', 'corpus', 'wordnet', 'linguistics', 'contextualized', 'contextualised', 'corpora', 'words'] , ['metaphors', 'metaphor', 'metaphorical', 'similes', 'metaphorically', 'chinese_metaphor_explanation', 'simile', 'figurative', 'linguistics', 'linguistically'] , ['semantic', 'wordnet', 'lexical', 'semantics', 'linguistic', 'disambiguation', 'semantically', 'parsing', 'nlp', 'parsers']","[""  Semantic textual similarity (STS) is a fundamental NLP task that measures the\nsemantic similarity between a pair of sentences. In order to reduce the\ninherent ambiguity posed from the sentences, a recent work called Conditional\nSTS (C-STS) has been proposed to measure the sentences' similarity conditioned\non a certain aspect. Despite the popularity of C-STS, we find that the current\nC-STS dataset suffers from various issues that could impede proper evaluation\non this task. In this paper, we reannotate the C-STS validation set and observe\nan annotator discrepancy on 55% of the instances resulting from the annotation\nerrors in the original label, ill-defined conditions, and the lack of clarity\nin the task definition. After a thorough dataset analysis, we improve the C-STS\ntask by leveraging the models' capability to understand the conditions under a\nQA task setting. With the generated answers, we present an automatic error\nidentification pipeline that is able to identify annotation errors from the\nC-STS data with over 80% F1 score. We also propose a new method that largely\nimproves the performance over baselines on the C-STS data by training the\nmodels with the answers. Finally we discuss the conditionality annotation based\non the typed-feature structure (TFS) of entity types. We show in examples that\nthe TFS is able to provide a linguistic foundation for constructing C-STS data\nwith new conditions.\n"", '  High-quality text embedding is pivotal in improving semantic textual\nsimilarity (STS) tasks, which are crucial components in Large Language Model\n(LLM) applications. However, a common challenge existing text embedding models\nface is the problem of vanishing gradients, primarily due to their reliance on\nthe cosine function in the optimization objective, which has saturation zones.\nTo address this issue, this paper proposes a novel angle-optimized text\nembedding model called AnglE. The core idea of AnglE is to introduce angle\noptimization in a complex space. This novel approach effectively mitigates the\nadverse effects of the saturation zone in the cosine function, which can impede\ngradient and hinder optimization processes. To set up a comprehensive STS\nevaluation, we experimented on existing short-text STS datasets and a newly\ncollected long-text STS dataset from GitHub Issues. Furthermore, we examine\ndomain-specific STS scenarios with limited labeled data and explore how AnglE\nworks with LLM-annotated data. Extensive experiments were conducted on various\ntasks including short-text STS, long-text STS, and domain-specific STS tasks.\nThe results show that AnglE outperforms the state-of-the-art (SOTA) STS models\nthat ignore the cosine saturation zone. These findings demonstrate the ability\nof AnglE to generate high-quality text embeddings and the usefulness of angle\noptimization in STS.\n', '  Since the introduction of BERT and RoBERTa, research on Semantic Textual\nSimilarity (STS) has made groundbreaking progress. Particularly, the adoption\nof contrastive learning has substantially elevated state-of-the-art performance\nacross various STS benchmarks. However, contrastive learning categorizes text\npairs as either semantically similar or dissimilar, failing to leverage\nfine-grained annotated information and necessitating large batch sizes to\nprevent model collapse. These constraints pose challenges for researchers\nengaged in STS tasks that require nuanced similarity levels or those with\nlimited computational resources, compelling them to explore alternatives like\nSentence-BERT. Nonetheless, Sentence-BERT tackles STS tasks from a\nclassification perspective, overlooking the progressive nature of semantic\nrelationships, which results in suboptimal performance. To bridge this gap,\nthis paper presents an innovative regression framework and proposes two simple\nyet effective loss functions: Translated ReLU and Smooth K2 Loss. Experimental\nanalyses demonstrate that our method achieves convincing performance across\nseven established STS benchmarks, especially when supplemented with\ntask-specific training data.\n'] , ['  Word meanings change over time, and word senses evolve, emerge or die out in\nthe process. For ancient languages, where the corpora are often small and\nsparse, modelling such changes accurately proves challenging, and quantifying\nuncertainty in sense-change estimates consequently becomes important. GASC\n(Genre-Aware Semantic Change) and DiSC (Diachronic Sense Change) are existing\ngenerative models that have been used to analyse sense change for target words\nfrom an ancient Greek text corpus, using unsupervised learning without the help\nof any pre-training. These models represent the senses of a given target word\nsuch as ""kosmos"" (meaning decoration, order or world) as distributions over\ncontext words, and sense prevalence as a distribution over senses. The models\nare fitted using Markov Chain Monte Carlo (MCMC) methods to measure temporal\nchanges in these representations. This paper introduces EDiSC, an Embedded DiSC\nmodel, which combines word embeddings with DiSC to provide superior model\nperformance. It is shown empirically that EDiSC offers improved predictive\naccuracy, ground-truth recovery and uncertainty quantification, as well as\nbetter sampling efficiency and scalability properties with MCMC methods. The\nchallenges of fitting these models are also discussed.\n', '  Despite the predominance of contextualized embeddings in NLP, approaches to\ndetect semantic change relying on these embeddings and clustering methods\nunderperform simpler counterparts based on static word embeddings. This stems\nfrom the poor quality of the clustering methods to produce sense clusters --\nwhich struggle to capture word senses, especially those with low frequency.\nThis issue hinders the next step in examining how changes in word senses in one\nlanguage influence another. To address this issue, we propose a graph-based\nclustering approach to capture nuanced changes in both high- and low-frequency\nword senses across time and languages, including the acquisition and loss of\nthese senses over time. Our experimental results show that our approach\nsubstantially surpasses previous approaches in the SemEval2020 binary\nclassification task across four languages. Moreover, we showcase the ability of\nour approach as a versatile visualization tool to detect semantic changes in\nboth intra-language and inter-language setups. We make our code and data\npublicly available.\n', ""  We use contextualized word definitions generated by large language models as\nsemantic representations in the task of diachronic lexical semantic change\ndetection (LSCD). In short, generated definitions are used as `senses', and the\nchange score of a target word is retrieved by comparing their distributions in\ntwo time periods under comparison. On the material of five datasets and three\nlanguages, we show that generated definitions are indeed specific and general\nenough to convey a signal sufficient to rank sets of words by the degree of\ntheir semantic change over time. Our approach is on par with or outperforms\nprior non-supervised sense-based LSCD methods. At the same time, it preserves\ninterpretability and allows to inspect the reasons behind a specific shift in\nterms of discrete definitions-as-senses. This is another step in the direction\nof explainable semantic change modeling.\n""] , [""  Metaphors are considered to pose challenges for a wide spectrum of NLP tasks.\nThis gives rise to the area of computational metaphor processing. However, it\nremains unclear what types of metaphors challenge current state-of-the-art\nmodels. In this paper, we test various NLP models on the VUA metaphor dataset\nand quantify to what extent metaphors affect models' performance on various\ndownstream tasks. Analysis reveals that VUA includes a large number of\nmetaphors that pose little difficulty to downstream tasks. We would like to\nshift the attention of researchers away from these metaphors to instead focus\non challenging metaphors. To identify hard metaphors, we propose an automatic\npipeline that identifies metaphors that challenge a particular model. Our\nanalysis demonstrates that our detected hard metaphors contrast significantly\nwith VUA and reduce the accuracy of machine translation by 16\\%, QA performance\nby 4\\%, NLI by 7\\%, and metaphor identification recall by over 14\\% for various\npopular NLP systems.\n"", '  This paper presents the results of the shared task on Chinese metaphor\ngeneration, hosted at the 13th CCF Conference on Natural Language Processing\nand Chinese Computing (NLPCC 2024). The goal of this shared task is to generate\nChinese metaphors using machine learning techniques and effectively identifying\nbasic components of metaphorical sentences. It is divided into two subtasks: 1)\nMetaphor Generation, which involves creating a metaphor from a provided tuple\nconsisting of TENOR, GROUND, and VEHICLE. The goal here is to synthesize a\nmetaphor that connects the subject (i.e. TENOR) with the object (i.e. VEHICLE),\nguided by the concept of the GROUND. 2) Metaphor Components Identification,\nwhich extracts the most fitting TENORs, GROUNDs, and VEHICLEs from a\nmetaphorical sentence. This component requires the identification of the most\nfitting metaphor elements that correspond to the specified grounds. In addition\nto overall results, we report on the setup and insights from the metaphor\ngeneration shared task, which attracted a total of 4 participating teams across\nboth subtasks.\n', ""  Metaphors, although occasionally unperceived, are ubiquitous in our everyday\nlanguage. Thus, it is crucial for Language Models to be able to grasp the\nunderlying meaning of this kind of figurative language. In this work, we\npresent Meta4XNLI, a novel parallel dataset for the tasks of metaphor detection\nand interpretation that contains metaphor annotations in both Spanish and\nEnglish. We investigate language models' metaphor identification and\nunderstanding abilities through a series of monolingual and cross-lingual\nexperiments by leveraging our proposed corpus. In order to comprehend how these\nnon-literal expressions affect models' performance, we look over the results\nand perform an error analysis. Additionally, parallel data offers many\npotential opportunities to investigate metaphor transferability between these\nlanguages and the impact of translation on the development of multilingual\nannotated resources.\n""] , ['  Word sense disambiguation primarily addresses the lexical ambiguity of common\nwords based on a predefined sense inventory. Conversely, proper names are\nusually considered to denote an ad-hoc real-world referent. Once the reference\nis decided, the ambiguity is purportedly resolved. However, proper names also\nexhibit ambiguities through appellativization, i.e., they act like common words\nand may denote different aspects of their referents. We proposed to address the\nambiguities of proper names through the light of regular polysemy, which we\nformalized as dot objects. This paper introduces a combined word sense\ndisambiguation (WSD) model for disambiguating common words against Chinese\nWordnet (CWN) and proper names as dot objects. The model leverages the\nflexibility of a gloss-based model architecture, which takes advantage of the\nglosses and example sentences of CWN. We show that the model achieves\ncompetitive results on both common and proper nouns, even on a relatively\nsparse sense dataset. Aside from being a performant WSD tool, the model further\nfacilitates the future development of the lexical resource.\n', '  This paper explores techniques that focus on understanding and resolving\nambiguity in language within the field of natural language processing (NLP),\nhighlighting the complexity of linguistic phenomena such as polysemy and\nhomonymy and their implications for computational models. Focusing extensively\non Word Sense Disambiguation (WSD), it outlines diverse approaches ranging from\ndeep learning techniques to leveraging lexical resources and knowledge graphs\nlike WordNet. The paper introduces cutting-edge methodologies like word sense\nextension (WSE) and neuromyotonic approaches, enhancing disambiguation accuracy\nby predicting new word senses. It examines specific applications in biomedical\ndisambiguation and language specific optimisation and discusses the\nsignificance of cognitive metaphors in discourse analysis. The research\nidentifies persistent challenges in the field, such as the scarcity of sense\nannotated corpora and the complexity of informal clinical texts. It concludes\nby suggesting future directions, including using large language models, visual\nWSD, and multilingual WSD systems, emphasising the ongoing evolution in\naddressing lexical complexities in NLP. This thinking perspective highlights\nthe advancement in this field to enable computers to understand language more\naccurately.\n', '  In this work, we propose a Distributional Semantic resource enriched with\nlinguistic and lexical information extracted from electronic dictionaries,\ndesigned to address the challenge of bridging the gap between the continuous\nsemantic values represented by distributional vectors and the discrete\ndescriptions offered by general semantics theory. Recently, many researchers\nhave concentrated on the nexus between embeddings and a comprehensive theory of\nsemantics and meaning. This often involves decoding the representation of word\nmeanings in Distributional Models into a set of discrete, manually constructed\nproperties such as semantic primitives or features, using neural decoding\ntechniques. Our approach introduces an alternative strategy grounded in\nlinguistic data. We have developed a collection of domain-specific\nco-occurrence matrices, derived from two sources: a classification of Italian\nnouns categorized into 4 semantic traits and 20 concrete noun sub-categories,\nand a list of Italian verbs classified according to their semantic classes. In\nthese matrices, the co-occurrence values for each word are calculated\nexclusively with a defined set of words pertinent to a particular lexical\ndomain. The resource comprises 21 domain-specific matrices, one comprehensive\nmatrix, and a Graphical User Interface. Our model facilitates the generation of\nreasoned semantic descriptions of concepts by selecting matrices directly\nassociated with concrete conceptual knowledge, such as a matrix based on\nlocation nouns and the concept of animal habitats. We assessed the utility of\nthe resource through two experiments, achieving promising outcomes in both: the\nautomatic classification of animal nouns and the extraction of animal features.\n']",Natural Language Processing and Semantic Analysis,Word Sense Disambiguation and Semantic Analysis
47,"Syntactic Parsing and Language Change Analysis , Psycholinguistics of Sentence Processing , Discourse Analysis and Linguistic Parsing , Linguistic Grammars and Semantics , Linguistic Semantics and Negation in Sentences","['parsers', 'treebanks', 'treebank', 'parsing', 'parser', 'parses', 'parse', 'syntactic', 'corpus', 'grammars'] , ['linguistic', 'linguistics', 'psycholinguistics', 'syntactic', 'sentences', 'psycholinguistic', 'linguists', 'grammaticality', 'judgments', 'language'] , ['discourse', 'linguistic', 'linguistics', 'corpus', 'treebank', 'annotation', 'parsing', 'parsers', 'rhetorical', 'connectives'] , ['linguistic', 'grammars', 'syntactic', 'semantics', 'languages', 'grammar', 'language', 'monadic', 'contextual', 'parsing'] , ['sentences', 'linguistic', 'semantics', 'syntactic', 'ambiguities', 'semantic', 'lexical', 'negation', 'nlg', 'ambiguity']","[""  Many studies have shown that human languages tend to optimize for lower\ncomplexity and increased communication efficiency. Syntactic dependency\ndistance, which measures the linear distance between dependent words, is often\nconsidered a key indicator of language processing difficulty and working memory\nload. The current paper looks at diachronic trends in syntactic language change\nin both English and German, using corpora of parliamentary debates from the\nlast c. 160 years. We base our observations on five dependency parsers,\nincluding the widely used Stanford CoreNLP as well as 4 newer alternatives. Our\nanalysis of syntactic language change goes beyond linear dependency distance\nand explores 15 metrics relevant to dependency distance minimization (DDM)\nand/or based on tree graph properties, such as the tree height and degree\nvariance. Even though we have evidence that recent parsers trained on modern\ntreebanks are not heavily affected by data 'noise' such as spelling changes and\nOCR errors in our historic data, we find that results of syntactic language\nchange are sensitive to the parsers involved, which is a caution against using\na single parser for evaluating syntactic language change as done in previous\nwork. We also show that syntactic language change over the time period\ninvestigated is largely similar between English and German for the different\nmetrics explored: only 4% of cases we examine yield opposite conclusions\nregarding upwards and downtrends of syntactic metrics across German and\nEnglish. We also show that changes in syntactic measures seem to be more\nfrequent at the tails of sentence length distributions. To our best knowledge,\nours is the most comprehensive analysis of syntactic language change using\nmodern NLP technology in recent corpora of English and German.\n"", '  Parsing is the process of breaking a sentence into its grammatical components\nand identifying the syntactic structure of the sentence. The syntactically\ncorrect sentence structure is achieved by assigning grammatical labels to its\nconstituents using lexicon and syntactic rules. In linguistics, parser is\nextremely useful due to the number of different applications like name entity\nrecognition, QA systems and information extraction, etc. The two most common\ntechniques used for parsing are phrase structure and dependency Structure.\nBecause Urdu is a low-resource language, there has been little progress in\nbuilding an Urdu parser. A comparison of several parsers revealed that the\ndependency parsing approach is better suited for order-free languages such as\nUrdu. We have made significant progress in parsing Urdu, a South Asian language\nwith a complex morphology. For Urdu dependency parsing, a basic feature model\nconsisting of word location, word head, and dependency relation is employed as\na starting point, followed by more complex feature models. The dependency\ntagset is designed after careful consideration of the complex morphological\nstructure of the Urdu language, word order variation, and lexical ambiguity and\nit contains 22 tags. Our dataset comprises of sentences from news articles, and\nwe tried to include sentences of different complexity (which is quite\nchallenging), to get reliable results. All experiments are performed using\nMaltParser, exploring all 9 algorithms and classifiers. We have achieved a 70\npercent overall best-labeled accuracy (LA), as well as an 84 percent overall\nbest-unlabeled attachment score (UAS) using the Nivreeager algorithm. The\ncomparison of output data with treebank test data that has been manually parsed\nis then used to carry out error assessment and to identify the errors produced\nby the parser.\n', '  Contemporary multilingual dependency parsers can parse a diverse set of\nlanguages, but for Morphologically Rich Languages (MRLs), performance is\nattested to be lower than other languages. The key challenge is that, due to\nhigh morphological complexity and ambiguity of the space-delimited input\ntokens, the linguistic units that act as nodes in the tree are not known in\nadvance. Pre-neural dependency parsers for MRLs subscribed to the joint\nmorpho-syntactic hypothesis, stating that morphological segmentation and\nsyntactic parsing should be solved jointly, rather than as a pipeline where\nsegmentation precedes parsing. However, neural state-of-the-art parsers to date\nuse a strict pipeline. In this paper we introduce a joint neural architecture\nwhere a lattice-based representation preserving all morphological ambiguity of\nthe input is provided to an arc-factored model, which then solves the\nmorphological segmentation and syntactic parsing tasks at once. Our experiments\non Hebrew, a rich and highly ambiguous MRL, demonstrate state-of-the-art\nperformance on parsing, tagging and segmentation of the Hebrew section of UD,\nusing a single model. This proposed architecture is LLM-based and language\nagnostic, providing a solid foundation for MRLs to obtain further performance\nimprovements and bridge the gap with other languages.\n'] , [""  To date, most investigations on surprisal and entropy effects in reading have\nbeen conducted on the group level, disregarding individual differences. In this\nwork, we revisit the predictive power of surprisal and entropy measures\nestimated from a range of language models (LMs) on data of human reading times\nas a measure of processing effort by incorporating information of language\nusers' cognitive capacities. To do so, we assess the predictive power of\nsurprisal and entropy estimated from generative LMs on reading data obtained\nfrom individuals who also completed a wide range of psychometric tests.\nSpecifically, we investigate if modulating surprisal and entropy relative to\ncognitive scores increases prediction accuracy of reading times, and we examine\nwhether LMs exhibit systematic biases in the prediction of reading times for\ncognitively high- or low-performing groups, revealing what type of\npsycholinguistic subject a given LM emulates. Our study finds that in most\ncases, incorporating cognitive capacities increases predictive power of\nsurprisal and entropy on reading times, and that generally, high performance in\nthe psychometric tests is associated with lower sensitivity to predictability\neffects. Finally, our results suggest that the analyzed LMs emulate readers\nwith lower verbal intelligence, suggesting that for a given target group (i.e.,\nindividuals with high verbal intelligence), these LMs provide less accurate\npredictability estimates.\n"", ""  The effect of syntactic priming exhibits three well-documented empirical\nproperties: the lexical boost, the inverse frequency effect, and the\nasymmetrical decay. We aim to show how these three empirical phenomena can be\nreconciled in a general learning framework, the hierarchical Bayesian model\n(HBM). The model represents syntactic knowledge in a hierarchical structure of\nsyntactic statistics, where a lower level represents the verb-specific biases\nof syntactic decisions, and a higher level represents the abstract bias as an\naggregation of verb-specific biases. This knowledge is updated in response to\nexperience by Bayesian inference. In simulations, we show that the HBM captures\nthe above-mentioned properties of syntactic priming. The results indicate that\nsome properties of priming which are usually explained by a residual activation\naccount can also be explained by an implicit learning account. We also discuss\nthe model's implications for the lexical basis of syntactic priming.\n"", '  There have been apparently conflicting claims over the syntax-semantics\nrelationship in child acquisition. However, few of them have assessed the\nchild\'s path toward the acquisition of recursive relative clauses (RRCs). The\nauthors of the current paper did experiments to investigate 3- to 11-year-olds\'\nmost-structured elicited production of eight Mandarin RRCs in a 4 (syntactic\ntypes)*2 (semantic conditions) design. The four syntactic types were RRCs with\na subject-gapped RC embedded in an object-gapped RC (SORRCs), RRCs with an\nobject-gapped RC embedded in another object-gapped RC (OORRCs), RRCs with an\nobject-gapped RC embedded in a subject-gapped RC (OSRRCs), and RRCs with a\nsubject-gapped RC embedded in another subject-gapped RC (SSRRCs). Each\nsyntactic type was put in two conditions differing in internal semantics:\nirreversible internal semantics (IIS) and reversible internal semantics (RIS).\nFor example, ""the balloon that [the girl that _ eats the banana] holds _"" is\nSORRCs in the IIS condition; ""the monkey that [the dog that _ bites the pig]\nhits_"" is SORRCs in the RIS condition. For each target, the participants were\nprovided with a speech-visual stimulus constructing a condition of irreversible\nexternal semantics (IES). The results showed that SSRRCs, OSRRCs and SORRCs in\nthe IIS-IES condition were produced two years earlier than their counterparts\nin the RIS-IES condition. Thus, a 2-stage development path is proposed: the\nlanguage acquisition device starts with the interface between (irreversible)\nsyntax and IIS, and ends with the interface between syntax and IES, both\nabiding by the syntax-semantic interface principle.\n'] , [""  This paper aims to quantitatively evaluate the performance of ChatGPT, an\ninteractive large language model, on inter-sentential relations such as\ntemporal relations, causal relations, and discourse relations. Given ChatGPT's\npromising performance across various tasks, we proceed to carry out thorough\nevaluations on the whole test sets of 11 datasets, including temporal and\ncausal relations, PDTB2.0-based, and dialogue-based discourse relations. To\nensure the reliability of our findings, we employ three tailored prompt\ntemplates for each task, including the zero-shot prompt template, zero-shot\nprompt engineering (PE) template, and in-context learning (ICL) prompt\ntemplate, to establish the initial baseline scores for all popular\nsentence-pair relation classification tasks for the first time. Through our\nstudy, we discover that ChatGPT exhibits exceptional proficiency in detecting\nand reasoning about causal relations, albeit it may not possess the same level\nof expertise in identifying the temporal order between two events. While it is\ncapable of identifying the majority of discourse relations with existing\nexplicit discourse connectives, the implicit discourse relation remains a\nformidable challenge. Concurrently, ChatGPT demonstrates subpar performance in\nthe dialogue discourse parsing task that requires structural understanding in a\ndialogue before being aware of the discourse relation.\n"", ""  The development of different theories of discourse structure has led to the\nestablishment of discourse corpora based on these theories. However, the\nexistence of discourse corpora established on different theoretical bases\ncreates challenges when it comes to exploring them in a consistent and cohesive\nway. This study has as its primary focus the conversion of PDTB annotations\ninto dependency structures. It employs refined BERT-based discourse parsers to\ntest the validity of the dependency data derived from the PDTB-style corpora in\nEnglish, Chinese, and several other languages. By converting both PDTB and RST\nannotations for the same texts into dependencies, this study also applies\n``dependency distance'' metrics to examine the correlation between RST\ndependencies and PDTB dependencies in English. The results show that the PDTB\ndependency data is valid and that there is a strong correlation between the two\ntypes of dependency distance. This study presents a comprehensive approach for\nanalyzing and evaluating discourse corpora by employing discourse dependencies\nto achieve unified analysis. By applying dependency representations, we can\nextract data from PDTB, RST, and SDRT corpora in a coherent and unified manner.\nMoreover, the cross-linguistic validation establishes the framework's\ngeneralizability beyond English. The establishment of this comprehensive\ndependency framework overcomes limitations of existing discourse corpora,\nsupporting a diverse range of algorithms and facilitating further studies in\ncomputational discourse analysis and language sciences.\n"", '  In this article we present Enhanced Rhetorical Structure Theory (eRST), a new\ntheoretical framework for computational discourse analysis, based on an\nexpansion of Rhetorical Structure Theory (RST). The framework encompasses\ndiscourse relation graphs with tree-breaking, nonprojective and concurrent\nrelations, as well as implicit and explicit signals which give explainable\nrationales to our analyses. We survey shortcomings of RST and other existing\nframeworks, such as Segmented Discourse Representation Theory (SDRT), the Penn\nDiscourse Treebank (PDTB) and Discourse Dependencies, and address these using\nconstructs in the proposed theory. We provide annotation, search and\nvisualization tools for data, and present and evaluate a freely available\ncorpus of English annotated according to our framework, encompassing 12 spoken\nand written genres with over 200K tokens. Finally, we discuss automatic\nparsing, evaluation metrics and applications for data in our framework.\n'] , [""  Word frequency is a strong predictor in most lexical processing tasks. Thus,\nany model of word recognition needs to account for how word frequency effects\narise. The Discriminative Lexicon Model (DLM; Baayen et al., 2018a, 2019)\nmodels lexical processing with linear mappings between words' forms and their\nmeanings. So far, the mappings can either be obtained incrementally via\nerror-driven learning, a computationally expensive process able to capture\nfrequency effects, or in an efficient, but frequency-agnostic solution\nmodelling the theoretical endstate of learning (EL) where all words are learned\noptimally. In this study we show how an efficient, yet frequency-informed\nmapping between form and meaning can be obtained (Frequency-informed learning;\nFIL). We find that FIL well approximates an incremental solution while being\ncomputationally much cheaper. FIL shows a relatively low type- and high\ntoken-accuracy, demonstrating that the model is able to process most word\ntokens encountered by speakers in daily life correctly. We use FIL to model\nreaction times in the Dutch Lexicon Project (Keuleers et al., 2010) and find\nthat FIL predicts well the S-shaped relationship between frequency and the mean\nof reaction times but underestimates the variance of reaction times for low\nfrequency words. FIL is also better able to account for priming effects in an\nauditory lexical decision task in Mandarin Chinese (Lee, 2007), compared to EL.\nFinally, we used ordered data from CHILDES (Brown, 1973; Demuth et al., 2006)\nto compare mappings obtained with FIL and incremental learning. The mappings\nare highly correlated, but with FIL some nuances based on word ordering effects\nare lost. Our results show how frequency effects in a learning model can be\nsimulated efficiently, and raise questions about how to best account for\nlow-frequency words in cognitive models.\n"", ""  TheBench is a tool to study monadic structures in natural language. It is for\nwriting monadic grammars to explore analyses, compare diverse languages through\ntheir categories, and to train models of grammar from form-meaning pairs where\nsyntax is latent variable.\n  Monadic structures are binary combinations of elements that employ semantics\nof composition only. TheBench is essentially old-school categorial grammar to\nsyntacticize the idea, with the implication that although syntax is autonomous\n(recall \\emph{colorless green ideas sleep furiously}), the treasure is in the\nbaggage it carries at every step, viz. semantics, more narrowly,\npredicate-argument structures indicating choice of categorial reference and its\nconsequent placeholders for decision in such structures.\n  There is some new thought in old school. Unlike traditional categorial\ngrammars, application is turned into composition in monadic analysis. Moreover,\nevery correspondence requires specifying two command relations, one on\nsyntactic command and the other on semantic command. A monadic grammar of\nTheBench contains only synthetic elements (called `objects' in category theory\nof mathematics) that are shaped by this analytic invariant, viz. composition.\nBoth ingredients (command relations) of any analytic step must therefore be\nfunctions (`arrows' in category theory). TheBench is one implementation of the\nidea for iterative development of such functions along with grammar of\nsynthetic elements.\n"", '  Neural network language models (LMs) have been shown to successfully capture\ncomplex linguistic knowledge. However, their utility for understanding language\nacquisition is still debated. We contribute to this debate by presenting a case\nstudy where we use LMs as simulated learners to derive novel experimental\nhypotheses to be tested with humans. We apply this paradigm to study\ncross-dative generalization (CDG): productive generalization of novel verbs\nacross dative constructions (she pilked me the ball/she pilked the ball to me)\n-- acquisition of which is known to involve a large space of contextual\nfeatures -- using LMs trained on child-directed speech. We specifically ask:\n""what properties of the training exposure facilitate a novel verb\'s\ngeneralization to the (unmodeled) alternate construction?"" To answer this, we\nsystematically vary the exposure context in which a novel dative verb occurs in\nterms of the properties of the theme and recipient, and then analyze the LMs\'\nusage of the novel verb in the unmodeled dative construction. We find LMs to\nreplicate known patterns of children\'s CDG, as a precondition to exploring\nnovel hypotheses. Subsequent simulations reveal a nuanced role of the features\nof the novel verbs\' exposure context on the LMs\' CDG. We find CDG to be\nfacilitated when the first postverbal argument of the exposure context is\npronominal, definite, short, and conforms to the prototypical animacy\nexpectations of the exposure dative. These patterns are characteristic of\nharmonic alignment in datives, where the argument with features ranking higher\non the discourse prominence scale tends to precede the other. This gives rise\nto a novel hypothesis that CDG is facilitated insofar as the features of the\nexposure context -- in particular, its first postverbal argument -- are\nharmonically aligned. We conclude by proposing future experiments that can test\nthis hypothesis in children.\n'] , ['  When reading temporarily ambiguous garden-path sentences, misinterpretations\nsometimes linger past the point of disambiguation. This phenomenon has\ntraditionally been studied in psycholinguistic experiments using online\nmeasures such as reading times and offline measures such as comprehension\nquestions. Here, we investigate the processing of garden-path sentences and the\nfate of lingering misinterpretations using four large language models (LLMs):\nGPT-2, LLaMA-2, Flan-T5, and RoBERTa. The overall goal is to evaluate whether\nhumans and LLMs are aligned in their processing of garden-path sentences and in\nthe lingering misinterpretations past the point of disambiguation, especially\nwhen extra-syntactic information (e.g., a comma delimiting a clause boundary)\nis present to guide processing. We address this goal using 24 garden-path\nsentences that have optional transitive and reflexive verbs leading to\ntemporary ambiguities. For each sentence, there are a pair of comprehension\nquestions corresponding to the misinterpretation and the correct\ninterpretation. In three experiments, we (1) measure the dynamic semantic\ninterpretations of LLMs using the question-answering task; (2) track whether\nthese models shift their implicit parse tree at the point of disambiguation (or\nby the end of the sentence); and (3) visualize the model components that attend\nto disambiguating information when processing the question probes. These\nexperiments show promising alignment between humans and LLMs in the processing\nof garden-path sentences, especially when extra-syntactic information is\navailable to guide processing.\n', '  Contradictory results about the encoding of the semantic impact of negation\nin pretrained language models (PLMs). have been drawn recently (e.g. Kassner\nand Sch{\\""u}tze (2020); Gubelmann and Handschuh (2022)). In this paper we focus\nrather on the way PLMs encode negation and its formal impact, through the\nphenomenon of the Negative Polarity Item (NPI) licensing in English. More\nprecisely, we use probes to identify which contextual representations best\nencode 1) the presence of negation in a sentence, and 2) the polarity of a\nneighboring masked polarity item. We find that contextual representations of\ntokens inside the negation scope do allow for (i) a better prediction of the\npresence of not compared to those outside the scope and (ii) a better\nprediction of the right polarity of a masked polarity item licensed by not,\nalthough the magnitude of the difference varies from PLM to PLM. Importantly,\nin both cases the trend holds even when controlling for distance to not. This\ntends to indicate that the embeddings of these models do reflect the notion of\nnegation scope, and do encode the impact of negation on NPI licensing. Yet,\nfurther control experiments reveal that the presence of other lexical items is\nalso better captured when using the contextual representation of a token within\nthe same syntactic clause than outside from it, suggesting that PLMs simply\ncapture the more general notion of syntactic clause.\n', ""  Previous works of negation understanding mainly focus on negation cue\ndetection and scope resolution, without identifying negation subject which is\nalso significant to the downstream tasks. In this paper, we propose a new\nnegation triplet extraction (NTE) task which aims to extract negation subject\nalong with negation cue and scope. To achieve NTE, we devise a novel\nSyntax&Semantic-Enhanced Negation Extraction model, namely SSENE, which is\nbuilt based on a generative pretrained language model (PLM) {of Encoder-Decoder\narchitecture} with a multi-task learning framework. Specifically, the given\nsentence's syntactic dependency tree is incorporated into the PLM's encoder to\ndiscover the correlations between the negation subject, cue and scope.\nMoreover, the semantic consistency between the sentence and the extracted\ntriplet is ensured by an auxiliary task learning. Furthermore, we have\nconstructed a high-quality Chinese dataset NegComment based on the users'\nreviews from the real-world platform of Meituan, upon which our evaluations\nshow that SSENE achieves the best NTE performance compared to the baselines.\nOur ablation and case studies also demonstrate that incorporating the syntactic\ninformation helps the PLM's recognize the distant dependency between the\nsubject and cue, and the auxiliary task learning is helpful to extract the\nnegation triplets with more semantic consistency.\n""]",Natural Language Processing and Linguistics,Linguistic Grammars and Semantics
48,"Spelling Correction Techniques and Corpora , Grammatical Error Correction Evaluation , Text Simplification and Readability","['spellchecking', 'corpus', 'speller', 'spell', 'spelling', 'misspelled', 'pinyin', 'diacritics', 'bspell', 'phonetic'] , ['corrections', 'grammar', 'corpus', 'sentences', 'annotated', 'annotation', 'edits', 'correction', 'fluently', 'evaluation'] , ['simplifications', 'simplification', 'readability', 'sentences', 'annotation', 'nlp', 'corpus', 'texts', 'lexical', 'text']","['  Chinese Spelling Correction (CSC) commonly lacks large-scale high-quality\ncorpora, due to the labor-intensive labeling of spelling errors in real-life\nhuman writing or typing scenarios. Two data augmentation methods are widely\nadopted: (1) \\textit{Random Replacement} with the guidance of confusion sets\nand (2) \\textit{OCR/ASR-based Generation} that simulates character misusing.\nHowever, both methods inevitably introduce noisy data (e.g., false spelling\nerrors), potentially leading to over-correction. By carefully analyzing the two\ntypes of corpora, we find that though the latter achieves more robust\ngeneralization performance, the former yields better-calibrated CSC models. We\nthen provide a theoretical analysis of this empirical observation, based on\nwhich a corpus refining strategy is proposed. Specifically, OCR/ASR-based data\nsamples are fed into a well-calibrated CSC model trained on random\nreplacement-based corpora and then filtered based on prediction confidence. By\nlearning a simple BERT-based model on the refined OCR/ASR-based corpus, we set\nup impressive state-of-the-art performance on three widely-used benchmarks,\nwhile significantly alleviating over-correction (e.g., lowering false positive\npredictions).\n', ""  This research introduces a state-of-the-art Persian spelling correction\nsystem that seamlessly integrates deep learning techniques with phonetic\nanalysis, significantly enhancing the accuracy and efficiency of natural\nlanguage processing (NLP) for Persian. Utilizing a fine-tuned language\nrepresentation model, our methodology effectively combines deep contextual\nanalysis with phonetic insights, adeptly correcting both non-word and real-word\nspelling errors. This strategy proves particularly effective in tackling the\nunique complexities of Persian spelling, including its elaborate morphology and\nthe challenge of homophony. A thorough evaluation on a wide-ranging dataset\nconfirms our system's superior performance compared to existing methods, with\nimpressive F1-Scores of 0.890 for detecting real-word errors and 0.905 for\ncorrecting them. Additionally, the system demonstrates a strong capability in\nnon-word error correction, achieving an F1-Score of 0.891. These results\nillustrate the significant benefits of incorporating phonetic insights into\ndeep learning models for spelling correction. Our contributions not only\nadvance Persian language processing by providing a versatile solution for a\nvariety of NLP applications but also pave the way for future research in the\nfield, emphasizing the critical role of phonetic analysis in developing\neffective spelling correction system.\n"", '  Automatic spelling correction stands as a pivotal challenge within the ambit\nof natural language processing (NLP), demanding nuanced solutions. Traditional\nspelling correction techniques are typically only capable of detecting and\ncorrecting non-word errors, such as typos and misspellings. However,\ncontext-sensitive errors, also known as real-word errors, are more challenging\nto detect because they are valid words that are used incorrectly in a given\ncontext. The Persian language, characterized by its rich morphology and complex\nsyntax, presents formidable challenges to automatic spelling correction\nsystems. Furthermore, the limited availability of Persian language resources\nmakes it difficult to train effective spelling correction models. This paper\nintroduces a cutting-edge approach for precise and efficient real-word error\ncorrection in Persian text. Our methodology adopts a structured, multi-tiered\napproach, employing semantic analysis, feature selection, and advanced\nclassifiers to enhance error detection and correction efficacy. The innovative\narchitecture discovers and stores semantic similarities between words and\nphrases in Persian text. The classifiers accurately identify real-word errors,\nwhile the semantic ranking algorithm determines the most probable corrections\nfor real-word errors, taking into account specific spelling correction and\ncontext properties such as context, semantic similarity, and edit-distance\nmeasures. Evaluations have demonstrated that our proposed method surpasses\nprevious Persian real-word error correction models. Our method achieves an\nimpressive F-measure of 96.6% in the detection phase and an accuracy of 99.1%\nin the correction phase. These results clearly indicate that our approach is a\nhighly promising solution for automatic real-word error correction in Persian\ntext.\n'] , ['  The paper focuses on improving the interpretability of Grammatical Error\nCorrection (GEC) metrics, which receives little attention in previous studies.\nTo bridge the gap, we propose CLEME2.0, a reference-based evaluation strategy\nthat can describe four elementary dimensions of GEC systems, namely\nhit-correction, error-correction, under-correction, and over-correction. They\ncollectively contribute to revealing the critical characteristics and locating\ndrawbacks of GEC systems. Evaluating systems by Combining these dimensions\nleads to high human consistency over other reference-based and reference-less\nmetrics. Extensive experiments on 2 human judgement datasets and 6 reference\ndatasets demonstrate the effectiveness and robustness of our method. All the\ncodes will be released after the peer review.\n', '  This paper investigates the application of GPT-3.5 for Grammatical Error\nCorrection (GEC) in multiple languages in several settings: zero-shot GEC,\nfine-tuning for GEC, and using GPT-3.5 to re-rank correction hypotheses\ngenerated by other GEC models. In the zero-shot setting, we conduct automatic\nevaluations of the corrections proposed by GPT-3.5 using several methods:\nestimating grammaticality with language models (LMs), the Scribendi test, and\ncomparing the semantic embeddings of sentences. GPT-3.5 has a known tendency to\nover-correct erroneous sentences and propose alternative corrections. For\nseveral languages, such as Czech, German, Russian, Spanish, and Ukrainian,\nGPT-3.5 substantially alters the source sentences, including their semantics,\nwhich presents significant challenges for evaluation with reference-based\nmetrics. For English, GPT-3.5 demonstrates high recall, generates fluent\ncorrections, and generally preserves sentence semantics. However, human\nevaluation for both English and Russian reveals that, despite its strong\nerror-detection capabilities, GPT-3.5 struggles with several error types,\nincluding punctuation mistakes, tense errors, syntactic dependencies between\nwords, and lexical compatibility at the sentence level.\n', '  Thanks to recent advances in generative AI, we are able to prompt large\nlanguage models (LLMs) to produce texts which are fluent and grammatical. In\naddition, it has been shown that we can elicit attempts at grammatical error\ncorrection (GEC) from LLMs when prompted with ungrammatical input sentences. We\nevaluate how well LLMs can perform at GEC by measuring their performance on\nestablished benchmark datasets. We go beyond previous studies, which only\nexamined GPT* models on a selection of English GEC datasets, by evaluating\nseven open-source and three commercial LLMs on four established GEC benchmarks.\nWe investigate model performance and report results against individual error\ntypes. Our results indicate that LLMs do not always outperform supervised\nEnglish GEC models except in specific contexts -- namely commercial LLMs on\nbenchmarks annotated with fluency corrections as opposed to minimal edits. We\nfind that several open-source models outperform commercial ones on minimal edit\nbenchmarks, and that in some settings zero-shot prompting is just as\ncompetitive as few-shot prompting.\n'] , [""  Text simplification is a common task where the text is adapted to make it\neasier to understand. Similarly, text elaboration can make a passage more\nsophisticated, offering a method to control the complexity of reading\ncomprehension tests. However, text simplification and elaboration tasks are\nlimited to only relatively alter the readability of texts. It is useful to\ndirectly modify the readability of any text to an absolute target readability\nlevel to cater to a diverse audience. Ideally, the readability of\nreadability-controlled generated text should be independent of the source text.\nTherefore, we propose a novel readability-controlled text modification task.\nThe task requires the generation of 8 versions at various target readability\nlevels for each input text. We introduce novel readability-controlled text\nmodification metrics. The baselines for this task use ChatGPT and Llama-2, with\nan extension approach introducing a two-step process (generating paraphrases by\npassing through the language model twice). The zero-shot approaches are able to\npush the readability of the paraphrases in the desired direction but the final\nreadability remains correlated with the original text's readability. We also\nfind greater drops in semantic and lexical similarity between the source and\ntarget texts with greater shifts in the readability.\n"", '  Text simplification aims to make the text easier to understand by applying\nrewriting transformations. There has been very little research on Chinese text\nsimplification for a long time. The lack of generic evaluation data is an\nessential reason for this phenomenon. In this paper, we introduce MCTS, a\nmulti-reference Chinese text simplification dataset. We describe the annotation\nprocess of the dataset and provide a detailed analysis. Furthermore, we\nevaluate the performance of several unsupervised methods and advanced large\nlanguage models. We additionally provide Chinese text simplification parallel\ndata that can be used for training, acquired by utilizing machine translation\nand English text simplification. We hope to build a basic understanding of\nChinese text simplification through the foundational work and provide\nreferences for future research. All of the code and data are released at\nhttps://github.com/blcuicall/mcts/.\n', ""  Sentence simplification, which rewrites a sentence to be easier to read and\nunderstand, is a promising technique to help people with various reading\ndifficulties. With the rise of advanced large language models (LLMs),\nevaluating their performance in sentence simplification has become imperative.\nRecent studies have used both automatic metrics and human evaluations to assess\nthe simplification abilities of LLMs. However, the suitability of existing\nevaluation methodologies for LLMs remains in question. First, the suitability\nof current automatic metrics on LLMs' simplification evaluation is still\nuncertain. Second, current human evaluation approaches in sentence\nsimplification often fall into two extremes: they are either too superficial,\nfailing to offer a clear understanding of the models' performance, or overly\ndetailed, making the annotation process complex and prone to inconsistency,\nwhich in turn affects the evaluation's reliability. To address these problems,\nthis study provides in-depth insights into LLMs' performance while ensuring the\nreliability of the evaluation. We design an error-based human annotation\nframework to assess the GPT-4's simplification capabilities. Results show that\nGPT-4 generally generates fewer erroneous simplification outputs compared to\nthe current state-of-the-art. However, LLMs have their limitations, as seen in\nGPT-4's struggles with lexical paraphrasing. Furthermore, we conduct\nmeta-evaluations on widely used automatic metrics using our human annotations.\nWe find that while these metrics are effective for significant quality\ndifferences, they lack sufficient sensitivity to assess the overall\nhigh-quality simplification by GPT-4.\n""]",Natural Language Processing for Text Correction and Simplification,Grammatical Error Correction Evaluation
49,"Detecting AI-Generated Text in Human-AI Collaborative Writing , ""Gender in Machine Translation"" , Text Style Transfer in Natural Language Processing","['texts', 'text', 'autextification', 'ai', 'mixtext', 'paraphrasing', 'linguistic', 'gram2vec', 'writing', 'plagiarism'] , ['genderless', 'gender', 'genders', 'translations', 'gendered', 'multilingual', 'translating', 'masculine', 'translation', 'feminine'] , ['text', 'stylistic', 'nlp', 'styles', 'sentiment', 'sentences', 'style', 'content', 'politeness', 'tst']","['  In recent years, there have been significant advancements in the development\nof Large Language Models (LLMs). While their practical applications are now\nwidespread, their potential for misuse, such as generating fake news and\ncommitting plagiarism, has posed significant concerns. To address this issue,\ndetectors have been developed to evaluate whether a given text is\nhuman-generated or AI-generated. Among others, zero-shot detectors stand out as\neffective approaches that do not require additional training data and are often\nlikelihood-based. In chat-based applications, users commonly input prompts and\nutilize the AI-generated texts. However, zero-shot detectors typically analyze\nthese texts in isolation, neglecting the impact of the original prompts. It is\nconceivable that this approach may lead to a discrepancy in likelihood\nassessments between the text generation phase and the detection phase. So far,\nthere remains an unverified gap concerning how the presence or absence of\nprompts impacts detection accuracy for zero-shot detectors. In this paper, we\nintroduce an evaluative framework to empirically analyze the impact of prompts\non the detection accuracy of AI-generated text. We assess various zero-shot\ndetectors using both white-box detection, which leverages the prompt, and\nblack-box detection, which operates without prompt information. Our experiments\nreveal the significant influence of prompts on detection accuracy. Remarkably,\ncompared with black-box detection without prompts, the white-box methods using\nprompts demonstrate an increase in AUC of at least $0.1$ across all zero-shot\ndetectors tested. Code is available:\n\\url{https://github.com/kaito25atugich/Detector}.\n', ""  This study explores the challenge of sentence-level AI-generated text\ndetection within human-AI collaborative hybrid texts. Existing studies of\nAI-generated text detection for hybrid texts often rely on synthetic datasets.\nThese typically involve hybrid texts with a limited number of boundaries. We\ncontend that studies of detecting AI-generated content within hybrid texts\nshould cover different types of hybrid texts generated in realistic settings to\nbetter inform real-world applications. Therefore, our study utilizes the\nCoAuthor dataset, which includes diverse, realistic hybrid texts generated\nthrough the collaboration between human writers and an intelligent writing\nsystem in multi-turn interactions. We adopt a two-step, segmentation-based\npipeline: (i) detect segments within a given hybrid text where each segment\ncontains sentences of consistent authorship, and (ii) classify the authorship\nof each identified segment. Our empirical findings highlight (1) detecting\nAI-generated sentences in hybrid texts is overall a challenging task because\n(1.1) human writers' selecting and even editing AI-generated sentences based on\npersonal preferences adds difficulty in identifying the authorship of segments;\n(1.2) the frequent change of authorship between neighboring sentences within\nthe hybrid text creates difficulties for segment detectors in identifying\nauthorship-consistent segments; (1.3) the short length of text segments within\nhybrid texts provides limited stylistic cues for reliable authorship\ndetermination; (2) before embarking on the detection process, it is beneficial\nto assess the average length of segments within the hybrid text. This\nassessment aids in deciding whether (2.1) to employ a text segmentation-based\nstrategy for hybrid texts with longer segments, or (2.2) to adopt a direct\nsentence-by-sentence classification strategy for those with shorter segments.\n"", ""  With the increasing prevalence of text generated by large language models\n(LLMs), there is a growing concern about distinguishing between LLM-generated\nand human-written texts in order to prevent the misuse of LLMs, such as the\ndissemination of misleading information and academic dishonesty. Previous\nresearch has primarily focused on classifying text as either entirely\nhuman-written or LLM-generated, neglecting the detection of mixed texts that\ncontain both types of content. This paper explores LLMs' ability to identify\nboundaries in human-written and machine-generated mixed texts. We approach this\ntask by transforming it into a token classification problem and regard the\nlabel turning point as the boundary. Notably, our ensemble model of LLMs\nachieved first place in the 'Human-Machine Mixed Text Detection' sub-task of\nthe SemEval'24 Competition Task 8. Additionally, we investigate factors that\ninfluence the capability of LLMs in detecting boundaries within mixed texts,\nincluding the incorporation of extra layers on top of LLMs, combination of\nsegmentation loss, and the impact of pretraining. Our findings aim to provide\nvaluable insights for future research in this area.\n""] , ['  In machine translation, the problem of ambiguously gendered input has been\npointed out, where the gender of an entity is not available in the source\nsentence. To address this ambiguity issue, the task of controlled translation\nthat takes the gender of the ambiguous entity as additional input have been\nproposed. However, most existing works have only considered a simplified setup\nof one target gender for input. In this paper, we tackle controlled translation\nin a more realistic setting of inputs with multiple entities and propose\nGender-of-Entity (GoE) prompting method for LLMs. Our proposed method instructs\nthe model with fine-grained entity-level gender information to translate with\ncorrect gender inflections. By utilizing four evaluation benchmarks, we\ninvestigate the controlled translation capability of LLMs in multiple\ndimensions and find that LLMs reach state-of-the-art performance in controlled\ntranslation. Furthermore, we discover an emergence of gender interference\nphenomenon when controlling the gender of multiple entities. Finally, we\naddress the limitations of existing gender accuracy evaluation metrics and\npropose leveraging LLMs as an evaluator for gender inflection in machine\ntranslation.\n', ""  While machine translation (MT) systems have seen significant improvements, it\nis still common for translations to reflect societal biases, such as gender\nbias. Decoder-only Large Language Models (LLMs) have demonstrated potential in\nMT, albeit with performance slightly lagging behind traditional encoder-decoder\nNeural Machine Translation (NMT) systems. However, LLMs offer a unique\nadvantage: the ability to control the properties of the output through prompts.\nIn this study, we leverage this flexibility to explore LLaMa's capability to\nproduce gender-specific translations. Our results indicate that LLaMa can\ngenerate gender-specific translations with translation accuracy and gender bias\ncomparable to NLLB, a state-of-the-art multilingual NMT system. Furthermore,\nour experiments reveal that LLaMa's gender-specific translations rely on\ncoreference resolution to determine gender, showing higher gender variance in\ngender-ambiguous datasets but maintaining consistency in less ambiguous\ncontexts. This research investigates the potential and challenges of using LLMs\nfor gender-specific translations as an instance of the controllability of\noutputs offered by LLMs.\n"", '  Gender bias has been a focal point in the study of bias in machine\ntranslation and language models. Existing machine translation gender bias\nevaluations are primarily focused on male and female genders, limiting the\nscope of the evaluation. To assess gender bias accurately, these studies often\nrely on calculating the accuracy of gender pronouns or the masculine and\nfeminine attributes of grammatical gender via the stereotypes triggered by\noccupations or sentiment words ({\\em i.e.}, clear positive or negative\nattitude), which cannot extend to non-binary groups. This study presents a\nbenchmark AmbGIMT (Gender-Inclusive Machine Translation with Ambiguous attitude\nwords), which assesses gender bias beyond binary gender. Meanwhile, we propose\na novel process to evaluate gender bias based on the Emotional Attitude Score\n(EAS), which is used to quantify ambiguous attitude words. In evaluating three\nrecent and effective open-source LLMs and one powerful multilingual\ntranslation-specific model, our main observations are: (1) The translation\nperformance within non-binary gender contexts is markedly inferior in terms of\ntranslation quality and exhibits more negative attitudes than binary-gender\ncontexts. (2) The analysis experiments indicate that incorporating constraint\ncontext in prompts for gender identity terms can substantially reduce\ntranslation bias, while the bias remains evident despite the presence of the\nconstraints. The code is publicly available at\n\\url{https://github.com/pppa2019/ambGIMT}.\n'] , ['  Text style transfer (TST) is an important task in controllable text\ngeneration, which aims to control selected attributes of language use, such as\npoliteness, formality, or sentiment, without altering the style-independent\ncontent of the text. The field has received considerable research attention in\nrecent years and has already been covered in several reviews, but the focus has\nmostly been on the development of new algorithms and learning from different\ntypes of data (supervised, unsupervised, out-of-domain, etc.) and not so much\non the application side. However, TST-related technologies are gradually\nreaching a production- and deployment-ready level, and therefore, the inclusion\nof the application perspective in TST research becomes crucial. Similarly, the\noften overlooked ethical considerations of TST technology have become a\npressing issue. This paper presents a comprehensive review of TST applications\nthat have been researched over the years, using both traditional linguistic\napproaches and more recent deep learning methods. We discuss current\nchallenges, future research directions, and ethical implications of TST\napplications in text generation. By providing a holistic overview of the\nlandscape of TST applications, we hope to stimulate further research and\ncontribute to a better understanding of the potential as well as ethical\nconsiderations associated with TST.\n', '  Text style transfer (TST) aims to vary the style polarity of text while\npreserving the semantic content. Although recent advancements have demonstrated\nremarkable progress in short TST, it remains a relatively straightforward task\nwith limited practical applications. The more comprehensive long TST task\npresents two challenges: (1) existing methods encounter difficulties in\naccurately evaluating content attributes in multiple words, leading to content\ndegradation; (2) the conventional vanilla style classifier loss encounters\nobstacles in maintaining consistent style across multiple generated sentences.\n  In this paper, we propose a novel method SC2, where a multilayer Joint\nStyle-Content Weighed (JSCW) module and a Style Consistency loss are designed\nto address the two issues. The JSCW simultaneously assesses the amounts of\nstyle and content attributes within a token, aiming to acquire a lossless\ncontent representation and thereby enhancing content preservation. The multiple\nJSCW layers further progressively refine content representations. We design a\nstyle consistency loss to ensure the generated multiple sentences consistently\nreflect the target style polarity. Moreover, we incorporate a denoising\nnon-autoregressive decoder to accelerate the training. We conduct plentiful\nexperiments and the results show significant improvements of SC2 over\ncompetitive baselines. Our code: https://github.com/jiezhao6/SC2.\n', '  Text Style Transfer (TST) is a pivotal task in natural language generation to\nmanipulate text style attributes while preserving style-independent content.\nThe attributes targeted in TST can vary widely, including politeness,\nauthorship, mitigation of offensive language, modification of feelings, and\nadjustment of text formality. TST has become a widely researched topic with\nsubstantial advancements in recent years. This paper provides an introductory\noverview of TST, addressing its challenges, existing approaches, datasets,\nevaluation measures, subtasks, and applications. This fundamental overview\nimproves understanding of the background and fundamentals of text style\ntransfer.\n']",Natural Language Processing for Text Analysis and Generation,Detecting AI-Generated Text in Human-AI Collaborative Writing
50,"Multi-Agent Reinforcement Learning (MARL) Exploration , ""Cooperation in Multi-Agent Reinforcement Learning"" , Multi-Agent Reinforcement Learning , Multi-Objective Reinforcement Learning , Multi-Agent Reinforcement Learning , ""Optimal Decision-Making in Multi-Agent Games""","['reinforcement', 'agents', 'agent', 'learning', 'rewards', 'multiagent', 'reward', 'starcraft', 'cooperation', 'exploration'] , ['reinforcement', 'cooperation', 'rewards', 'cooperative', 'games', 'agents', 'incentives', 'emergence', 'agent', 'game'] , ['reinforcement', 'agents', 'games', 'agent', 'rewards', 'cooperation', 'reward', 'opponents', 'strategies', 'cooperative'] , ['reinforcement', 'objectives', 'optimal', 'critic', 'learning', 'reward', 'objective', 'rewards', 'optimization', 'control'] , ['agents', 'reinforcement', 'agent', 'cooperative', 'cooperation', 'learning', 'coordination', 'cooperate', 'collaborative', 'centralized'] , ['reinforcement', 'optimal', 'games', 'bandits', 'strategy', 'allocation', 'strategies', 'game', 'risk', 'optimize']","['  Recently, deep multi-agent reinforcement learning (MARL) has gained\nsignificant popularity due to its success in various cooperative multi-agent\ntasks. However, exploration still remains a challenging problem in MARL due to\nthe partial observability of the agents and the exploration space that can grow\nexponentially as the number of agents increases. Firstly, in order to address\nthe scalability issue of the exploration space, we define a formation-based\nequivalence relation on the exploration space and aim to reduce the search\nspace by exploring only meaningful states in different formations. Then, we\npropose a novel formation-aware exploration (FoX) framework that encourages\npartially observable agents to visit the states in diverse formations by\nguiding them to be well aware of their current formation solely based on their\nown observations. Numerical results show that the proposed FoX framework\nsignificantly outperforms the state-of-the-art MARL algorithms on Google\nResearch Football (GRF) and sparse Starcraft II multi-agent challenge (SMAC)\ntasks.\n', '  Multi-Agent Reinforcement Learning (MARL) algorithms are widely adopted in\ntackling complex tasks that require collaboration and competition among agents\nin dynamic Multi-Agent Systems (MAS). However, learning such tasks from scratch\nis arduous and may not always be feasible, particularly for MASs with a large\nnumber of interactive agents due to the extensive sample complexity. Therefore,\nreusing knowledge gained from past experiences or other agents could\nefficiently accelerate the learning process and upscale MARL algorithms. In\nthis study, we introduce a novel framework that enables transfer learning for\nMARL through unifying various state spaces into fixed-size inputs that allow\none unified deep-learning policy viable in different scenarios within a MAS. We\nevaluated our approach in a range of scenarios within the StarCraft Multi-Agent\nChallenge (SMAC) environment, and the findings show significant enhancements in\nmulti-agent learning performance using maneuvering skills learned from other\nscenarios compared to agents learning from scratch. Furthermore, we adopted\nCurriculum Transfer Learning (CTL), enabling our deep learning policy to\nprogressively acquire knowledge and skills across pre-designed homogeneous\nlearning scenarios organized by difficulty levels. This process promotes inter-\nand intra-agent knowledge transfer, leading to high multi-agent learning\nperformance in more complicated heterogeneous scenarios.\n', ""  The rise of multi-agent systems, especially the success of multi-agent\nreinforcement learning (MARL), is reshaping our future across diverse domains\nlike autonomous vehicle networks. However, MARL still faces significant\nchallenges, particularly in achieving zero-shot scalability, which allows\ntrained MARL models to be directly applied to unseen tasks with varying numbers\nof agents. In addition, real-world multi-agent systems usually contain agents\nwith different functions and strategies, while the existing scalable MARL\nmethods only have limited heterogeneity. To address this, we propose a novel\nMARL framework named Scalable and Heterogeneous Proximal Policy Optimization\n(SHPPO), integrating heterogeneity into parameter-shared PPO-based MARL\nnetworks. we first leverage a latent network to adaptively learn strategy\npatterns for each agent. Second, we introduce a heterogeneous layer for\ndecision-making, whose parameters are specifically generated by the learned\nlatent variables. Our approach is scalable as all the parameters are shared\nexcept for the heterogeneous layer, and gains both inter-individual and\ntemporal heterogeneity at the same time. We implement our approach based on the\nstate-of-the-art backbone PPO-based algorithm as SHPPO, while our approach is\nagnostic to the backbone and can be seamlessly plugged into any\nparameter-shared MARL method. SHPPO exhibits superior performance over the\nbaselines such as MAPPO and HAPPO in classic MARL environments like Starcraft\nMulti-Agent Challenge (SMAC) and Google Research Football (GRF), showcasing\nenhanced zero-shot scalability and offering insights into the learned latent\nrepresentation's impact on team performance by visualization.\n""] , [""  The significance of network structures in promoting group cooperation within\nsocial dilemmas has been widely recognized. Prior studies attribute this\nfacilitation to the assortment of strategies driven by spatial interactions.\nAlthough reinforcement learning has been employed to investigate the impact of\ndynamic interaction on the evolution of cooperation, there remains a lack of\nunderstanding about how agents develop neighbour selection behaviours and the\nformation of strategic assortment within an explicit interaction structure. To\naddress this, our study introduces a computational framework based on\nmulti-agent reinforcement learning in the spatial Prisoner's Dilemma game. This\nframework allows agents to select dilemma strategies and interacting neighbours\nbased on their long-term experiences, differing from existing research that\nrelies on preset social norms or external incentives. By modelling each agent\nusing two distinct Q-networks, we disentangle the coevolutionary dynamics\nbetween cooperation and interaction. The results indicate that long-term\nexperience enables agents to develop the ability to identify non-cooperative\nneighbours and exhibit a preference for interaction with cooperative ones. This\nemergent self-organizing behaviour leads to the clustering of agents with\nsimilar strategies, thereby increasing network reciprocity and enhancing group\ncooperation.\n"", ""  Understanding the emergence of cooperation in systems of computational agents\nis crucial for the development of effective cooperative AI. Interaction among\nindividuals in real-world settings are often sparse and occur within a broad\nspectrum of incentives, which often are only partially known. In this work, we\nexplore how cooperation can arise among reinforcement learning agents in\nscenarios characterised by infrequent encounters, and where agents face\nuncertainty about the alignment of their incentives with those of others. To do\nso, we train the agents under a wide spectrum of environments ranging from\nfully competitive, to fully cooperative, to mixed-motives. Under this type of\nuncertainty we study the effects of mechanisms, such as reputation and\nintrinsic rewards, that have been proposed in the literature to foster\ncooperation in mixed-motives environments. Our findings show that uncertainty\nsubstantially lowers the agents' ability to engage in cooperative behaviour,\nwhen that would be the best course of action. In this scenario, the use of\neffective reputation mechanisms and intrinsic rewards boosts the agents'\ncapability to act nearly-optimally in cooperative environments, while greatly\nenhancing cooperation in mixed-motive environments as well.\n"", ""  Cooperation is fundamental in Multi-Agent Systems (MAS) and Multi-Agent\nReinforcement Learning (MARL), often requiring agents to balance individual\ngains with collective rewards. In this regard, this paper aims to investigate\nstrategies to invoke cooperation in game-theoretic scenarios, namely the\nIterated Prisoner's Dilemma, where agents must optimize both individual and\ngroup outcomes. Existing cooperative strategies are analyzed for their\neffectiveness in promoting group-oriented behavior in repeated games.\nModifications are proposed where encouraging group rewards will also result in\na higher individual gain, addressing real-world dilemmas seen in distributed\nsystems. The study extends to scenarios with exponentially growing agent\npopulations ($N \\longrightarrow +\\infty$), where traditional computation and\nequilibrium determination are challenging. Leveraging mean-field game theory,\nequilibrium solutions and reward structures are established for infinitely\nlarge agent sets in repeated games. Finally, practical insights are offered\nthrough simulations using the Multi Agent-Posthumous Credit Assignment trainer,\nand the paper explores adapting simulation algorithms to create scenarios\nfavoring cooperation for group rewards. These practical implementations bridge\ntheoretical concepts with real-world applications.\n""] , ['  Recent advances in reinforcement learning (RL) heavily rely on a variety of\nwell-designed benchmarks, which provide environmental platforms and consistent\ncriteria to evaluate existing and novel algorithms. Specifically, in\nmulti-agent RL (MARL), a plethora of benchmarks based on cooperative games have\nspurred the development of algorithms that improve the scalability of\ncooperative multi-agent systems. However, for the competitive setting, a\nlightweight and open-sourced benchmark with challenging gaming dynamics and\nvisual inputs has not yet been established. In this work, we present\nFightLadder, a real-time fighting game platform, to empower competitive MARL\nresearch. Along with the platform, we provide implementations of\nstate-of-the-art MARL algorithms for competitive games, as well as a set of\nevaluation metrics to characterize the performance and exploitability of\nagents. We demonstrate the feasibility of this platform by training a general\nagent that consistently defeats 12 built-in characters in single-player mode,\nand expose the difficulty of training a non-exploitable agent without human\nknowledge and demonstrations in two-player mode. FightLadder provides\nmeticulously designed environments to address critical challenges in\ncompetitive MARL research, aiming to catalyze a new era of discovery and\nadvancement in the field. Videos and code at\nhttps://sites.google.com/view/fightladder/home.\n', '  Multi-agent reinforcement learning (MARL) methods, while effective in\nzero-sum or positive-sum games, often yield suboptimal outcomes in general-sum\ngames where cooperation is essential for achieving globally optimal outcomes.\nMatrix game social dilemmas, which abstract key aspects of general-sum\ninteractions, such as cooperation, risk, and trust, fail to model the temporal\nand spatial dynamics characteristic of real-world scenarios. In response, our\nstudy extends matrix game social dilemmas into more complex, higher-dimensional\nMARL environments. We adapt a gridworld implementation of the Stag Hunt dilemma\nto more closely match the decision-space of a one-shot matrix game while also\nintroducing variable environment complexity. Our findings indicate that as\ncomplexity increases, MARL agents trained in these environments converge to\nsuboptimal strategies, consistent with the risk-dominant Nash equilibria\nstrategies found in matrix games. Our work highlights the impact of environment\ncomplexity on achieving optimal outcomes in higher-dimensional game-theoretic\nMARL environments.\n', '  Measuring the contribution of individual agents is challenging in cooperative\nmulti-agent reinforcement learning (MARL). In cooperative MARL, team\nperformance is typically inferred from a single shared global reward. Arguably,\namong the best current approaches to effectively measure individual agent\ncontributions is to use Shapley values. However, calculating these values is\nexpensive as the computational complexity grows exponentially with respect to\nthe number of agents. In this paper, we adapt difference rewards into an\nefficient method for quantifying the contribution of individual agents,\nreferred to as Agent Importance, offering a linear computational complexity\nrelative to the number of agents. We show empirically that the computed values\nare strongly correlated with the true Shapley values, as well as the true\nunderlying individual agent rewards, used as the ground truth in environments\nwhere these are available. We demonstrate how Agent Importance can be used to\nhelp study MARL systems by diagnosing algorithmic failures discovered in prior\nMARL benchmarking work. Our analysis illustrates Agent Importance as a valuable\nexplainability component for future MARL benchmarks.\n'] , ['  Multi-objective reinforcement learning (MORL) is essential for addressing the\nintricacies of real-world RL problems, which often require trade-offs between\nmultiple utility functions. However, MORL is challenging due to unstable\nlearning dynamics with deep learning-based function approximators. The research\npath most taken has been to explore different value-based loss functions for\nMORL to overcome this issue. Our work empirically explores model-free policy\nlearning loss functions and the impact of different architectural choices. We\nintroduce two different approaches: Multi-objective Proximal Policy\nOptimization (MOPPO), which extends PPO to MORL, and Multi-objective Advantage\nActor Critic (MOA2C), which acts as a simple baseline in our ablations. Our\nproposed approach is straightforward to implement, requiring only small\nmodifications at the level of function approximator. We conduct comprehensive\nevaluations on the MORL Deep Sea Treasure, Minecart, and Reacher environments\nand show that MOPPO effectively captures the Pareto front. Our extensive\nablation studies and empirical analyses reveal the impact of different\narchitectural choices, underscoring the robustness and versatility of MOPPO\ncompared to popular MORL approaches like Pareto Conditioned Networks (PCN) and\nEnvelope Q-learning in terms of MORL metrics, including hypervolume and\nexpected utility.\n', '  Many real-world continuous control problems are in the dilemma of weighing\nthe pros and cons, multi-objective reinforcement learning (MORL) serves as a\ngeneric framework of learning control policies for different preferences over\nobjectives. However, the existing MORL methods either rely on multiple passes\nof explicit search for finding the Pareto front and therefore are not\nsample-efficient, or utilizes a shared policy network for coarse knowledge\nsharing among policies. To boost the sample efficiency of MORL, we propose\nQ-Pensieve, a policy improvement scheme that stores a collection of Q-snapshots\nto jointly determine the policy update direction and thereby enables data\nsharing at the policy level. We show that Q-Pensieve can be naturally\nintegrated with soft policy iteration with convergence guarantee. To\nsubstantiate this concept, we propose the technique of Q replay buffer, which\nstores the learned Q-networks from the past iterations, and arrive at a\npractical actor-critic implementation. Through extensive experiments and an\nablation study, we demonstrate that with much fewer samples, the proposed\nalgorithm can outperform the benchmark MORL methods on a variety of MORL\nbenchmark tasks.\n', '  For a control problem with multiple conflicting objectives, there exists a\nset of Pareto-optimal policies called the Pareto set instead of a single\noptimal policy. When a multi-objective control problem is continuous and\ncomplex, traditional multi-objective reinforcement learning (MORL) algorithms\nsearch for many Pareto-optimal deep policies to approximate the Pareto set,\nwhich is quite resource-consuming. In this paper, we propose a simple and\nresource-efficient MORL algorithm that learns a continuous representation of\nthe Pareto set in a high-dimensional policy parameter space using a single\nhypernet. The learned hypernet can directly generate various well-trained\npolicy networks for different user preferences. We compare our method with two\nstate-of-the-art MORL algorithms on seven multi-objective continuous robot\ncontrol problems. Experimental results show that our method achieves the best\noverall performance with the least training parameters. An interesting\nobservation is that the Pareto set is well approximated by a curved line or\nsurface in a high-dimensional parameter space. This observation will provide\ninsight for researchers to design new MORL algorithms.\n'] , ['  Multi-Agent Reinforcement Learning (MARL) comprises a broad area of research\nwithin the field of multi-agent systems. Several recent works have focused\nspecifically on the study of communication approaches in MARL. While multiple\ncommunication methods have been proposed, these might still be too complex and\nnot easily transferable to more practical contexts. One of the reasons for that\nis due to the use of the famous parameter sharing trick. In this paper, we\ninvestigate how independent learners in MARL that do not share parameters can\ncommunicate. We demonstrate that this setting might incur into some problems,\nto which we propose a new learning scheme as a solution. Our results show that,\ndespite the challenges, independent agents can still learn communication\nstrategies following our method. Additionally, we use this method to\ninvestigate how communication in MARL is affected by different network\ncapacities, both for sharing and not sharing parameters. We observe that\ncommunication may not always be needed and that the chosen agent network sizes\nneed to be considered when used together with communication in order to achieve\nefficient learning.\n', ""  Traditional multi-agent reinforcement learning algorithms are difficultly\napplied in a large-scale multi-agent environment. The introduction of mean\nfield theory has enhanced the scalability of multi-agent reinforcement learning\nin recent years. This paper considers partially observable multi-agent\nreinforcement learning (MARL), where each agent can only observe other agents\nwithin a fixed range. This partial observability affects the agent's ability to\nassess the quality of the actions of surrounding agents. This paper focuses on\ndeveloping a method to capture more effective information from local\nobservations in order to select more effective actions. Previous work in this\nfield employs probability distributions or weighted mean field to update the\naverage actions of neighborhood agents, but it does not fully consider the\nfeature information of surrounding neighbors and leads to a local optimum. In\nthis paper, we propose a novel multi-agent reinforcement learning algorithm,\nPartially Observable Mean Field Multi-Agent Reinforcement Learning based on\nGraph--Attention (GAMFQ) to remedy this flaw. GAMFQ uses a graph attention\nmodule and a mean field module to describe how an agent is influenced by the\nactions of other agents at each time step. This graph attention module consists\nof a graph attention encoder and a differentiable attention mechanism, and this\nmechanism outputs a dynamic graph to represent the effectiveness of\nneighborhood agents against central agents. The mean--field module approximates\nthe effect of a neighborhood agent on a central agent as the average effect of\neffective neighborhood agents. We evaluate GAMFQ on three challenging tasks in\nthe MAgents framework. Experiments show that GAMFQ outperforms baselines\nincluding the state-of-the-art partially observable mean-field reinforcement\nlearning algorithms.\n"", '  Achieving distributed reinforcement learning (RL) for large-scale cooperative\nmulti-agent systems (MASs) is challenging because: (i) each agent has access to\nonly limited information; (ii) issues on convergence or computational\ncomplexity emerge due to the curse of dimensionality. In this paper, we propose\na general computationally efficient distributed framework for cooperative\nmulti-agent reinforcement learning (MARL) by utilizing the structures of graphs\ninvolved in this problem. We introduce three coupling graphs describing three\ntypes of inter-agent couplings in MARL, namely, the state graph, the\nobservation graph and the reward graph. By further considering a communication\ngraph, we propose two distributed RL approaches based on local value-functions\nderived from the coupling graphs. The first approach is able to reduce sample\ncomplexity significantly under specific conditions on the aforementioned four\ngraphs. The second approach provides an approximate solution and can be\nefficient even for problems with dense coupling graphs. Here there is a\ntrade-off between minimizing the approximation error and reducing the\ncomputational complexity. Simulations show that our RL algorithms have a\nsignificantly improved scalability to large-scale MASs compared with\ncentralized and consensus-based distributed RL algorithms.\n'] , ['  Addressing the question of how to achieve optimal decision-making under risk\nand uncertainty is crucial for enhancing the capabilities of artificial agents\nthat collaborate with or support humans. In this work, we address this question\nin the context of Public Goods Games. We study learning in a novel\nmulti-objective version of the Public Goods Game where agents have different\nrisk preferences, by means of multi-objective reinforcement learning. We\nintroduce a parametric non-linear utility function to model risk preferences at\nthe level of individual agents, over the collective and individual reward\ncomponents of the game. We study the interplay between such preference\nmodelling and environmental uncertainty on the incentive alignment level in the\ngame. We demonstrate how different combinations of individual preferences and\nenvironmental uncertainties sustain the emergence of cooperative patterns in\nnon-cooperative environments (i.e., where competitive strategies are dominant),\nwhile others sustain competitive patterns in cooperative environments (i.e.,\nwhere cooperative strategies are dominant).\n', '  This paper proposes a new framework of Markov $\\alpha$-potential games to\nstudy Markov games. In this new framework, Markov games are shown to be Markov\n$\\alpha$-potential games, and the existence of an associated $\\alpha$-potential\nfunction is established. Any optimizer of an $\\alpha$-potential function is\nshown to be an $\\alpha$-stationary NE. Two important classes of practically\nsignificant Markov games, Markov congestion games and the perturbed Markov team\ngames, are studied via this framework of Markov $\\alpha$-potential games, with\nexplicit characterization of an upper bound for $\\alpha$ and its relation to\ngame parameters. Additionally, a semi-infinite linear programming based\nformulation is presented to obtain an upper bound for $\\alpha$ for any Markov\ngame. Furthermore, two equilibrium approximation algorithms, namely the\nprojected gradient-ascent algorithm and the sequential maximum improvement\nalgorithm, are presented along with their Nash regret analysis, and\ncorroborated by numerical experiments.\n', '  We study risk-sensitive multi-agent reinforcement learning under general-sum\nMarkov games, where agents optimize the entropic risk measure of rewards with\npossibly diverse risk preferences. We show that using the regret naively\nadapted from existing literature as a performance metric could induce policies\nwith equilibrium bias that favor the most risk-sensitive agents and overlook\nthe other agents. To address such deficiency of the naive regret, we propose a\nnovel notion of regret, which we call risk-balanced regret, and show through a\nlower bound that it overcomes the issue of equilibrium bias. Furthermore, we\ndevelop a self-play algorithm for learning Nash, correlated, and coarse\ncorrelated equilibria in risk-sensitive Markov games. We prove that the\nproposed algorithm attains near-optimal regret guarantees with respect to the\nrisk-balanced regret.\n']",Multi-Agent Decision Making and Reinforcement Learning,Multi-Agent Reinforcement Learning
51,"""Optimal Bandit Algorithms for Adaptive Regret Minimization"" , ""Bandit Learning and Regret Optimization in Games"" , ""Online Learning and Control with Regret Guarantees""","['bandit', 'bandits', 'optimal', 'regret', 'optimality', 'reward', 'rewards', 'exploration', 'exploitation', 'guarantees'] , ['bandits', 'bandit', 'optimal', 'regret', 'optimization', 'reward', 'games', 'guarantees', 'exploration', 'rewards'] , ['reinforcement', 'optimal', 'regret', 'adaptive', 'learning', 'optimization', 'guarantees', 'exploration', 'control', 'reward']","['  Fast changing states or volatile environments pose a significant challenge to\nonline optimization, which needs to perform rapid adaptation under limited\nobservation. In this paper, we give query and regret optimal bandit algorithms\nunder the strict notion of strongly adaptive regret, which measures the maximum\nregret over any contiguous interval $I$. Due to its worst-case nature, there is\nan almost-linear $\\Omega(|I|^{1-\\epsilon})$ regret lower bound, when only one\nquery per round is allowed [Daniely el al, ICML 2015]. Surprisingly, with just\ntwo queries per round, we give Strongly Adaptive Bandit Learner (StABL) that\nachieves $\\tilde{O}(\\sqrt{n|I|})$ adaptive regret for multi-armed bandits with\n$n$ arms. The bound is tight and cannot be improved in general. Our algorithm\nleverages a multiplicative update scheme of varying stepsizes and a carefully\nchosen observation distribution to control the variance. Furthermore, we extend\nour results and provide optimal algorithms in the bandit convex optimization\nsetting. Finally, we empirically demonstrate the superior performance of our\nalgorithms under volatile environments and for downstream tasks, such as\nalgorithm selection for hyperparameter optimization.\n', '  We present substantial evidence demonstrating the benefits of integrating\nLarge Language Models (LLMs) with a Contextual Multi-Armed Bandit framework.\nContextual bandits have been widely used in recommendation systems to generate\npersonalized suggestions based on user-specific contexts. We show that LLMs,\npre-trained on extensive corpora rich in human knowledge and preferences, can\nsimulate human behaviours well enough to jump-start contextual multi-armed\nbandits to reduce online learning regret. We propose an initialization\nalgorithm for contextual bandits by prompting LLMs to produce a pre-training\ndataset of approximate human preferences for the bandit. This significantly\nreduces online learning regret and data-gathering costs for training such\nmodels. Our approach is validated empirically through two sets of experiments\nwith different bandit setups: one which utilizes LLMs to serve as an oracle and\na real-world experiment utilizing data from a conjoint survey experiment.\n', ""  Contextual dueling bandit is used to model the bandit problems, where a\nlearner's goal is to find the best arm for a given context using observed noisy\npreference feedback over the selected arms for the past contexts. However,\nexisting algorithms assume the reward function is linear, which can be complex\nand non-linear in many real-life applications like online recommendations or\nranking web search results. To overcome this challenge, we use a neural network\nto estimate the reward function using preference feedback for the previously\nselected arms. We propose upper confidence bound- and Thompson sampling-based\nalgorithms with sub-linear regret guarantees that efficiently select arms in\neach round. We then extend our theoretical results to contextual bandit\nproblems with binary feedback, which is in itself a non-trivial contribution.\nExperimental results on the problem instances derived from synthetic datasets\ncorroborate our theoretical results.\n""] , ['  In this work, we study potential games and Markov potential games under\nstochastic cost and bandit feedback. We propose a variant of the Frank-Wolfe\nalgorithm with sufficient exploration and recursive gradient estimation, which\nprovably converges to the Nash equilibrium while attaining sublinear regret for\neach individual player. Our algorithm simultaneously achieves a Nash regret and\na regret bound of $O(T^{4/5})$ for potential games, which matches the best\navailable result, without using additional projection steps. Through carefully\nbalancing the reuse of past samples and exploration of new samples, we then\nextend the results to Markov potential games and improve the best available\nNash regret from $O(T^{5/6})$ to $O(T^{4/5})$. Moreover, our algorithm requires\nno knowledge of the game, such as the distribution mismatch coefficient, which\nprovides more flexibility in its practical implementation. Experimental results\ncorroborate our theoretical findings and underscore the practical effectiveness\nof our method.\n', ""  Large language models (LLMs) have been increasingly employed for\n(interactive) decision-making, via the development of LLM-based autonomous\nagents. Despite their emerging successes, the performance of LLM agents in\ndecision-making has not been fully investigated through quantitative metrics,\nespecially in the multi-agent setting when they interact with each other, a\ntypical scenario in real-world LLM-agent applications. To better understand the\nlimits of LLM agents in these interactive environments, we propose to study\ntheir interactions in benchmark decision-making settings in online learning and\ngame theory, through the performance metric of \\emph{regret}. We first\nempirically study the {no-regret} behaviors of LLMs in canonical\n(non-stationary) online learning problems, as well as the emergence of\nequilibria when LLM agents interact through playing repeated games. We then\nprovide some theoretical insights into the no-regret behaviors of LLM agents,\nunder certain assumptions on the supervised pre-training and the rationality\nmodel of human decision-makers who generate the data. Notably, we also identify\n(simple) cases where advanced LLMs such as GPT-4 fail to be no-regret. To\npromote the no-regret behaviors, we propose a novel \\emph{unsupervised}\ntraining loss of \\emph{regret-loss}, which, in contrast to the supervised\npre-training loss, does not require the labels of (optimal) actions. We then\nestablish the statistical guarantee of generalization bound for regret-loss\nminimization, followed by the optimization guarantee that minimizing such a\nloss may automatically lead to known no-regret learning algorithms. Our further\nexperiments demonstrate the effectiveness of our regret-loss, especially in\naddressing the above ``regrettable'' cases.\n"", ""  We consider online no-regret learning in unknown games with bandit feedback,\nwhere each player can only observe its reward at each time -- determined by all\nplayers' current joint action -- rather than its gradient. We focus on the\nclass of \\textit{smooth and strongly monotone} games and study optimal\nno-regret learning therein. Leveraging self-concordant barrier functions, we\nfirst construct a new bandit learning algorithm and show that it achieves the\nsingle-agent optimal regret of $\\tilde{\\Theta}(n\\sqrt{T})$ under smooth and\nstrongly concave reward functions ($n \\geq 1$ is the problem dimension). We\nthen show that if each player applies this no-regret learning algorithm in\nstrongly monotone games, the joint action converges in the \\textit{last\niterate} to the unique Nash equilibrium at a rate of\n$\\tilde{\\Theta}(nT^{-1/2})$. Prior to our work, the best-known convergence rate\nin the same class of games is $\\tilde{O}(n^{2/3}T^{-1/3})$ (achieved by a\ndifferent algorithm), thus leaving open the problem of optimal no-regret\nlearning algorithms (since the known lower bound is $\\Omega(nT^{-1/2})$). Our\nresults thus settle this open problem and contribute to the broad landscape of\nbandit game-theoretical learning by identifying the first doubly optimal bandit\nlearning algorithm, in that it achieves (up to log factors) both optimal regret\nin the single-agent learning and optimal last-iterate convergence rate in the\nmulti-agent learning. We also present preliminary numerical results on several\napplication problems to demonstrate the efficacy of our algorithm in terms of\niteration count.\n""] , ['  Recent advancement in online optimization and control has provided novel\ntools to study online linear quadratic regulator (LQR) problems, where cost\nmatrices are varying adversarially over time. However, the controller\nparameterization of existing works may not satisfy practical conditions like\nsparsity due to physical connections. In this work, we study online linear\nquadratic Gaussian problems with a given linear constraint imposed on the\ncontroller. Inspired by the recent work of [1] which proposed, for a linearly\nconstrained policy optimization of an offline LQR, a second order method\nequipped with a Riemannian metric that emerges naturally in the context of\noptimal control problems, we propose online optimistic Newton on manifold\n(OONM) which provides an online controller based on the prediction on the first\nand second order information of the function sequence. To quantify the proposed\nalgorithm, we leverage the notion of regret defined as the sub-optimality of\nits cumulative cost to that of a (locally) minimizing controller sequence and\nprovide the regret bound in terms of the path-length of the minimizer sequence.\nSimulation results are also provided to verify the property of OONM.\n', '  In this paper, we propose Posterior Sampling Reinforcement Learning for\nZero-sum Stochastic Games (PSRL-ZSG), the first online learning algorithm that\nachieves Bayesian regret bound of $O(HS\\sqrt{AT})$ in the infinite-horizon\nzero-sum stochastic games with average-reward criterion. Here $H$ is an upper\nbound on the span of the bias function, $S$ is the number of states, $A$ is the\nnumber of joint actions and $T$ is the horizon. We consider the online setting\nwhere the opponent can not be controlled and can take any arbitrary\ntime-adaptive history-dependent strategy. Our regret bound improves on the best\nexisting regret bound of $O(\\sqrt[3]{DS^2AT^2})$ by Wei et al. (2017) under the\nsame assumption and matches the theoretical lower bound in $T$.\n', '  We consider the online control problem with an unknown linear dynamical\nsystem in the presence of adversarial perturbations and adversarial convex loss\nfunctions. Although the problem is widely studied in model-based control, it\nremains unclear whether data-driven approaches, which bypass the system\nidentification step, can solve the problem. In this work, we present a novel\ndata-driven online adaptive control algorithm to address this online control\nproblem. Our algorithm leverages the behavioral systems theory to learn a\nnon-parametric system representation and then adopts a perturbation-based\ncontroller updated by online gradient descent. We prove that our algorithm\nguarantees an $\\tmO(T^{2/3})$ regret bound with high probability, which matches\nthe best-known regret bound for this problem. Furthermore, we extend our\nalgorithm and performance guarantee to the cases with output feedback.\n']",Optimization and Learning in Bandit and Game-Theoretic Settings,"""Bandit Learning and Regret Optimization in Games"""
52,"""Optimality and Complexity in Markov Decision Processes"" , Planning under Uncertainty in POMDPs","['optimality', 'optimal', 'reinforcement', 'mdps', 'reward', 'mdp', 'complexity', 'markov', 'minimax', 'discount'] , ['planning', 'planner', 'pomdps', 'pomdp', 'reinforcement', 'rpomdps', 'markov', 'stochastic', 'optimal', 'pomcp']","['  We study Markov potential games under the infinite horizon average reward\ncriterion. Most previous studies have been for discounted rewards. We prove\nthat both algorithms based on independent policy gradient and independent\nnatural policy gradient converge globally to a Nash equilibrium for the average\nreward criterion. To set the stage for gradient-based methods, we first\nestablish that the average reward is a smooth function of policies and provide\nsensitivity bounds for the differential value functions, under certain\nconditions on ergodicity and the second largest eigenvalue of the underlying\nMarkov decision process (MDP). We prove that three algorithms, policy gradient,\nproximal-Q, and natural policy gradient (NPG), converge to an $\\epsilon$-Nash\nequilibrium with time complexity $O(\\frac{1}{\\epsilon^2})$, given a\ngradient/differential Q function oracle. When policy gradients have to be\nestimated, we propose an algorithm with\n$\\tilde{O}(\\frac{1}{\\min_{s,a}\\pi(a|s)\\delta})$ sample complexity to achieve\n$\\delta$ approximation error w.r.t~the $\\ell_2$ norm. Equipped with the\nestimator, we derive the first sample complexity analysis for a policy gradient\nascent algorithm, featuring a sample complexity of $\\tilde{O}(1/\\epsilon^5)$.\nSimulation studies are presented.\n', '  We study the sample complexity of learning an $\\varepsilon$-optimal policy in\nan average-reward Markov decision process (MDP) under a generative model. We\nestablish the complexity bound $\\widetilde{O}\\left(SA\\frac{H}{\\varepsilon^2}\n\\right)$, where $H$ is the span of the bias function of the optimal policy and\n$SA$ is the cardinality of the state-action space. Our result is the first that\nis minimax optimal (up to log factors) in all parameters $S,A,H$ and\n$\\varepsilon$, improving on existing work that either assumes uniformly bounded\nmixing times for all policies or has suboptimal dependence on the parameters.\n  Our result is based on reducing the average-reward MDP to a discounted MDP.\nTo establish the optimality of this reduction, we develop improved bounds for\n$\\gamma$-discounted MDPs, showing that\n$\\widetilde{O}\\left(SA\\frac{H}{(1-\\gamma)^2\\varepsilon^2} \\right)$ samples\nsuffice to learn a $\\varepsilon$-optimal policy in weakly communicating MDPs\nunder the regime that $\\gamma \\geq 1 - \\frac{1}{H}$, circumventing the\nwell-known lower bound of\n$\\widetilde{\\Omega}\\left(SA\\frac{1}{(1-\\gamma)^3\\varepsilon^2} \\right)$ for\ngeneral $\\gamma$-discounted MDPs. Our analysis develops upper bounds on certain\ninstance-dependent variance parameters in terms of the span parameter. These\nbounds are tighter than those based on the mixing time or diameter of the MDP\nand may be of broader use.\n', '  We study the sample complexity of learning an $\\varepsilon$-optimal policy in\nan average-reward Markov decision process (MDP) under a generative model. For\nweakly communicating MDPs, we establish the complexity bound\n$\\widetilde{O}(SA\\frac{H}{\\varepsilon^2} )$, where $H$ is the span of the bias\nfunction of the optimal policy and $SA$ is the cardinality of the state-action\nspace. Our result is the first that is minimax optimal (up to log factors) in\nall parameters $S,A,H$, and $\\varepsilon$, improving on existing work that\neither assumes uniformly bounded mixing times for all policies or has\nsuboptimal dependence on the parameters. We also initiate the study of sample\ncomplexity in general (multichain) average-reward MDPs. We argue a new\ntransient time parameter $B$ is necessary, establish an\n$\\widetilde{O}(SA\\frac{B + H}{\\varepsilon^2})$ complexity bound, and prove a\nmatching (up to log factors) minimax lower bound. Both results are based on\nreducing the average-reward MDP to a discounted MDP, which requires new ideas\nin the general setting. To optimally analyze this reduction, we develop\nimproved bounds for $\\gamma$-discounted MDPs, showing that\n$\\widetilde{O}(SA\\frac{H}{(1-\\gamma)^2\\varepsilon^2} )$ and\n$\\widetilde{O}(SA\\frac{B + H}{(1-\\gamma)^2\\varepsilon^2} )$ samples suffice to\nlearn $\\varepsilon$-optimal policies in weakly communicating and in general\nMDPs, respectively. Both these results circumvent the well-known minimax lower\nbound of $\\widetilde{\\Omega}(SA\\frac{1}{(1-\\gamma)^3\\varepsilon^2} )$ for\n$\\gamma$-discounted MDPs, and establish a quadratic rather than cubic horizon\ndependence for a fixed MDP instance.\n'] , [""  Many sequential decision problems involve optimizing one objective function\nwhile imposing constraints on other objectives. Constrained Partially\nObservable Markov Decision Processes (C-POMDP) model this case with transition\nuncertainty and partial observability. In this work, we first show that\nC-POMDPs violate the optimal substructure property over successive decision\nsteps and thus may exhibit behaviors that are undesirable for some (e.g.,\nsafety critical) applications. Additionally, online re-planning in C-POMDPs is\noften ineffective due to the inconsistency resulting from this violation. To\naddress these drawbacks, we introduce the Recursively-Constrained POMDP\n(RC-POMDP), which imposes additional history-dependent cost constraints on the\nC-POMDP. We show that, unlike C-POMDPs, RC-POMDPs always have deterministic\noptimal policies and that optimal policies obey Bellman's principle of\noptimality. We also present a point-based dynamic programming algorithm for\nRC-POMDPs. Evaluations on benchmark problems demonstrate the efficacy of our\nalgorithm and show that policies for RC-POMDPs produce more desirable behaviors\nthan policies for C-POMDPs.\n"", '  To plan safely in uncertain environments, agents must balance utility with\nsafety constraints. Safe planning problems can be modeled as a\nchance-constrained partially observable Markov decision process (CC-POMDP) and\nsolutions often use expensive rollouts or heuristics to estimate the optimal\nvalue and action-selection policy. This work introduces the ConstrainedZero\npolicy iteration algorithm that solves CC-POMDPs in belief space by learning\nneural network approximations of the optimal value and policy with an\nadditional network head that estimates the failure probability given a belief.\nThis failure probability guides safe action selection during online Monte Carlo\ntree search (MCTS). To avoid overemphasizing search based on the failure\nestimates, we introduce $\\Delta$-MCTS, which uses adaptive conformal inference\nto update the failure threshold during planning. The approach is tested on a\nsafety-critical POMDP benchmark, an aircraft collision avoidance system, and\nthe sustainability problem of safe CO$_2$ storage. Results show that by\nseparating safety constraints from the objective we can achieve a target level\nof safety without optimizing the balance between rewards and costs.\n', '  Real-world planning problems, including autonomous driving and sustainable\nenergy applications like carbon storage and resource exploration, have recently\nbeen modeled as partially observable Markov decision processes (POMDPs) and\nsolved using approximate methods. To solve high-dimensional POMDPs in practice,\nstate-of-the-art methods use online planning with problem-specific heuristics\nto reduce planning horizons and make the problems tractable. Algorithms that\nlearn approximations to replace heuristics have recently found success in\nlarge-scale fully observable domains. The key insight is the combination of\nonline Monte Carlo tree search with offline neural network approximations of\nthe optimal policy and value function. In this work, we bring this insight to\npartially observable domains and propose BetaZero, a belief-state planning\nalgorithm for high-dimensional POMDPs. BetaZero learns offline approximations\nthat replace heuristics to enable online decision making in long-horizon\nproblems. We address several challenges inherent in large-scale partially\nobservable domains; namely challenges of transitioning in stochastic\nenvironments, prioritizing action branching with a limited search budget, and\nrepresenting beliefs as input to the network. To formalize the use of all\nlimited search information, we train against a novel $Q$-weighted visit counts\npolicy. We test BetaZero on various well-established POMDP benchmarks found in\nthe literature and a real-world problem of critical mineral exploration.\nExperiments show that BetaZero outperforms state-of-the-art POMDP solvers on a\nvariety of tasks.\n']",Decision Making under Uncertainty in Markov Decision Processes,"""Optimality and Complexity in Markov Decision Processes"""
53,"Offline Reinforcement Learning , Reinforcement Learning in MDPs , Offline Reinforcement Learning in MDPs , Offline Reinforcement Learning with Preferences","['offline', 'reinforcement', 'learning', 'learned', 'reward', 'rewards', 'critic', 'exploration', 'imitation', 'learn'] , ['reinforcement', 'rewards', 'learning', 'reward', 'optimal', 'mdps', 'optimization', 'approximation', 'policies', 'gradient'] , ['optimality', 'reinforcement', 'optimal', 'bandit', 'mdps', 'learning', 'mdp', 'policies', 'exploration', 'policy'] , ['reinforcement', 'reward', 'rewards', 'learning', 'rl', 'offline', 'learned', 'learner', 'optimal', 'exploration']","['  Deep generative models (DGMs) have demonstrated great success across various\ndomains, particularly in generating texts, images, and videos using models\ntrained from offline data. Similarly, data-driven decision-making and robotic\ncontrol also necessitate learning a generator function from the offline data to\nserve as the strategy or policy. In this case, applying deep generative models\nin offline policy learning exhibits great potential, and numerous studies have\nexplored in this direction. However, this field still lacks a comprehensive\nreview and so developments of different branches are relatively independent. In\nthis paper, we provide the first systematic review on the applications of deep\ngenerative models for offline policy learning. In particular, we cover five\nmainstream deep generative models, including Variational Auto-Encoders,\nGenerative Adversarial Networks, Normalizing Flows, Transformers, and Diffusion\nModels, and their applications in both offline reinforcement learning (offline\nRL) and imitation learning (IL). Offline RL and IL are two main branches of\noffline policy learning and are widely-adopted techniques for sequential\ndecision-making. Notably, for each type of DGM-based offline policy learning,\nwe distill its fundamental scheme, categorize related works based on the usage\nof the DGM, and sort out the development process of algorithms in that field.\nSubsequent to the main content, we provide in-depth discussions on deep\ngenerative models and offline policy learning as a summary, based on which we\npresent our perspectives on future research directions. This work offers a\nhands-on reference for the research progress in deep generative models for\noffline policy learning, and aims to inspire improved DGM-based offline RL or\nIL algorithms. For convenience, we maintain a paper list on\nhttps://github.com/LucasCJYSDL/DGMs-for-Offline-Policy-Learning.\n', '  Despite recent progress in offline learning, these methods are still trained\nand tested on the same environment. In this paper, we compare the\ngeneralization abilities of widely used online and offline learning methods\nsuch as online reinforcement learning (RL), offline RL, sequence modeling, and\nbehavioral cloning. Our experiments show that offline learning algorithms\nperform worse on new environments than online learning ones. We also introduce\nthe first benchmark for evaluating generalization in offline learning,\ncollecting datasets of varying sizes and skill-levels from Procgen (2D video\ngames) and WebShop (e-commerce websites). The datasets contain trajectories for\na limited number of game levels or natural language instructions and at test\ntime, the agent has to generalize to new levels or instructions. Our\nexperiments reveal that existing offline learning algorithms struggle to match\nthe performance of online RL on both train and test environments. Behavioral\ncloning is a strong baseline, outperforming state-of-the-art offline RL and\nsequence modeling approaches when trained on data from multiple environments\nand tested on new ones. Finally, we find that increasing the diversity of the\ndata, rather than its size, improves performance on new environments for all\noffline learning algorithms. Our study demonstrates the limited generalization\nof current offline learning algorithms highlighting the need for more research\nin this area.\n', '  While imitation learning requires access to high-quality data, offline\nreinforcement learning (RL) should, in principle, perform similarly or better\nwith substantially lower data quality by using a value function. However,\ncurrent results indicate that offline RL often performs worse than imitation\nlearning, and it is often unclear what holds back the performance of offline\nRL. Motivated by this observation, we aim to understand the bottlenecks in\ncurrent offline RL algorithms. While poor performance of offline RL is\ntypically attributed to an imperfect value function, we ask: is the main\nbottleneck of offline RL indeed in learning the value function, or something\nelse? To answer this question, we perform a systematic empirical study of (1)\nvalue learning, (2) policy extraction, and (3) policy generalization in offline\nRL problems, analyzing how these components affect performance. We make two\nsurprising observations. First, we find that the choice of a policy extraction\nalgorithm significantly affects the performance and scalability of offline RL,\noften more so than the value learning objective. For instance, we show that\ncommon value-weighted behavioral cloning objectives (e.g., AWR) do not fully\nleverage the learned value function, and switching to behavior-constrained\npolicy gradient objectives (e.g., DDPG+BC) often leads to substantial\nimprovements in performance and scalability. Second, we find that a big barrier\nto improving offline RL performance is often imperfect policy generalization on\ntest-time states out of the support of the training data, rather than policy\nlearning on in-distribution states. We then show that the use of suboptimal but\nhigh-coverage data or test-time policy training techniques can address this\ngeneralization issue in practice. Specifically, we propose two simple test-time\npolicy improvement methods and show that these methods lead to better\nperformance.\n'] , ['  The goal of reinforcement learning is estimating a policy that maps states to\nactions and maximizes the cumulative reward of a Markov Decision Process (MDP).\nThis is oftentimes achieved by estimating first the optimal (reward) value\nfunction (VF) associated with each state-action pair. When the MDP has an\ninfinite horizon, the optimal VFs and policies are stationary under mild\nconditions. However, in finite-horizon MDPs, the VFs (hence, the policies) vary\nwith time. This poses a challenge since the number of VFs to estimate grows not\nonly with the size of the state-action space but also with the time horizon.\nThis paper proposes a non-parametric low-rank stochastic algorithm to\napproximate the VFs of finite-horizon MDPs. First, we represent the (unknown)\nVFs as a multi-dimensional array, or tensor, where time is one of the\ndimensions. Then, we use rewards sampled from the MDP to estimate the optimal\nVFs. More precisely, we use the (truncated) PARAFAC decomposition to design an\nonline low-rank algorithm that recovers the entries of the tensor of VFs. The\nsize of the low-rank PARAFAC model grows additively with respect to each of its\ndimensions, rendering our approach efficient, as demonstrated via numerical\nexperiments.\n', ""  Most methods in reinforcement learning use a Policy Gradient (PG) approach to\nlearn a parametric stochastic policy that maps states to actions. The standard\napproach is to implement such a mapping via a neural network (NN) whose\nparameters are optimized using stochastic gradient descent. However, PG methods\nare prone to large policy updates that can render learning inefficient. Trust\nregion algorithms, like Trust Region Policy Optimization (TRPO), constrain the\npolicy update step, ensuring monotonic improvements. This paper introduces\nlow-rank matrix-based models as an efficient alternative for estimating the\nparameters of TRPO algorithms. By gathering the stochastic policy's parameters\ninto a matrix and applying matrix-completion techniques, we promote and enforce\nlow rank. Our numerical studies demonstrate that low-rank matrix-based policy\nmodels effectively reduce both computational and sample complexities compared\nto NN models, while maintaining comparable aggregated rewards.\n"", '  Neural Network based approximations of the Value function make up the core of\nleading Policy Based methods such as Trust Regional Policy Optimization (TRPO)\nand Proximal Policy Optimization (PPO). While this adds significant value when\ndealing with very complex environments, we note that in sufficiently low State\nand action space environments, a computationally expensive Neural Network\narchitecture offers marginal improvement over simpler Value approximation\nmethods. We present an implementation of Natural Actor Critic algorithms with\nactor updates through Natural Policy Gradient methods. This paper proposes that\nNatural Policy Gradient (NPG) methods with Linear Function Approximation as a\nparadigm for value approximation may surpass the performance and speed of\nNeural Network based models such as TRPO and PPO within these environments.\nOver Reinforcement Learning benchmarks Cart Pole and Acrobot, we observe that\nour algorithm trains much faster than complex neural network architectures, and\nobtains an equivalent or greater result. This allows us to recommend the use of\nNPG methods with Linear Function Approximation over TRPO and PPO for both\ntraditional and sparse reward low dimensional problems.\n'] , ['  In offline reinforcement learning (RL), the absence of active exploration\ncalls for attention on the model robustness to tackle the sim-to-real gap,\nwhere the discrepancy between the simulated and deployed environments can\nsignificantly undermine the performance of the learned policy. To endow the\nlearned policy with robustness in a sample-efficient manner in the presence of\nhigh-dimensional state-action space, this paper considers the sample complexity\nof distributionally robust linear Markov decision processes (MDPs) with an\nuncertainty set characterized by the total variation distance using offline\ndata. We develop a pessimistic model-based algorithm and establish its sample\ncomplexity bound under minimal data coverage assumptions, which outperforms\nprior art by at least $\\widetilde{O}(d)$, where $d$ is the feature dimension.\nWe further improve the performance guarantee of the proposed algorithm by\nincorporating a carefully-designed variance estimator.\n', '  We study offline reinforcement learning (RL) with linear MDPs under the\ninfinite-horizon discounted setting which aims to learn a policy that maximizes\nthe expected discounted cumulative reward using a pre-collected dataset.\nExisting algorithms for this setting either require a uniform data coverage\nassumptions or are computationally inefficient for finding an\n$\\epsilon$-optimal policy with $O(\\epsilon^{-2})$ sample complexity. In this\npaper, we propose a primal dual algorithm for offline RL with linear MDPs in\nthe infinite-horizon discounted setting. Our algorithm is the first\ncomputationally efficient algorithm in this setting that achieves sample\ncomplexity of $O(\\epsilon^{-2})$ with partial data coverage assumption. Our\nwork is an improvement upon a recent work that requires $O(\\epsilon^{-4})$\nsamples. Moreover, we extend our algorithm to work in the offline constrained\nRL setting that enforces constraints on additional reward signals.\n', '  Hybrid Reinforcement Learning (RL), where an agent learns from both an\noffline dataset and online explorations in an unknown environment, has garnered\nsignificant recent interest. A crucial question posed by Xie et al. (2022) is\nwhether hybrid RL can improve upon the existing lower bounds established in\npurely offline and purely online RL without relying on the single-policy\nconcentrability assumption. While Li et al. (2023) provided an affirmative\nanswer to this question in the tabular PAC RL case, the question remains\nunsettled for both the regret-minimizing RL case and the non-tabular case.\n  In this work, building upon recent advancements in offline RL and\nreward-agnostic exploration, we develop computationally efficient algorithms\nfor both PAC and regret-minimizing RL with linear function approximation,\nwithout single-policy concentrability. We demonstrate that these algorithms\nachieve sharper error or regret bounds that are no worse than, and can improve\non, the optimal sample complexity in offline RL (the first algorithm, for PAC\nRL) and online RL (the second algorithm, for regret-minimizing RL) in linear\nMarkov decision processes (MDPs), regardless of the quality of the behavior\npolicy. To our knowledge, this work establishes the tightest theoretical\nguarantees currently available for hybrid RL in linear MDPs.\n'] , [""  In preference-based reinforcement learning (PbRL), a reward function is\nlearned from a type of human feedback called preference. To expedite preference\ncollection, recent works have leveraged \\emph{offline preferences}, which are\npreferences collected for some offline data. In this scenario, the learned\nreward function is fitted on the offline data. If a learning agent exhibits\nbehaviors that do not overlap with the offline data, the learned reward\nfunction may encounter generalizability issues. To address this problem, the\npresent study introduces a framework that consolidates offline preferences and\n\\emph{virtual preferences} for PbRL, which are comparisons between the agent's\nbehaviors and the offline data. Critically, the reward function can track the\nagent's behaviors using the virtual preferences, thereby offering well-aligned\nguidance to the agent. Through experiments on continuous control tasks, this\nstudy demonstrates the effectiveness of incorporating the virtual preferences\nin PbRL.\n"", ""  Inverse Reinforcement Learning (IRL) is a powerful framework for learning\ncomplex behaviors from expert demonstrations. However, it traditionally\nrequires repeatedly solving a computationally expensive reinforcement learning\n(RL) problem in its inner loop. It is desirable to reduce the exploration\nburden by leveraging expert demonstrations in the inner-loop RL. As an example,\nrecent work resets the learner to expert states in order to inform the learner\nof high-reward expert states. However, such an approach is infeasible in the\nreal world. In this work, we consider an alternative approach to speeding up\nthe RL subroutine in IRL: \\emph{pessimism}, i.e., staying close to the expert's\ndata distribution, instantiated via the use of offline RL algorithms. We\nformalize a connection between offline RL and IRL, enabling us to use an\narbitrary offline RL algorithm to improve the sample efficiency of IRL. We\nvalidate our theory experimentally by demonstrating a strong correlation\nbetween the efficacy of an offline RL algorithm and how well it works as part\nof an IRL procedure. By using a strong offline RL algorithm as part of an IRL\nprocedure, we are able to find policies that match expert performance\nsignificantly more efficiently than the prior art.\n"", '  Offline reinforcement learning has become one of the most practical RL\nsettings. A recent success story has been RLHF, offline preference-based RL\n(PBRL) with preference from humans. However, most existing works on offline RL\nfocus on the standard setting with scalar reward feedback. It remains unknown\nhow to universally transfer the existing rich understanding of offline RL from\nthe reward-based to the preference-based setting. In this work, we propose a\ngeneral framework to bridge this gap. Our key insight is transforming\npreference feedback to scalar rewards via optimal reward labeling (ORL), and\nthen any reward-based offline RL algorithms can be applied to the dataset with\nthe reward labels. We theoretically show the connection between several recent\nPBRL techniques and our framework combined with specific offline RL algorithms\nin terms of how they utilize the preference signals. By combining reward\nlabeling with different algorithms, our framework can lead to new and\npotentially more efficient offline PBRL algorithms. We empirically test our\nframework on preference datasets based on the standard D4RL benchmark. When\ncombined with a variety of efficient reward-based offline RL algorithms, the\nlearning result achieved under our framework is comparable to training the same\nalgorithm on the dataset with actual rewards in many cases and better than the\nrecent PBRL baselines in most cases.\n']",Reinforcement Learning Methods and Applications,Offline Reinforcement Learning in MDPs
54,"""Meta-Reinforcement Learning and Reward Structures"" , ""Renewable Energy Management with Reinforcement Learning"" , ""Portfolio Optimization with Reinforcement Learning"" , ""Reinforcement Learning with Actor-Critic Algorithms"" , ""Optimizing Diffusion Models with Reinforcement Learning"" , ""Reinforcement Learning with Adaptive Control and Exploration""","['reinforcement', 'reward', 'rewards', 'learning', 'learned', 'exploration', 'learn', 'deepmind', 'tasks', 'rl'] , ['microgrid', 'renewable', 'scheduling', 'reinforcement', 'electricity', 'ev', 'energy', 'discharging', 'hvac', 'agent'] , ['trading', 'portfolio', 'portfolios', 'learning', 'stocks', 'investment', 'finance', 'agents', 'markets', 'arbitrage'] , ['reinforcement', 'critic', 'learning', 'optimism', 'replay', 'reward', 'exploration', 'bias', 'control', 'prioritized'] , ['reward', 'rewards', 'reinforcement', 'backpropagation', 'gradient', 'backpropagate', 'generative', 'diffusion', 'bias', 'optimizing'] , ['reinforcement', 'planning', 'learning', 'exploration', 'actions', 'rewards', 'learned', 'reward', 'critic', 'control']","['  Meta-reinforcement learning (meta-RL) is a promising framework for tackling\nchallenging domains requiring efficient exploration. Existing meta-RL\nalgorithms are characterized by low sample efficiency, and mostly focus on\nlow-dimensional task distributions. In parallel, model-based RL methods have\nbeen successful in solving partially observable MDPs, of which meta-RL is a\nspecial case. In this work, we leverage this success and propose a new\nmodel-based approach to meta-RL, based on elements from existing\nstate-of-the-art model-based and meta-RL methods. We demonstrate the\neffectiveness of our approach on common meta-RL benchmark domains, attaining\ngreater return with better sample efficiency (up to $15\\times$) while requiring\nvery little hyperparameter tuning. In addition, we validate our approach on a\nslate of more challenging, higher-dimensional domains, taking a step towards\nreal-world generalizing agents.\n', '  Improving sample efficiency is central to Reinforcement Learning (RL),\nespecially in environments where the rewards are sparse. Some recent approaches\nhave proposed to specify reward functions as manually designed or learned\nreward structures whose integrations in the RL algorithms are claimed to\nsignificantly improve the learning efficiency. Manually designed reward\nstructures can suffer from inaccuracy and existing automatically learning\nmethods are often computationally intractable for complex tasks. The\nintegration of inaccurate or partial reward structures in RL algorithms fail to\nlearn optimal policies. In this work, we propose an RL algorithm that can\nautomatically structure the reward function for sample efficiency, given a set\nof labels that signify subtasks. Given such minimal knowledge about the task,\nwe train a high-level policy that selects optimal sub-tasks in each state\ntogether with a low-level policy that efficiently learns to complete each\nsub-task. We evaluate our algorithm in a variety of sparse-reward environments.\nThe experiment results show that our approach significantly outperforms the\nstate-of-art baselines as the difficulty of the task increases.\n', '  Meta reinforcement learning (meta-RL) methods such as RL$^2$ have emerged as\npromising approaches for learning data-efficient RL algorithms tailored to a\ngiven task distribution. However, they show poor asymptotic performance and\nstruggle with out-of-distribution tasks because they rely on sequence models,\nsuch as recurrent neural networks or transformers, to process experiences\nrather than summarize them using general-purpose RL components such as value\nfunctions. In contrast, traditional RL algorithms are data-inefficient as they\ndo not use domain knowledge, but they do converge to an optimal policy in the\nlimit. We propose RL$^3$, a principled hybrid approach that incorporates\naction-values, learned per task through traditional RL, in the inputs to\nmeta-RL. We show that RL$^3$ earns greater cumulative reward in the long term,\ncompared to RL$^2$, while maintaining data-efficiency in the short term, and\ngeneralizes better to out-of-distribution tasks. Experiments are conducted on\nboth custom and benchmark discrete domains from the meta-RL literature that\nexhibit a range of short-term, long-term, and complex dependencies.\n'] , ['  Dairy farming consumes a significant amount of energy, making it an\nenergy-intensive sector within agriculture. Integrating renewable energy\ngeneration into dairy farming could help address this challenge. Effective\nbattery management is important for integrating renewable energy generation.\nManaging battery charging and discharging poses significant challenges because\nof fluctuations in electrical consumption, the intermittent nature of renewable\nenergy generation, and fluctuations in energy prices. Artificial Intelligence\n(AI) has the potential to significantly improve the use of renewable energy in\ndairy farming, however, there is limited research conducted in this particular\ndomain. This research considers Ireland as a case study as it works towards\nattaining its 2030 energy strategy centered on the utilization of renewable\nsources. This study proposes a Q-learning-based algorithm for scheduling\nbattery charging and discharging in a dairy farm setting. This research also\nexplores the effect of the proposed algorithm by adding wind generation data\nand considering additional case studies. The proposed algorithm reduces the\ncost of imported electricity from the grid by 13.41%, peak demand by 2%, and\n24.49% when utilizing wind generation. These results underline how\nreinforcement learning is highly effective in managing batteries in the dairy\nfarming sector.\n', ""  The increasing integration of electric vehicles (EVs) into the grid can pose\na significant risk to the distribution system operation in the absence of\ncoordination. In response to the need for effective coordination of EVs within\nthe distribution network, this paper presents a safety-aware reinforcement\nlearning (RL) algorithm designed to manage EV charging stations while ensuring\nthe satisfaction of system constraints. Unlike existing methods, our proposed\nalgorithm does not rely on explicit penalties for constraint violations,\neliminating the need for penalty coefficient tuning. Furthermore, managing EV\ncharging stations is further complicated by multiple uncertainties, notably the\nvariability in solar energy generation and energy prices. To address this\nchallenge, we develop an off-policy RL algorithm to efficiently utilize data to\nlearn patterns in such uncertain environments. Our algorithm also incorporates\na maximum entropy framework to enhance the RL algorithm's exploratory process,\npreventing convergence to local optimal solutions. Simulation results\ndemonstrate that our algorithm outperforms traditional RL algorithms in\nmanaging EV charging in the distribution network.\n"", '  The widespread adoption of electric vehicles (EVs) poses several challenges\nto power distribution networks and smart grid infrastructure due to the\npossibility of significantly increasing electricity demands, especially during\npeak hours. Furthermore, when EVs participate in demand-side management\nprograms, charging expenses can be reduced by using optimal charging control\npolicies that fully utilize real-time pricing schemes. However, devising\noptimal charging methods and control strategies for EVs is challenging due to\nvarious stochastic and uncertain environmental factors. Currently, most EV\ncharging controllers operate based on a centralized model. In this paper, we\nintroduce a novel approach for distributed and cooperative charging strategy\nusing a Multi-Agent Reinforcement Learning (MARL) framework. Our method is\nbuilt upon the Deep Deterministic Policy Gradient (DDPG) algorithm for a group\nof EVs in a residential community, where all EVs are connected to a shared\ntransformer. This method, referred to as CTDE-DDPG, adopts a Centralized\nTraining Decentralized Execution (CTDE) approach to establish cooperation\nbetween agents during the training phase, while ensuring a distributed and\nprivacy-preserving operation during execution. We theoretically examine the\nperformance of centralized and decentralized critics for the DDPG-based MARL\nimplementation and demonstrate their trade-offs. Furthermore, we numerically\nexplore the efficiency, scalability, and performance of centralized and\ndecentralized critics. Our theoretical and numerical results indicate that,\ndespite higher policy gradient variances and training complexity, the CTDE-DDPG\nframework significantly improves charging efficiency by reducing total\nvariation by approximately %36 and charging cost by around %9.1 on average...\n'] , ['  In recent years, deep or reinforcement learning approaches have been applied\nto optimise investment portfolios through learning the spatial and temporal\ninformation under the dynamic financial market. Yet in most cases, the existing\napproaches may produce biased trading signals based on the conventional price\ndata due to a lot of market noises, which possibly fails to balance the\ninvestment returns and risks. Accordingly, a multi-agent and self-adaptive\nportfolio optimisation framework integrated with attention mechanisms and time\nseries, namely the MASAAT, is proposed in this work in which multiple trading\nagents are created to observe and analyse the price series and directional\nchange data that recognises the significant changes of asset prices at\ndifferent levels of granularity for enhancing the signal-to-noise ratio of\nprice series. Afterwards, by reconstructing the tokens of financial data in a\nsequence, the attention-based cross-sectional analysis module and temporal\nanalysis module of each agent can effectively capture the correlations between\nassets and the dependencies between time points. Besides, a portfolio generator\nis integrated into the proposed framework to fuse the spatial-temporal\ninformation and then summarise the portfolios suggested by all trading agents\nto produce a newly ensemble portfolio for reducing biased trading actions and\nbalancing the overall returns and risks. The experimental results clearly\ndemonstrate that the MASAAT framework achieves impressive enhancement when\ncompared with many well-known portfolio optimsation approaches on three\nchallenging data sets of DJIA, S&P 500 and CSI 300. More importantly, our\nproposal has potential strengths in many possible applications for future\nstudy.\n', '  High-frequency trading (HFT) that executes algorithmic trading in short time\nscales, has recently occupied the majority of cryptocurrency market. Besides\ntraditional quantitative trading methods, reinforcement learning (RL) has\nbecome another appealing approach for HFT due to its terrific ability of\nhandling high-dimensional financial data and solving sophisticated sequential\ndecision-making problems, \\emph{e.g.,} hierarchical reinforcement learning\n(HRL) has shown its promising performance on second-level HFT by training a\nrouter to select only one sub-agent from the agent pool to execute the current\ntransaction. However, existing RL methods for HFT still have some defects: 1)\nstandard RL-based trading agents suffer from the overfitting issue, preventing\nthem from making effective policy adjustments based on financial context; 2)\ndue to the rapid changes in market conditions, investment decisions made by an\nindividual agent are usually one-sided and highly biased, which might lead to\nsignificant loss in extreme markets. To tackle these problems, we propose a\nnovel Memory Augmented Context-aware Reinforcement learning method On HFT,\n\\emph{a.k.a.} MacroHFT, which consists of two training phases: 1) we first\ntrain multiple types of sub-agents with the market data decomposed according to\nvarious financial indicators, specifically market trend and volatility, where\neach agent owns a conditional adapter to adjust its trading policy according to\nmarket conditions; 2) then we train a hyper-agent to mix the decisions from\nthese sub-agents and output a consistently profitable meta-policy to handle\nrapid market fluctuations, equipped with a memory mechanism to enhance the\ncapability of decision-making. Extensive experiments on various cryptocurrency\nmarkets demonstrate that MacroHFT can achieve state-of-the-art performance on\nminute-level trading tasks.\n', '  As a model-free algorithm, deep reinforcement learning (DRL) agent learns and\nmakes decisions by interacting with the environment in an unsupervised way. In\nrecent years, DRL algorithms have been widely applied by scholars for portfolio\noptimization in consecutive trading periods, since the DRL agent can\ndynamically adapt to market changes and does not rely on the specification of\nthe joint dynamics across the assets. However, typical DRL agents for portfolio\noptimization cannot learn a policy that is aware of the dynamic correlation\nbetween portfolio asset returns. Since the dynamic correlations among portfolio\nassets are crucial in optimizing the portfolio, the lack of such knowledge\nmakes it difficult for the DRL agent to maximize the return per unit of risk,\nespecially when the target market permits short selling (i.e., the US stock\nmarket). In this research, we propose a hybrid portfolio optimization model\ncombining the DRL agent and the Black-Litterman (BL) model to enable the DRL\nagent to learn the dynamic correlation between the portfolio asset returns and\nimplement an efficacious long/short strategy based on the correlation.\nEssentially, the DRL agent is trained to learn the policy to apply the BL model\nto determine the target portfolio weights. To test our DRL agent, we construct\nthe portfolio based on all the Dow Jones Industrial Average constitute stocks.\nEmpirical results of the experiments conducted on real-world United States\nstock market data demonstrate that our DRL agent significantly outperforms\nvarious comparison portfolio choice strategies and alternative DRL frameworks\nby at least 42% in terms of accumulated return. In terms of the return per unit\nof risk, our DRL agent significantly outperforms various comparative portfolio\nchoice strategies and alternative strategies based on other machine learning\nframeworks.\n'] , ['  Temporal difference (TD) learning is a fundamental technique in reinforcement\nlearning that updates value estimates for states or state-action pairs using a\nTD target. This target represents an improved estimate of the true value by\nincorporating both immediate rewards and the estimated value of subsequent\nstates. Traditionally, TD learning relies on the value of a single subsequent\nstate. We propose an enhanced multi-state TD (MSTD) target that utilizes the\nestimated values of multiple subsequent states. Building on this new MSTD\nconcept, we develop complete actor-critic algorithms that include management of\nreplay buffers in two modes, and integrate with deep deterministic policy\noptimization (DDPG) and soft actor-critic (SAC). Experimental results\ndemonstrate that algorithms employing the MSTD target significantly improve\nlearning performance compared to traditional methods.The code is provided on\nGitHub.\n', ""  Off-policy actor-critic algorithms have shown promise in deep reinforcement\nlearning for continuous control tasks. Their success largely stems from\nleveraging pessimistic state-action value function updates, which effectively\naddress function approximation errors and improve performance. However, such\npessimism can lead to under-exploration, constraining the agent's ability to\nexplore/refine its policies. Conversely, optimism can counteract\nunder-exploration, but it also carries the risk of excessive risk-taking and\npoor convergence if not properly balanced. Based on these insights, we\nintroduce Utility Soft Actor-Critic (USAC), a novel framework within the\nactor-critic paradigm that enables independent control over the degree of\npessimism/optimism for both the actor and the critic via interpretable\nparameters. USAC adapts its exploration strategy based on the uncertainty of\ncritics through a utility function that allows us to balance between pessimism\nand optimism separately. By going beyond binary choices of optimism and\npessimism, USAC represents a significant step towards achieving balance within\noff-policy actor-critic algorithms. Our experiments across various continuous\ncontrol problems show that the degree of pessimism or optimism depends on the\nnature of the task. Furthermore, we demonstrate that USAC can outperform\nstate-of-the-art algorithms for appropriately configured pessimism/optimism\nparameters.\n"", '  Actor-critic algorithms address the dual goals of reinforcement learning\n(RL), policy evaluation and improvement via two separate function\napproximators. The practicality of this approach comes at the expense of\ntraining instability, caused mainly by the destructive effect of the\napproximation errors of the critic on the actor. We tackle this bottleneck by\nemploying an existing Probably Approximately Correct (PAC) Bayesian bound for\nthe first time as the critic training objective of the Soft Actor-Critic (SAC)\nalgorithm. We further demonstrate that online learning performance improves\nsignificantly when a stochastic actor explores multiple futures by\ncritic-guided random search. We observe our resulting algorithm to compare\nfavorably against the state-of-the-art SAC implementation on multiple classical\ncontrol and locomotion tasks in terms of both sample efficiency and regret.\n'] , [""  Diffusion models are a class of flexible generative models trained with an\napproximation to the log-likelihood objective. However, most use cases of\ndiffusion models are not concerned with likelihoods, but instead with\ndownstream objectives such as human-perceived image quality or drug\neffectiveness. In this paper, we investigate reinforcement learning methods for\ndirectly optimizing diffusion models for such objectives. We describe how\nposing denoising as a multi-step decision-making problem enables a class of\npolicy gradient algorithms, which we refer to as denoising diffusion policy\noptimization (DDPO), that are more effective than alternative reward-weighted\nlikelihood approaches. Empirically, DDPO is able to adapt text-to-image\ndiffusion models to objectives that are difficult to express via prompting,\nsuch as image compressibility, and those derived from human feedback, such as\naesthetic quality. Finally, we show that DDPO can improve prompt-image\nalignment using feedback from a vision-language model without the need for\nadditional data collection or human annotation. The project's website can be\nfound at http://rl-diffusion.github.io .\n"", '  Reward finetuning has emerged as a promising approach to aligning foundation\nmodels with downstream objectives. Remarkable success has been achieved in the\nlanguage domain by using reinforcement learning (RL) to maximize rewards that\nreflect human preference. However, in the vision domain, existing RL-based\nreward finetuning methods are limited by their instability in large-scale\ntraining, rendering them incapable of generalizing to complex, unseen prompts.\nIn this paper, we propose Proximal Reward Difference Prediction (PRDP),\nenabling stable black-box reward finetuning for diffusion models for the first\ntime on large-scale prompt datasets with over 100K prompts. Our key innovation\nis the Reward Difference Prediction (RDP) objective that has the same optimal\nsolution as the RL objective while enjoying better training stability.\nSpecifically, the RDP objective is a supervised regression objective that tasks\nthe diffusion model with predicting the reward difference of generated image\npairs from their denoising trajectories. We theoretically prove that the\ndiffusion model that obtains perfect reward difference prediction is exactly\nthe maximizer of the RL objective. We further develop an online algorithm with\nproximal updates to stably optimize the RDP objective. In experiments, we\ndemonstrate that PRDP can match the reward maximization ability of\nwell-established RL-based methods in small-scale training. Furthermore, through\nlarge-scale training on text prompts from the Human Preference Dataset v2 and\nthe Pick-a-Pic v1 dataset, PRDP achieves superior generation quality on a\ndiverse set of complex, unseen prompts whereas RL-based methods completely\nfail.\n', ""  Using reinforcement learning with human feedback (RLHF) has shown significant\npromise in fine-tuning diffusion models. Previous methods start by training a\nreward model that aligns with human preferences, then leverage RL techniques to\nfine-tune the underlying models. However, crafting an efficient reward model\ndemands extensive datasets, optimal architecture, and manual hyperparameter\ntuning, making the process both time and cost-intensive. The direct preference\noptimization (DPO) method, effective in fine-tuning large language models,\neliminates the necessity for a reward model. However, the extensive GPU memory\nrequirement of the diffusion model's denoising process hinders the direct\napplication of the DPO method. To address this issue, we introduce the Direct\nPreference for Denoising Diffusion Policy Optimization (D3PO) method to\ndirectly fine-tune diffusion models. The theoretical analysis demonstrates that\nalthough D3PO omits training a reward model, it effectively functions as the\noptimal reward model trained using human feedback data to guide the learning\nprocess. This approach requires no training of a reward model, proving to be\nmore direct, cost-effective, and minimizing computational overhead. In\nexperiments, our method uses the relative scale of objectives as a proxy for\nhuman preference, delivering comparable results to methods using ground-truth\nrewards. Moreover, D3PO demonstrates the ability to reduce image distortion\nrates and generate safer images, overcoming challenges lacking robust reward\nmodels. Our code is publicly available at https://github.com/yk7333/D3PO.\n""] , [""  Deploying controllers trained with Reinforcement Learning (RL) on real robots\ncan be challenging: RL relies on agents' policies being modeled as Markov\nDecision Processes (MDPs), which assume an inherently discrete passage of time.\nThe use of MDPs results in that nearly all RL-based control systems employ a\nfixed-rate control strategy with a period (or time step) typically chosen based\non the developer's experience or specific characteristics of the application\nenvironment. Unfortunately, the system should be controlled at the highest,\nworst-case frequency to ensure stability, which can demand significant\ncomputational and energy resources and hinder the deployability of the\ncontroller on onboard hardware. Adhering to the principles of reactive\nprogramming, we surmise that applying control actions only when necessary\nenables the use of simpler hardware and helps reduce energy consumption. We\nchallenge the fixed frequency assumption by proposing a variant of RL with\nvariable control rate. In this approach, the policy decides the action the\nagent should take as well as the duration of the time step associated with that\naction. In our new setting, we expand Soft Actor-Critic (SAC) to compute the\noptimal policy with a variable control rate, introducing the Soft Elastic\nActor-Critic (SEAC) algorithm. We show the efficacy of SEAC through a\nproof-of-concept simulation driving an agent with Newtonian kinematics. Our\nexperiments show higher average returns, shorter task completion times, and\nreduced computational resources when compared to fixed rate policies.\n"", ""  Traditional reinforcement learning (RL) methods typically employ a fixed\ncontrol loop, where each cycle corresponds to an action. This rigidity poses\nchallenges in practical applications, as the optimal control frequency is\ntask-dependent. A suboptimal choice can lead to high computational demands and\nreduced exploration efficiency. Variable Time Step Reinforcement Learning\n(VTS-RL) addresses these issues by using adaptive frequencies for the control\nloop, executing actions only when necessary. This approach, rooted in reactive\nprogramming principles, reduces computational load and extends the action space\nby including action durations. However, VTS-RL's implementation is often\ncomplicated by the need to tune multiple hyperparameters that govern\nexploration in the multi-objective action-duration space (i.e., balancing task\nperformance and number of time steps to achieve a goal). To overcome these\nchallenges, we introduce the Multi-Objective Soft Elastic Actor-Critic (MOSEAC)\nmethod. This method features an adaptive reward scheme that adjusts\nhyperparameters based on observed trends in task rewards during training. This\nscheme reduces the complexity of hyperparameter tuning, requiring a single\nhyperparameter to guide exploration, thereby simplifying the learning process\nand lowering deployment costs. We validate the MOSEAC method through\nsimulations in a Newtonian kinematics environment, demonstrating high task and\ntraining performance with fewer time steps, ultimately lowering energy\nconsumption. This validation shows that MOSEAC streamlines RL algorithm\ndeployment by automatically tuning the agent control loop frequency using a\nsingle parameter. Its principles can be applied to enhance any RL algorithm,\nmaking it a versatile solution for various applications.\n"", ""  Reinforcement learning (RL) on high-dimensional and complex problems relies\non abstraction for improved efficiency and generalization. In this paper, we\nstudy abstraction in the continuous-control setting, and extend the definition\nof Markov decision process (MDP) homomorphisms to the setting of continuous\nstate and action spaces. We derive a policy gradient theorem on the abstract\nMDP for both stochastic and deterministic policies. Our policy gradient results\nallow for leveraging approximate symmetries of the environment for policy\noptimization. Based on these theorems, we propose a family of actor-critic\nalgorithms that are able to learn the policy and the MDP homomorphism map\nsimultaneously, using the lax bisimulation metric. Finally, we introduce a\nseries of environments with continuous symmetries to further demonstrate the\nability of our algorithm for action abstraction in the presence of such\nsymmetries. We demonstrate the effectiveness of our method on our environments,\nas well as on challenging visual control tasks from the DeepMind Control Suite.\nOur method's ability to utilize MDP homomorphisms for representation learning\nleads to improved performance, and the visualizations of the latent space\nclearly demonstrate the structure of the learned abstraction.\n""]",Reinforcement Learning Applications and Methodologies,"""Reinforcement Learning with Actor-Critic Algorithms"""
55,"Safe Reinforcement Learning for Autonomous Control , Distributional Reinforcement Learning , Reinforcement Learning in Recommender Systems , Reinforcement Learning with Logical Specifications and Explainability , Reinforcement Learning with Policy Gradient Methods , Reinforcement Learning Methods , Reinforcement Learning for Planning and Robotics , ""Autonomous Planning and Scheduling with Reinforcement Learning""","['safety', 'reinforcement', 'unsafe', 'safely', 'safe', 'autonomous', 'control', 'learning', 'barrier', 'learned'] , ['distributional', 'quantile', 'reinforcement', 'learns', 'distributions', 'distribution', 'learning', 'dqn', 'reward', 'expectile'] , ['recommender', 'recommenders', 'reinforcement', 'rewards', 'reward', 'recommendation', 'planning', 'rl', 'offline', 'agent'] , ['reinforcement', 'reward', 'rewards', 'learning', 'agents', 'agent', 'learned', 'automaton', 'autonomous', 'tasks'] , ['reinforcement', 'optimality', 'learning', 'optimal', 'critic', 'policies', 'policy', 'markovian', 'reward', 'exploration'] , ['learns', 'reinforcement', 'learning', 'planning', 'supervised', 'learned', 'trained', 'reward', 'rewards', 'learn'] , ['planning', 'reinforcement', 'planner', 'maze', 'robotics', 'exploration', 'robot', 'autonomous', 'robotic', 'learning'] , ['planning', 'planner', 'reinforcement', 'autonomous', 'plan', 'policies', 'schedule', 'temporal', 'projects', 'actions']","['  We develop provably safe and convergent reinforcement learning (RL)\nalgorithms for control of nonlinear dynamical systems, bridging the gap between\nthe hard safety guarantees of control theory and the convergence guarantees of\nRL theory. Recent advances at the intersection of control and RL follow a\ntwo-stage, safety filter approach to enforcing hard safety constraints:\nmodel-free RL is used to learn a potentially unsafe controller, whose actions\nare projected onto safe sets prescribed, for example, by a control barrier\nfunction. Though safe, such approaches lose any convergence guarantees enjoyed\nby the underlying RL methods. In this paper, we develop a single-stage,\nsampling-based approach to hard constraint satisfaction that learns RL\ncontrollers enjoying classical convergence guarantees while satisfying hard\nsafety constraints throughout training and deployment. We validate the efficacy\nof our approach in simulation, including safe control of a quadcopter in a\nchallenging obstacle avoidance problem, and demonstrate that it outperforms\nexisting benchmarks.\n', ""  Recently, safe reinforcement learning (RL) with the actor-critic structure\nfor continuous control tasks has received increasing attention. It is still\nchallenging to learn a near-optimal control policy with safety and convergence\nguarantees. Also, few works have addressed the safe RL algorithm design under\ntime-varying safety constraints. This paper proposes a safe RL algorithm for\noptimal control of nonlinear systems with time-varying state and control\nconstraints. In the proposed approach, we construct a novel barrier force-based\ncontrol policy structure to guarantee control safety. A multi-step policy\nevaluation mechanism is proposed to predict the policy's safety risk under\ntime-varying safety constraints and guide the policy to update safely.\nTheoretical results on stability and robustness are proven. Also, the\nconvergence of the actor-critic implementation is analyzed. The performance of\nthe proposed algorithm outperforms several state-of-the-art RL algorithms in\nthe simulated Safety Gym environment. Furthermore, the approach is applied to\nthe integrated path following and collision avoidance problem for two\nreal-world intelligent vehicles. A differential-drive vehicle and an\nAckermann-drive one are used to verify offline deployment and online learning\nperformance, respectively. Our approach shows an impressive sim-to-real\ntransfer capability and a satisfactory online control performance in the\nexperiment.\n"", '  Deep reinforcement learning (DRL) has demonstrated remarkable performance in\nmany continuous control tasks. However, a significant obstacle to the\nreal-world application of DRL is the lack of safety guarantees. Although DRL\nagents can satisfy system safety in expectation through reward shaping,\ndesigning agents to consistently meet hard constraints (e.g., safety\nspecifications) at every time step remains a formidable challenge. In contrast,\nexisting work in the field of safe control provides guarantees on persistent\nsatisfaction of hard safety constraints. However, these methods require\nexplicit analytical system dynamics models to synthesize safe control, which\nare typically inaccessible in DRL settings. In this paper, we present a\nmodel-free safe control algorithm, the implicit safe set algorithm, for\nsynthesizing safeguards for DRL agents that ensure provable safety throughout\ntraining. The proposed algorithm synthesizes a safety index (barrier\ncertificate) and a subsequent safe control law solely by querying a black-box\ndynamic function (e.g., a digital twin simulator). Moreover, we theoretically\nprove that the implicit safe set algorithm guarantees finite time convergence\nto the safe set and forward invariance for both continuous-time and\ndiscrete-time systems. We validate the proposed algorithm on the\nstate-of-the-art Safety Gym benchmark, where it achieves zero safety violations\nwhile gaining $95\\% \\pm 9\\%$ cumulative reward compared to state-of-the-art\nsafe DRL methods. Furthermore, the resulting algorithm scales well to\nhigh-dimensional systems with parallel computing.\n'] , ['  We propose a new algorithm for model-based distributional reinforcement\nlearning (RL), and prove that it is minimax-optimal for approximating return\ndistributions with a generative model (up to logarithmic factors), resolving an\nopen question of Zhang et al. (2023). Our analysis provides new theoretical\nresults on categorical approaches to distributional RL, and also introduces a\nnew distributional Bellman equation, the stochastic categorical CDF Bellman\nequation, which we expect to be of independent interest. We also provide an\nexperimental study comparing several model-based distributional RL algorithms,\nwith several takeaways for practitioners.\n', '  Distributional Reinforcement Learning (RL) estimates return distribution\nmainly by learning quantile values via minimizing the quantile Huber loss\nfunction, entailing a threshold parameter often selected heuristically or via\nhyperparameter search, which may not generalize well and can be suboptimal.\nThis paper introduces a generalized quantile Huber loss function derived from\nWasserstein distance (WD) calculation between Gaussian distributions, capturing\nnoise in predicted (current) and target (Bellman-updated) quantile values.\nCompared to the classical quantile Huber loss, this innovative loss function\nenhances robustness against outliers. Notably, the classical Huber loss\nfunction can be seen as an approximation of our proposed loss, enabling\nparameter adjustment by approximating the amount of noise in the data during\nthe learning process. Empirical tests on Atari games, a common application in\ndistributional RL, and a recent hedging strategy using distributional RL,\nvalidate the effectiveness of our proposed loss function and its potential for\nparameter adjustments in distributional RL. The implementation of the proposed\nloss function is available here.\n', '  Distributional reinforcement learning (RL) has proven useful in multiple\nbenchmarks as it enables approximating the full distribution of returns and\nmakes a better use of environment samples. The commonly used quantile\nregression approach to distributional RL -- based on asymmetric $L_1$ losses --\nprovides a flexible and effective way of learning arbitrary return\ndistributions. In practice, it is often improved by using a more efficient,\nhybrid asymmetric $L_1$-$L_2$ Huber loss for quantile regression. However, by\ndoing so, distributional estimation guarantees vanish, and we empirically\nobserve that the estimated distribution rapidly collapses to its mean. Indeed,\nasymmetric $L_2$ losses, corresponding to expectile regression, cannot be\nreadily used for distributional temporal difference learning. Motivated by the\nefficiency of $L_2$-based learning, we propose to jointly learn expectiles and\nquantiles of the return distribution in a way that allows efficient learning\nwhile keeping an estimate of the full distribution of returns. We prove that\nour approach approximately learns the correct return distribution, and we\nbenchmark a practical implementation on a toy example and at scale. On the\nAtari benchmark, our approach matches the performance of the Huber-based IQN-1\nbaseline after $200$M training frames but avoids distributional collapse and\nkeeps estimates of the full distribution of returns.\n'] , [""  In recent years, there has been a growing interest in utilizing reinforcement\nlearning (RL) to optimize long-term rewards in recommender systems. Since\nindustrial recommender systems are typically designed as multi-stage systems,\nRL methods with a single agent face challenges when optimizing multiple stages\nsimultaneously. The reason is that different stages have different observation\nspaces, and thus cannot be modeled by a single agent. To address this issue, we\npropose a novel UNidirectional-EXecution-based multi-agent Reinforcement\nLearning (UNEX-RL) framework to reinforce the long-term rewards in multi-stage\nrecommender systems. We show that the unidirectional execution is a key feature\nof multi-stage recommender systems, bringing new challenges to the applications\nof multi-agent reinforcement learning (MARL), namely the observation dependency\nand the cascading effect. To tackle these challenges, we provide a cascading\ninformation chain (CIC) method to separate the independent observations from\naction-dependent observations and use CIC to train UNEX-RL effectively. We also\ndiscuss practical variance reduction techniques for UNEX-RL. Finally, we show\nthe effectiveness of UNEX-RL on both public datasets and an online recommender\nsystem with over 100 million users. Specifically, UNEX-RL reveals a 0.558%\nincrease in users' usage time compared with single-agent RL algorithms in\nonline A/B experiments, highlighting the effectiveness of UNEX-RL in industrial\nrecommender systems.\n"", ""  As the last critical stage of RSs, Multi-Task Fusion (MTF) is responsible for\ncombining multiple scores outputted by Multi-Task Learning (MTL) into a final\nscore to maximize user satisfaction, which determines the ultimate\nrecommendation results. Recently, to optimize long-term user satisfaction\nwithin a recommendation session, Reinforcement Learning (RL) is used for MTF in\nthe industry. However, the off-policy RL algorithms used for MTF so far have\nthe following severe problems: 1) to avoid out-of-distribution (OOD) problem,\ntheir constraints are overly strict, which seriously damage their performance;\n2) they are unaware of the exploration policy used for producing training data\nand never interact with real environment, so only suboptimal policy can be\nlearned; 3) the traditional exploration policies are inefficient and hurt user\nexperience. To solve the above problems, we propose a novel method named\nIntegratedRL-MTF customized for MTF in large-scale RSs. IntegratedRL-MTF\nintegrates off-policy RL model with our online exploration policy to relax\noverstrict and complicated constraints, which significantly improves its\nperformance. We also design an extremely efficient exploration policy, which\neliminates low-value exploration space and focuses on exploring potential\nhigh-value state-action pairs. Moreover, we adopt progressive training mode to\nfurther enhance our model's performance with the help of our exploration\npolicy. We conduct extensive offline and online experiments in the short video\nchannel of Tencent News. The results demonstrate that our model outperforms\nother models remarkably. IntegratedRL-MTF has been fully deployed in our RS and\nother large-scale RSs in Tencent, which have achieved significant improvements.\n"", ""  Sequential recommendation, where user preference is dynamically inferred from\nsequential historical behaviors, is a critical task in recommender systems\n(RSs). To further optimize long-term user engagement, offline\nreinforcement-learning-based RSs have become a mainstream technique as they\nprovide an additional advantage in avoiding global explorations that may harm\nonline users' experiences. However, previous studies mainly focus on discrete\naction and policy spaces, which might have difficulties in handling\ndramatically growing items efficiently.\n  To mitigate this issue, in this paper, we aim to design an algorithmic\nframework applicable to continuous policies. To facilitate the control in the\nlow-dimensional but dense user preference space, we propose an\n\\underline{\\textbf{E}}fficient \\underline{\\textbf{Co}}ntinuous\n\\underline{\\textbf{C}}ontrol framework (ECoC). Based on a statistically tested\nassumption, we first propose the novel unified action representation abstracted\nfrom normalized user and item spaces. Then, we develop the corresponding policy\nevaluation and policy improvement procedures. During this process, strategic\nexploration and directional control in terms of unified actions are carefully\ndesigned and crucial to final recommendation decisions. Moreover, beneficial\nfrom unified actions, the conservatism regularization for policies and value\nfunctions are combined and perfectly compatible with the continuous framework.\nThe resulting dual regularization ensures the successful offline training of\nRL-based recommendation policies. Finally, we conduct extensive experiments to\nvalidate the effectiveness of our framework. The results show that compared to\nthe discrete baselines, our ECoC is trained far more efficiently. Meanwhile,\nthe final policies outperform baselines in both capturing the offline data and\ngaining long-term rewards.\n""] , ['  Reinforcement Learning (RL) has made significant strides in enabling\nartificial agents to learn diverse behaviors. However, learning an effective\npolicy often requires a large number of environment interactions. To mitigate\nsample complexity issues, recent approaches have used high-level task\nspecifications, such as Linear Temporal Logic (LTL$_f$) formulas or Reward\nMachines (RM), to guide the learning progress of the agent. In this work, we\npropose a novel approach, called Logical Specifications-guided Dynamic Task\nSampling (LSTS), that learns a set of RL policies to guide an agent from an\ninitial state to a goal state based on a high-level task specification, while\nminimizing the number of environmental interactions. Unlike previous work, LSTS\ndoes not assume information about the environment dynamics or the Reward\nMachine, and dynamically samples promising tasks that lead to successful goal\npolicies. We evaluate LSTS on a gridworld and show that it achieves improved\ntime-to-threshold performance on complex sequential decision-making problems\ncompared to state-of-the-art RM and Automaton-guided RL baselines, such as\nQ-Learning for Reward Machines and Compositional RL from logical Specifications\n(DIRL). Moreover, we demonstrate that our method outperforms RM and\nAutomaton-guided RL baselines in terms of sample-efficiency, both in a\npartially observable robotic task and in a continuous control robotic\nmanipulation task.\n', ""  Explanation is a key component for the adoption of reinforcement learning\n(RL) in many real-world decision-making problems. In the literature, the\nexplanation is often provided by saliency attribution to the features of the RL\nagent's state. In this work, we propose a complementary approach to these\nexplanations, particularly for offline RL, where we attribute the policy\ndecisions of a trained RL agent to the trajectories encountered by it during\ntraining. To do so, we encode trajectories in offline training data\nindividually as well as collectively (encoding a set of trajectories). We then\nattribute policy decisions to a set of trajectories in this encoded space by\nestimating the sensitivity of the decision with respect to that set. Further,\nwe demonstrate the effectiveness of the proposed approach in terms of quality\nof attributions as well as practical scalability in diverse environments that\ninvolve both discrete and continuous state and action spaces such as\ngrid-worlds, video games (Atari) and continuous control (MuJoCo). We also\nconduct a human study on a simple navigation task to observe how their\nunderstanding of the task compares with data attributed for a trained RL\npolicy. Keywords -- Explainable AI, Verifiability of AI Decisions, Explainable\nRL.\n"", '  Properly defining a reward signal to efficiently train a reinforcement\nlearning (RL) agent is a challenging task. Designing balanced objective\nfunctions from which a desired behavior can emerge requires expert knowledge,\nespecially for complex environments. Learning rewards from human feedback or\nusing large language models (LLMs) to directly provide rewards are promising\nalternatives, allowing non-experts to specify goals for the agent. However,\nblack-box reward models make it difficult to debug the reward. In this work, we\npropose Object-Centric Assessment with Language Models (OCALM) to derive\ninherently interpretable reward functions for RL agents from natural language\ntask descriptions. OCALM uses the extensive world-knowledge of LLMs while\nleveraging the object-centric nature common to many environments to derive\nreward functions focused on relational concepts, providing RL agents with the\nability to derive policies from task descriptions.\n'] , ['  Entropy regularization is an efficient technique for encouraging exploration\nand preventing a premature convergence of (vanilla) policy gradient methods in\nreinforcement learning (RL). However, the theoretical understanding of\nentropy-regularized RL algorithms has been limited. In this paper, we revisit\nthe classical entropy regularized policy gradient methods with the soft-max\npolicy parametrization, whose convergence has so far only been established\nassuming access to exact gradient oracles. To go beyond this scenario, we\npropose the first set of (nearly) unbiased stochastic policy gradient\nestimators with trajectory-level entropy regularization, with one being an\nunbiased visitation measure-based estimator and the other one being a nearly\nunbiased yet more practical trajectory-based estimator. We prove that although\nthe estimators themselves are unbounded in general due to the additional\nlogarithmic policy rewards introduced by the entropy term, the variances are\nuniformly bounded. We then propose a two-phase stochastic policy gradient (PG)\nalgorithm that uses a large batch size in the first phase to overcome the\nchallenge of the stochastic approximation due to the non-coercive landscape,\nand uses a small batch size in the second phase by leveraging the curvature\ninformation around the optimal policy. We establish a global optimality\nconvergence result and a sample complexity of\n$\\widetilde{\\mathcal{O}}(\\frac{1}{\\epsilon^2})$ for the proposed algorithm. Our\nresult is the first global convergence and sample complexity results for the\nstochastic entropy-regularized vanilla PG method.\n', '  Since the objective functions of reinforcement learning problems are\ntypically highly nonconvex, it is desirable that policy gradient, the most\npopular algorithm, escapes saddle points and arrives at second-order stationary\npoints. Existing results only consider vanilla policy gradient algorithms with\nunbiased gradient estimators, but practical implementations under the\ninfinite-horizon discounted reward setting are biased due to finite-horizon\nsampling. Moreover, actor-critic methods, whose second-order convergence has\nnot yet been established, are also biased due to the critic approximation of\nthe value function. We provide a novel second-order analysis of biased policy\ngradient methods, including the vanilla gradient estimator computed from\nMonte-Carlo sampling of trajectories as well as the double-loop actor-critic\nalgorithm, where in the inner loop the critic improves the approximation of the\nvalue function via TD(0) learning. Separately, we also establish the\nconvergence of TD(0) on Markov chains irrespective of initial state\ndistribution.\n', '  The infinite horizon setting is widely adopted for problems of reinforcement\nlearning (RL). These invariably result in stationary policies that are optimal.\nIn many situations, finite horizon control problems are of interest and for\nsuch problems, the optimal policies are time-varying in general. Another\nsetting that has become popular in recent times is of Constrained Reinforcement\nLearning, where the agent maximizes its rewards while it also aims to satisfy\nsome given constraint criteria. However, this setting has only been studied in\nthe context of infinite horizon MDPs where stationary policies are optimal. We\npresent an algorithm for constrained RL in the Finite Horizon Setting where the\nhorizon terminates after a fixed (finite) time. We use function approximation\nin our algorithm which is essential when the state and action spaces are large\nor continuous and use the policy gradient method to find the optimal policy.\nThe optimal policy that we obtain depends on the stage and so is non-stationary\nin general. To the best of our knowledge, our paper presents the first policy\ngradient algorithm for the finite horizon setting with constraints. We show the\nconvergence of our algorithm to a constrained optimal policy. We also compare\nand analyze the performance of our algorithm through experiments and show that\nour algorithm performs better than some other well known algorithms.\n'] , ['  Recent works have shown the remarkable superiority of transformer models in\nreinforcement learning (RL), where the decision-making problem is formulated as\nsequential generation. Transformer-based agents could emerge with\nself-improvement in online environments by providing task contexts, such as\nmultiple trajectories, called in-context RL. However, due to the quadratic\ncomputation complexity of attention in transformers, current in-context RL\nmethods suffer from huge computational costs as the task horizon increases. In\ncontrast, the Mamba model is renowned for its efficient ability to process\nlong-term dependencies, which provides an opportunity for in-context RL to\nsolve tasks that require long-term memory. To this end, we first implement\nDecision Mamba (DM) by replacing the backbone of Decision Transformer (DT).\nThen, we propose a Decision Mamba-Hybrid (DM-H) with the merits of transformers\nand Mamba in high-quality prediction and long-term memory. Specifically, DM-H\nfirst generates high-value sub-goals from long-term memory through the Mamba\nmodel. Then, we use sub-goals to prompt the transformer, establishing\nhigh-quality predictions. Experimental results demonstrate that DM-H achieves\nstate-of-the-art in long and short-term tasks, such as D4RL, Grid World, and\nTmaze benchmarks. Regarding efficiency, the online testing of DM-H in the\nlong-term task is 28$\\times$ times faster than the transformer-based baselines.\n', '  Real-world reinforcement learning (RL) environments, whether in robotics or\nindustrial settings, often involve non-visual observations and require not only\nefficient but also reliable and thus interpretable and flexible RL approaches.\nTo improve efficiency, agents that perform state representation learning with\nauxiliary tasks have been widely studied in visual observation contexts.\nHowever, for real-world problems, dedicated representation learning modules\nthat are decoupled from RL agents are more suited to meet requirements. This\nstudy compares common auxiliary tasks based on, to the best of our knowledge,\nthe only decoupled representation learning method for low-dimensional\nnon-visual observations. We evaluate potential improvements in sample\nefficiency and returns for environments ranging from a simple pendulum to a\ncomplex simulated robotics task. Our findings show that representation learning\nwith auxiliary tasks only provides performance gains in sufficiently complex\nenvironments and that learning environment dynamics is preferable to predicting\nrewards. These insights can inform future development of interpretable\nrepresentation learning approaches for non-visual observations and advance the\nuse of RL solutions in real-world scenarios.\n', '  Unsupervised and self-supervised objectives, such as next token prediction,\nhave enabled pre-training generalist models from large amounts of unlabeled\ndata. In reinforcement learning (RL), however, finding a truly general and\nscalable unsupervised pre-training objective for generalist policies from\noffline data remains a major open question. While a number of methods have been\nproposed to enable generic self-supervised RL, based on principles such as\ngoal-conditioned RL, behavioral cloning, and unsupervised skill learning, such\nmethods remain limited in terms of either the diversity of the discovered\nbehaviors, the need for high-quality demonstration data, or the lack of a clear\nadaptation mechanism for downstream tasks. In this work, we propose a novel\nunsupervised framework to pre-train generalist policies that capture diverse,\noptimal, long-horizon behaviors from unlabeled offline data such that they can\nbe quickly adapted to any arbitrary new tasks in a zero-shot manner. Our key\ninsight is to learn a structured representation that preserves the temporal\nstructure of the underlying environment, and then to span this learned latent\nspace with directional movements, which enables various zero-shot policy\n""prompting"" schemes for downstream tasks. Through our experiments on simulated\nrobotic locomotion and manipulation benchmarks, we show that our unsupervised\npolicies can solve goal-conditioned and general RL tasks in a zero-shot\nfashion, even often outperforming prior methods designed specifically for each\nsetting. Our code and videos are available at\nhttps://seohong.me/projects/hilp/.\n'] , ['  The Value Iteration Network (VIN) is an end-to-end differentiable\narchitecture that performs value iteration on a latent MDP for planning in\nreinforcement learning (RL). However, VINs struggle to scale to long-term and\nlarge-scale planning tasks, such as navigating a $100\\times 100$ maze -- a task\nwhich typically requires thousands of planning steps to solve. We observe that\nthis deficiency is due to two issues: the representation capacity of the latent\nMDP and the planning module\'s depth. We address these by augmenting the latent\nMDP with a dynamic transition kernel, dramatically improving its\nrepresentational capacity, and, to mitigate the vanishing gradient problem,\nintroducing an ""adaptive highway loss"" that constructs skip connections to\nimprove gradient flow. We evaluate our method on both 2D maze navigation\nenvironments and the ViZDoom 3D navigation benchmark. We find that our new\nmethod, named Dynamic Transition VIN (DT-VIN), easily scales to 5000 layers and\ncasually solves challenging versions of the above tasks. Altogether, we believe\nthat DT-VIN represents a concrete step forward in performing long-term\nlarge-scale planning in RL environments.\n', ""  This work investigates the potential of Reinforcement Learning (RL) to tackle\nrobot motion planning challenges in the dynamic RoboCup Small Size League\n(SSL). Using a heuristic control approach, we evaluate RL's effectiveness in\nobstacle-free and single-obstacle path-planning environments. Ablation studies\nreveal significant performance improvements. Our method achieved a 60% time\ngain in obstacle-free environments compared to baseline algorithms.\nAdditionally, our findings demonstrated dynamic obstacle avoidance\ncapabilities, adeptly navigating around moving blocks. These findings highlight\nthe potential of RL to enhance robot motion planning in the challenging and\nunpredictable SSL environment.\n"", '  General-purpose agents require fine-grained controls and rich sensory inputs\nto perform a wide range of tasks. However, this complexity often leads to\nintractable decision-making. Traditionally, agents are provided with\ntask-specific action and observation spaces to mitigate this challenge, but\nthis reduces autonomy. Instead, agents must be capable of building state-action\nspaces at the correct abstraction level from their sensorimotor experiences. We\nleverage the structure of a given set of temporally-extended actions to learn\nabstract Markov decision processes (MDPs) that operate at a higher level of\ntemporal and state granularity. We characterize state abstractions necessary to\nensure that planning with these skills, by simulating trajectories in the\nabstract MDP, results in policies with bounded value loss in the original MDP.\nWe evaluate our approach in goal-based navigation environments that require\ncontinuous abstract states to plan successfully and show that abstract model\nlearning improves the sample efficiency of planning and learning.\n'] , [""  Recently there has been a growing interest in industry and academia,\nregarding the use of wireless chargers to prolong the operational longevity of\nunmanned aerial vehicles (commonly knowns as drones). In this paper we consider\na charger-assisted drone application: a drone is deployed to observe a set\npoints of interest, while a charger can move to recharge the drone's battery.\nWe focus on the route and charging schedule of the drone and the mobile\ncharger, to obtain high observation utility with the shortest possible time,\nwhile ensuring the drone remains operational during task execution.\nEssentially, this proposed drone-charger scheduling problem is a multi-stage\ndecision-making process, in which the drone and the mobile charger act as two\nagents who cooperate to finish a task. The discrete-continuous hybrid action\nspace of the two agents poses a significant challenge in our problem. To\naddress this issue, we present a hybrid-action deep reinforcement learning\nframework, called HaDMC, which uses a standard policy learning algorithm to\ngenerate latent continuous actions. Motivated by representation learning, we\nspecifically design and train an action decoder. It involves two pipelines to\nconvert the latent continuous actions into original discrete and continuous\nactions, by which the drone and the charger can directly interact with\nenvironment. We embed a mutual learning scheme in model training, emphasizing\nthe collaborative rather than individual actions. We conduct extensive\nnumerical experiments to evaluate HaDMC and compare it with state-of-the-art\ndeep reinforcement learning approaches. The experimental results show the\neffectiveness and efficiency of our solution.\n"", ""  In recent years, Deep Reinforcement Learning (DRL) has emerged as an\neffective approach to solving real-world tasks. However, despite their\nsuccesses, DRL-based policies suffer from poor reliability, which limits their\ndeployment in safety-critical domains. Various methods have been put forth to\naddress this issue by providing formal safety guarantees. Two main approaches\ninclude shielding and verification. While shielding ensures the safe behavior\nof the policy by employing an external online component (i.e., a ``shield'')\nthat overrides potentially dangerous actions, this approach has a significant\ncomputational cost as the shield must be invoked at runtime to validate every\ndecision. On the other hand, verification is an offline process that can\nidentify policies that are unsafe, prior to their deployment, yet, without\nproviding alternative actions when such a policy is deemed unsafe. In this\nwork, we present verification-guided shielding -- a novel approach that bridges\nthe DRL reliability gap by integrating these two methods. Our approach combines\nboth formal and probabilistic verification tools to partition the input domain\ninto safe and unsafe regions. In addition, we employ clustering and symbolic\nrepresentation procedures that compress the unsafe regions into a compact\nrepresentation. This, in turn, allows to temporarily activate the shield solely\nin (potentially) unsafe regions, in an efficient manner. Our novel approach\nallows to significantly reduce runtime overhead while still preserving formal\nsafety guarantees. We extensively evaluate our approach on two benchmarks from\nthe robotic navigation domain, as well as provide an in-depth analysis of its\nscalability and completeness.\n"", '  Standard temporal planning assumes that planning takes place offline and then\nexecution starts at time 0. Recently, situated temporal planning was\nintroduced, where planning starts at time 0 and execution occurs after planning\nterminates. Situated temporal planning reflects a more realistic scenario where\ntime passes during planning. However, in situated temporal planning a complete\nplan must be generated before any action is executed. In some problems with\ntime pressure, timing is too tight to complete planning before the first action\nmust be executed. For example, an autonomous car that has a truck backing\ntowards it should probably move out of the way now and plan how to get to its\ndestination later. In this paper, we propose a new problem setting: concurrent\nplanning and execution, in which actions can be dispatched (executed) before\nplanning terminates. Unlike previous work on planning and execution, we must\nhandle wall clock deadlines that affect action applicability and goal\nachievement (as in situated planning) while also supporting dispatching actions\nbefore a complete plan has been found. We extend previous work on metareasoning\nfor situated temporal planning to develop an algorithm for this new setting.\nOur empirical evaluation shows that when there is strong time pressure, our\napproach outperforms situated temporal planning.\n']",Reinforcement Learning Methods and Applications,Reinforcement Learning with Policy Gradient Methods
56,"""UAV Trajectory Optimization for Wireless Communication"" , ""Quadrotor Control and Aerodynamics"" , ""UAV Dogfight Tactics and Maneuvers"" , ""Autonomous UAV Swarms and Trajectory Planning"" , ""UAV Tracking and Sensing""","['uav', 'uavs', 'unmanned', 'drone', 'aerial', 'swarm', 'offloading', 'optimization', 'optimizing', 'transmit'] , ['quadrotor', 'quadrotors', 'drones', 'drone', 'controllers', 'aerial', 'multirotor', 'aerodynamic', 'trajectory', 'robotics'] , ['tactics', 'uav', 'maneuvers', 'uavs', 'evader', 'combat', 'drones', 'maneuver', 'maneuverability', 'drone'] , ['uav', 'unmanned', 'uavs', 'swarm', 'swarms', 'autonomous', 'flying', 'planning', 'aircraft', 'aerial'] , ['uav', 'drones', 'drone', 'uavs', 'unmanned', 'lidar', 'sensing', 'flying', 'quadrotor', 'aerial']","['  In this paper, the problem of using one active unmanned aerial vehicle (UAV)\nand four passive UAVs to localize a 3D target UAV in real time is investigated.\nIn the considered model, each passive UAV receives reflection signals from the\ntarget UAV, which are initially transmitted by the active UAV. The received\nreflection signals allow each passive UAV to estimate the signal transmission\ndistance which will be transmitted to a base station (BS) for the estimation of\nthe position of the target UAV. Due to the movement of the target UAV, each\nactive/passive UAV must optimize its trajectory to continuously localize the\ntarget UAV. Meanwhile, since the accuracy of the distance estimation depends on\nthe signal-to-noise ratio of the transmission signals, the active UAV must\noptimize its transmit power. This problem is formulated as an optimization\nproblem whose goal is to jointly optimize the transmit power of the active UAV\nand trajectories of both active and passive UAVs so as to maximize the target\nUAV positioning accuracy. To solve this problem, a Z function decomposition\nbased reinforcement learning (ZD-RL) method is proposed. Compared to value\nfunction decomposition based RL (VD-RL), the proposed method can find the\nprobability distribution of the sum of future rewards to accurately estimate\nthe expected value of the sum of future rewards thus finding better transmit\npower of the active UAV and trajectories for both active and passive UAVs and\nimproving target UAV positioning accuracy. Simulation results show that the\nproposed ZD-RL method can reduce the positioning errors by up to 39.4% and\n64.6%, compared to VD-RL and independent deep RL methods, respectively.\n', '  Effective solutions for intelligent data collection in terrestrial cellular\nnetworks are crucial, especially in the context of Internet of Things\napplications. The limited spectrum and coverage area of terrestrial base\nstations pose challenges in meeting the escalating data rate demands of network\nusers. Unmanned aerial vehicles, known for their high agility, mobility, and\nflexibility, present an alternative means to offload data traffic from\nterrestrial BSs, serving as additional access points. This paper introduces a\nnovel approach to efficiently maximize the utilization of multiple UAVs for\ndata traffic offloading from terrestrial BSs. Specifically, the focus is on\nmaximizing user association with UAVs by jointly optimizing UAV trajectories\nand users association indicators under quality of service constraints. Since,\nthe formulated UAVs control problem is nonconvex and combinatorial, this study\nleverages the multi agent reinforcement learning framework. In this framework,\neach UAV acts as an independent agent, aiming to maintain inter UAV cooperative\nbehavior. The proposed approach utilizes the finite state Markov decision\nprocess to account for UAVs velocity constraints and the relationship between\ntheir trajectories and state space. A low complexity distributed state action\nreward state action algorithm is presented to determine UAVs optimal sequential\ndecision making policies over training episodes. The extensive simulation\nresults validate the proposed analysis and offer valuable insights into the\noptimal UAV trajectories. The derived trajectories demonstrate superior average\nUAV association performance compared to benchmark techniques such as Q learning\nand particle swarm optimization.\n', '  Recently, Unmanned Aerial Vehicles (UAVs) have attracted the attention of\nresearchers in academia and industry for providing wireless services to ground\nusers in diverse scenarios like festivals, large sporting events, natural and\nman-made disasters due to their advantages in terms of versatility and\nmaneuverability. However, the limited resources of UAVs (e.g., energy budget\nand different service requirements) can pose challenges for adopting UAVs for\nsuch applications. Our system model considers a UAV swarm that navigates an\narea, providing wireless communication to ground users with RIS support to\nimprove the coverage of the UAVs. In this work, we introduce an optimization\nmodel with the aim of maximizing the throughput and UAVs coverage through\noptimal path planning of UAVs and multi-RIS phase configurations. The\nformulated optimization is challenging to solve using standard linear\nprogramming techniques, limiting its applicability in real-time\ndecision-making. Therefore, we introduce a two-step solution using deep\nreinforcement learning and particle swarm optimization. We conduct extensive\nsimulations and compare our approach to two competitive solutions presented in\nthe recent literature. Our simulation results demonstrate that our adopted\napproach is 20 \\% better than the brute-force approach and 30\\% better than the\nbaseline solution in terms of QoS.\n'] , ['  Learning-based methods, particularly Reinforcement Learning (RL), hold great\npromise for streamlining deployment, enhancing performance, and achieving\ngeneralization in the control of autonomous multirotor aerial vehicles. Deep RL\nhas been able to control complex systems with impressive fidelity and agility\nin simulation but the simulation-to-reality transfer often brings a\nhard-to-bridge reality gap. Moreover, RL is commonly plagued by prohibitively\nlong training times. In this work, we propose a novel asymmetric\nactor-critic-based architecture coupled with a highly reliable RL-based\ntraining paradigm for end-to-end quadrotor control. We show how curriculum\nlearning and a highly optimized simulator enhance sample complexity and lead to\nfast training times. To precisely discuss the challenges related to\nlow-level/end-to-end multirotor control, we also introduce a taxonomy that\nclassifies the existing levels of control abstractions as well as\nnon-linearities and domain parameters. Our framework enables\nSimulation-to-Reality (Sim2Real) transfer for direct RPM control after only 18\nseconds of training on a consumer-grade laptop as well as its deployment on\nmicrocontrollers to control a multirotor under real-time guarantees. Finally,\nour solution exhibits competitive performance in trajectory tracking, as\ndemonstrated through various experimental comparisons with existing\nstate-of-the-art control solutions using a real Crazyflie nano quadrotor. We\nopen source the code including a very fast multirotor dynamics simulator that\ncan simulate about 5 months of flight per second on a laptop GPU. The fast\ntraining times and deployment to a cheap, off-the-shelf quadrotor lower the\nbarriers to entry and help democratize the research and development of these\nsystems.\n', '  Ensuring the reliability and validity of data-driven quadrotor model\npredictions is essential for their accepted and practical use. This is\nespecially true for grey- and black-box models wherein the mapping of inputs to\npredictions is not transparent and subsequent reliability notoriously difficult\nto ascertain. Nonetheless, such techniques are frequently and successfully used\nto identify quadrotor models. Prediction intervals (PIs) may be employed to\nprovide insight into the consistency and accuracy of model predictions. This\npaper estimates such PIs for polynomial and Artificial Neural Network (ANN)\nquadrotor aerodynamic models. Two existing ANN PI estimation techniques - the\nbootstrap method and the quality driven method - are validated numerically for\nquadrotor aerodynamic models using an existing high-fidelity quadrotor\nsimulation. Quadrotor aerodynamic models are then identified on real quadrotor\nflight data to demonstrate their utility and explore their sensitivity to model\ninterpolation and extrapolation. It is found that the ANN-based PIs widen\nconsiderably when extrapolating and remain constant, or shrink, when\ninterpolating. While this behaviour also occurs for the polynomial PIs, it is\nof lower magnitude. The estimated PIs establish probabilistic bounds within\nwhich the quadrotor model outputs will likely lie, subject to modelling and\nmeasurement uncertainties that are reflected through the PI widths.\n', '  Motivated by the increasing use of quadrotors for payload delivery, we\nconsider a joint trajectory generation and feedback control design problem for\na quadrotor experiencing aerodynamic wrenches. Unmodeled aerodynamic drag\nforces from carried payloads can lead to catastrophic outcomes. Prior work\nmodel aerodynamic effects as residual dynamics or external disturbances in the\ncontrol problem leading to a reactive policy that could be catastrophic.\nMoreover, redesigning controllers and tuning control gains on hardware\nplatforms is a laborious effort. In this paper, we argue that adapting the\ntrajectory generation component keeping the controller fixed can improve\ntrajectory tracking for quadrotor systems experiencing drag forces. To achieve\nthis, we formulate a drag-aware planning problem by applying a suitable\nrelaxation to an optimal quadrotor control problem, introducing a tracking cost\nfunction which measures the ability of a controller to follow a reference\ntrajectory. This tracking cost function acts as a regularizer in trajectory\ngeneration and is learned from data obtained from simulation. Our experiments\nin both simulation and on the Crazyflie hardware platform show that changing\nthe planner reduces tracking error by as much as 83%. Evaluation on hardware\ndemonstrates that our planned path, as opposed to a baseline, avoids controller\nsaturation and catastrophic outcomes during aggressive maneuvers.\n'] , ['  This paper proposes a three-layer unmanned combat aerial vehicle (UCAV)\ndogfight frame where Deep reinforcement learning (DRL) is responsible for\nhigh-level maneuver decision. A four-channel low-level control law is firstly\nconstructed, followed by a library containing eight basic flight maneuvers\n(BFMs). Double deep Q network (DDQN) is applied for BFM selection in UCAV\ndogfight, where the opponent strategy during the training process is\nconstructed with DT. Our simulation result shows that, the agent can achieve a\nwin rate of 85.75% against the DT strategy, and positive results when facing\nvarious unseen opponents. Based on the proposed frame, interpretability of the\nDRL-based dogfight is significantly improved. The agent performs yo-yo to\nadjust its turn rate and gain higher maneuverability. Emergence of ""Dive and\nChase"" behavior also indicates the agent can generate a novel tactic that\nutilizes the drawback of its opponent.\n', ""  Unmanned Combat Aerial Vehicle (UCAV) dogfight, which refers to a fight\nbetween two or more UCAVs usually at close quarters, plays a decisive role on\nthe aerial battlefields. With the evolution of artificial intelligence,\ndogfight progressively transits towards intelligent and autonomous modes.\nHowever, the development of autonomous dogfight policy learning is hindered by\nchallenges such as weak exploration capabilities, low learning efficiency, and\nunrealistic simulated environments. To overcome these challenges, this paper\nproposes a novel imitative reinforcement learning framework, which efficiently\nleverages expert data while enabling autonomous exploration. The proposed\nframework not only enhances learning efficiency through expert imitation, but\nalso ensures adaptability to dynamic environments via autonomous exploration\nwith reinforcement learning. Therefore, the proposed framework can learn a\nsuccessful dogfight policy of 'pursuit-lock-launch' for UCAVs. To support\ndata-driven learning, we establish a dogfight environment based on the\nHarfang3D sandbox, where we conduct extensive experiments. The results indicate\nthat the proposed framework excels in multistage dogfight, significantly\noutperforms state-of-the-art reinforcement learning and imitation learning\nmethods. Thanks to the ability of imitating experts and autonomous exploration,\nour framework can quickly learn the critical knowledge in complex aerial combat\ntasks, achieving up to a 100% success rate and demonstrating excellent\nrobustness.\n"", '  Dogfighting is a challenging scenario in aerial applications that requires a\ncomprehensive understanding of both strategic maneuvers and the aerodynamics of\nagile aircraft. The aerial agent needs to not only understand tactically\nevolving maneuvers of fighter jets from a long-term perspective but also react\nto rapidly changing aerodynamics of aircraft from a short-term viewpoint. In\nthis paper, we introduce TempFuser, a novel long short-term temporal fusion\ntransformer architecture that can learn agile, tactical, and acrobatic flight\nmaneuvers in complex dogfight problems. Our approach integrates two distinct\ntemporal transition embeddings into a transformer-based network to\ncomprehensively capture both the long-term tactics and short-term agility of\naerial agents. By incorporating these perspectives, our policy network\ngenerates end-to-end flight commands that secure dominant positions over the\nlong term and effectively outmaneuver agile opponents. After training in a\nhigh-fidelity flight simulator, our model successfully learns to execute\nstrategic maneuvers, outperforming baseline policy models against various types\nof opponent aircraft. Notably, our model exhibits human-like acrobatic\nmaneuvers even when facing adversaries with superior specifications, all\nwithout relying on explicit prior knowledge. Moreover, it demonstrates robust\npursuit performance in challenging supersonic and low-altitude situations. Demo\nvideos are available at https://sites.google.com/view/tempfuser.\n'] , ['  With the impact of artificial intelligence on the traditional UAV industry,\nautonomous UAV flight has become a current hot research field. Based on the\ndemand for research on critical technologies for autonomous flying UAVs, this\npaper addresses the field of flight state recognition and trajectory prediction\nof UAVs. This paper proposes a method to improve the accuracy of UAV trajectory\nprediction based on UAV flight state recognition and verifies it using two\nprediction models. Firstly, UAV flight data acquisition and data preprocessing\nare carried out; secondly, UAV flight trajectory features are extracted based\non data fusion and a UAV flight state recognition model based on PCA-DAGSVM\nmodel is established; finally, two UAV flight trajectory prediction models are\nestablished and the trajectory prediction errors of the two prediction models\nare compared and analyzed after flight state recognition. The results show\nthat: 1) the UAV flight state recognition model based on PCA-DAGSVM has good\nrecognition effect. 2) compared with the traditional UAV trajectory prediction\nmodel, the prediction model based on flight state recognition can effectively\nreduce the prediction error.\n', '  As the demands for immediate and effective responses increase in both\ncivilian and military domains, the unmanned aerial vehicle (UAV) swarms emerge\nas effective solutions, in which multiple cooperative UAVs can work together to\nachieve specific goals. However, how to manage such complex systems to ensure\nreal-time adaptability lack sufficient researches. Hence, in this paper, we\npropose the cooperative cognitive dynamic system (CCDS), to optimize the\nmanagement for UAV swarms. CCDS leverages a hierarchical and cooperative\ncontrol structure that enables real-time data processing and decision.\nAccordingly, CCDS optimizes the UAV swarm management via dynamic\nreconfigurability and adaptive intelligent optimization. In addition, CCDS can\nbe integrated with the biomimetic mechanism to efficiently allocate tasks for\nUAV swarms. Further, the distributed coordination of CCDS ensures reliable and\nresilient control, thus enhancing the adaptability and robustness. Finally, the\npotential challenges and future directions are analyzed, to provide insights\ninto managing UAV swarms in dynamic heterogeneous networking.\n', ""  This paper addresses the increasing significance of UAVs (Unmanned Aerial\nVehicles) and the emergence of UAV swarms for collaborative operations in\nvarious domains. However, the effectiveness of UAV swarms can be severely\ncompromised by jamming technology, necessitating robust antijamming strategies.\nWhile existing methods such as frequency hopping and physical path planning\nhave been explored, there remains a gap in research on path planning for UAV\nswarms when the jammer's location is unknown. To address this, a novel\napproach, where UAV swarms leverage collective intelligence to predict jamming\nareas, evade them, and efficiently reach target destinations, is proposed. This\napproach utilizes Graph Convolutional Networks (GCN) to predict the location\nand intensity of jamming areas based on information gathered from each UAV. A\nmulti-agent control algorithm is then employed to disperse the UAV swarm, avoid\njamming, and regroup upon reaching the target. Through simulations, the\neffectiveness of the proposed method is demonstrated, showcasing accurate\nprediction of jamming areas and successful evasion through obstacle avoidance\nalgorithms, ultimately achieving the mission objective. Proposed method offers\nrobustness, scalability, and computational efficiency, making it applicable\nacross various scenarios where UAV swarms operate in potentially hostile\nenvironments.\n""] , ['  UAV tracking and pose estimation plays an imperative role in various\nUAV-related missions, such as formation control and anti-UAV measures.\nAccurately detecting and tracking UAVs in a 3D space remains a particularly\nchallenging problem, as it requires extracting sparse features of micro UAVs\nfrom different flight environments and continuously matching correspondences,\nespecially during agile flight. Generally, cameras and LiDARs are the two main\ntypes of sensors used to capture UAV trajectories in flight. However, both\nsensors have limitations in UAV classification and pose estimation. This\ntechnical report briefly introduces the method proposed by our team ""NTU-ICG""\nfor the CVPR 2024 UG2+ Challenge Track 5. This work develops a clustering-based\nlearning detection approach, CL-Det, for UAV tracking and pose estimation using\ntwo types of LiDARs, namely Livox Avia and LiDAR 360. We combine the\ninformation from the two data sources to locate drones in 3D. We first align\nthe timestamps of Livox Avia data and LiDAR 360 data and then separate the\npoint cloud of objects of interest (OOIs) from the environment. The point cloud\nof OOIs is clustered using the DBSCAN method, with the midpoint of the largest\ncluster assumed to be the UAV position. Furthermore, we utilize historical\nestimations to fill in missing data. The proposed method shows competitive pose\nestimation performance and ranks 5th on the final leaderboard of the CVPR 2024\nUG2+ Challenge.\n', '  Unmanned Aerial Vehicles (UAVs) have emerged as a transformative technology\nacross diverse sectors, offering adaptable solutions to complex challenges in\nboth military and civilian domains. Their expanding capabilities present a\nplatform for further advancement by integrating cutting-edge computational\ntools like Artificial Intelligence (AI) and Machine Learning (ML) algorithms.\nThese advancements have significantly impacted various facets of human life,\nfostering an era of unparalleled efficiency and convenience. Large Language\nModels (LLMs), a key component of AI, exhibit remarkable learning and\nadaptation capabilities within deployed environments, demonstrating an evolving\nform of intelligence with the potential to approach human-level proficiency.\nThis work explores the significant potential of integrating UAVs and LLMs to\npropel the development of autonomous systems. We comprehensively review LLM\narchitectures, evaluating their suitability for UAV integration. Additionally,\nwe summarize the state-of-the-art LLM-based UAV architectures and identify\nnovel opportunities for LLM embedding within UAV frameworks. Notably, we focus\non leveraging LLMs to refine data analysis and decision-making processes,\nspecifically for enhanced spectral sensing and sharing in UAV applications.\nFurthermore, we investigate how LLM integration expands the scope of existing\nUAV applications, enabling autonomous data processing, improved\ndecision-making, and faster response times in emergency scenarios like disaster\nresponse and network restoration. Finally, we highlight crucial areas for\nfuture research that are critical for facilitating the effective integration of\nLLMs and UAVs.\n', '  In the last twenty years, unmanned aerial vehicles (UAVs) have garnered\ngrowing interest due to their expanding applications in both military and\ncivilian domains. Detecting non-cooperative aerial vehicles with efficiency and\nestimating collisions accurately are pivotal for achieving fully autonomous\naircraft and facilitating Advanced Air Mobility (AAM). This paper presents a\ndeep-learning framework that utilizes optical sensors for the detection,\ntracking, and distance estimation of non-cooperative aerial vehicles. In\nimplementing this comprehensive sensing framework, the availability of depth\ninformation is essential for enabling autonomous aerial vehicles to perceive\nand navigate around obstacles. In this work, we propose a method for estimating\nthe distance information of a detected aerial object in real time using only\nthe input of a monocular camera. In order to train our deep learning components\nfor the object detection, tracking and depth estimation tasks we utilize the\nAmazon Airborne Object Tracking (AOT) Dataset. In contrast to previous\napproaches that integrate the depth estimation module into the object detector,\nour method formulates the problem as image-to-image translation. We employ a\nseparate lightweight encoder-decoder network for efficient and robust depth\nestimation. In a nutshell, the object detection module identifies and localizes\nobstacles, conveying this information to both the tracking module for\nmonitoring obstacle movement and the depth estimation module for calculating\ndistances. Our approach is evaluated on the Airborne Object Tracking (AOT)\ndataset which is the largest (to the best of our knowledge) air-to-air airborne\nobject dataset.\n']",Unmanned Aerial Vehicle (UAV) Systems and Technologies,"""Autonomous UAV Swarms and Trajectory Planning"""
57,"Imitation Learning from Demonstrations , Imitation Learning and Reinforcement Exploration , ""Robotics and Imitation Learning""","['imitation', 'reinforcement', 'mimic', 'demonstrations', 'imitate', 'learning', 'adversarial', 'cloning', 'reward', 'learned'] , ['learns', 'imitation', 'reinforcement', 'learning', 'exploration', 'learned', 'actions', 'demonstrations', 'tasks', 'robotic'] , ['learns', 'reinforcement', 'robotic', 'robotics', 'robot', 'learning', 'robots', 'exploration', 'learned', 'imitation']","[""  We focus on offline imitation learning (IL), which aims to mimic an expert's\nbehavior using demonstrations without any interaction with the environment. One\nof the main challenges in offline IL is the limited support of expert\ndemonstrations, which typically cover only a small fraction of the state-action\nspace. While it may not be feasible to obtain numerous expert demonstrations,\nit is often possible to gather a larger set of sub-optimal demonstrations. For\nexample, in treatment optimization problems, there are varying levels of doctor\ntreatments available for different chronic conditions. These range from\ntreatment specialists and experienced general practitioners to less experienced\ngeneral practitioners. Similarly, when robots are trained to imitate humans in\nroutine tasks, they might learn from individuals with different levels of\nexpertise and efficiency.\n  In this paper, we propose an offline IL approach that leverages the larger\nset of sub-optimal demonstrations while effectively mimicking expert\ntrajectories. Existing offline IL methods based on behavior cloning or\ndistribution matching often face issues such as overfitting to the limited set\nof expert demonstrations or inadvertently imitating sub-optimal trajectories\nfrom the larger dataset. Our approach, which is based on inverse soft-Q\nlearning, learns from both expert and sub-optimal demonstrations. It assigns\nhigher importance (through learned weights) to aligning with expert\ndemonstrations and lower importance to aligning with sub-optimal ones. A key\ncontribution of our approach, called SPRINQL, is transforming the offline IL\nproblem into a convex optimization over the space of Q functions. Through\ncomprehensive experimental evaluations, we demonstrate that the SPRINQL\nalgorithm achieves state-of-the-art (SOTA) performance on offline IL\nbenchmarks. Code is available at https://github.com/hmhuy2000/SPRINQL.\n"", '  Imitation learning is often used in addition to reinforcement learning in\nenvironments where reward design is difficult or where the reward is sparse,\nbut it is difficult to be able to imitate well in unknown states from a small\namount of expert data and sampling data. Supervised learning methods such as\nBehavioral Cloning do not require sampling data, but usually suffer from\ndistribution shift. The methods based on reinforcement learning, such as\ninverse reinforcement learning and Generative Adversarial imitation learning\n(GAIL), can learn from only a few expert data. However, they often need to\ninteract with the environment. Soft Q imitation learning (SQIL) addressed the\nproblems, and it was shown that it could learn efficiently by combining\nBehavioral Cloning and soft Q-learning with constant rewards. In order to make\nthis algorithm more robust to distribution shift, we propose more efficient and\nrobust algorithm by adding to this method a reward function based on\nadversarial inverse reinforcement learning that rewards the agent for\nperforming actions in status similar to the demo. We call this algorithm\nDiscriminator Soft Q Imitation Learning (DSQIL). We evaluated it on MuJoCo\nenvironments.\n', ""  In many real-world settings, an agent must learn to act in environments where\nno reward signal can be specified, but a set of expert demonstrations is\navailable. Imitation learning (IL) is a popular framework for learning policies\nfrom such demonstrations. However, in some cases, differences in observability\nbetween the expert and the agent can give rise to an imitation gap such that\nthe expert's policy is not optimal for the agent and a naive application of IL\ncan fail catastrophically. In particular, if the expert observes the Markov\nstate and the agent does not, then the expert will not demonstrate the\ninformation-gathering behavior needed by the agent but not the expert. In this\npaper, we propose a Bayesian solution to the Imitation Gap (BIG), first using\nthe expert demonstrations, together with a prior specifying the cost of\nexploratory behavior that is not demonstrated, to infer a posterior over\nrewards with Bayesian inverse reinforcement learning (IRL). BIG then uses the\nreward posterior to learn a Bayes-optimal policy. Our experiments show that\nBIG, unlike IL, allows the agent to explore at test time when presented with an\nimitation gap, whilst still learning to behave optimally using expert\ndemonstrations when no such gap exists.\n""] , [""  Unsupervised skill discovery is a learning paradigm that aims to acquire\ndiverse behaviors without explicit rewards. However, it faces challenges in\nlearning complex behaviors and often leads to learning unsafe or undesirable\nbehaviors. For instance, in various continuous control tasks, current\nunsupervised skill discovery methods succeed in learning basic locomotions like\nstanding but struggle with learning more complex movements such as walking and\nrunning. Moreover, they may acquire unsafe behaviors like tripping and rolling\nor navigate to undesirable locations such as pitfalls or hazardous areas. In\nresponse, we present DoDont (Do's and Don'ts), an instruction-based skill\ndiscovery algorithm composed of two stages. First, in an instruction learning\nstage, DoDont leverages action-free instruction videos to train an instruction\nnetwork to distinguish desirable transitions from undesirable ones. Then, in\nthe skill learning stage, the instruction network adjusts the reward function\nof the skill discovery algorithm to weight the desired behaviors. Specifically,\nwe integrate the instruction network into a distance-maximizing skill discovery\nalgorithm, where the instruction network serves as the distance function.\nEmpirically, with less than 8 instruction videos, DoDont effectively learns\ndesirable behaviors and avoids undesirable ones across complex continuous\ncontrol tasks. Code and videos are available at\nhttps://mynsng.github.io/dodont/\n"", '  Online Imitation Learning methods struggle with the gap between extensive\nonline exploration space and limited expert trajectories, which hinder\nefficient exploration due to inaccurate task-aware reward estimation. Inspired\nby the findings from cognitive neuroscience that task decomposition could\nfacilitate cognitive processing for efficient learning, we hypothesize that an\nagent could estimate precise task-aware imitation rewards for efficient online\nexploration by decomposing the target task into the objectives of ""what to do""\nand the mechanisms of ""how to do"". In this work, we introduce the hybrid\nKey-state guided Online Imitation (KOI) learning approach, which leverages the\nintegration of semantic and motion key states as guidance for task-aware reward\nestimation. Initially, we utilize the visual-language models to segment the\nexpert trajectory into semantic key states, indicating the objectives of ""what\nto do"". Within the intervals between semantic key states, optical flow is\nemployed to capture motion key states to understand the process of ""how to do"".\nBy integrating a thorough grasp of both semantic and motion key states, we\nrefine the trajectory-matching reward computation, encouraging task-aware\nexploration for efficient online imitation learning. Our experiment results\nprove that our method is more sample efficient in the Meta-World and LIBERO\nenvironments. We also conduct real-world robotic manipulation experiments to\nvalidate the efficacy of our method, demonstrating the practical applicability\nof our KOI method.\n', '  One-shot Imitation Learning~(OSIL) aims to imbue AI agents with the ability\nto learn a new task from a single demonstration. To supervise the learning,\nOSIL typically requires a prohibitively large number of paired expert\ndemonstrations -- i.e. trajectories corresponding to different variations of\nthe same semantic task. To overcome this limitation, we introduce the\nsemi-supervised OSIL problem setting, where the learning agent is presented\nwith a large dataset of trajectories with no task labels (i.e. an unpaired\ndataset), along with a small dataset of multiple demonstrations per semantic\ntask (i.e. a paired dataset). This presents a more realistic and practical\nembodiment of few-shot learning and requires the agent to effectively leverage\nweak supervision from a large dataset of trajectories. Subsequently, we develop\nan algorithm specifically applicable to this semi-supervised OSIL setting. Our\napproach first learns an embedding space where different tasks cluster\nuniquely. We utilize this embedding space and the clustering it supports to\nself-generate pairings between trajectories in the large unpaired dataset.\nThrough empirical results on simulated control tasks, we demonstrate that OSIL\nmodels trained on such self-generated pairings are competitive with OSIL models\ntrained with ground-truth labels, presenting a major advancement in the\nlabel-efficiency of OSIL.\n'] , ['  Imitation learning (IL), aiming to learn optimal control policies from expert\ndemonstrations, has been an effective method for robot manipulation tasks.\nHowever, previous IL methods either only use expensive expert demonstrations\nand omit imperfect demonstrations or rely on interacting with the environment\nand learning from online experiences. In the context of robotic manipulation,\nwe aim to conquer the above two challenges and propose a novel framework named\nSimilarity Weighted Behavior Transformer (SWBT). SWBT effectively learn from\nboth expert and imperfect demonstrations without interaction with environments.\nWe reveal that the easy-to-get imperfect demonstrations, such as forward and\ninverse dynamics, significantly enhance the network by learning fruitful\ninformation. To the best of our knowledge, we are the first to attempt to\nintegrate imperfect demonstrations into the offline imitation learning setting\nfor robot manipulation tasks. Extensive experiments on the ManiSkill2 benchmark\nbuilt on the high-fidelity Sapien simulator and real-world robotic manipulation\ntasks demonstrated that the proposed method can extract better features and\nimprove the success rates for all tasks. Our code will be released upon\nacceptance of the paper.\n', ""  Reinforcement Learning (RL) has achieved great success in sequential\ndecision-making problems, but often at the cost of a large number of\nagent-environment interactions. To improve sample efficiency, methods like\nReinforcement Learning from Expert Demonstrations (RLED) introduce external\nexpert demonstrations to facilitate agent exploration during the learning\nprocess. In practice, these demonstrations, which are often collected from\nhuman users, are costly and hence often constrained to a limited amount. How to\nselect the best set of human demonstrations that is most beneficial for\nlearning therefore becomes a major concern. This paper presents EARLY (Episodic\nActive Learning from demonstration querY), an algorithm that enables a learning\nagent to generate optimized queries of expert demonstrations in a\ntrajectory-based feature space. Based on a trajectory-level estimate of\nuncertainty in the agent's current policy, EARLY determines the optimized\ntiming and content for feature-based queries. By querying episodic\ndemonstrations as opposed to isolated state-action pairs, EARLY improves the\nhuman teaching experience and achieves better learning performance. We validate\nthe effectiveness of our method in three simulated navigation tasks of\nincreasing difficulty. The results show that our method is able to achieve\nexpert-level performance for all three tasks with convergence over 30\\% faster\nthan other baseline methods when demonstrations are generated by simulated\noracle policies. The results of a follow-up pilot user study (N=18) further\nvalidate that our method can still maintain a significantly better convergence\nin the case of human expert demonstrators while achieving a better user\nexperience in perceived task load and consuming significantly less human time.\n"", '  Most reinforcement learning (RL) methods focus on learning optimal policies\nover low-level action spaces. While these methods can perform well in their\ntraining environments, they lack the flexibility to transfer to new tasks.\nInstead, RL agents that can act over useful, temporally extended skills rather\nthan low-level actions can learn new tasks more easily. Prior work in\nskill-based RL either requires expert supervision to define useful skills,\nwhich is hard to scale, or learns a skill-space from offline data with\nheuristics that limit the adaptability of the skills, making them difficult to\ntransfer during downstream RL. Our approach, EXTRACT, instead utilizes\npre-trained vision language models to extract a discrete set of semantically\nmeaningful skills from offline data, each of which is parameterized by\ncontinuous arguments, without human supervision. This skill parameterization\nallows robots to learn new tasks by only needing to learn when to select a\nspecific skill and how to modify its arguments for the specific task. We\ndemonstrate through experiments in sparse-reward, image-based, robot\nmanipulation environments that EXTRACT can more quickly learn new tasks than\nprior works, with major gains in sample efficiency and performance over prior\nskill-based RL. Website at https://www.jessezhang.net/projects/extract/.\n']",Imitation Learning and Robotics,Imitation Learning and Reinforcement Exploration
58,"""On-Device Learning for Nano-Drones"" , Soft Robotics and Robot Learning","['drones', 'drone', 'unmanned', 'uavs', 'nano', 'dronet', 'aerial', 'onboard', 'pose', 'chip'] , ['robotic', 'robotics', 'robot', 'robots', 'manipulators', 'grasping', 'softmac', 'manipulator', 'exoskeleton', 'mechanical']","['  Miniaturized cyber-physical systems (CPSes) powered by tiny machine learning\n(TinyML), such as nano-drones, are becoming an increasingly attractive\ntechnology. Their small form factor (i.e., ~10cm diameter) ensures vast\napplicability, ranging from the exploration of narrow disaster scenarios to\nsafe human-robot interaction. Simple electronics make these CPSes inexpensive,\nbut strongly limit the computational, memory, and sensing resources available\non board. In real-world applications, these limitations are further exacerbated\nby domain shift. This fundamental machine learning problem implies that model\nperception performance drops when moving from the training domain to a\ndifferent deployment one. To cope with and mitigate this general problem, we\npresent a novel on-device fine-tuning approach that relies only on the limited\nultra-low power resources available aboard nano-drones. Then, to overcome the\nlack of ground-truth training labels aboard our CPS, we also employ a\nself-supervised method based on ego-motion consistency. Albeit our work builds\non top of a specific real-world vision-based human pose estimation task, it is\nwidely applicable for many embedded TinyML use cases. Our 512-image on-device\ntraining procedure is fully deployed aboard an ultra-low power GWT GAP9\nSystem-on-Chip and requires only 1MB of memory while consuming as low as 19mW\nor running in just 510ms (at 38mW). Finally, we demonstrate the benefits of our\non-device learning approach by field-testing our closed-loop CPS, showing a\nreduction in horizontal position error of up to 26% vs. a non-fine-tuned\nstate-of-the-art baseline. In the most challenging never-seen-before\nenvironment, our on-device learning procedure makes the difference between\nsucceeding or failing the mission.\n', '  Sub-\\SI{50}{\\gram} nano-drones are gaining momentum in both academia and\nindustry. Their most compelling applications rely on onboard deep learning\nmodels for perception despite severe hardware constraints (\\ie\nsub-\\SI{100}{\\milli\\watt} processor). When deployed in unknown environments not\nrepresented in the training data, these models often underperform due to domain\nshift. To cope with this fundamental problem, we propose, for the first time,\non-device learning aboard nano-drones, where the first part of the in-field\nmission is dedicated to self-supervised fine-tuning of a pre-trained\nconvolutional neural network (CNN). Leveraging a real-world vision-based\nregression task, we thoroughly explore performance-cost trade-offs of the\nfine-tuning phase along three axes: \\textit{i}) dataset size (more data\nincreases the regression performance but requires more memory and longer\ncomputation); \\textit{ii}) methodologies (\\eg fine-tuning all model parameters\nvs. only a subset); and \\textit{iii}) self-supervision strategy. Our approach\ndemonstrates an improvement in mean absolute error up to 30\\% compared to the\npre-trained baseline, requiring only \\SI{22}{\\second} fine-tuning on an\nultra-low-power GWT GAP9 System-on-Chip. Addressing the domain shift problem\nvia on-device learning aboard nano-drones not only marks a novel result for\nhardware-limited robots but lays the ground for more general advancements for\nthe entire robotics community.\n', ""  Sub-10cm diameter nano-drones are gaining momentum thanks to their\napplicability in scenarios prevented to bigger flying drones, such as in narrow\nenvironments and close to humans. However, their tiny form factor also brings\ntheir major drawback: ultra-constrained memory and processors for the onboard\nexecution of their perception pipelines. Therefore, lightweight deep\nlearning-based approaches are becoming increasingly popular, stressing how\ncomputational efficiency and energy-saving are paramount as they can make the\ndifference between a fully working closed-loop system and a failing one. In\nthis work, to maximize the exploitation of the ultra-limited resources aboard\nnano-drones, we present a novel adaptive deep learning-based mechanism for the\nefficient execution of a vision-based human pose estimation task. We leverage\ntwo State-of-the-Art (SoA) convolutional neural networks (CNNs) with different\nregression performance vs. computational costs trade-offs. By combining these\nCNNs with three novel adaptation strategies based on the output's temporal\nconsistency and on auxiliary tasks to swap the CNN being executed proactively,\nwe present six different systems. On a real-world dataset and the actual\nnano-drone hardware, our best-performing system, compared to executing only the\nbigger and most accurate SoA model, shows 28% latency reduction while keeping\nthe same mean absolute error (MAE), 3% MAE reduction while being iso-latency,\nand the absolute peak performance, i.e., 6% better than SoA model.\n""] , ['  Soft robots have many advantages over rigid robots thanks to their compliant\nand passive nature. However, it is generally challenging to model the dynamics\nof soft robots due to their high spatial dimensionality, making it difficult to\nuse model-based methods to accurately control soft robots. It often requires\ndirect numerical simulation of partial differential equations to simulate soft\nrobots. This not only requires an accurate numerical model, but also makes soft\nrobot modeling slow and expensive. Deep learning algorithms have shown promises\nin data-driven modeling of soft robots. However, these algorithms usually\nrequire a large amount of data, which are difficult to obtain in either\nsimulation or real-world experiments of soft robots. In this work, we propose\nKNODE-Cosserat, a framework that combines first-principle physics models and\nneural ordinary differential equations. We leverage the best from both worlds\n-- the generalization ability of physics-based models and the fast speed of\ndeep learning methods. We validate our framework in both simulation and\nreal-world experiments. In both cases, we show that the robot model\nsignificantly improves over the baseline models under different metrics.\n', ""  The safety and accuracy of robotic navigation hold paramount importance,\nespecially in the realm of soft continuum robotics, where the limitations of\ntraditional rigid sensors become evident. Encoders, piezoresistive, and\npotentiometer sensors often fail to integrate well with the flexible nature of\nthese robots, adding unwanted bulk and rigidity. To overcome these hurdles, our\nstudy presents a new approach to shape sensing in soft continuum robots through\nthe use of soft e-textile resistive sensors. This sensor, designed to\nflawlessly integrate with the robot's structure, utilizes a resistive material\nthat adjusts its resistance in response to the robot's movements and\ndeformations. This adjustment facilitates the capture of multidimensional force\nmeasurements across the soft sensor layers. A deep Convolutional Neural Network\n(CNN) is employed to decode the sensor signals, enabling precise estimation of\nthe robot's shape configuration based on the detailed data from the e-textile\nsensor. Our research investigates the efficacy of this e-textile sensor in\ndetermining the curvature parameters of soft continuum robots. The findings are\nencouraging, showing that the soft e-textile sensor not only matches but\npotentially exceeds the capabilities of traditional rigid sensors in terms of\nshape sensing and estimation. This advancement significantly boosts the safety\nand efficiency of robotic navigation systems.\n"", '  The dominant paradigm for end-to-end robot learning focuses on optimizing\ntask-specific objectives that solve a single robotic problem such as picking up\nan object or reaching a target position. However, recent work on high-capacity\nmodels in robotics has shown promise toward being trained on large collections\nof diverse and task-agnostic datasets of video demonstrations. These models\nhave shown impressive levels of generalization to unseen circumstances,\nespecially as the amount of data and the model complexity scale. Surgical robot\nsystems that learn from data have struggled to advance as quickly as other\nfields of robot learning for a few reasons: (1) there is a lack of existing\nlarge-scale open-source data to train models, (2) it is challenging to model\nthe soft-body deformations that these robots work with during surgery because\nsimulation cannot match the physical and visual complexity of biological\ntissue, and (3) surgical robots risk harming patients when tested in clinical\ntrials and require more extensive safety measures. This perspective article\naims to provide a path toward increasing robot autonomy in robot-assisted\nsurgery through the development of a multi-modal, multi-task,\nvision-language-action model for surgical robots. Ultimately, we argue that\nsurgical robots are uniquely positioned to benefit from general-purpose models\nand provide three guiding actions toward increased autonomy in robot-assisted\nsurgery.\n']",Robotics and Autonomous Systems,"""On-Device Learning for Nano-Drones"""
59,"""Autonomous Driving with Vision and HD Maps"" , ""Surgical Robotics and Computer Vision"" , ""Assistive Technology for Visually Impaired Individuals""","['vision', 'driving', 'lanesegnet', 'trafficvlm', 'vehicles', 'cameras', 'camera', 'vehicle', 'hdmaps', 'road'] , ['videos', 'recognition', 'surgicalpart', 'pose', 'surgeons', 'laparoscopic', 'neurosurgical', 'camera', 'vision', 'intraoperative'] , ['braille', 'impaired', 'accessibility', 'blind', 'impairments', 'disabilities', 'accessible', 'sighted', 'recognition', 'visual']","['  Lane detection is a vital task for vehicles to navigate and localize their\nposition on the road. To ensure reliable driving, lane detection models must\nhave robust generalization performance in various road environments. However,\ndespite the advanced performance in the trained domain, their generalization\nperformance still falls short of expectations due to the domain discrepancy. To\nbridge this gap, we propose a novel generative framework using HD Maps for\nSingle-Source Domain Generalization (SSDG) in lane detection. We first generate\nnumerous front-view images from lane markings of HD Maps. Next, we\nstrategically select a core subset among the generated images using (i) lane\nstructure and (ii) road surrounding criteria to maximize their diversity. In\nthe end, utilizing this core set, we train lane detection models to boost their\ngeneralization performance. We validate that our generative framework from HD\nMaps outperforms the Domain Adaptation model MLDA with +3.01%p accuracy\nimprovement, even though we do not access the target domain images.\n', '  Vision-based ego-lane inference using High-Definition (HD) maps is essential\nin autonomous driving and advanced driver assistance systems. The traditional\napproach necessitates well-calibrated cameras, which confines variation of\ncamera configuration, as the algorithm relies on intrinsic and extrinsic\ncalibration. In this paper, we propose a learning-based ego-lane inference by\ndirectly estimating the ego-lane index from a single image. To enhance robust\nperformance, our model incorporates the two-head structure inferring ego-lane\nin two perspectives simultaneously. Furthermore, we utilize an attention\nmechanism guided by vanishing point-and-line to adapt to changes in viewpoint\nwithout requiring accurate calibration. The high adaptability of our model was\nvalidated in diverse environments, devices, and camera mounting points and\norientations.\n', '  Sharing and joint processing of camera feeds and sensor measurements, known\nas Cooperative Perception (CP), has emerged as a new technique to achieve\nhigher perception qualities. CP can enhance the safety of Autonomous Vehicles\n(AVs) where their individual visual perception quality is compromised by\nadverse weather conditions (haze as foggy weather), low illumination, winding\nroads, and crowded traffic. To cover the limitations of former methods, in this\npaper, we propose a novel approach to realize an optimized CP under constrained\ncommunications. At the core of our approach is recruiting the best helper from\nthe available list of front vehicles to augment the visual range and enhance\nthe Object Detection (OD) accuracy of the ego vehicle. In this two-step\nprocess, we first select the helper vehicles that contribute the most to CP\nbased on their visual range and lowest motion blur. Next, we implement a radio\nblock optimization among the candidate vehicles to further improve\ncommunication efficiency. We specifically focus on pedestrian detection as an\nexemplary scenario. To validate our approach, we used the CARLA simulator to\ncreate a dataset of annotated videos for different driving scenarios where\npedestrian detection is challenging for an AV with compromised vision. Our\nresults demonstrate the efficacy of our two-step optimization process in\nimproving the overall performance of cooperative perception in challenging\nscenarios, substantially improving driving safety under adverse conditions.\nFinally, we note that the networking assumptions are adopted from LTE Release\n14 Mode 4 side-link communication, commonly used for Vehicle-to-Vehicle (V2V)\ncommunication. Nonetheless, our method is flexible and applicable to arbitrary\nV2V communications.\n'] , ['  Purpose: Depth estimation in robotic surgery is vital in 3D reconstruction,\nsurgical navigation and augmented reality visualization. Although the\nfoundation model exhibits outstanding performance in many vision tasks,\nincluding depth estimation (e.g., DINOv2), recent works observed its\nlimitations in medical and surgical domain-specific applications. This work\npresents a low-ranked adaptation (LoRA) of the foundation model for surgical\ndepth estimation. Methods: We design a foundation model-based depth estimation\nmethod, referred to as Surgical-DINO, a low-rank adaptation of the DINOv2 for\ndepth estimation in endoscopic surgery. We build LoRA layers and integrate them\ninto DINO to adapt with surgery-specific domain knowledge instead of\nconventional fine-tuning. During training, we freeze the DINO image encoder,\nwhich shows excellent visual representation capacity, and only optimize the\nLoRA layers and depth decoder to integrate features from the surgical scene.\nResults: Our model is extensively validated on a MICCAI challenge dataset of\nSCARED, which is collected from da Vinci Xi endoscope surgery. We empirically\nshow that Surgical-DINO significantly outperforms all the state-of-the-art\nmodels in endoscopic depth estimation tasks. The analysis with ablation studies\nhas shown evidence of the remarkable effect of our LoRA layers and adaptation.\nConclusion: Surgical-DINO shed some light on the successful adaptation of the\nfoundation models into the surgical domain for depth estimation. There is clear\nevidence in the results that zero-shot prediction on pre-trained weights in\ncomputer vision datasets or naive fine-tuning is not sufficient to use the\nfoundation model in the surgical domain directly. Code is available at\nhttps://github.com/BeileiCui/SurgicalDINO.\n', '  Phase recognition in surgical videos is crucial for enhancing computer-aided\nsurgical systems as it enables automated understanding of sequential procedural\nstages. Existing methods often rely on fixed temporal windows for video\nanalysis to identify dynamic surgical phases. Thus, they struggle to\nsimultaneously capture short-, mid-, and long-term information necessary to\nfully understand complex surgical procedures. To address these issues, we\npropose Multi-Scale Transformers for Surgical Phase Recognition (MuST), a novel\nTransformer-based approach that combines a Multi-Term Frame encoder with a\nTemporal Consistency Module to capture information across multiple temporal\nscales of a surgical video. Our Multi-Term Frame Encoder computes\ninterdependencies across a hierarchy of temporal scales by sampling sequences\nat increasing strides around the frame of interest. Furthermore, we employ a\nlong-term Transformer encoder over the frame embeddings to further enhance\nlong-term reasoning. MuST achieves higher performance than previous\nstate-of-the-art methods on three different public benchmarks.\n', ""  Automation in surgical robotics has the potential to improve patient safety\nand surgical efficiency, but it is difficult to achieve due to the need for\nrobust perception algorithms. In particular, 6D pose estimation of surgical\ninstruments is critical to enable the automatic execution of surgical maneuvers\nbased on visual feedback. In recent years, supervised deep learning algorithms\nhave shown increasingly better performance at 6D pose estimation tasks; yet,\ntheir success depends on the availability of large amounts of annotated data.\nIn household and industrial settings, synthetic data, generated with 3D\ncomputer graphics software, has been shown as an alternative to minimize\nannotation costs of 6D pose datasets. However, this strategy does not translate\nwell to surgical domains as commercial graphics software have limited tools to\ngenerate images depicting realistic instrument-tissue interactions. To address\nthese limitations, we propose an improved simulation environment for surgical\nrobotics that enables the automatic generation of large and diverse datasets\nfor 6D pose estimation of surgical instruments. Among the improvements, we\ndeveloped an automated data generation pipeline and an improved surgical scene.\nTo show the applicability of our system, we generated a dataset of 7.5k images\nwith pose annotations of a surgical needle that was used to evaluate a\nstate-of-the-art pose estimation network. The trained model obtained a mean\ntranslational error of 2.59mm on a challenging dataset that presented varying\nlevels of occlusion. These results highlight our pipeline's success in training\nand evaluating novel vision algorithms for surgical robotics applications.\n""] , ['  In recent years, advancements in Natural Language Processing (NLP) techniques\nhave revolutionized the field of accessibility and exclusivity of testing,\nparticularly for visually impaired students (VIS). CBT has shown in years back\nits relevance in terms of administering exams electronically, making the test\nprocess easier, providing quicker and more accurate results, and offering\ngreater flexibility and accessibility for candidates. Yet, its relevance was\nnot felt by the visually impaired students as they cannot access printed\ndocuments. Hence, in this paper, we present an NLP-driven Computer-Based Test\nguide for visually impaired students. It employs a speech technology\npre-trained methods to provide real-time assistance and support to visually\nimpaired students. The system utilizes NLP technologies to convert the\ntext-based questions and the associated options in a machine-readable format.\nSubsequently, the speech technology pre-trained model processes the converted\ntext enabling the VIS to comprehend and analyze the content. Furthermore, we\nvalidated that this pre-trained model is not perverse by testing for accuracy\nusing sample audio datasets labels (A, B, C, D, E, F, G) to compare with the\nvoice recordings obtained from 20 VIS which is been predicted by the system to\nattain values for precision, recall, and F1-scores. These metrics are used to\nassess the performance of the pre-trained model and have indicated that it is\nproficient enough to give its better performance to the evaluated system. The\nmethodology adopted for this system is Object Oriented Analysis and Design\nMethodology (OOADM) where Objects are discussed and built by modeling\nreal-world instances.\n', '  Visually impaired people are a large group who can only use braille for\nreading and writing. However, the lack of special educational resources is the\nbottleneck for educating them. Educational equity is a reflection of the level\nof social civilization, cultural equality, and individual dignity. Facilitating\nand improving lifelong learning channels for the visually impaired is of great\nsignificance. Their written braille homework or exam papers cannot be\nunderstood by sighted teachers, because of the lack of a highly accurate\nbraille translation system, especially in Chinese which has tone marks. braille\nwriters often omit tone marks to save space, leading to confusion when braille\nwith the same consonants and vowels is translated into Chinese. Previous\nalgorithms were insufficient in extracting contextual information, resulting in\nlow accuracy of braille translations into Chinese. This project informatively\nfine-tuned the mT5 model with an Encoder-decoder architecture for braille to\nChinese character conversion. This research created a training set of braille\nand corresponding Chinese text from the Leipzig Corpora. This project\nsignificantly reduced the confusion in braille, achieving $62.4$ and $62.3$\nBLEU scores in the validation and test sets, with a curriculum learning\nfine-tuning method. By incorporating the braille recognition algorithm, this\nproject is the first publicly available braille translation system and can\nbenefit lots of visually impaired students and families who are preparing for\nthe Chinese College Test and help to propel their college dreams in the future.\nThere is a demo on our homepage\\footnote{\\url{https://vision-braille.com/}}.\n', ""  The prevalence of mobile technology offers unique opportunities for\naddressing healthcare challenges, especially for individuals with visual\nimpairments. This paper explores the development and implementation of a deep\nlearning-based mobile application designed to assist blind and visually\nimpaired individuals in real-time pill identification. Utilizing the YOLO\nframework, the application aims to accurately recognize and differentiate\nbetween various pill types through real-time image processing on mobile\ndevices. The system incorporates Text-to- Speech (TTS) to provide immediate\nauditory feedback, enhancing usability and independence for visually impaired\nusers. Our study evaluates the application's effectiveness in terms of\ndetection accuracy and user experience, highlighting its potential to improve\nmedication management and safety among the visually impaired community.\nKeywords-Deep Learning; YOLO Framework; Mobile Application; Visual Impairment;\nPill Identification; Healthcare\n""]",Computer Vision Applications,"""Surgical Robotics and Computer Vision"""
60,"""Robot Manipulation and Embodied Intelligence"" , Human-Robot Interaction and Robotics , ""Robot Motion Planning and Task Planning""","['robotic', 'robot', 'robotics', 'robots', 'grasping', 'manipulator', 'teleoperation', 'embodied', 'grasp', 'humanoid'] , ['robot', 'robots', 'robotic', 'robotics', 'humanoid', 'interactive', 'cobots', 'gestures', 'conversations', 'cobot'] , ['planning', 'robotics', 'planner', 'robotic', 'robot', 'planners', 'robots', 'motions', 'motion', 'trajectory']","[""  Large Language Models (LLMs) have been recently used in robot applications\nfor grounding LLM common-sense reasoning with the robot's perception and\nphysical abilities. In humanoid robots, memory also plays a critical role in\nfostering real-world embodiment and facilitating long-term interactive\ncapabilities, especially in multi-task setups where the robot must remember\nprevious task states, environment states, and executed actions. In this paper,\nwe address incorporating memory processes with LLMs for generating cross-task\nrobot actions, while the robot effectively switches between tasks. Our proposed\ndual-layered architecture features two LLMs, utilizing their complementary\nskills of reasoning and following instructions, combined with a memory model\ninspired by human cognition. Our results show a significant improvement in\nperformance over a baseline of five robotic tasks, demonstrating the potential\nof integrating memory with LLMs for combining the robot's action and perception\nfor adaptive task execution.\n"", '  Considering how to make the model accurately understand and follow natural\nlanguage instructions and perform actions consistent with world knowledge is a\nkey challenge in robot manipulation. This mainly includes human fuzzy\ninstruction reasoning and the following of physical knowledge. Therefore, the\nembodied intelligence agent must have the ability to model world knowledge from\ntraining data. However, most existing vision and language robot manipulation\nmethods mainly operate in less realistic simulator and language settings and\nlack explicit modeling of world knowledge. To bridge this gap, we introduce a\nnovel and simple robot manipulation framework, called Surfer. It is based on\nthe world model, treats robot manipulation as a state transfer of the visual\nscene, and decouples it into two parts: action and scene. Then, the\ngeneralization ability of the model on new instructions and new scenes is\nenhanced by explicit modeling of the action and scene prediction in multi-modal\ninformation. In addition to the framework, we also built a robot manipulation\nsimulator that supports full physics execution based on the MuJoCo physics\nengine. It can automatically generate demonstration training data and test\ndata, effectively reducing labor costs. To conduct a comprehensive and\nsystematic evaluation of the robot manipulation model in terms of language\nunderstanding and physical execution, we also created a robotic manipulation\nbenchmark with progressive reasoning tasks, called SeaWave. It contains 4\nlevels of progressive reasoning tasks and can provide a standardized testing\nplatform for embedded AI agents in multi-modal environments. On average, Surfer\nachieved a success rate of 54.74% on the defined four levels of manipulation\ntasks, exceeding the best baseline performance of 47.64%.\n', '  To substantially enhance robot intelligence, there is a pressing need to\ndevelop a large model that enables general-purpose robots to proficiently\nundertake a broad spectrum of manipulation tasks, akin to the versatile\ntask-planning ability exhibited by LLMs. The vast diversity in objects, robots,\nand manipulation tasks presents huge challenges. Our work introduces a\ncomprehensive framework to develop a foundation model for general robotic\nmanipulation that formalizes a manipulation task as contact synthesis.\nSpecifically, our model takes as input object and robot manipulator point\nclouds, object physical attributes, target motions, and manipulation region\nmasks. It outputs contact points on the object and associated contact forces or\npost-contact motions for robots to achieve the desired manipulation task. We\nperform extensive experiments both in the simulation and real-world settings,\nmanipulating articulated rigid objects, rigid objects, and deformable objects\nthat vary in dimensionality, ranging from one-dimensional objects like ropes to\ntwo-dimensional objects like cloth and extending to three-dimensional objects\nsuch as plasticine. Our model achieves average success rates of around 90\\%.\nSupplementary materials and videos are available on our project website at\nhttps://manifoundationmodel.github.io/.\n'] , ['  With robotics rapidly advancing, more effective human-robot interaction is\nincreasingly needed to realize the full potential of robots for society. While\nspoken language must be part of the solution, our ability to provide spoken\nlanguage interaction capabilities is still very limited. The National Science\nFoundation accordingly convened a workshop, bringing together speech, language,\nand robotics researchers to discuss what needs to be done. The result is this\nreport, in which we identify key scientific and engineering advances needed.\n  Our recommendations broadly relate to eight general themes. First, meeting\nhuman needs requires addressing new challenges in speech technology and user\nexperience design. Second, this requires better models of the social and\ninteractive aspects of language use. Third, for robustness, robots need\nhigher-bandwidth communication with users and better handling of uncertainty,\nincluding simultaneous consideration of multiple hypotheses and goals. Fourth,\nmore powerful adaptation methods are needed, to enable robots to communicate in\nnew environments, for new tasks, and with diverse user populations, without\nextensive re-engineering or the collection of massive training data. Fifth,\nsince robots are embodied, speech should function together with other\ncommunication modalities, such as gaze, gesture, posture, and motion. Sixth,\nsince robots operate in complex environments, speech components need access to\nrich yet efficient representations of what the robot knows about objects,\nlocations, noise sources, the user, and other humans. Seventh, since robots\noperate in real time, their speech and language processing components must\nalso. Eighth, in addition to more research, we need more work on infrastructure\nand resources, including shareable software modules and internal interfaces,\ninexpensive hardware, baseline systems, and diverse corpora.\n', ""  Users develop mental models of robots to conceptualize what kind of\ninteractions they can have with those robots. The conceptualizations are often\nformed before interactions with the robot and are based only on observing the\nrobot's physical design. As a result, understanding conceptualizations formed\nfrom physical design is necessary to understand how users intend to interact\nwith the robot. We propose to use multimodal features of robot embodiments to\npredict what kinds of expectations users will have about a given robot's social\nand physical capabilities. We show that using such features provides\ninformation about general mental models of the robots that generalize across\nsocially interactive robots. We describe how these models can be incorporated\ninto interaction design and physical design for researchers working with\nsocially interactive robots.\n"", ""  Assistive robots have attracted significant attention due to their potential\nto enhance the quality of life for vulnerable individuals like the elderly. The\nconvergence of computer vision, large language models, and robotics has\nintroduced the `visuolinguomotor' mode for assistive robots, where visuals and\nlinguistics are incorporated into assistive robots to enable proactive and\ninteractive assistance. This raises the question: \\textit{In circumstances\nwhere visuals become unreliable or unavailable, can we rely solely on language\nto control robots, i.e., the viability of the `linguomotor` mode for assistive\nrobots?} This work takes the initial steps to answer this question by: 1)\nevaluating the responses of assistive robots to language prompts of varying\ngranularities; and 2) exploring the necessity and feasibility of controlling\nthe robot on-the-fly. We have designed and conducted experiments on a Sawyer\ncobot to support our arguments. A Turtlebot robot case is designed to\ndemonstrate the adaptation of the solution to scenarios where assistive robots\nneed to maneuver to assist. Codes will be released on GitHub soon to benefit\nthe community.\n""] , ['  Conventional Task and Motion Planning (TAMP) approaches rely on manually\ncrafted interfaces connecting symbolic task planning with continuous motion\ngeneration. These domain-specific and labor-intensive modules are limited in\naddressing emerging tasks in real-world settings. Here, we present LLM^3, a\nnovel Large Language Model (LLM)-based TAMP framework featuring a\ndomain-independent interface. Specifically, we leverage the powerful reasoning\nand planning capabilities of pre-trained LLMs to propose symbolic action\nsequences and select continuous action parameters for motion planning.\nCrucially, LLM^3 incorporates motion planning feedback through prompting,\nallowing the LLM to iteratively refine its proposals by reasoning about motion\nfailure. Consequently, LLM^3 interfaces between task planning and motion\nplanning, alleviating the intricate design process of handling domain-specific\nmessages between them. Through a series of simulations in a box-packing domain,\nwe quantitatively demonstrate the effectiveness of LLM^3 in solving TAMP\nproblems and the efficiency in selecting action parameters. Ablation studies\nunderscore the significant contribution of motion failure reasoning to the\nsuccess of LLM^3. Furthermore, we conduct qualitative experiments on a physical\nmanipulator, demonstrating the practical applicability of our approach in\nreal-world settings.\n', '  To control how a robot moves, motion planning algorithms must compute paths\nin high-dimensional state spaces while accounting for physical constraints\nrelated to motors and joints, generating smooth and stable motions, avoiding\nobstacles, and preventing collisions. A motion planning algorithm must\ntherefore balance competing demands, and should ideally incorporate uncertainty\nto handle noise, model errors, and facilitate deployment in complex\nenvironments. To address these issues, we introduce a framework for robot\nmotion planning based on variational Gaussian processes, which unifies and\ngeneralizes various probabilistic-inference-based motion planning algorithms,\nand connects them with optimization-based planners. Our framework provides a\nprincipled and flexible way to incorporate equality-based, inequality-based,\nand soft motion-planning constraints during end-to-end training, is\nstraightforward to implement, and provides both interval-based and\nMonte-Carlo-based uncertainty estimates. We conduct experiments using different\nenvironments and robots, comparing against baseline approaches based on the\nfeasibility of the planned paths, and obstacle avoidance quality. Results show\nthat our proposed approach yields a good balance between success rates and path\nquality.\n', '  Learning priors on trajectory distributions can help accelerate robot motion\nplanning optimization. Given previously successful plans, learning trajectory\ngenerative models as priors for a new planning problem is highly desirable.\nPrior works propose several ways on utilizing this prior to bootstrapping the\nmotion planning problem. Either sampling the prior for initializations or using\nthe prior distribution in a maximum-a-posterior formulation for trajectory\noptimization. In this work, we propose learning diffusion models as priors. We\nthen can sample directly from the posterior trajectory distribution conditioned\non task goals, by leveraging the inverse denoising process of diffusion models.\nFurthermore, diffusion has been recently shown to effectively encode data\nmultimodality in high-dimensional settings, which is particularly well-suited\nfor large trajectory dataset. To demonstrate our method efficacy, we compare\nour proposed method - Motion Planning Diffusion - against several baselines in\nsimulated planar robot and 7-dof robot arm manipulator environments. To assess\nthe generalization capabilities of our method, we test it in environments with\npreviously unseen obstacles. Our experiments show that diffusion models are\nstrong priors to encode high-dimensional trajectory distributions of robot\nmotions.\n']",Robotics and Artificial Intelligence,"""Robot Manipulation and Embodied Intelligence"""
61,"""Legged Robot Locomotion and Navigation"" , ""Robot Navigation in Crowded Environments"" , Hippocampal-inspired models for navigation and planning , ""Robot Navigation and Exploration in Unknown Environments""","['locomotion', 'stepping', 'terrain', 'legged', 'robotics', 'walking', 'jumps', 'terrains', 'jumping', 'robotic'] , ['robot', 'robots', 'robotic', 'planning', 'autonomous', 'navigate', 'navigating', 'pedestrians', 'navigation', 'agent'] , ['hippocampal', 'planning', 'hippocampus', 'exploration', 'cortex', 'cognitive', 'environments', 'reinforcement', 'behaviors', 'adaptive'] , ['navigate', 'navigating', 'navigation', 'planning', 'robotics', 'robot', 'terrain', 'slam', 'obstacle', 'exploration']","[""  Autonomous wheeled-legged robots have the potential to transform logistics\nsystems, improving operational efficiency and adaptability in urban\nenvironments. Navigating urban environments, however, poses unique challenges\nfor robots, necessitating innovative solutions for locomotion and navigation.\nThese challenges include the need for adaptive locomotion across varied\nterrains and the ability to navigate efficiently around complex dynamic\nobstacles. This work introduces a fully integrated system comprising adaptive\nlocomotion control, mobility-aware local navigation planning, and large-scale\npath planning within the city. Using model-free reinforcement learning (RL)\ntechniques and privileged learning, we develop a versatile locomotion\ncontroller. This controller achieves efficient and robust locomotion over\nvarious rough terrains, facilitated by smooth transitions between walking and\ndriving modes. It is tightly integrated with a learned navigation controller\nthrough a hierarchical RL framework, enabling effective navigation through\nchallenging terrain and various obstacles at high speed. Our controllers are\nintegrated into a large-scale urban navigation system and validated by\nautonomous, kilometer-scale navigation missions conducted in Zurich,\nSwitzerland, and Seville, Spain. These missions demonstrate the system's\nrobustness and adaptability, underscoring the importance of integrated control\nsystems in achieving seamless navigation in complex environments. Our findings\nsupport the feasibility of wheeled-legged robots and hierarchical RL for\nautonomous navigation, with implications for last-mile delivery and beyond.\n"", '  While most recent advancements in legged robot control have been driven by\nmodel-free reinforcement learning, we explore the potential of differentiable\nsimulation. Differentiable simulation promises faster convergence and more\nstable training by computing low-variant first-order gradients using the robot\nmodel, but so far, its use for legged robot control has remained limited to\nsimulation. The main challenge with differentiable simulation lies in the\ncomplex optimization landscape of robotic tasks due to discontinuities in\ncontact-rich environments, e.g., quadruped locomotion. This work proposes a\nnew, differentiable simulation framework to overcome these challenges. The key\nidea involves decoupling the complex whole-body simulation, which may exhibit\ndiscontinuities due to contact, into two separate continuous domains.\nSubsequently, we align the robot state resulting from the simplified model with\na more precise, non-differentiable simulator to maintain sufficient simulation\naccuracy. Our framework enables learning quadruped walking in minutes using a\nsingle simulated robot without any parallelization. When augmented with GPU\nparallelization, our approach allows the quadruped robot to master diverse\nlocomotion skills, including trot, pace, bound, and gallop, on challenging\nterrains in minutes. Additionally, our policy achieves robust locomotion\nperformance in the real world zero-shot. To the best of our knowledge, this\nwork represents the first demonstration of using differentiable simulation for\ncontrolling a real quadruped robot. This work provides several important\ninsights into using differentiable simulations for legged locomotion in the\nreal world.\n', '  Recent advances of locomotion controllers utilizing deep reinforcement\nlearning (RL) have yielded impressive results in terms of achieving rapid and\nrobust locomotion across challenging terrain, such as rugged rocks, non-rigid\nground, and slippery surfaces. However, while these controllers primarily\naddress challenges underneath the robot, relatively little research has\ninvestigated legged mobility through confined 3D spaces, such as narrow tunnels\nor irregular voids, which impose all-around constraints. The cyclic gait\npatterns resulted from existing RL-based methods to learn parameterized\nlocomotion skills characterized by motion parameters, such as velocity and body\nheight, may not be adequate to navigate robots through challenging confined 3D\nspaces, requiring both agile 3D obstacle avoidance and robust legged\nlocomotion. Instead, we propose to learn locomotion skills end-to-end from\ngoal-oriented navigation in confined 3D spaces. To address the inefficiency of\ntracking distant navigation goals, we introduce a hierarchical locomotion\ncontroller that combines a classical planner tasked with planning waypoints to\nreach a faraway global goal location, and an RL-based policy trained to follow\nthese waypoints by generating low-level motion commands. This approach allows\nthe policy to explore its own locomotion skills within the entire solution\nspace and facilitates smooth transitions between local goals, enabling\nlong-term navigation towards distant goals. In simulation, our hierarchical\napproach succeeds at navigating through demanding confined 3D environments,\noutperforming both pure end-to-end learning approaches and parameterized\nlocomotion skills. We further demonstrate the successful real-world deployment\nof our simulation-trained controller on a real robot.\n'] , [""  Recent research on mobile robot navigation has focused on socially aware\nnavigation in crowded environments. However, existing methods do not adequately\naccount for human robot interactions and demand accurate location information\nfrom omnidirectional sensors, rendering them unsuitable for practical\napplications. In response to this need, this study introduces a novel\nalgorithm, BNBRL+, predicated on the partially observable Markov decision\nprocess framework to assess risks in unobservable areas and formulate movement\nstrategies under uncertainty. BNBRL+ consolidates belief algorithms with\nBayesian neural networks to probabilistically infer beliefs based on the\npositional data of humans. It further integrates the dynamics between the\nrobot, humans, and inferred beliefs to determine the navigation paths and\nembeds social norms within the reward function, thereby facilitating socially\naware navigation. Through experiments in various risk laden scenarios, this\nstudy validates the effectiveness of BNBRL+ in navigating crowded environments\nwith blind spots. The model's ability to navigate effectively in spaces with\nlimited visibility and avoid obstacles dynamically can significantly improve\nthe safety and reliability of autonomous vehicles.\n"", '  Learning robot navigation strategies among pedestrian is crucial for domain\nbased applications. Combining perception, planning and prediction allows us to\nmodel the interactions between robots and pedestrians, resulting in impressive\noutcomes especially with recent approaches based on deep reinforcement learning\n(RL). However, these works do not consider multi-robot scenarios. In this\npaper, we present MultiSoc, a new method for learning multi-agent socially\naware navigation strategies using RL. Inspired by recent works on multi-agent\ndeep RL, our method leverages graph-based representation of agent interactions,\ncombining the positions and fields of view of entities (pedestrians and\nagents). Each agent uses a model based on two Graph Neural Network combined\nwith attention mechanisms. First an edge-selector produces a sparse graph, then\na crowd coordinator applies node attention to produce a graph representing the\ninfluence of each entity on the others. This is incorporated into a model-free\nRL framework to learn multi-agent policies. We evaluate our approach on\nsimulation and provide a series of experiments in a set of various conditions\n(number of agents / pedestrians). Empirical results show that our method learns\nfaster than social navigation deep RL mono-agent techniques, and enables\nefficient multi-agent implicit coordination in challenging crowd navigation\nwith multiple heterogeneous humans. Furthermore, by incorporating customizable\nmeta-parameters, we can adjust the neighborhood density to take into account in\nour navigation strategy.\n', ""  Mobile robots are being used on a large scale in various crowded situations\nand become part of our society. The socially acceptable navigation behavior of\na mobile robot with individual human consideration is an essential requirement\nfor scalable applications and human acceptance. Deep Reinforcement Learning\n(DRL) approaches are recently used to learn a robot's navigation policy and to\nmodel the complex interactions between robots and humans. We propose to divide\nexisting DRL-based navigation approaches based on the robot's exhibited social\nbehavior and distinguish between social collision avoidance with a lack of\nsocial behavior and socially aware approaches with explicit predefined social\nbehavior. In addition, we propose a novel socially integrated navigation\napproach where the robot's social behavior is adaptive and emerges from the\ninteraction with humans. The formulation of our approach is derived from a\nsociological definition, which states that social acting is oriented toward the\nacting of others. The DRL policy is trained in an environment where other\nagents interact socially integrated and reward the robot's behavior\nindividually. The simulation results indicate that the proposed socially\nintegrated navigation approach outperforms a socially aware approach in terms\nof ego navigation performance while significantly reducing the negative impact\non all agents within the environment.\n""] , [""  The vertebrate hippocampus is believed to use recurrent connectivity in area\nCA3 to support episodic memory recall from partial cues. This brain area also\ncontains place cells, whose location-selective firing fields implement maps\nsupporting spatial memory. Here we show that place cells emerge in networks\ntrained to remember temporally continuous sensory episodes. We model CA3 as a\nrecurrent autoencoder that recalls and reconstructs sensory experiences from\nnoisy and partially occluded observations by agents traversing simulated rooms.\nThe agents move in realistic trajectories modeled from rodents and environments\nare modeled as high-dimensional sensory experience maps. Training our\nautoencoder to pattern-complete and reconstruct experiences with a constraint\non total activity causes spatially localized firing fields, i.e., place cells,\nto emerge in the encoding layer. The emergent place fields reproduce key\naspects of hippocampal phenomenology: a) remapping (maintenance of and\nreversion to distinct learned maps in different environments), implemented via\nrepositioning of experience manifolds in the network's hidden layer, b)\northogonality of spatial representations in different arenas, c) robust place\nfield emergence in differently shaped rooms, with single units showing multiple\nplace fields in large or complex spaces, and d) slow representational drift of\nplace fields. We argue that these results arise because continuous traversal of\nspace makes sensory experience temporally continuous. We make testable\npredictions: a) rapidly changing sensory context will disrupt place fields, b)\nplace fields will form even if recurrent connections are blocked, but reversion\nto previously learned representations upon remapping will be abolished, c) the\ndimension of temporally smooth experience sets the dimensionality of place\nfields, including during virtual navigation of abstract spaces.\n"", ""  Drawing inspiration from animal navigation strategies, we introduce a novel\ncomputational model for navigation and mapping, rooted in biologically inspired\nprinciples. Animals exhibit remarkable navigation abilities by efficiently\nusing memory, imagination, and strategic decision-making to navigate complex\nand aliased environments. Building on these insights, we integrate traditional\ncognitive mapping approaches with an Active Inference Framework (AIF) to learn\nan environment structure in a few steps. Through the incorporation of\ntopological mapping for long-term memory and AIF for navigation planning and\nstructure learning, our model can dynamically apprehend environmental\nstructures and expand its internal map with predicted beliefs during\nexploration. Comparative experiments with the Clone-Structured Graph (CSCG)\nmodel highlight our model's ability to rapidly learn environmental structures\nin a single episode, with minimal navigation overlap. this is achieved without\nprior knowledge of the dimensions of the environment or the type of\nobservations, showcasing its robustness and effectiveness in navigating\nambiguous environments.\n"", '  By dynamic planning, we refer to the ability of the human brain to infer and\nimpose motor trajectories related to cognitive decisions. A recent paradigm,\nactive inference, brings fundamental insights into the adaptation of biological\norganisms, constantly striving to minimize prediction errors to restrict\nthemselves to life-compatible states. Over the past years, many studies have\nshown how human and animal behavior could be explained in terms of an active\ninferential process - either as discrete decision-making or continuous motor\ncontrol - inspiring innovative solutions in robotics and artificial\nintelligence. Still, the literature lacks a comprehensive outlook on how to\neffectively plan actions in changing environments. Setting ourselves the goal\nof modeling tool use, we delve into the topic of dynamic planning in active\ninference, keeping in mind two crucial aspects of biological goal-directed\nbehavior: the capacity to understand and exploit affordances for object\nmanipulation, and to learn the hierarchical interactions between the self and\nthe environment, including other agents. We start from a simple unit and\ngradually describe more advanced structures, comparing recently proposed design\nchoices and providing basic examples for each section. This study distances\nitself from traditional views centered on neural networks and reinforcement\nlearning, and points toward a yet unexplored direction in active inference:\nhybrid representations in hierarchical models.\n'] , ['  Autonomous robots exploring unknown environments face a significant\nchallenge: navigating effectively without prior maps and with limited external\nfeedback. This challenge intensifies in sparse reward environments, where\ntraditional exploration techniques often fail. In this paper, we present\nTopoNav, a novel topological navigation framework that integrates active\nmapping, hierarchical reinforcement learning, and intrinsic motivation to\nenable efficient goal-oriented exploration and navigation in sparse-reward\nsettings. TopoNav dynamically constructs a topological map of the environment,\ncapturing key locations and pathways. A two-level hierarchical policy\narchitecture, comprising a high-level graph traversal policy and low-level\nmotion control policies, enables effective navigation and obstacle avoidance\nwhile maintaining focus on the overall goal. Additionally, TopoNav incorporates\nintrinsic motivation to guide exploration toward relevant regions and frontier\nnodes in the topological map, addressing the challenges of sparse extrinsic\nrewards. We evaluate TopoNav both in the simulated and real-world off-road\nenvironments using a Clearpath Jackal robot, across three challenging\nnavigation scenarios: goal-reaching, feature-based navigation, and navigation\nin complex terrains. We observe an increase in exploration coverage by 7- 20%,\nin success rates by 9-19%, and reductions in navigation times by 15-36% across\nvarious scenarios, compared to state-of-the-art methods\n', ""  Recent results suggest that splitting topological navigation into\nrobot-independent and robot-specific components improves navigation performance\nby enabling the robot-independent part to be trained with data collected by\nrobots of different types. However, the navigation methods' performance is\nstill limited by the scarcity of suitable training data and they suffer from\npoor computational scaling.\n  In this work, we present PlaceNav, subdividing the robot-independent part\ninto navigation-specific and generic computer vision components. We utilize\nvisual place recognition for the subgoal selection of the topological\nnavigation pipeline. This makes subgoal selection more efficient and enables\nleveraging large-scale datasets from non-robotics sources, increasing training\ndata availability. Bayesian filtering, enabled by place recognition, further\nimproves navigation performance by increasing the temporal consistency of\nsubgoals. Our experimental results verify the design and the new method obtains\na 76% higher success rate in indoor and 23% higher in outdoor navigation tasks\nwith higher computational efficiency.\n"", '  Legged navigation is typically examined within open-world, off-road, and\nchallenging environments. In these scenarios, estimating external disturbances\nrequires a complex synthesis of multi-modal information. This underlines a\nmajor limitation in existing works that primarily focus on avoiding obstacles.\nIn this work, we propose TOP-Nav, a novel legged navigation framework that\nintegrates a comprehensive path planner with Terrain awareness, Obstacle\navoidance and close-loop Proprioception. TOP-Nav underscores the synergies\nbetween vision and proprioception in both path and motion planning. Within the\npath planner, we present and integrate a terrain estimator that enables the\nrobot to select waypoints on terrains with higher traversability while\neffectively avoiding obstacles. In the motion planning level, we not only\nimplement a locomotion controller to track the navigation commands, but also\nconstruct a proprioception advisor to provide motion evaluations for the path\nplanner. Based on the close-loop motion feedback, we make online corrections\nfor the vision-based terrain and obstacle estimations. Consequently, TOP-Nav\nachieves open-world navigation that the robot can handle terrains or\ndisturbances beyond the distribution of prior knowledge and overcomes\nconstraints imposed by visual conditions. Building upon extensive experiments\nconducted in both simulation and real-world environments, TOP-Nav demonstrates\nsuperior performance in open-world navigation compared to existing methods.\n']",Robot Navigation and Locomotion,"""Robot Navigation and Exploration in Unknown Environments"""
62,"Federated Learning for Collaborative Model Training , Wireless Federated Learning with AirComp and Digital Modulation , Federated Learning Fairness , Federated Learning Algorithms and Convergence Guarantees , Federated Learning for Vehicular Networks , Multimodal Federated Learning with Missing Modalities , Federated Learning for Distributed Systems , Federated Learning for Edge Computing and IoT , Federated Learning with Asynchronous and Adaptive Methods","['federated', 'collaborative', 'learning', 'collaboratively', 'distributed', 'generalization', 'datasets', 'classifier', 'clients', 'sharing'] , ['wireless', 'decoding', 'transmit', 'bandwidth', 'channels', 'channel', 'broadcast', 'modulation', 'aircomp', 'quantization'] , ['federated', 'fairness', 'fairfrs', 'distributed', 'collaborative', 'incentive', 'collaboratively', 'fedms', 'equitable', 'decentralized'] , ['federated', 'learning', 'distributed', 'generalization', 'adaptive', 'algorithms', 'training', 'centralized', 'fedams', 'gradients'] , ['vehicular', 'supervised', 'vehicles', 'vehicle', 'driving', 'autonomous', 'cloud', 'federated', 'learning', 'networks'] , ['multimodal', 'modality', 'crossmodal', 'federated', 'learning', 'fedmml', 'modal', 'modalities', 'regularization', 'distributed'] , ['federated', 'privacy', 'distributed', 'cloud', 'collaborative', 'shared', 'cluster', 'sharing', 'networks', 'fedmap'] , ['cloud', 'edge', 'federated', 'networks', 'servers', 'bandwidth', 'iot', 'serverless', 'network', 'distributed'] , ['federated', 'distributed', 'learning', 'synchronization', 'adaptive', 'sgd', 'stochastic', 'centralized', 'asynchronous', 'gradient']","[""  Federated Learning (FL) is a machine learning paradigm that enables clients\nto jointly train a global model by aggregating the locally trained models\nwithout sharing any local training data. In practice, there can often be\nsubstantial heterogeneity (e.g., class imbalance) across the local data\ndistributions observed by each of these clients. Under such non-iid data\ndistributions across clients, FL suffers from the 'client-drift' problem where\nevery client drifts to its own local optimum. This results in slower\nconvergence and poor performance of the aggregated model. To address this\nlimitation, we propose a novel regularization technique based on adaptive\nself-distillation (ASD) for training models on the client side. Our\nregularization scheme adaptively adjusts to the client's training data based on\nthe global model entropy and the client's label distribution. The proposed\nregularization can be easily integrated atop existing, state-of-the-art FL\nalgorithms, leading to a further boost in the performance of these\noff-the-shelf methods. We theoretically explain how ASD reduces client-drift\nand also explain its generalization ability. We demonstrate the efficacy of our\napproach through extensive experiments on multiple real-world benchmarks and\nshow substantial gains in performance over state-of-the-art methods.\n"", ""  Federated Learning (FL) allows several clients to construct a common global\nmachine-learning model without having to share their data. FL, however, faces\nthe challenge of statistical heterogeneity between the client's data, which\ndegrades performance and slows down the convergence toward the global model. In\nthis paper, we provide theoretical proof that minimizing heterogeneity between\nclients facilitates the convergence of a global model for every single client.\nThis becomes particularly important under empirical concept shifts among\nclients, rather than merely considering imbalanced classes, which have been\nstudied until now. Therefore, we propose a method for knowledge transfer\nbetween clients where the server trains client-specific generators. Each\ngenerator generates samples for the corresponding client to remove the conflict\nwith other clients' models. Experiments conducted on synthetic and real data,\nalong with a theoretical study, support the effectiveness of our method in\nconstructing a well-generalizable global model by reducing the conflict between\nlocal models.\n"", ""  Knowledge distillation (KD) can enable collaborative learning among\ndistributed clients that have different model architectures and do not share\ntheir local data and model parameters with others. Each client updates its\nlocal model using the average model output/feature of all client models as the\ntarget, known as federated KD. However, existing federated KD methods often do\nnot perform well when clients' local models are trained with heterogeneous\nlocal datasets. In this paper, we propose Federated knowledge distillation\nenabled by Adversarial Learning (FedAL) to address the data heterogeneity among\nclients. First, to alleviate the local model output divergence across clients\ncaused by data heterogeneity, the server acts as a discriminator to guide\nclients' local model training to achieve consensus model outputs among clients\nthrough a min-max game between clients and the discriminator. Moreover,\ncatastrophic forgetting may happen during the clients' local training and\nglobal knowledge transfer due to clients' heterogeneous local data. Towards\nthis challenge, we design the less-forgetting regularization for both local\ntraining and global knowledge transfer to guarantee clients' ability to\ntransfer/learn knowledge to/from others. Experimental results show that FedAL\nand its variants achieve higher accuracy than other federated KD baselines.\n""] , ['  In this paper, we quantitatively compare these two effective communication\nschemes, i.e., digital and analog ones, for wireless federated learning (FL)\nover resource-constrained networks, highlighting their essential differences as\nwell as their respective application scenarios. We first examine both digital\nand analog transmission methods, together with a unified and fair comparison\nscheme under practical constraints. A universal convergence analysis under\nvarious imperfections is established for FL performance evaluation in wireless\nnetworks. These analytical results reveal that the fundamental difference\nbetween the two paradigms lies in whether communication and computation are\njointly designed or not. The digital schemes decouple the communication design\nfrom specific FL tasks, making it difficult to support simultaneous uplink\ntransmission of massive devices with limited bandwidth. In contrast, the analog\ncommunication allows over-the-air computation (AirComp), thus achieving\nefficient spectrum utilization. However, computation-oriented analog\ntransmission reduces power efficiency, and its performance is sensitive to\ncomputational errors. Finally, numerical simulations are conducted to verify\nthese theoretical observations.\n', '  In this paper, the performance optimization of federated learning (FL), when\ndeployed over a realistic wireless multiple-input multiple-output (MIMO)\ncommunication system with digital modulation and over-the-air computation\n(AirComp) is studied. In particular, a MIMO system is considered in which edge\ndevices transmit their local FL models (trained using their locally collected\ndata) to a parameter server (PS) using beamforming to maximize the number of\ndevices scheduled for transmission. The PS, acting as a central controller,\ngenerates a global FL model using the received local FL models and broadcasts\nit back to all devices. Due to the limited bandwidth in a wireless network,\nAirComp is adopted to enable efficient wireless data aggregation. However,\nfading of wireless channels can produce aggregate distortions in an\nAirComp-based FL scheme. To tackle this challenge, we propose a modified\nfederated averaging (FedAvg) algorithm that combines digital modulation with\nAirComp to mitigate wireless fading while ensuring the communication\nefficiency. This is achieved by a joint transmit and receive beamforming\ndesign, which is formulated as an optimization problem to dynamically adjust\nthe beamforming matrices based on current FL model parameters so as to minimize\nthe transmitting error and ensure the FL performance. To achieve this goal, we\nfirst analytically characterize how the beamforming matrices affect the\nperformance of the FedAvg in different iterations. Based on this relationship,\nan artificial neural network (ANN) is used to estimate the local FL models of\nall devices and adjust the beamforming matrices at the PS for future model\ntransmission. The algorithmic advantages and improved performance of the\nproposed methodologies are demonstrated through extensive numerical\nexperiments.\n', '  Wireless federated learning (FL) relies on efficient uplink communications to\naggregate model updates across distributed edge devices. Over-the-air\ncomputation (a.k.a. AirComp) has emerged as a promising approach for addressing\nthe scalability challenge of FL over wireless links with limited communication\nresources. Unlike conventional methods, AirComp allows multiple edge devices to\ntransmit uplink signals simultaneously, enabling the parameter server to\ndirectly decode the average global model. However, existing AirComp solutions\nare intrinsically analog, while modern wireless systems predominantly adopt\ndigital modulations. Consequently, careful constellation designs are necessary\nto accurately decode the sum model updates without ambiguity. In this paper, we\npropose an end-to-end communication system supporting AirComp with digital\nmodulation, aiming to overcome the challenges associated with accurate decoding\nof the sum signal with constellation designs. We leverage autoencoder network\nstructures and explore the joint optimization of transmitter and receiver\ncomponents. Our approach fills an important gap in the context of accurately\ndecoding the sum signal in digital modulation-based AirComp, which can advance\nthe deployment of FL in contemporary wireless systems.\n'] , [""  Federated Learning (FL) is an emerging paradigm in machine learning without\nexposing clients' raw data. In practical scenarios with numerous clients,\nencouraging fair and efficient client participation in federated learning is of\nutmost importance, which is also challenging given the heterogeneity in data\ndistribution and device properties. Existing works have proposed different\nclient-selection methods that consider fairness; however, they fail to select\nclients with high utilities while simultaneously achieving fair accuracy\nlevels. In this paper, we propose a fair client-selection approach that unlocks\nthreefold fairness in federated learning. In addition to having a fair\nclient-selection strategy, we enforce an equitable number of rounds for client\nparticipation and ensure a fair accuracy distribution over the clients. The\nexperimental results demonstrate that FedFair^3, in comparison to the\nstate-of-the-art baselines, achieves 18.15% less accuracy variance on the IID\ndata and 54.78% on the non-IID data, without decreasing the global accuracy.\nFurthermore, it shows 24.36% less wall-clock training time on average.\n"", ""  Federated learning (FL) has emerged as a prospective solution for\ncollaboratively learning a shared model across clients without sacrificing\ntheir data privacy. However, the federated learned model tends to be biased\nagainst certain demographic groups (e.g., racial and gender groups) due to the\ninherent FL properties, such as data heterogeneity and party selection. Unlike\ncentralized learning, mitigating bias in FL is particularly challenging as\nprivate training datasets and their sensitive attributes are typically not\ndirectly accessible. Most prior research in this field only focuses on global\nfairness while overlooking the local fairness of individual clients. Moreover,\nexisting methods often require sensitive information about the client's local\ndatasets to be shared, which is not desirable. To address these issues, we\npropose GLOCALFAIR, a client-server co-design fairness framework that can\njointly improve global and local group fairness in FL without the need for\nsensitive statistics about the client's private datasets. Specifically, we\nutilize constrained optimization to enforce local fairness on the client side\nand adopt a fairness-aware clustering-based aggregation on the server to\nfurther ensure the global model fairness across different sensitive groups\nwhile maintaining high utility. Experiments on two image datasets and one\ntabular dataset with various state-of-the-art fairness baselines show that\nGLOCALFAIR can achieve enhanced fairness under both global and local data\ndistributions while maintaining a good level of utility and client fairness.\n"", ""  Federated Learning (FL) is a privacy-enhancing technology for distributed ML.\nBy training models locally and aggregating updates - a federation learns\ntogether, while bypassing centralised data collection. FL is increasingly\npopular in healthcare, finance and personal computing. However, it inherits\nfairness challenges from classical ML and introduces new ones, resulting from\ndifferences in data quality, client participation, communication constraints,\naggregation methods and underlying hardware. Fairness remains an unresolved\nissue in FL and the community has identified an absence of succinct definitions\nand metrics to quantify fairness; to address this, we propose Federated\nFairness Analytics - a methodology for measuring fairness. Our definition of\nfairness comprises four notions with novel, corresponding metrics. They are\nsymptomatically defined and leverage techniques originating from XAI,\ncooperative game-theory and networking engineering. We tested a range of\nexperimental settings, varying the FL approach, ML task and data settings. The\nresults show that statistical heterogeneity and client participation affect\nfairness and fairness conscious approaches such as Ditto and q-FedAvg\nmarginally improve fairness-performance trade-offs. Using our techniques, FL\npractitioners can uncover previously unobtainable insights into their system's\nfairness, at differing levels of granularity in order to address fairness\nchallenges in FL. We have open-sourced our work at:\nhttps://github.com/oscardilley/federated-fairness.\n""] , ['  There are two paradigms in Federated Learning (FL): parallel FL (PFL), where\nmodels are trained in a parallel manner across clients; and sequential FL\n(SFL), where models are trained in a sequential manner across clients. In\ncontrast to that of PFL, the convergence theory of SFL on heterogeneous data is\nstill lacking. To resolve the theoretical dilemma of SFL, we establish sharp\nconvergence guarantees for SFL on heterogeneous data with both upper and lower\nbounds. Specifically, we derive the upper bounds for strongly convex, general\nconvex and non-convex objective functions, and construct the matching lower\nbounds for the strongly convex and general convex objective functions. Then, we\ncompare the upper bounds of SFL with those of PFL, showing that SFL outperforms\nPFL (at least, when the level of heterogeneity is relatively high).\nExperimental results on quadratic functions and real data sets validate the\ncounterintuitive comparison result.\n', '  There are two categories of methods in Federated Learning (FL) for joint\ntraining across multiple clients: i) parallel FL (PFL), where clients train\nmodels in a parallel manner; and ii) sequential FL (SFL), where clients train\nmodels in a sequential manner. In contrast to that of PFL, the convergence\ntheory of SFL on heterogeneous data is still lacking. In this paper, we\nestablish the convergence guarantees of SFL for strongly/general/non-convex\nobjectives on heterogeneous data. The convergence guarantees of SFL are better\nthan that of PFL on heterogeneous data with both full and partial client\nparticipation. Experimental results validate the counterintuitive analysis\nresult that SFL outperforms PFL on extremely heterogeneous data in cross-device\nsettings.\n', '  Federated learning is a paradigm of distributed machine learning in which\nmultiple clients coordinate with a central server to learn a model, without\nsharing their own training data. Standard federated optimization methods such\nas Federated Averaging (FedAvg) ensure balance among the clients by using the\nsame stepsize for local updates on all clients. However, this means that all\nclients need to respect the global geometry of the function which could yield\nslow convergence. In this work, we propose locally adaptive federated learning\nalgorithms, that leverage the local geometric information for each client\nfunction. We show that such locally adaptive methods with uncoordinated\nstepsizes across all clients can be particularly efficient in interpolated\n(overparameterized) settings, and analyze their convergence in the presence of\nheterogeneous data for convex and strongly convex settings. We validate our\ntheoretical claims by performing illustrative experiments for both i.i.d.\nnon-i.i.d. cases. Our proposed algorithms match the optimization performance of\ntuned FedAvg in the convex setting, outperform FedAvg as well as\nstate-of-the-art adaptive federated algorithms like FedAMS for non-convex\nexperiments, and come with superior generalization performance.\n'] , ['  Roadside unit (RSU) can significantly improve the safety and robustness of\nautonomous vehicles through Vehicle-to-Everything (V2X) communication.\nCurrently, the usage of a single RSU mainly focuses on real-time inference and\nV2X collaboration, while neglecting the potential value of the high-quality\ndata collected by RSU sensors. Integrating the vast amounts of data from\nnumerous RSUs can provide a rich source of data for model training. However,\nthe absence of ground truth annotations and the difficulty of transmitting\nenormous volumes of data are two inevitable barriers to fully exploiting this\nhidden value. In this paper, we introduce FedRSU, an innovative federated\nlearning framework for self-supervised scene flow estimation. In FedRSU, we\npresent a recurrent self-supervision training paradigm, where for each RSU, the\nscene flow prediction of points at every timestamp can be supervised by its\nsubsequent future multi-modality observation. Another key component of FedRSU\nis federated learning, where multiple devices collaboratively train an ML model\nwhile keeping the training data local and private. With the power of the\nrecurrent self-supervised learning paradigm, FL is able to leverage innumerable\nunderutilized data from RSU. To verify the FedRSU framework, we construct a\nlarge-scale multi-modality dataset RSU-SF. The dataset consists of 17 RSU\nclients, covering various scenarios, modalities, and sensor settings. Based on\nRSU-SF, we show that FedRSU can greatly improve model performance in ITS and\nprovide a comprehensive benchmark under diverse FL scenarios. To the best of\nour knowledge, we provide the first real-world LiDAR-camera multi-modal dataset\nand benchmark for the FL community.\n', ""  Vehicular edge intelligence (VEI) is a promising paradigm for enabling future\nintelligent transportation systems by accommodating artificial intelligence\n(AI) at the vehicular edge computing (VEC) system. Federated learning (FL)\nstands as one of the fundamental technologies facilitating collaborative model\ntraining locally and aggregation, while safeguarding the privacy of vehicle\ndata in VEI. However, traditional FL faces challenges in adapting to vehicle\nheterogeneity, training large models on resource-constrained vehicles, and\nremaining susceptible to model weight privacy leakage. Meanwhile, split\nlearning (SL) is proposed as a promising collaborative learning framework which\ncan mitigate the risk of model wights leakage, and release the training\nworkload on vehicles. SL sequentially trains a model between a vehicle and an\nedge cloud (EC) by dividing the entire model into a vehicle-side model and an\nEC-side model at a given cut layer. In this work, we combine the advantages of\nSL and FL to develop an Adaptive Split Federated Learning scheme for Vehicular\nEdge Computing (ASFV). The ASFV scheme adaptively splits the model and\nparallelizes the training process, taking into account mobile vehicle selection\nand resource allocation. Our extensive simulations, conducted on\nnon-independent and identically distributed data, demonstrate that the proposed\nASFV solution significantly reduces training latency compared to existing\nbenchmarks, while adapting to network dynamics and vehicles' mobility.\n"", '  Federated Learning (FL) is an advanced distributed machine learning approach,\nthat protects the privacy of each vehicle by allowing the model to be trained\non multiple devices simultaneously without the need to upload all data to a\nroad side unit (RSU). This enables FL to handle scenarios with sensitive or\nwidely distributed data. However, in these fields, it is well known that the\nlabeling costs can be a significant expense, and models relying on labels are\nnot suitable for these rapidly evolving fields especially in vehicular\nnetworks, or mobile internet of things (MIoT), where new data emerges\nconstantly. To handle this issue, the self-supervised learning paves the way\nfor training without labels. Additionally, for vehicles with high velocity,\nowing to blurred images, simple aggregation not only impacts the accuracy of\nthe aggregated model but also reduces the convergence speed of FL. This paper\nproposes a FL algorithm based on image blur level to aggregation, called\nFLSimCo, which does not require labels and serves as a pre-training stage for\nself-supervised learning in the vehicular environment. Simulation results\ndemonstrate that the proposed algorithm exhibits fast and stable convergence.\n'] , ['  In real-world scenarios, multimodal federated learning often faces the\npractical challenge of intricate modality missing, which poses constraints on\nbuilding federated frameworks and significantly degrades model inference\naccuracy. Existing solutions for addressing missing modalities generally\ninvolve developing modality-specific encoders on clients and training modality\nfusion modules on servers. However, these methods are primarily constrained to\nspecific scenarios with either unimodal clients or complete multimodal clients,\nstruggling to generalize effectively in the intricate modality missing\nscenarios. In this paper, we introduce a prototype library into the\nFedAvg-based Federated Learning framework, thereby empowering the framework\nwith the capability to alleviate the global model performance degradation\nresulting from modality missing during both training and testing. The proposed\nmethod utilizes prototypes as masks representing missing modalities to\nformulate a task-calibrated training loss and a model-agnostic uni-modality\ninference strategy. In addition, a proximal term based on prototypes is\nconstructed to enhance local training. Experimental results demonstrate the\nstate-of-the-art performance of our approach. Compared to the baselines, our\nmethod improved inference accuracy by 3.7\\% with 50\\% modality missing during\ntraining and by 23.8\\% during uni-modality inference. Code is available at\nhttps://github.com/BaoGuangYin/PmcmFL.\n', '  Federated learning (FL) underpins advancements in privacy-preserving\ndistributed computing by collaboratively training neural networks without\nexposing clients\' raw data. Current FL paradigms primarily focus on uni-modal\ndata, while exploiting the knowledge from distributed multimodal data remains\nlargely unexplored. Existing multimodal FL (MFL) solutions are mainly designed\nfor statistical or modality heterogeneity from the input side, however, have\nyet to solve the fundamental issue,""modality imbalance"", in distributed\nconditions, which can lead to inadequate information exploitation and\nheterogeneous knowledge aggregation on different modalities.In this paper, we\npropose a novel Cross-Modal Infiltration Federated Learning (FedCMI) framework\nthat effectively alleviates modality imbalance and knowledge heterogeneity via\nknowledge transfer from the global dominant modality. To avoid the loss of\ninformation in the weak modality due to merely imitating the behavior of\ndominant modality, we design the two-projector module to integrate the\nknowledge from dominant modality while still promoting the local feature\nexploitation of weak modality. In addition, we introduce a class-wise\ntemperature adaptation scheme to achieve fair performance across different\nclasses. Extensive experiments over popular datasets are conducted and give us\na gratifying confirmation of the proposed framework for fully exploring the\ninformation of each modality in MFL.\n', '  Multimodal federated learning (MFL) has emerged as a decentralized machine\nlearning paradigm, allowing multiple clients with different modalities to\ncollaborate on training a machine learning model across diverse data sources\nwithout sharing their private data. However, challenges, such as data\nheterogeneity and severely missing modalities, pose crucial hindrances to the\nrobustness of MFL, significantly impacting the performance of global model. The\nabsence of a modality introduces misalignment during the local training phase,\nstemming from zero-filling in the case of clients with missing modalities.\nConsequently, achieving robust generalization in global model becomes\nimperative, especially when dealing with clients that have incomplete data. In\nthis paper, we propose Multimodal Federated Cross Prototype Learning (MFCPL), a\nnovel approach for MFL under severely missing modalities by conducting the\ncomplete prototypes to provide diverse modality knowledge in modality-shared\nlevel with the cross-modal regularization and modality-specific level with\ncross-modal contrastive mechanism. Additionally, our approach introduces the\ncross-modal alignment to provide regularization for modality-specific features,\nthereby enhancing overall performance, particularly in scenarios involving\nseverely missing modalities. Through extensive experiments on three multimodal\ndatasets, we demonstrate the effectiveness of MFCPL in mitigating these\nchallenges and improving the overall performance.\n'] , ['  Federated Learning(FL) is a privacy-preserving machine learning paradigm\nwhere a global model is trained in-situ across a large number of distributed\nedge devices. These systems are often comprised of millions of user devices and\nonly a subset of available devices can be used for training in each epoch.\nDesigning a device selection strategy is challenging, given that devices are\nhighly heterogeneous in both their system resources and training data. This\nheterogeneity makes device selection very crucial for timely model convergence\nand sufficient model accuracy. To tackle the FL client heterogeneity problem,\nvarious client selection algorithms have been developed, showing promising\nperformance improvement in terms of model coverage and accuracy. In this work,\nwe study the overhead of client selection algorithms in a large scale FL\nenvironment. Then we propose an efficient data distribution summary calculation\nalgorithm to reduce the overhead in a real-world large scale FL environment.\nThe evaluation shows that our proposed solution could achieve up to 30x\nreduction in data summary time, and up to 360x reduction in clustering time.\n', '  Federated Learning (FL) is a promising paradigm that offers significant\nadvancements in privacy-preserving, decentralized machine learning by enabling\ncollaborative training of models across distributed devices without\ncentralizing data. However, the practical deployment of FL systems faces a\nsignificant bottleneck: the communication overhead caused by frequently\nexchanging large model updates between numerous devices and a central server.\nThis communication inefficiency can hinder training speed, model performance,\nand the overall feasibility of real-world FL applications. In this survey, we\ninvestigate various strategies and advancements made in communication-efficient\nFL, highlighting their impact and potential to overcome the communication\nchallenges inherent in FL systems. Specifically, we define measures for\ncommunication efficiency, analyze sources of communication inefficiency in FL\nsystems, and provide a taxonomy and comprehensive review of state-of-the-art\ncommunication-efficient FL methods. Additionally, we discuss promising future\nresearch directions for enhancing the communication efficiency of FL systems.\nBy addressing the communication bottleneck, FL can be effectively applied and\nenable scalable and practical deployment across diverse applications that\nrequire privacy-preserving, decentralized machine learning, such as IoT,\nhealthcare, or finance.\n', '  Federated learning (FL), as an emerging collaborative learning paradigm, has\ngarnered significant attention due to its capacity to preserve privacy within\ndistributed learning systems. In these systems, clients collaboratively train a\nunified neural network model using their local datasets and share model\nparameters rather than raw data, enhancing privacy. Predominantly, FL systems\nare designed for mobile and edge computing environments where training\ntypically occurs over wireless networks. Consequently, as model sizes increase,\nthe conventional FL frameworks increasingly consume substantial communication\nresources. To address this challenge and improve communication efficiency, this\npaper introduces a novel hierarchical FL framework that integrates the benefits\nof clustered FL and model compression. We present an adaptive clustering\nalgorithm that identifies a core client and dynamically organizes clients into\nclusters. Furthermore, to enhance transmission efficiency, each core client\nimplements a local aggregation with compression (LC aggregation) algorithm\nafter collecting compressed models from other clients within the same cluster.\nSimulation results affirm that our proposed algorithms not only maintain\ncomparable predictive accuracy but also significantly reduce energy consumption\nrelative to existing FL mechanisms.\n'] , ['  Federated learning (FL) enables edge nodes to collaboratively contribute to\nconstructing a global model without sharing their data. This is accomplished by\ndevices computing local, private model updates that are then aggregated by a\nserver. However, computational resource constraints and network communication\ncan become a severe bottleneck for larger model sizes typical for deep learning\napplications. Edge nodes tend to have limited hardware resources (RAM, CPU),\nand the network bandwidth and reliability at the edge is a concern for scaling\nfederated fleet applications. In this paper, we propose and evaluate a FL\nstrategy inspired by transfer learning in order to reduce resource utilization\non devices, as well as the load on the server and network in each global\ntraining round. For each local model update, we randomly select layers to\ntrain, freezing the remaining part of the model. In doing so, we can reduce\nboth server load and communication costs per round by excluding all untrained\nlayer weights from being transferred to the server. The goal of this study is\nto empirically explore the potential trade-off between resource utilization on\ndevices and global model convergence under the proposed strategy. We implement\nthe approach using the federated learning framework FEDn. A number of\nexperiments were carried out over different datasets (CIFAR-10, CASA, and\nIMDB), performing different tasks using different deep-learning model\narchitectures. Our results show that training the model partially can\naccelerate the training process, efficiently utilizes resources on-device, and\nreduce the data transmission by around 75% and 53% when we train 25%, and 50%\nof the model layers, respectively, without harming the resulting global model\naccuracy.\n', '  Federated Learning (FL) plays a critical role in distributed systems. In\nthese systems, data privacy and confidentiality hold paramount importance,\nparticularly within edge-based data processing systems such as IoT devices\ndeployed in smart homes. FL emerges as a privacy-enforcing sub-domain of\nmachine learning that enables model training on client devices, eliminating the\nnecessity to share private data with a central server. While existing research\nhas predominantly addressed challenges pertaining to data heterogeneity, there\nremains a current gap in addressing issues such as varying device capabilities\nand efficient communication. These unaddressed issues raise a number of\nimplications in resource-constrained environments. In particular, the practical\nimplementation of FL-based IoT or edge systems is extremely inefficient. In\nthis paper, we propose ""Resource-Efficient Federated Training Framework for\nHeterogeneous and Resource-Constrained Environments (REFT),"" a novel approach\nspecifically devised to address these challenges in resource-limited devices.\nOur proposed method uses Variable Pruning to optimize resource utilization by\nadapting pruning strategies to the computational capabilities of each client.\nFurthermore, our proposed REFT technique employs knowledge distillation to\nminimize the need for continuous bidirectional client-server communication.\nThis achieves a significant reduction in communication bandwidth, thereby\nenhancing the overall resource efficiency. We conduct experiments for an image\nclassification task, and the results demonstrate the effectiveness of our\napproach in resource-limited settings. Our technique not only preserves data\nprivacy and performance standards but also accommodates heterogeneous model\narchitectures, facilitating the participation of a broader array of diverse\nclient devices in the training process, all while consuming minimal bandwidth.\n', '  To enable large-scale and efficient deployment of artificial intelligence\n(AI), the combination of AI and edge computing has spawned Edge Intelligence,\nwhich leverages the computing and communication capabilities of end devices and\nedge servers to process data closer to where it is generated. A key technology\nfor edge intelligence is the privacy-protecting machine learning paradigm known\nas Federated Learning (FL), which enables data owners to train models without\nhaving to transfer raw data to third-party servers. However, FL networks are\nexpected to involve thousands of heterogeneous distributed devices. As a\nresult, communication efficiency remains a key bottleneck. To reduce node\nfailures and device exits, a Hierarchical Federated Learning (HFL) framework is\nproposed, where a designated cluster leader supports the data owner through\nintermediate model aggregation. Therefore, based on the improvement of edge\nserver resource utilization, this paper can effectively make up for the\nlimitation of cache capacity. In order to mitigate the impact of soft clicks on\nthe quality of user experience (QoE), the authors model the user QoE as a\ncomprehensive system cost. To solve the formulaic problem, the authors propose\na decentralized caching algorithm with federated deep reinforcement learning\n(DRL) and federated learning (FL), where multiple agents learn and make\ndecisions independently\n'] , ['  Federated Learning (FL) is a distributed machine learning paradigm that\nallows clients to train models on their data while preserving their privacy. FL\nalgorithms, such as Federated Averaging (FedAvg) and its variants, have been\nshown to converge well in many scenarios. However, these methods require\nclients to upload their local updates to the server in a synchronous manner,\nwhich can be slow and unreliable in realistic FL settings. To address this\nissue, researchers have developed asynchronous FL methods that allow clients to\ncontinue training on their local data using a stale global model. However, most\nof these methods simply aggregate all of the received updates without\nconsidering their relative contributions, which can slow down convergence. In\nthis paper, we propose a contribution-aware asynchronous FL method that takes\ninto account the staleness and statistical heterogeneity of the received\nupdates. Our method dynamically adjusts the contribution of each update based\non these factors, which can speed up convergence compared to existing methods.\n', ""  Federated learning (FL) algorithms usually sample a fraction of clients in\neach round (partial participation) when the number of participants is large and\nthe server's communication bandwidth is limited. Recent works on the\nconvergence analysis of FL have focused on unbiased client sampling, e.g.,\nsampling uniformly at random, which suffers from slow wall-clock time for\nconvergence due to high degrees of system heterogeneity and statistical\nheterogeneity. This paper aims to design an adaptive client sampling algorithm\nfor FL over wireless networks that tackles both system and statistical\nheterogeneity to minimize the wall-clock convergence time. We obtain a new\ntractable convergence bound for FL algorithms with arbitrary client sampling\nprobability. Based on the bound, we analytically establish the relationship\nbetween the total learning time and sampling probability with an adaptive\nbandwidth allocation scheme, which results in a non-convex optimization\nproblem. We design an efficient algorithm for learning the unknown parameters\nin the convergence bound and develop a low-complexity algorithm to\napproximately solve the non-convex problem. Our solution reveals the impact of\nsystem and statistical heterogeneity parameters on the optimal client sampling\ndesign. Moreover, our solution shows that as the number of sampled clients\nincreases, the total convergence time first decreases and then increases\nbecause a larger sampling number reduces the number of rounds for convergence\nbut results in a longer expected time per-round due to limited wireless\nbandwidth. Experimental results from both hardware prototype and simulation\ndemonstrate that our proposed sampling scheme significantly reduces the\nconvergence time compared to several baseline sampling schemes.\n"", '  Federated learning (FL) was recently proposed to securely train models with\ndata held over multiple locations (""clients"") under the coordination of a\ncentral server. Two major challenges hindering the performance of FL algorithms\nare long training times caused by straggling clients, and a decline in model\naccuracy under non-iid local data distributions (""client drift""). In this work,\nwe propose and analyze Asynchronous Exact Averaging (AREA), a new stochastic\n(sub)gradient algorithm that utilizes asynchronous communication to speed up\nconvergence and enhance scalability, and employs client memory to correct the\nclient drift caused by variations in client update frequencies. Moreover, AREA\nis, to the best of our knowledge, the first method that is guaranteed to\nconverge under arbitrarily long delays, without the use of delay-adaptive\nstepsizes, and (i) for strongly convex, smooth functions, asymptotically\nconverges to an error neighborhood whose size depends only on the variance of\nthe stochastic gradients used with respect to the number of iterations, and\n(ii) for convex, non-smooth functions, matches the convergence rate of the\ncentralized stochastic subgradient method up to a constant factor, which\ndepends on the average of the individual client update frequencies instead of\ntheir minimum (or maximum). Our numerical results validate our theoretical\nanalysis and indicate AREA outperforms state-of-the-art methods when local data\nare highly non-iid, especially as the number of clients grows.\n']",Federated Learning,Federated Learning for Distributed Systems
63,"Federated Learning Security and Adversarial Attacks , Differential Privacy in Federated Learning , Federated Learning for Healthcare Data Privacy , Federated Learning and Unlearning for Data Privacy , Federated Recommendation Systems with Privacy Protection , Federated Learning for Distributed Data Privacy , Federated Learning Security and Privacy Attacks , Federated Learning for Medical Data Privacy , Federated Learning for Anomaly Detection and Security","['adversarial', 'fedsecurity', 'adversary', 'datadefense', 'feddefender', 'malicious', 'attacks', 'federated', 'aggregators', 'adversaries'] , ['privacy', 'federated', 'private', 'distributed', 'decentralized', 'sharing', 'security', 'secret', 'cryptographic', 'secure'] , ['privacy', 'federated', 'datasets', 'learning', 'personalized', 'distributed', 'sharing', 'data', 'collaborative', 'healthcare'] , ['unlearning', 'federated', 'privacy', 'learning', 'unlearned', 'distributed', 'disclosure', 'collaborative', 'private', 'decentralized'] , ['recommender', 'federated', 'privacy', 'collaborative', 'personalization', 'personalized', 'collaboratively', 'shared', 'recommendation', 'sharing'] , ['federated', 'privacy', 'learning', 'datasets', 'collaborative', 'distributed', 'trained', 'sharing', 'fedllm', 'collaboratively'] , ['privacy', 'adversary', 'security', 'attacks', 'secure', 'federated', 'confidentiality', 'malicious', 'attacker', 'protect'] , ['privacy', 'federated', 'private', 'adversarial', 'distributed', 'sharing', 'learning', 'secret', 'security', 'decentralized'] , ['federated', 'privacy', 'intrusion', 'distributed', 'decentralized', 'ddos', 'cloud', 'security', 'anomaly', 'malicious']","[""  Federated Learning (FL) is a decentralized machine learning method that\nenables participants to collaboratively train a model without sharing their\nprivate data. Despite its privacy and scalability benefits, FL is susceptible\nto backdoor attacks, where adversaries poison the local training data of a\nsubset of clients using a backdoor trigger, aiming to make the aggregated model\nproduce malicious results when the same backdoor condition is met by an\ninference-time input. Existing backdoor attacks in FL suffer from common\ndeficiencies: fixed trigger patterns and reliance on the assistance of model\npoisoning. State-of-the-art defenses based on Byzantine-robust aggregation\nexhibit a good defense performance on these attacks because of the significant\ndivergence between malicious and benign model updates. To effectively conceal\nmalicious model updates among benign ones, we propose DPOT, a backdoor attack\nstrategy in FL that dynamically constructs backdoor objectives by optimizing a\nbackdoor trigger, making backdoor data have minimal effect on model updates. We\nprovide theoretical justifications for DPOT's attacking principle and display\nexperimental results showing that DPOT, via only a data-poisoning attack,\neffectively undermines state-of-the-art defenses and outperforms existing\nbackdoor attack techniques on various datasets.\n"", ""  Federated Learning (FL) is susceptible to poisoning attacks, wherein\ncompromised clients manipulate the global model by modifying local datasets or\nsending manipulated model updates. Experienced defenders can readily detect and\nmitigate the poisoning effects of malicious behaviors using Byzantine-robust\naggregation rules. However, the exploration of poisoning attacks in scenarios\nwhere such behaviors are absent remains largely unexplored for Byzantine-robust\nFL. This paper addresses the challenging problem of poisoning Byzantine-robust\nFL by introducing catastrophic forgetting. To fill this gap, we first formally\ndefine generalization error and establish its connection to catastrophic\nforgetting, paving the way for the development of a clean-label data poisoning\nattack named BadSampler. This attack leverages only clean-label data (i.e.,\nwithout poisoned data) to poison Byzantine-robust FL and requires the adversary\nto selectively sample training data with high loss to feed model training and\nmaximize the model's generalization error. We formulate the attack as an\noptimization problem and present two elegant adversarial sampling strategies,\nTop-$\\kappa$ sampling, and meta-sampling, to approximately solve it.\nAdditionally, our formal error upper bound and time complexity analysis\ndemonstrate that our design can preserve attack utility with high efficiency.\nExtensive evaluations on two real-world datasets illustrate the effectiveness\nand performance of our proposed attacks.\n"", '  Federated Learning (FL) is an emerging distributed machine learning paradigm\nthat allows multiple clients to collaboratively train a global model without\nsharing private local data. However, FL systems are vulnerable to attacks from\nmalicious clients, who can degrade the global model performance through data\npoisoning and model poisoning. Existing defense methods typically focus on a\nsingle type of attack, such as Byzantine attacks or backdoor attacks, and are\noften ineffective against potential data poisoning attacks like label flipping\nand label shuffling. Additionally, these methods often lack accuracy and\nrobustness in detecting and handling malicious updates. To address these\nissues, we propose a novel method based on model confidence scores, which\nevaluates the uncertainty of client model updates to detect and defend against\nmalicious clients. Our approach is comprehensively effective for both model\npoisoning and data poisoning attacks and is capable of accurately identifying\nand mitigating potential malicious updates from being aggregated. Experimental\nresults demonstrate that our method significantly improves the robustness of FL\nsystems against various types of attacks, also achieving higher model accuracy\nand stability across various scenarios.\n'] , ['  Federated learning (FL) enables multiple clients to collaboratively learn a\nshared model without sharing their individual data. Concerns about utility,\nprivacy, and training efficiency in FL have garnered significant research\nattention. Differential privacy has emerged as a prevalent technique in FL,\nsafeguarding the privacy of individual user data while impacting utility and\ntraining efficiency. Within Differential Privacy Federated Learning (DPFL),\nprevious studies have primarily focused on the utility-privacy trade-off,\nneglecting training efficiency, which is crucial for timely completion.\nMoreover, differential privacy achieves privacy by introducing controlled\nrandomness (noise) on selected clients in each communication round. Previous\nwork has mainly examined the impact of noise level ($\\sigma$) and communication\nrounds ($T$) on the privacy-utility dynamic, overlooking other influential\nfactors like the sample ratio ($q$, the proportion of selected clients). This\npaper systematically formulates an efficiency-constrained utility-privacy\nbi-objective optimization problem in DPFL, focusing on $\\sigma$, $T$, and $q$.\nWe provide a comprehensive theoretical analysis, yielding analytical solutions\nfor the Pareto front. Extensive empirical experiments verify the validity and\nefficacy of our analysis, offering valuable guidance for low-cost parameter\ndesign in DPFL.\n', '  In recent years, privacy and security concerns in machine learning have\npromoted trusted federated learning to the forefront of research. Differential\nprivacy has emerged as the de facto standard for privacy protection in\nfederated learning due to its rigorous mathematical foundation and provable\nguarantee. Despite extensive research on algorithms that incorporate\ndifferential privacy within federated learning, there remains an evident\ndeficiency in systematic reviews that categorize and synthesize these studies.\n  Our work presents a systematic overview of the differentially private\nfederated learning. Existing taxonomies have not adequately considered objects\nand level of privacy protection provided by various differential privacy models\nin federated learning. To rectify this gap, we propose a new taxonomy of\ndifferentially private federated learning based on definition and guarantee of\nvarious differential privacy models and federated scenarios. Our classification\nallows for a clear delineation of the protected objects across various\ndifferential privacy models and their respective neighborhood levels within\nfederated learning environments. Furthermore, we explore the applications of\ndifferential privacy in federated learning scenarios. Our work provide valuable\ninsights into privacy-preserving federated learning and suggest practical\ndirections for future research.\n', ""  Federated Learning (FL) is an emerging paradigm that holds great promise for\nprivacy-preserving machine learning using distributed data. To enhance privacy,\nFL can be combined with Differential Privacy (DP), which involves adding\nGaussian noise to the model weights. However, FL faces a significant challenge\nin terms of large communication overhead when transmitting these model weights.\nTo address this issue, quantization is commonly employed. Nevertheless, the\npresence of quantized Gaussian noise introduces complexities in understanding\nprivacy protection. This research paper investigates the impact of quantization\non privacy in FL systems. We examine the privacy guarantees of quantized\nGaussian mechanisms using R\\'enyi Differential Privacy (RDP). By deriving the\nprivacy budget of quantized Gaussian mechanisms, we demonstrate that lower\nquantization bit levels provide improved privacy protection. To validate our\ntheoretical findings, we employ Membership Inference Attacks (MIA), which gauge\nthe accuracy of privacy leakage. The numerical results align with our\ntheoretical analysis, confirming that quantization can indeed enhance privacy\nprotection. This study not only enhances our understanding of the correlation\nbetween privacy and communication in FL but also underscores the advantages of\nquantization in preserving privacy.\n""] , [""  Machine learning (ML) and Artificial Intelligence (AI) have fueled remarkable\nadvancements, particularly in healthcare. Within medical imaging, ML models\nhold the promise of improving disease diagnoses, treatment planning, and\npost-treatment monitoring. Various computer vision tasks like image\nclassification, object detection, and image segmentation are poised to become\nroutine in clinical analysis. However, privacy concerns surrounding patient\ndata hinder the assembly of large training datasets needed for developing and\ntraining accurate, robust, and generalizable models. Federated Learning (FL)\nemerges as a compelling solution, enabling organizations to collaborate on ML\nmodel training by sharing model training information (gradients) rather than\ndata (e.g., medical images). FL's distributed learning framework facilitates\ninter-institutional collaboration while preserving patient privacy. However,\nFL, while robust in privacy preservation, faces several challenges. Sensitive\ninformation can still be gleaned from shared gradients that are passed on\nbetween organizations during model training. Additionally, in medical imaging,\nquantifying model confidence\\uncertainty accurately is crucial due to the noise\nand artifacts present in the data. Uncertainty estimation in FL encounters\nunique hurdles due to data heterogeneity across organizations. This paper\noffers a comprehensive review of FL, privacy preservation, and uncertainty\nestimation, with a focus on medical imaging. Alongside a survey of current\nresearch, we identify gaps in the field and suggest future directions for FL\nresearch to enhance privacy and address noisy medical imaging data challenges.\n"", ""  Data privacy has become a major concern in healthcare due to the increasing\ndigitization of medical records and data-driven medical research. Protecting\nsensitive patient information from breaches and unauthorized access is\ncritical, as such incidents can have severe legal and ethical complications.\nFederated Learning (FL) addresses this concern by enabling multiple healthcare\ninstitutions to collaboratively learn from decentralized data without sharing\nit. FL's scope in healthcare covers areas such as disease prediction, treatment\ncustomization, and clinical trial research. However, implementing FL poses\nchallenges, including model convergence in non-IID (independent and identically\ndistributed) data environments, communication overhead, and managing\nmulti-institutional collaborations. A systematic review of FL in healthcare is\nnecessary to evaluate how effectively FL can provide privacy while maintaining\nthe integrity and usability of medical data analysis. In this study, we analyze\nexisting literature on FL applications in healthcare. We explore the current\nstate of model security practices, identify prevalent challenges, and discuss\npractical applications and their implications. Additionally, the review\nhighlights promising future research directions to refine FL implementations,\nenhance data security protocols, and expand FL's use to broader healthcare\napplications, which will benefit future researchers and practitioners.\n"", '  Distributed training can facilitate the processing of large medical image\ndatasets, and improve the accuracy and efficiency of disease diagnosis while\nprotecting patient privacy, which is crucial for achieving efficient medical\nimage analysis and accelerating medical research progress. This paper presents\nan innovative approach to medical image classification, leveraging Federated\nLearning (FL) to address the dual challenges of data privacy and efficient\ndisease diagnosis. Traditional Centralized Machine Learning models, despite\ntheir widespread use in medical imaging for tasks such as disease diagnosis,\nraise significant privacy concerns due to the sensitive nature of patient data.\nAs an alternative, FL emerges as a promising solution by allowing the training\nof a collective global model across local clients without centralizing the\ndata, thus preserving privacy. Focusing on the application of FL in Magnetic\nResonance Imaging (MRI) brain tumor detection, this study demonstrates the\neffectiveness of the Federated Learning framework coupled with EfficientNet-B0\nand the FedAvg algorithm in enhancing both privacy and diagnostic accuracy.\nThrough a meticulous selection of preprocessing methods, algorithms, and\nhyperparameters, and a comparative analysis of various Convolutional Neural\nNetwork (CNN) architectures, the research uncovers optimal strategies for image\nclassification. The experimental results reveal that EfficientNet-B0\noutperforms other models like ResNet in handling data heterogeneity and\nachieving higher accuracy and lower loss, highlighting the potential of FL in\novercoming the limitations of traditional models. The study underscores the\nsignificance of addressing data heterogeneity and proposes further research\ndirections for broadening the applicability of FL in medical image analysis.\n'] , ['  Federated learning (FL) offers a compelling framework for training large\nlanguage models (LLMs) while addressing data privacy and decentralization\nchallenges. This paper surveys recent advancements in the federated learning of\nlarge language models, with a particular focus on machine unlearning, a crucial\naspect for complying with privacy regulations like the Right to be Forgotten.\nMachine unlearning in the context of federated LLMs involves systematically and\nsecurely removing individual data contributions from the learned model without\nretraining from scratch. We explore various strategies that enable effective\nunlearning, such as perturbation techniques, model decomposition, and\nincremental learning, highlighting their implications for maintaining model\nperformance and data privacy. Furthermore, we examine case studies and\nexperimental results from recent literature to assess the effectiveness and\nefficiency of these approaches in real-world scenarios. Our survey reveals a\ngrowing interest in developing more robust and scalable federated unlearning\nmethods, suggesting a vital area for future research in the intersection of AI\nethics and distributed machine learning technologies.\n', '  We study federated unlearning, a novel problem to eliminate the impact of\nspecific clients or data points on the global model learned via federated\nlearning (FL). This problem is driven by the right to be forgotten and the\nprivacy challenges in FL. We introduce a new framework for exact federated\nunlearning that meets two essential criteria: \\textit{communication efficiency}\nand \\textit{exact unlearning provability}. To our knowledge, this is the first\nwork to tackle both aspects coherently. We start by giving a rigorous\ndefinition of \\textit{exact} federated unlearning, which guarantees that the\nunlearned model is statistically indistinguishable from the one trained without\nthe deleted data. We then pinpoint the key property that enables fast exact\nfederated unlearning: total variation (TV) stability, which measures the\nsensitivity of the model parameters to slight changes in the dataset.\nLeveraging this insight, we develop a TV-stable FL algorithm called\n\\texttt{FATS}, which modifies the classical\n\\texttt{\\underline{F}ed\\underline{A}vg} algorithm for \\underline{T}V\n\\underline{S}tability and employs local SGD with periodic averaging to lower\nthe communication round. We also design efficient unlearning algorithms for\n\\texttt{FATS} under two settings: client-level and sample-level unlearning. We\nprovide theoretical guarantees for our learning and unlearning algorithms,\nproving that they achieve exact federated unlearning with reasonable\nconvergence rates for both the original and unlearned models. We empirically\nvalidate our framework on 6 benchmark datasets, and show its superiority over\nstate-of-the-art methods in terms of accuracy, communication cost, computation\ncost, and unlearning efficacy.\n', '  Federated learning (FL), introduced in 2017, facilitates collaborative\nlearning between non-trusting parties with no need for the parties to\nexplicitly share their data among themselves. This allows training models on\nuser data while respecting privacy regulations such as GDPR and CPRA. However,\nemerging privacy requirements may mandate model owners to be able to\n\\emph{forget} some learned data, e.g., when requested by data owners or law\nenforcement. This has given birth to an active field of research called\n\\emph{machine unlearning}. In the context of FL, many techniques developed for\nunlearning in centralized settings are not trivially applicable! This is due to\nthe unique differences between centralized and distributed learning, in\nparticular, interactivity, stochasticity, heterogeneity, and limited\naccessibility in FL. In response, a recent line of work has focused on\ndeveloping unlearning mechanisms tailored to FL.\n  This SoK paper aims to take a deep look at the \\emph{federated unlearning}\nliterature, with the goal of identifying research trends and challenges in this\nemerging field. By carefully categorizing papers published on FL unlearning\n(since 2020), we aim to pinpoint the unique complexities of federated\nunlearning, highlighting limitations on directly applying centralized\nunlearning methods. We compare existing federated unlearning methods regarding\ninfluence removal and performance recovery, compare their threat models and\nassumptions, and discuss their implications and limitations. For instance, we\nanalyze the experimental setup of FL unlearning studies from various\nperspectives, including data heterogeneity and its simulation, the datasets\nused for demonstration, and evaluation metrics. Our work aims to offer insights\nand suggestions for future research on federated unlearning.\n'] , ['  Sequential recommender systems have made significant progress. Recently, due\nto increasing concerns about user data privacy, some researchers have\nimplemented federated learning for sequential recommendation, a.k.a., Federated\nSequential Recommender Systems (FedSeqRecs), in which a public sequential\nrecommender model is shared and frequently transmitted between a central server\nand clients to achieve collaborative learning. Although these solutions\nmitigate user privacy to some extent, they present two significant limitations\nthat affect their practical usability: (1) They require a globally shared\nsequential recommendation model. However, in real-world scenarios, the\nrecommendation model constitutes a critical intellectual property for platform\nand service providers. Therefore, service providers may be reluctant to\ndisclose their meticulously developed models. (2) The communication costs are\nhigh as they correlate with the number of model parameters. This becomes\nparticularly problematic as the current FedSeqRec will be inapplicable when\nsequential recommendation marches into a large language model era.\n  To overcome the above challenges, this paper proposes a parameter\ntransmission-free federated sequential recommendation framework (PTF-FSR),\nwhich ensures both model and data privacy protection to meet the privacy needs\nof service providers and system users alike. Furthermore, since PTF-FSR only\ntransmits prediction results under privacy protection, which are independent of\nmodel sizes, this new federated learning architecture can accommodate more\ncomplex and larger sequential recommendation models. Extensive experiments\nconducted on three widely used recommendation datasets, employing various\nsequential recommendation models from both ID-based and ID-free paradigms,\ndemonstrate the effectiveness and generalization capability of our proposed\nframework.\n', ""  The federated recommendation system is an emerging AI service architecture\nthat provides recommendation services in a privacy-preserving manner. Using\nuser-relation graphs to enhance federated recommendations is a promising topic.\nHowever, it is still an open challenge to construct the user-relation graph\nwhile preserving data locality-based privacy protection in federated settings.\nInspired by a simple motivation, similar users share a similar vision\n(embeddings) to the same item set, this paper proposes a novel Graph-guided\nPersonalization for Federated Recommendation (GPFedRec). The proposed method\nconstructs a user-relation graph from user-specific personalized item\nembeddings at the server without accessing the users' interaction records. The\npersonalized item embedding is locally fine-tuned on each device, and then a\nuser-relation graph will be constructed by measuring the similarity among\nclient-specific item embeddings. Without accessing users' historical\ninteractions, we embody the data locality-based privacy protection of vanilla\nfederated learning. Furthermore, a graph-guided aggregation mechanism is\ndesigned to leverage the user-relation graph and federated optimization\nframework simultaneously. Extensive experiments on five benchmark datasets\ndemonstrate GPFedRec's superior performance. The in-depth study validates that\nGPFedRec can generally improve existing federated recommendation methods as a\nplugin while keeping user privacy safe. Code is available to ease\nreproducibility\n"", ""  With the growing concerns regarding user data privacy, Federated Recommender\nSystem (FedRec) has garnered significant attention recently due to its\nprivacy-preserving capabilities. Existing FedRecs generally adhere to a\nlearning protocol in which a central server shares a global recommendation\nmodel with clients, and participants achieve collaborative learning by\nfrequently communicating the model's public parameters. Nevertheless, this\nlearning framework has two drawbacks that limit its practical usability: (1) It\nnecessitates a global-sharing recommendation model; however, in real-world\nscenarios, information related to the recommender model, including its\nalgorithm and parameters, constitutes the platforms' intellectual property.\nHence, service providers are unlikely to release such information actively. (2)\nThe communication costs of model parameter transmission are expensive since the\nmodel parameters are usually high-dimensional matrices. With the model size\nincreasing, the communication burden will be the bottleneck for such\ntraditional FedRecs.\n  Given the above limitations, this paper introduces a novel parameter\ntransmission-free federated recommendation framework that balances the\nprotection between users' data privacy and platforms' model privacy, namely\nPTF-FedRec. Specifically, participants in PTF-FedRec collaboratively exchange\nknowledge by sharing their predictions within a privacy-preserving mechanism.\nThrough this way, the central server can learn a recommender model without\ndisclosing its model parameters or accessing clients' raw data, preserving both\nthe server's model privacy and users' data privacy. Besides, since clients and\nthe central server only need to communicate prediction scores which are just a\nfew real numbers, the overhead is significantly reduced compared to traditional\nFedRecs. The code is available\nat\\url{https://github.com/hi-weiyuan/PTF-FedRec}.\n""] , [""  Federated learning, a distributed learning paradigm, utilizes multiple\nclients to build a robust global model. In real-world applications, local\nclients often operate within their limited domains, leading to a `domain shift'\nacross clients. Privacy concerns limit each client's learning to its own domain\ndata, which increase the risk of overfitting. Moreover, the process of\naggregating models trained on own limited domain can be potentially lead to a\nsignificant degradation in the global model performance. To deal with these\nchallenges, we introduce the concept of federated feature diversification. Each\nclient diversifies the own limited domain data by leveraging global feature\nstatistics, i.e., the aggregated average statistics over all participating\nclients, shared through the global model's parameters. This data\ndiversification helps local models to learn client-invariant representations\nwhile preserving privacy. Our resultant global model shows robust performance\non unseen test domain data. To enhance performance further, we develop an\ninstance-adaptive inference approach tailored for test domain data. Our\nproposed instance feature adapter dynamically adjusts feature statistics to\nalign with the test input, thereby reducing the domain gap between the test and\ntraining domains. We show that our method achieves state-of-the-art performance\non several domain generalization benchmarks within a federated learning\nsetting.\n"", ""  Random forests are considered a cornerstone in machine learning for their\nrobustness and versatility. Despite these strengths, their conventional\ncentralized training is ill-suited for the modern landscape of data that is\noften distributed, sensitive, and subject to privacy concerns. Federated\nlearning (FL) provides a compelling solution to this problem, enabling models\nto be trained across a group of clients while maintaining the privacy of each\nclient's data. However, adapting tree-based methods like random forests to\nfederated settings introduces significant challenges, particularly when it\ncomes to non-identically distributed (non-IID) data across clients, which is a\ncommon scenario in real-world applications. This paper presents a federated\nrandom forest approach that employs a novel ensemble construction method aimed\nat improving performance under non-IID data. Instead of growing trees\nindependently in each client, our approach ensures each decision tree in the\nensemble is iteratively and collectively grown across clients. To preserve the\nprivacy of the client's data, we confine the information stored in the leaf\nnodes to the majority class label identified from the samples of the client's\nlocal data that reach each node. This limited disclosure preserves the\nconfidentiality of the underlying data distribution of clients, thereby\nenhancing the privacy of the federated learning process. Furthermore, our\ncollaborative ensemble construction strategy allows the ensemble to better\nreflect the data's heterogeneity across different clients, enhancing its\nperformance on non-IID data, as our experimental results confirm.\n"", '  Federated Learning (FL) is a recent model training paradigm in which client\ndevices collaboratively train a model without ever aggregating their data.\nCrucially, this scheme offers users potential privacy and security benefits by\nonly ever communicating updates to the model weights to a central server as\nopposed to traditional machine learning (ML) training which directly\ncommunicates and aggregates data. However, FL training suffers from statistical\nheterogeneity as clients may have differing local data distributions. Large\nlanguage models (LLMs) offer a potential solution to this issue of\nheterogeneity given that they have consistently been shown to be able to learn\non vast amounts of noisy data. While LLMs are a promising development for\nresolving the consistent issue of non-I.I.D. Clients in federated settings\nexacerbate two other bottlenecks in FL: limited local computing and expensive\ncommunication. This thesis aims to develop efficient training methods for LLMs\nin FL. To this end, we employ two critical techniques in enabling efficient\ntraining. First, we use low-rank adaptation (LoRA) to reduce the computational\nload of local model training. Second, we communicate sparse updates throughout\ntraining to significantly cut down on communication costs. Taken together, our\nmethod reduces communication costs by up to 10x over vanilla LoRA and up to 5x\nover more complex sparse LoRA baselines while achieving greater utility. We\nemphasize the importance of carefully applying sparsity and picking effective\nrank and sparsity configurations for federated LLM training.\n'] , [""  In this paper, we initiate the study of local model reconstruction attacks\nfor federated learning, where a honest-but-curious adversary eavesdrops the\nmessages exchanged between a targeted client and the server, and then\nreconstructs the local/personalized model of the victim. The local model\nreconstruction attack allows the adversary to trigger other classical attacks\nin a more effective way, since the local model only depends on the client's\ndata and can leak more private information than the global model learned by the\nserver. Additionally, we propose a novel model-based attribute inference attack\nin federated learning leveraging the local model reconstruction attack. We\nprovide an analytical lower-bound for this attribute inference attack.\nEmpirical results using real world datasets confirm that our local\nreconstruction attack works well for both regression and classification tasks.\nMoreover, we benchmark our novel attribute inference attack against the\nstate-of-the-art attacks in federated learning. Our attack results in higher\nreconstruction accuracy especially when the clients' datasets are\nheterogeneous. Our work provides a new angle for designing powerful and\nexplainable attacks to effectively quantify the privacy risk in FL.\n"", ""  Federated Learning (FL) has garnered significant attention for its potential\nto protect user privacy while enhancing model training efficiency. For that\nreason, FL has found its use in various domains, from healthcare to industrial\nengineering, especially where data cannot be easily exchanged due to sensitive\ninformation or privacy laws. However, recent research has demonstrated that FL\nprotocols can be easily compromised by active reconstruction attacks executed\nby dishonest servers. These attacks involve the malicious modification of\nglobal model parameters, allowing the server to obtain a verbatim copy of\nusers' private data by inverting their gradient updates. Tackling this class of\nattack remains a crucial challenge due to the strong threat model. In this\npaper, we propose a defense mechanism, namely OASIS, based on image\naugmentation that effectively counteracts active reconstruction attacks while\npreserving model performance. We first uncover the core principle of gradient\ninversion that enables these attacks and theoretically identify the main\nconditions by which the defense can be robust regardless of the attack\nstrategies. We then construct our defense with image augmentation showing that\nit can undermine the attack principle. Comprehensive evaluations demonstrate\nthe efficacy of the defense mechanism highlighting its feasibility as a\nsolution.\n"", '  Deep learning has shown incredible potential across a vast array of tasks and\naccompanying this growth has been an insatiable appetite for data. However, a\nlarge amount of data needed for enabling deep learning is stored on personal\ndevices and recent concerns on privacy have further highlighted challenges for\naccessing such data. As a result, federated learning (FL) has emerged as an\nimportant privacy-preserving technology enabling collaborative training of\nmachine learning models without the need to send the raw, potentially\nsensitive, data to a central server. However, the fundamental premise that\nsending model updates to a server is privacy-preserving only holds if the\nupdates cannot be ""reverse engineered"" to infer information about the private\ntraining data. It has been shown under a wide variety of settings that this\npremise for privacy does {\\em not} hold.\n  In this survey paper, we provide a comprehensive literature review of the\ndifferent privacy attacks and defense methods in FL. We identify the current\nlimitations of these attacks and highlight the settings in which FL client\nprivacy can be broken. We dissect some of the successful industry applications\nof FL and draw lessons for future successful adoption. We survey the emerging\nlandscape of privacy regulation for FL. We conclude with future directions for\ntaking FL toward the cherished goal of generating accurate models while\npreserving the privacy of the data from its participants.\n'] , ['  Federated learning (FL) is gaining increasing popularity in the medical\ndomain for analyzing medical images, which is considered an effective technique\nto safeguard sensitive patient data and comply with privacy regulations.\nHowever, several recent studies have revealed that the default settings of FL\nmay leak private training data under privacy attacks. Thus, it is still unclear\nwhether and to what extent such privacy risks of FL exist in the medical\ndomain, and if so, ""how to mitigate such risks?"". In this paper, first, we\npropose a holistic framework for Medical data Privacy risk analysis and\nmitigation in Federated Learning (MedPFL) to analyze privacy risks and develop\neffective mitigation strategies in FL for protecting private medical data.\nSecond, we demonstrate the substantial privacy risks of using FL to process\nmedical images, where adversaries can easily perform privacy attacks to\nreconstruct private medical images accurately. Third, we show that the defense\napproach of adding random noises may not always work effectively to protect\nmedical images against privacy attacks in FL, which poses unique and pressing\nchallenges associated with medical data for privacy protection.\n', ""  Federated Learning (FL) is a decentralized machine learning (ML) approach\nthat keeps data localized and often incorporates Differential Privacy (DP) to\nenhance privacy guarantees. Similar to previous work on DP in ML, we observed\nthat differentially private federated learning (DPFL) introduces performance\ndisparities, particularly affecting minority groups. Recent work has attempted\nto address performance fairness in vanilla FL through clustering, but this\nmethod remains sensitive and prone to errors, which are further exacerbated by\nthe DP noise in DPFL. To fill this gap, in this paper, we propose a novel\nclustered DPFL algorithm designed to effectively identify clients' clusters in\nhighly heterogeneous settings while maintaining high accuracy with DP\nguarantees. To this end, we propose to cluster clients based on both their\nmodel updates and training loss values. Our proposed approach also addresses\nthe server's uncertainties in clustering clients' model updates by employing\nlarger batch sizes along with Gaussian Mixture Model (GMM) to alleviate the\nimpact of noise and potential clustering errors, especially in\nprivacy-sensitive scenarios. We provide theoretical analysis of the\neffectiveness of our proposed approach. We also extensively evaluate our\napproach across diverse data distributions and privacy budgets and show its\neffectiveness in mitigating the disparate impact of DP in FL settings with a\nsmall computational cost.\n"", '  Despite recent progress in enhancing the privacy of federated learning (FL)\nvia differential privacy (DP), the trade-off of DP between privacy protection\nand performance is still underexplored for real-world medical scenario. In this\npaper, we propose to optimize the trade-off under the context of client-level\nDP, which focuses on privacy during communications. However, FL for medical\nimaging involves typically much fewer participants (hospitals) than other\ndomains (e.g., mobile devices), thus ensuring clients be differentially private\nis much more challenging. To tackle this problem, we propose an adaptive\nintermediary strategy to improve performance without harming privacy.\nSpecifically, we theoretically find splitting clients into sub-clients, which\nserve as intermediaries between hospitals and the server, can mitigate the\nnoises introduced by DP without harming privacy. Our proposed approach is\nempirically evaluated on both classification and segmentation tasks using two\npublic datasets, and its effectiveness is demonstrated with significant\nperformance improvements and comprehensive analytical studies. Code is\navailable at: https://github.com/med-air/Client-DP-FL.\n'] , [""  The emergence of federated learning (FL) presents a promising approach to\nleverage decentralized data while preserving privacy. Furthermore, the\ncombination of FL and anomaly detection is particularly compelling because it\nallows for detecting rare and critical anomalies (usually also rare in locally\ngathered data) in sensitive data from multiple sources, such as cybersecurity\nand healthcare. However, benchmarking the performance of anomaly detection\nmethods in FL environments remains an underexplored area. This paper introduces\nFedAD-Bench, a unified benchmark for evaluating unsupervised anomaly detection\nalgorithms within the context of FL. We systematically analyze and compare the\nperformance of recent deep learning anomaly detection models under federated\nsettings, which were typically assessed solely in centralized settings.\nFedAD-Bench encompasses diverse datasets and metrics to provide a holistic\nevaluation. Through extensive experiments, we identify key challenges such as\nmodel aggregation inefficiencies and metric unreliability. We present insights\ninto FL's regularization effects, revealing scenarios in which it outperforms\ncentralized approaches due to its inherent ability to mitigate overfitting. Our\nwork aims to establish a standardized benchmark to guide future research and\ndevelopment in federated anomaly detection, promoting reproducibility and fair\ncomparison across studies.\n"", ""  Federated learning (FL) is a decentralized learning technique that enables\nparticipating devices to collaboratively build a shared Machine Leaning (ML) or\nDeep Learning (DL) model without revealing their raw data to a third party. Due\nto its privacy-preserving nature, FL has sparked widespread attention for\nbuilding Intrusion Detection Systems (IDS) within the realm of cybersecurity.\nHowever, the data heterogeneity across participating domains and entities\npresents significant challenges for the reliable implementation of an FL-based\nIDS. In this paper, we propose an effective method called Statistical Averaging\n(StatAvg) to alleviate non-independently and identically (non-iid) distributed\nfeatures across local clients' data in FL. In particular, StatAvg allows the FL\nclients to share their individual data statistics with the server, which then\naggregates this information to produce global statistics. The latter are shared\nwith the clients and used for universal data normalisation. It is worth\nmentioning that StatAvg can seamlessly integrate with any FL aggregation\nstrategy, as it occurs before the actual FL training process. The proposed\nmethod is evaluated against baseline approaches using datasets for network and\nhost Artificial Intelligence (AI)-powered IDS. The experimental results\ndemonstrate the efficiency of StatAvg in mitigating non-iid feature\ndistributions across the FL clients compared to the baseline methods.\n"", '  The increasing security and privacy concerns in the Smart Grid sector have\nled to a significant demand for robust intrusion detection systems within\ncritical smart grid infrastructure. To address the challenges posed by privacy\npreservation and decentralized power system zones with distinct data ownership,\nFederated Learning (FL) has emerged as a promising privacy-preserving solution\nwhich facilitates collaborative training of attack detection models without\nnecessitating the sharing of raw data. However, FL presents several\nimplementation limitations in the power system domain due to its heavy reliance\non a centralized aggregator and the risks of privacy leakage during model\nupdate transmission. To overcome these technical bottlenecks, this paper\nintroduces a novel decentralized federated anomaly detection scheme based on\ntwo main gossip protocols namely Random Walk and Epidemic. Our findings\nindicate that the Random Walk protocol exhibits superior performance compared\nto the Epidemic protocol, highlighting its efficacy in decentralized federated\nlearning environments. Experimental validation of the proposed framework\nutilizing publicly available industrial control systems datasets demonstrates\nsuperior attack detection accuracy while safeguarding data confidentiality and\nmitigating the impact of communication latency and stragglers. Furthermore, our\napproach yields a notable 35% improvement in training time compared to\nconventional FL, underscoring the efficacy and robustness of our decentralized\nlearning method.\n']",Federated Learning for Privacy-Preserving Machine Learning,Federated Learning and Unlearning for Data Privacy
64,"Backdoor Attacks on Graph Neural Networks , Backdoor Attacks on Large Foundation Models","['adversarial', 'backdoors', 'backdoor', 'attacks', 'networks', 'vulnerabilities', 'graphs', 'graphmu', 'gnns', 'adversary'] , ['backdoors', 'backdoor', 'adversarial', 'attacks', 'backdoored', 'attacker', 'vulnerability', 'malicious', 'security', 'obfuscation']","['  Graph Neural Networks (GNNs) have gained popularity in numerous domains, yet\nthey are vulnerable to backdoor attacks that can compromise their performance\nand ethical application. The detection of these attacks is crucial for\nmaintaining the reliability and security of GNN classification tasks, but\neffective detection techniques are lacking. Following an initial investigation,\nwe observed that while graph-level explanations can offer limited insights,\ntheir effectiveness in detecting backdoor triggers is inconsistent and\nincomplete. To bridge this gap, we extract and transform secondary outputs of\nGNN explanation mechanisms, designing seven novel metrics that more effectively\ndetect backdoor attacks. Additionally, we develop an adaptive attack to\nrigorously evaluate our approach. We test our method on multiple benchmark\ndatasets and examine its efficacy against various attack models. Our results\nshow that our method can achieve high detection performance, marking a\nsignificant advancement in safeguarding GNNs against backdoor attacks.\n', '  Graph Neural Networks (GNNs) are a class of deep learning models capable of\nprocessing graph-structured data, and they have demonstrated significant\nperformance in a variety of real-world applications. Recent studies have found\nthat GNN models are vulnerable to backdoor attacks. When specific patterns\n(called backdoor triggers, e.g., subgraphs, nodes, etc.) appear in the input\ndata, the backdoor embedded in the GNN models is activated, which misclassifies\nthe input data into the target class label specified by the attacker, whereas\nwhen there are no backdoor triggers in the input, the backdoor embedded in the\nGNN models is not activated, and the models work normally. Backdoor attacks are\nhighly stealthy and expose GNN models to serious security risks. Currently,\nresearch on backdoor attacks against GNNs mainly focus on tasks such as graph\nclassification and node classification, and backdoor attacks against link\nprediction tasks are rarely studied. In this paper, we propose a backdoor\nattack against the link prediction tasks based on GNNs and reveal the existence\nof such security vulnerability in GNN models, which make the backdoored GNN\nmodels to incorrectly predict unlinked two nodes as having a link relationship\nwhen a trigger appear. The method uses a single node as the trigger and poison\nselected node pairs in the training graph, and then the backdoor will be\nembedded in the GNN models through the training process. In the inference\nstage, the backdoor in the GNN models can be activated by simply linking the\ntrigger node to the two end nodes of the unlinked node pairs in the input data,\ncausing the GNN models to produce incorrect link prediction results for the\ntarget node pairs.\n', '  Graph Neural Networks (GNNs) are gaining popularity across various domains\ndue to their effectiveness in learning graph-structured data. Nevertheless,\nthey have been shown to be susceptible to backdoor poisoning attacks, which\npose serious threats to real-world applications. Meanwhile, graph reduction\ntechniques, including coarsening and sparsification, which have long been\nemployed to improve the scalability of large graph computational tasks, have\nrecently emerged as effective methods for accelerating GNN training on\nlarge-scale graphs. However, the current development and deployment of graph\nreduction techniques for large graphs overlook the potential risks of data\npoisoning attacks against GNNs. It is not yet clear how graph reduction\ninteracts with existing backdoor attacks. This paper conducts a thorough\nexamination of the robustness of graph reduction methods in scalable GNN\ntraining in the presence of state-of-the-art backdoor attacks. We performed a\ncomprehensive robustness analysis across six coarsening methods and six\nsparsification methods for graph reduction, under three GNN backdoor attacks\nagainst three GNN architectures. Our findings indicate that the effectiveness\nof graph reduction methods in mitigating attack success rates varies\nsignificantly, with some methods even exacerbating the attacks. Through\ndetailed analyses of triggers and poisoned nodes, we interpret our findings and\nenhance our understanding of how graph reduction influences robustness against\nbackdoor attacks. These results highlight the critical need for incorporating\nrobustness considerations in graph reduction for GNN training, ensuring that\nenhancements in computational efficiency do not compromise the security of GNN\nsystems.\n'] , ['  One key challenge in backdoor attacks against large foundation models is the\nresource limits. Backdoor attacks usually require retraining the target model,\nwhich is impractical for very large foundation models. Existing backdoor\nattacks are mainly designed for supervised classifiers or small foundation\nmodels (e.g., BERT). None of these attacks has successfully compromised a very\nlarge foundation model, such as Llama-3-70B, especially with limited\ncomputational resources. In this paper, we propose TrojFM, a novel backdoor\nattack tailored for very large foundation models. Our primary technical\ncontribution is the development of a novel backdoor injection method. This\nmethod forces a backdoored model to generate similar hidden representations for\npoisoned inputs regardless of their actual semantics. Our approach injects such\nbackdoors by fine-tuning only a very small proportion of model parameters. This\nenables TrojFM to efficiently launch downstream task-agnostic backdoor attacks\nagainst very large foundation models under limited computational resources.\nMoreover, we optimize the fine-tuning process with our customized QLoRA\ntechnique, enabling launching our attack via only~\\textit{one A100 GPU}.\nFurthermore, we design a new trigger injection method to ensure our attack\nstealthiness. Through extensive experiments, we first demonstrate that TrojFM\ncan launch effective backdoor attacks against widely used large GPT-style\nmodels without jeopardizing their normal functionalities (and outperforming\nexisting attacks on BERT-style models). Furthermore, we show that TrojFM is\nresilient to SOTA defenses and is insensitive to changes in key\nhyper-parameters. Finally, we conduct a resource analysis to quantify that our\nmethod can significantly save computational and memory costs compared to\nexisting backdoor attacks.\n', '  The field of few-shot learning (FSL) has shown promising results in scenarios\nwhere training data is limited, but its vulnerability to backdoor attacks\nremains largely unexplored. We first explore this topic by first evaluating the\nperformance of the existing backdoor attack methods on few-shot learning\nscenarios. Unlike in standard supervised learning, existing backdoor attack\nmethods failed to perform an effective attack in FSL due to two main issues.\nFirstly, the model tends to overfit to either benign features or trigger\nfeatures, causing a tough trade-off between attack success rate and benign\naccuracy. Secondly, due to the small number of training samples, the dirty\nlabel or visible trigger in the support set can be easily detected by victims,\nwhich reduces the stealthiness of attacks. It seemed that FSL could survive\nfrom backdoor attacks. However, in this paper, we propose the Few-shot Learning\nBackdoor Attack (FLBA) to show that FSL can still be vulnerable to backdoor\nattacks. Specifically, we first generate a trigger to maximize the gap between\npoisoned and benign features. It enables the model to learn both benign and\ntrigger features, which solves the problem of overfitting. To make it more\nstealthy, we hide the trigger by optimizing two types of imperceptible\nperturbation, namely attractive and repulsive perturbation, instead of\nattaching the trigger directly. Once we obtain the perturbations, we can poison\nall samples in the benign support set into a hidden poisoned support set and\nfine-tune the model on it. Our method demonstrates a high Attack Success Rate\n(ASR) in FSL tasks with different few-shot learning paradigms while preserving\nclean accuracy and maintaining stealthiness. This study reveals that few-shot\nlearning still suffers from backdoor attacks, and its security should be given\nattention.\n', '  Backdoor attacks involve the injection of a limited quantity of poisoned\nexamples containing triggers into the training dataset. During the inference\nstage, backdoor attacks can uphold a high level of accuracy for normal\nexamples, yet when presented with trigger-containing instances, the model may\nerroneously predict them as the targeted class designated by the attacker. This\npaper explores strategies for mitigating the risks associated with backdoor\nattacks by examining the filtration of poisoned samples.We primarily leverage\ntwo key characteristics of backdoor attacks: the ability for multiple backdoors\nto exist simultaneously within a single model, and the discovery through\nComposite Backdoor Attack (CBA) that altering two triggers in a sample to new\ntarget labels does not compromise the original functionality of the triggers,\nyet enables the prediction of the data as a new target class when both triggers\nare present simultaneously.Therefore, a novel three-stage poisoning data\nfiltering approach, known as Composite Backdoor Poison Filtering (CBPF), is\nproposed as an effective solution. Firstly, utilizing the identified\ndistinctions in output between poisoned and clean samples, a subset of data is\npartitioned to include both poisoned and clean instances. Subsequently, benign\ntriggers are incorporated and labels are adjusted to create new target and\nbenign target classes, thereby prompting the poisoned and clean data to be\nclassified as distinct entities during the inference stage. The experimental\nresults indicate that CBPF is successful in filtering out malicious data\nproduced by six advanced attacks on CIFAR10 and ImageNet-12. On average, CBPF\nattains a notable filtering success rate of 99.91% for the six attacks on\nCIFAR10. Additionally, the model trained on the uncontaminated samples exhibits\nsustained high accuracy levels.\n']",Backdoor Attacks on AI Models,Backdoor Attacks on Large Foundation Models
65,"Adversarial Attacks on Deep Learning Models , Backdoor Attacks on Large Language Models , Adversarial Attacks on NLP Models , ""Adversarial Attacks on Multimodal Vision-Language Models"" , ""Adversarial Attacks on Reinforcement Learning"" , ""Adversarial Attacks on Retrieval-Augmented Language Models"" , ""Adversarial Attacks on Large Language Models"" , ""Adversarial Attacks on Large Language Models""","['adversarial', 'adversarially', 'adversary', 'attacks', 'attacker', 'adversaries', 'backdoors', 'defenses', 'classifiers', 'threats'] , ['backdoors', 'adversarial', 'backdoor', 'attacks', 'vulnerabilities', 'malicious', 'security', 'vulnerability', 'backdoored', 'anydoor'] , ['adversarial', 'adversary', 'adversaries', 'malicious', 'attacks', 'vulnerabilities', 'attacking', 'threat', 'attack', 'vulnerability'] , ['adversarial', 'captioner', 'multimodal', 'captioning', 'embeddings', 'captions', 'visual', 'adversary', 'vulnerabilities', 'attacking'] , ['adversarial', 'adversary', 'attackers', 'attacks', 'attacker', 'adversaries', 'attack', 'defenses', 'threat', 'reinforcement'] , ['adversarial', 'exploitability', 'attacker', 'malicious', 'vulnerability', 'attacks', 'retrieval', 'security', 'attackers', 'adversary'] , ['adversarial', 'malicious', 'vulnerabilities', 'attacker', 'threats', 'vulnerability', 'security', 'attacks', 'attackers', 'vulnerable'] , ['adversarial', 'teaming', 'threat', 'unsafe', 'attacker', 'attackers', 'attacking', 'vulnerability', 'vulnerabilities', 'attacks']","[""  Deep neural networks (DNNs) are easily fooled by adversarial perturbations\nthat are imperceptible to humans. Adversarial training, a process where\nadversarial examples are added to the training set, is the current\nstate-of-the-art defense against adversarial attacks, but it lowers the model's\naccuracy on clean inputs, is computationally expensive, and offers less\nrobustness to natural noise. In contrast, energy-based models (EBMs), which\nwere designed for efficient implementation in neuromorphic hardware and\nphysical systems, incorporate feedback connections from each layer to the\nprevious layer, yielding a recurrent, deep-attractor architecture which we\nhypothesize should make them naturally robust. Our work is the first to explore\nthe robustness of EBMs to both natural corruptions and adversarial attacks,\nwhich we do using the CIFAR-10 and CIFAR-100 datasets. We demonstrate that EBMs\nare more robust than transformers and display comparable robustness to\nadversarially-trained DNNs on gradient-based (white-box) attacks, query-based\n(black-box) attacks, and natural perturbations without sacrificing clean\naccuracy, and without the need for adversarial training or additional training\ntechniques.\n"", '  As deep learning (DL) models are increasingly being integrated into our\neveryday lives, ensuring their safety by making them robust against adversarial\nattacks has become increasingly critical. DL models have been found to be\nsusceptible to adversarial attacks which can be achieved by introducing small,\ntargeted perturbations to disrupt the input data. Adversarial training has been\npresented as a mitigation strategy which can result in more robust models. This\nadversarial robustness comes with additional computational costs required to\ndesign adversarial attacks during training. The two objectives -- adversarial\nrobustness and computational efficiency -- then appear to be in conflict of\neach other. In this work, we explore the effects of two different model\ncompression methods -- structured weight pruning and quantization -- on\nadversarial robustness. We specifically explore the effects of fine-tuning on\ncompressed models, and present the trade-off between standard fine-tuning and\nadversarial fine-tuning. Our results show that compression does not inherently\nlead to loss in model robustness and adversarial fine-tuning of a compressed\nmodel can yield large improvement to the robustness performance of models. We\npresent experiments on two benchmark datasets showing that adversarial\nfine-tuning of compressed models can achieve robustness performance comparable\nto adversarially trained models, while also improving computational efficiency.\n', '  The rapid advancement of artificial intelligence within the realm of\ncybersecurity raises significant security concerns. The vulnerability of deep\nlearning models in adversarial attacks is one of the major issues. In\nadversarial machine learning, malicious users try to fool the deep learning\nmodel by inserting adversarial perturbation inputs into the model during its\ntraining or testing phase. Subsequently, it reduces the model confidence score\nand results in incorrect classifications. The novel key contribution of the\nresearch is to empirically test the black-box adversarial transferability\nphenomena in cyber attack detection systems. It indicates that the adversarial\nperturbation input generated through the surrogate model has a similar impact\non the target model in producing the incorrect classification. To empirically\nvalidate this phenomenon, surrogate and target models are used. The adversarial\nperturbation inputs are generated based on the surrogate-model for which the\nhacker has complete information. Based on these adversarial perturbation\ninputs, both surrogate and target models are evaluated during the inference\nphase. We have done extensive experimentation over the CICDDoS-2019 dataset,\nand the results are classified in terms of various performance metrics like\naccuracy, precision, recall, and f1-score. The findings indicate that any deep\nlearning model is highly susceptible to adversarial attacks, even if the\nattacker does not have access to the internal details of the target model. The\nresults also indicate that white-box adversarial attacks have a severe impact\ncompared to black-box adversarial attacks. There is a need to investigate and\nexplore adversarial defence techniques to increase the robustness of the deep\nlearning models against adversarial attacks.\n'] , ['  Large Language Models (LLMs) have shown significant promise in\ndecision-making tasks when fine-tuned on specific applications, leveraging\ntheir inherent common sense and reasoning abilities learned from vast amounts\nof data. However, these systems are exposed to substantial safety and security\nrisks during the fine-tuning phase. In this work, we propose the first\ncomprehensive framework for Backdoor Attacks against LLM-enabled\nDecision-making systems (BALD), systematically exploring how such attacks can\nbe introduced during the fine-tuning phase across various channels.\nSpecifically, we propose three attack mechanisms and corresponding backdoor\noptimization methods to attack different components in the LLM-based\ndecision-making pipeline: word injection, scenario manipulation, and knowledge\ninjection. Word injection embeds trigger words directly into the query prompt.\nScenario manipulation occurs in the physical environment, where a high-level\nbackdoor semantic scenario triggers the attack. Knowledge injection conducts\nbackdoor attacks on retrieval augmented generation (RAG)-based LLM systems,\nstrategically injecting word triggers into poisoned knowledge while ensuring\nthe information remains factually accurate for stealthiness. We conduct\nextensive experiments with three popular LLMs (GPT-3.5, LLaMA2, PaLM2), using\ntwo datasets (HighwayEnv, nuScenes), and demonstrate the effectiveness and\nstealthiness of our backdoor triggers and mechanisms. Finally, we critically\nassess the strengths and weaknesses of our proposed approaches, highlight the\ninherent vulnerabilities of LLMs in decision-making tasks, and evaluate\npotential defenses to safeguard LLM-based decision making systems.\n', '  Leveraging the rapid development of Large Language Models LLMs, LLM-based\nagents have been developed to handle various real-world applications, including\nfinance, healthcare, and shopping, etc. It is crucial to ensure the reliability\nand security of LLM-based agents during applications. However, the safety\nissues of LLM-based agents are currently under-explored. In this work, we take\nthe first step to investigate one of the typical safety threats, backdoor\nattack, to LLM-based agents. We first formulate a general framework of agent\nbackdoor attacks, then we present a thorough analysis on the different forms of\nagent backdoor attacks. Specifically, from the perspective of the final\nattacking outcomes, the attacker can either choose to manipulate the final\noutput distribution, or only introduce malicious behavior in the intermediate\nreasoning process, while keeping the final output correct. Furthermore, the\nformer category can be divided into two subcategories based on trigger\nlocations: the backdoor trigger can be hidden either in the user query or in an\nintermediate observation returned by the external environment. We propose the\ncorresponding data poisoning mechanisms to implement the above variations of\nagent backdoor attacks on two typical agent tasks, web shopping and tool\nutilization. Extensive experiments show that LLM-based agents suffer severely\nfrom backdoor attacks, indicating an urgent need for further research on the\ndevelopment of defenses against backdoor attacks on LLM-based agents. Warning:\nThis paper may contain biased content.\n', '  The large language models (LLMs), which bridge the gap between human language\nunderstanding and complex problem-solving, achieve state-of-the-art performance\non several NLP tasks, particularly in few-shot and zero-shot settings. Despite\nthe demonstrable efficacy of LMMs, due to constraints on computational\nresources, users have to engage with open-source language models or outsource\nthe entire training process to third-party platforms. However, research has\ndemonstrated that language models are susceptible to potential security\nvulnerabilities, particularly in backdoor attacks. Backdoor attacks are\ndesigned to introduce targeted vulnerabilities into language models by\npoisoning training samples or model weights, allowing attackers to manipulate\nmodel responses through malicious triggers. While existing surveys on backdoor\nattacks provide a comprehensive overview, they lack an in-depth examination of\nbackdoor attacks specifically targeting LLMs. To bridge this gap and grasp the\nlatest trends in the field, this paper presents a novel perspective on backdoor\nattacks for LLMs by focusing on fine-tuning methods. Specifically, we\nsystematically classify backdoor attacks into three categories: full-parameter\nfine-tuning, parameter-efficient fine-tuning, and attacks without fine-tuning.\nBased on insights from a substantial review, we also discuss crucial issues for\nfuture research on backdoor attacks, such as further exploring attack\nalgorithms that do not require fine-tuning, or developing more covert attack\nalgorithms.\n'] , ['  Recent studies have revealed the vulnerability of pre-trained language models\nto adversarial attacks. Existing adversarial defense techniques attempt to\nreconstruct adversarial examples within feature or text spaces. However, these\nmethods struggle to effectively repair the semantics in adversarial examples,\nresulting in unsatisfactory performance and limiting their practical utility.\nTo repair the semantics in adversarial examples, we introduce a novel approach\nnamed Reactive Perturbation Defocusing (Rapid). Rapid employs an adversarial\ndetector to identify fake labels of adversarial examples and leverage\nadversarial attackers to repair the semantics in adversarial examples. Our\nextensive experimental results conducted on four public datasets, convincingly\ndemonstrate the effectiveness of Rapid in various adversarial attack scenarios.\nTo address the problem of defense performance validation in previous works, we\nprovide a demonstration of adversarial detection and repair based on our work,\nwhich can be easily evaluated at https://tinyurl.com/22ercuf8.\n', '  Text classification systems have been proven vulnerable to adversarial text\nexamples, modified versions of the original text examples that are often\nunnoticed by human eyes, yet can force text classification models to alter\ntheir classification. Often, research works quantifying the impact of\nadversarial text attacks have been applied only to models trained in English.\nIn this paper, we introduce the first word-level study of adversarial attacks\nin Arabic. Specifically, we use a synonym (word-level) attack using a Masked\nLanguage Modeling (MLM) task with a BERT model in a black-box setting to assess\nthe robustness of the state-of-the-art text classification models to\nadversarial attacks in Arabic. To evaluate the grammatical and semantic\nsimilarities of the newly produced adversarial examples using our synonym\nBERT-based attack, we invite four human evaluators to assess and compare the\nproduced adversarial examples with their original examples. We also study the\ntransferability of these newly produced Arabic adversarial examples to various\nmodels and investigate the effectiveness of defense mechanisms against these\nadversarial examples on the BERT models. We find that fine-tuned BERT models\nwere more susceptible to our synonym attacks than the other Deep Neural\nNetworks (DNN) models like WordCNN and WordLSTM we trained. We also find that\nfine-tuned BERT models were more susceptible to transferred attacks. We,\nlastly, find that fine-tuned BERT models successfully regain at least 2% in\naccuracy after applying adversarial training as an initial defense mechanism.\n', '  Deep neural networks have been proven to be vulnerable to adversarial\nexamples and various methods have been proposed to defend against adversarial\nattacks for natural language processing tasks. However, previous defense\nmethods have limitations in maintaining effective defense while ensuring the\nperformance of the original task. In this paper, we propose a malicious\nperturbation based adversarial training method (MPAT) for building robust deep\nneural networks against textual adversarial attacks. Specifically, we construct\na multi-level malicious example generation strategy to generate adversarial\nexamples with malicious perturbations, which are used instead of original\ninputs for model training. Additionally, we employ a novel training objective\nfunction to ensure achieving the defense goal without compromising the\nperformance on the original task. We conduct comprehensive experiments to\nevaluate our defense method by attacking five victim models on three benchmark\ndatasets. The result demonstrates that our method is more effective against\nmalicious adversarial attacks compared with previous defense methods while\nmaintaining or further improving the performance on the original task.\n'] , [""  Vision-enabled language models (VLMs) are now used to build autonomous\nmultimodal agents capable of taking actions in real environments. In this\npaper, we show that multimodal agents raise new safety risks, even though\nattacking agents is more challenging than prior attacks due to limited access\nto and knowledge about the environment. Our attacks use adversarial text\nstrings to guide gradient-based perturbation over one trigger image in the\nenvironment: (1) our captioner attack attacks white-box captioners if they are\nused to process images into captions as additional inputs to the VLM; (2) our\nCLIP attack attacks a set of CLIP models jointly, which can transfer to\nproprietary VLMs. To evaluate the attacks, we curated VisualWebArena-Adv, a set\nof adversarial tasks based on VisualWebArena, an environment for web-based\nmultimodal agent tasks. Within an L-infinity norm of $16/256$ on a single\nimage, the captioner attack can make a captioner-augmented GPT-4V agent execute\nthe adversarial goals with a 75% success rate. When we remove the captioner or\nuse GPT-4V to generate its own captions, the CLIP attack can achieve success\nrates of 21% and 43%, respectively. Experiments on agents based on other VLMs,\nsuch as Gemini-1.5, Claude-3, and GPT-4o, show interesting differences in their\nrobustness. Further analysis reveals several key factors contributing to the\nattack's success, and we also discuss the implications for defenses as well.\nProject page: https://chenwu.io/attack-agent Code and data:\nhttps://github.com/ChenWu98/agent-attack\n"", ""  Vision-language models (VLMs) have achieved significant strides in recent\ntimes specially in multimodal tasks, yet they remain susceptible to adversarial\nattacks on their vision components. To address this, we propose Sim-CLIP, an\nunsupervised adversarial fine-tuning method that enhances the robustness of the\nwidely-used CLIP vision encoder against such attacks while maintaining semantic\nrichness and specificity. By employing a Siamese architecture with cosine\nsimilarity loss, Sim-CLIP learns semantically meaningful and attack-resilient\nvisual representations without requiring large batch sizes or momentum\nencoders. Our results demonstrate that VLMs enhanced with Sim-CLIP's fine-tuned\nCLIP encoder exhibit significantly enhanced robustness against adversarial\nattacks, while preserving semantic meaning of the perturbed images. Notably,\nSim-CLIP does not require additional training or fine-tuning of the VLM itself;\nreplacing the original vision encoder with our fine-tuned Sim-CLIP suffices to\nprovide robustness. This work underscores the significance of reinforcing\nfoundational models like CLIP to safeguard the reliability of downstream VLM\napplications, paving the way for more secure and effective multimodal systems.\n"", '  Vision-Language Models (VLMs) are becoming increasingly vulnerable to\nadversarial attacks as various novel attack strategies are being proposed\nagainst these models. While existing defenses excel in unimodal contexts, they\ncurrently fall short in safeguarding VLMs against adversarial threats. To\nmitigate this vulnerability, we propose a novel, yet elegantly simple approach\nfor detecting adversarial samples in VLMs. Our method leverages Text-to-Image\n(T2I) models to generate images based on captions produced by target VLMs.\nSubsequently, we calculate the similarities of the embeddings of both input and\ngenerated images in the feature space to identify adversarial samples.\nEmpirical evaluations conducted on different datasets validate the efficacy of\nour approach, outperforming baseline methods adapted from image classification\ndomains. Furthermore, we extend our methodology to classification tasks,\nshowcasing its adaptability and model-agnostic nature. Theoretical analyses and\nempirical findings also show the resilience of our approach against adaptive\nattacks, positioning it as an excellent defense mechanism for real-world\ndeployment against adversarial threats.\n'] , [""  To ensure the usefulness of Reinforcement Learning (RL) in real systems, it\nis crucial to ensure they are robust to noise and adversarial attacks. In\nadversarial RL, an external attacker has the power to manipulate the victim\nagent's interaction with the environment. We study the full class of online\nmanipulation attacks, which include (i) state attacks, (ii) observation attacks\n(which are a generalization of perceived-state attacks), (iii) action attacks,\nand (iv) reward attacks. We show the attacker's problem of designing a stealthy\nattack that maximizes its own expected reward, which often corresponds to\nminimizing the victim's value, is captured by a Markov Decision Process (MDP)\nthat we call a meta-MDP since it is not the true environment but a higher level\nenvironment induced by the attacked interaction. We show that the attacker can\nderive optimal attacks by planning in polynomial time or learning with\npolynomial sample complexity using standard RL techniques. We argue that the\noptimal defense policy for the victim can be computed as the solution to a\nstochastic Stackelberg game, which can be further simplified into a\npartially-observable turn-based stochastic game (POTBSG). Neither the attacker\nnor the victim would benefit from deviating from their respective optimal\npolicies, thus such solutions are truly robust. Although the defense problem is\nNP-hard, we show that optimal Markovian defenses can be computed (learned) in\npolynomial time (sample complexity) in many scenarios.\n"", '  Most existing works focus on direct perturbations to the victim\'s\nstate/action or the underlying transition dynamics to demonstrate the\nvulnerability of reinforcement learning agents to adversarial attacks. However,\nsuch direct manipulations may not be always realizable. In this paper, we\nconsider a multi-agent setting where a well-trained victim agent $\\nu$ is\nexploited by an attacker controlling another agent $\\alpha$ with an\n\\textit{adversarial policy}. Previous models do not account for the possibility\nthat the attacker may only have partial control over $\\alpha$ or that the\nattack may produce easily detectable ""abnormal"" behaviors. Furthermore, there\nis a lack of provably efficient defenses against these adversarial policies. To\naddress these limitations, we introduce a generalized attack framework that has\nthe flexibility to model to what extent the adversary is able to control the\nagent, and allows the attacker to regulate the state distribution shift and\nproduce stealthier adversarial policies. Moreover, we offer a provably\nefficient defense with polynomial convergence to the most robust victim policy\nthrough adversarial training with timescale separation. This stands in sharp\ncontrast to supervised learning, where adversarial training typically provides\nonly \\textit{empirical} defenses. Using the Robosumo competition experiments,\nwe show that our generalized attack formulation results in much stealthier\nadversarial policies when maintaining the same winning rate as baselines.\nAdditionally, our adversarial training approach yields stable learning dynamics\nand less exploitable victim policies.\n', ""  This study considers the attack on reinforcement learning agents where the\nadversary aims to control the victim's behavior as specified by the adversary\nby adding adversarial modifications to the victim's state observation. While\nsome attack methods reported success in manipulating the victim agent's\nbehavior, these methods often rely on environment-specific heuristics. In\naddition, all existing attack methods require white-box access to the victim's\npolicy. In this study, we propose a novel method for manipulating the victim\nagent in the black-box (i.e., the adversary is allowed to observe the victim's\nstate and action only) and no-box (i.e., the adversary is allowed to observe\nthe victim's state only) setting without requiring environment-specific\nheuristics. Our attack method is formulated as a bi-level optimization problem\nthat is reduced to a distribution matching problem and can be solved by an\nexisting imitation learning algorithm in the black-box and no-box settings.\nEmpirical evaluations on several reinforcement learning benchmarks show that\nour proposed method has superior attack performance to baselines.\n""] , [""  Retrieval Augmented Generation (RAG) expands the capabilities of modern large\nlanguage models (LLMs) in chatbot applications, enabling developers to adapt\nand personalize the LLM output without expensive training or fine-tuning. RAG\nsystems use an external knowledge database to retrieve the most relevant\ndocuments for a given query, providing this context to the LLM generator. While\nRAG achieves impressive utility in many applications, its adoption to enable\npersonalized generative models introduces new security risks. In this work, we\npropose new attack surfaces for an adversary to compromise a victim's RAG\nsystem, by injecting a single malicious document in its knowledge database. We\ndesign Phantom, general two-step attack framework against RAG augmented LLMs.\nThe first step involves crafting a poisoned document designed to be retrieved\nby the RAG system within the top-k results only when an adversarial trigger, a\nspecific sequence of words acting as backdoor, is present in the victim's\nqueries. In the second step, a specially crafted adversarial string within the\npoisoned document triggers various adversarial attacks in the LLM generator,\nincluding denial of service, reputation damage, privacy violations, and harmful\nbehaviors. We demonstrate our attacks on multiple LLM architectures, including\nGemma, Vicuna, and Llama.\n"", '  Large Language Models (LLMs) are constrained by outdated information and a\ntendency to generate incorrect data, commonly referred to as ""hallucinations.""\nRetrieval-Augmented Generation (RAG) addresses these limitations by combining\nthe strengths of retrieval-based methods and generative models. This approach\ninvolves retrieving relevant information from a large, up-to-date dataset and\nusing it to enhance the generation process, leading to more accurate and\ncontextually appropriate responses. Despite its benefits, RAG introduces a new\nattack surface for LLMs, particularly because RAG databases are often sourced\nfrom public data, such as the web. In this paper, we propose \\TrojRAG{} to\nidentify the vulnerabilities and attacks on retrieval parts (RAG database) and\ntheir indirect attacks on generative parts (LLMs). Specifically, we identify\nthat poisoning several customized content passages could achieve a retrieval\nbackdoor, where the retrieval works well for clean queries but always returns\ncustomized poisoned adversarial queries. Triggers and poisoned passages can be\nhighly customized to implement various attacks. For example, a trigger could be\na semantic group like ""The Republican Party, Donald Trump, etc."" Adversarial\npassages can be tailored to different contents, not only linked to the triggers\nbut also used to indirectly attack generative LLMs without modifying them.\nThese attacks can include denial-of-service attacks on RAG and semantic\nsteering attacks on LLM generations conditioned by the triggers. Our\nexperiments demonstrate that by just poisoning 10 adversarial passages can\ninduce 98.2\\% success rate to retrieve the adversarial passages. Then, these\npassages can increase the reject ratio of RAG-based GPT-4 from 0.01\\% to 74.6\\%\nor increase the rate of negative responses from 0.22\\% to 72\\% for targeted\nqueries.\n', '  Large language models (LLMs) have achieved remarkable success due to their\nexceptional generative capabilities. Despite their success, they also have\ninherent limitations such as a lack of up-to-date knowledge and hallucination.\nRetrieval-Augmented Generation (RAG) is a state-of-the-art technique to\nmitigate these limitations. The key idea of RAG is to ground the answer\ngeneration of an LLM on external knowledge retrieved from a knowledge database.\nExisting studies mainly focus on improving the accuracy or efficiency of RAG,\nleaving its security largely unexplored. We aim to bridge the gap in this work.\nWe find that the knowledge database in a RAG system introduces a new and\npractical attack surface. Based on this attack surface, we propose PoisonedRAG,\nthe first knowledge corruption attack to RAG, where an attacker could inject a\nfew malicious texts into the knowledge database of a RAG system to induce an\nLLM to generate an attacker-chosen target answer for an attacker-chosen target\nquestion. We formulate knowledge corruption attacks as an optimization problem,\nwhose solution is a set of malicious texts. Depending on the background\nknowledge (e.g., black-box and white-box settings) of an attacker on a RAG\nsystem, we propose two solutions to solve the optimization problem,\nrespectively. Our results show PoisonedRAG could achieve a 90% attack success\nrate when injecting five malicious texts for each target question into a\nknowledge database with millions of texts. We also evaluate several defenses\nand our results show they are insufficient to defend against PoisonedRAG,\nhighlighting the need for new defenses.\n'] , ['  As Large Language Models (LLMs) increasingly become key components in various\nAI applications, understanding their security vulnerabilities and the\neffectiveness of defense mechanisms is crucial. This survey examines the\nsecurity challenges of LLMs, focusing on two main areas: Prompt Hacking and\nAdversarial Attacks, each with specific types of threats. Under Prompt Hacking,\nwe explore Prompt Injection and Jailbreaking Attacks, discussing how they work,\ntheir potential impacts, and ways to mitigate them. Similarly, we analyze\nAdversarial Attacks, breaking them down into Data Poisoning Attacks and\nBackdoor Attacks. This structured examination helps us understand the\nrelationships between these vulnerabilities and the defense strategies that can\nbe implemented. The survey highlights these security challenges and discusses\nrobust defensive frameworks to protect LLMs against these threats. By detailing\nthese security issues, the survey contributes to the broader discussion on\ncreating resilient AI systems that can resist sophisticated attacks.\n', '  Large Language Models (LLMs) have performed exceptionally in various\ntext-generative tasks, including question answering, translation, code\ncompletion, etc. However, the over-assistance of LLMs has raised the challenge\nof ""jailbreaking"", which induces the model to generate malicious responses\nagainst the usage policy and society by designing adversarial prompts. With the\nemergence of jailbreak attack methods exploiting different vulnerabilities in\nLLMs, the corresponding safety alignment measures are also evolving. In this\npaper, we propose a comprehensive and detailed taxonomy of jailbreak attack and\ndefense methods. For instance, the attack methods are divided into black-box\nand white-box attacks based on the transparency of the target model. Meanwhile,\nwe classify defense methods into prompt-level and model-level defenses.\nAdditionally, we further subdivide these attack and defense methods into\ndistinct sub-classes and present a coherent diagram illustrating their\nrelationships. We also conduct an investigation into the current evaluation\nmethods and compare them from different perspectives. Our findings aim to\ninspire future research and practical implementations in safeguarding LLMs\nagainst adversarial attacks. Above all, although jailbreak remains a\nsignificant concern within the community, we believe that our work enhances the\nunderstanding of this domain and provides a foundation for developing more\nsecure LLMs.\n', ""  Although safely enhanced Large Language Models (LLMs) have achieved\nremarkable success in tackling various complex tasks in a zero-shot manner,\nthey remain susceptible to jailbreak attacks, particularly the unknown\njailbreak attack. To enhance LLMs' generalized defense capabilities, we propose\na two-stage adversarial tuning framework, which generates adversarial prompts\nto explore worst-case scenarios by optimizing datasets containing pairs of\nadversarial prompts and their safe responses. In the first stage, we introduce\nthe hierarchical meta-universal adversarial prompt learning to efficiently and\neffectively generate token-level adversarial prompts. In the second stage, we\npropose the automatic adversarial prompt learning to iteratively refine\nsemantic-level adversarial prompts, further enhancing LLM's defense\ncapabilities. We conducted comprehensive experiments on three widely used\njailbreak datasets, comparing our framework with six defense baselines under\nfive representative attack scenarios. The results underscore the superiority of\nour proposed methods. Furthermore, our adversarial tuning framework exhibits\nempirical generalizability across various attack strategies and target LLMs,\nhighlighting its potential as a transferable defense mechanism.\n""] , [""  AI safety training and red-teaming of large language models (LLMs) are\nmeasures to mitigate the generation of unsafe content. Our work exposes the\ninherent cross-lingual vulnerability of these safety mechanisms, resulting from\nthe linguistic inequality of safety training data, by successfully\ncircumventing GPT-4's safeguard through translating unsafe English inputs into\nlow-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafe\ntranslated inputs and provides actionable items that can get the users towards\ntheir harmful goals 79% of the time, which is on par with or even surpassing\nstate-of-the-art jailbreaking attacks. Other high-/mid-resource languages have\nsignificantly lower attack success rate, which suggests that the cross-lingual\nvulnerability mainly applies to low-resource languages. Previously, limited\ntraining on low-resource languages primarily affects speakers of those\nlanguages, causing technological disparities. However, our work highlights a\ncrucial shift: this deficiency now poses a risk to all LLMs users. Publicly\navailable translation APIs enable anyone to exploit LLMs' safety\nvulnerabilities. Therefore, our work calls for a more holistic red-teaming\nefforts to develop robust multilingual safeguards with wide language coverage.\n"", '  Manual Red teaming is a commonly-used method to identify vulnerabilities in\nlarge language models (LLMs), which, is costly and unscalable. In contrast,\nautomated red teaming uses a Red LLM to automatically generate adversarial\nprompts to the Target LLM, offering a scalable way for safety vulnerability\ndetection. However, the difficulty of building a powerful automated Red LLM\nlies in the fact that the safety vulnerabilities of the Target LLM are\ndynamically changing with the evolution of the Target LLM. To mitigate this\nissue, we propose a Deep Adversarial Automated Red Teaming (DART) framework in\nwhich the Red LLM and Target LLM are deeply and dynamically interacting with\neach other in an iterative manner. In each iteration, in order to generate\nsuccessful attacks as many as possible, the Red LLM not only takes into account\nthe responses from the Target LLM, but also adversarially adjust its attacking\ndirections by monitoring the global diversity of generated attacks across\nmultiple iterations. Simultaneously, to explore dynamically changing safety\nvulnerabilities of the Target LLM, we allow the Target LLM to enhance its\nsafety via an active learning based data selection mechanism. Experimential\nresults demonstrate that DART significantly reduces the safety risk of the\ntarget LLM. For human evaluation on Anthropic Harmless dataset, compared to the\ninstruction-tuning target LLM, DART eliminates the violation risks by 53.4\\%.\nWe will release the datasets and codes of DART soon.\n', '  Red-teaming, or identifying prompts that elicit harmful responses, is a\ncritical step in ensuring the safe and responsible deployment of large language\nmodels (LLMs). Developing effective protection against many modes of attack\nprompts requires discovering diverse attacks. Automated red-teaming typically\nuses reinforcement learning to fine-tune an attacker language model to generate\nprompts that elicit undesirable responses from a target LLM, as measured, for\nexample, by an auxiliary toxicity classifier. We show that even with explicit\nregularization to favor novelty and diversity, existing approaches suffer from\nmode collapse or fail to generate effective attacks. As a flexible and\nprobabilistically principled alternative, we propose to use GFlowNet\nfine-tuning, followed by a secondary smoothing phase, to train the attacker\nmodel to generate diverse and effective attack prompts. We find that the\nattacks generated by our method are effective against a wide range of target\nLLMs, both with and without safety tuning, and transfer well between target\nLLMs. Finally, we demonstrate that models safety-tuned using a dataset of\nred-teaming prompts generated by our method are robust to attacks from other\nRL-based red-teaming approaches.\n']",Adversarial Attacks and Defenses in AI and Machine Learning Models,"""Adversarial Attacks on Large Language Models"""
66,"""Adversarial Attacks and Privacy Preservation in AI Systems"" , Explainable AI Security and Privacy Risks","['adversarial', 'privacy', 'security', 'anonymization', 'protected', 'adversary', 'secure', 'confidentiality', 'protection', 'attacks'] , ['adversarially', 'adversarial', 'explainers', 'explainability', 'explanations', 'interpretability', 'privacy', 'ai', 'evasion', 'security']","[""  The increasing prevalence of adversarial attacks on Artificial Intelligence\n(AI) systems has created a need for innovative security measures. However, the\ncurrent methods of defending against these attacks often come with a high\ncomputing cost and require back-end processing, making real-time defense\nchallenging. Fortunately, there have been remarkable advancements in\nedge-computing, which make it easier to deploy neural networks on edge devices.\nBuilding upon these advancements, we propose an edge framework design to enable\nuniversal and efficient detection of adversarial attacks. This framework\nincorporates an attention-based adversarial detection methodology and a\nlightweight detection network formation, making it suitable for a wide range of\nneural networks and can be deployed on edge devices. To assess the\neffectiveness of our proposed framework, we conducted evaluations on five\nneural networks. The results indicate an impressive 97.43% F-score can be\nachieved, demonstrating the framework's proficiency in detecting adversarial\nattacks. Moreover, our proposed framework also exhibits significantly reduced\ncomputing complexity and cost in comparison to previous detection methods. This\naspect is particularly beneficial as it ensures that the defense mechanism can\nbe efficiently implemented in real-time on-edge devices.\n"", '  As a booming research area in the past decade, deep learning technologies\nhave been driven by big data collected and processed on an unprecedented scale.\nHowever, privacy concerns arise due to the potential leakage of sensitive\ninformation from the training data. Recent research has revealed that deep\nlearning models are vulnerable to various privacy attacks, including membership\ninference attacks, attribute inference attacks, and gradient inversion attacks.\nNotably, the efficacy of these attacks varies from model to model. In this\npaper, we answer a fundamental question: Does model architecture affect model\nprivacy? By investigating representative model architectures from convolutional\nneural networks (CNNs) to Transformers, we demonstrate that Transformers\ngenerally exhibit higher vulnerability to privacy attacks than CNNs.\nAdditionally, we identify the micro design of activation layers, stem layers,\nand LN layers, as major factors contributing to the resilience of CNNs against\nprivacy attacks, while the presence of attention modules is another main factor\nthat exacerbates the privacy vulnerability of Transformers. Our discovery\nreveals valuable insights for deep learning models to defend against privacy\nattacks and inspires the research community to develop privacy-friendly model\narchitectures.\n', '  The training phase of deep neural networks requires substantial resources and\nas such is often performed on cloud servers. However, this raises privacy\nconcerns when the training dataset contains sensitive content, e.g., face\nimages. In this work, we propose a method to perform the training phase of a\ndeep learning model on both an edge device and a cloud server that prevents\nsensitive content being transmitted to the cloud while retaining the desired\ninformation. The proposed privacy-preserving method uses adversarial early\nexits to suppress the sensitive content at the edge and transmits the\ntask-relevant information to the cloud. This approach incorporates noise\naddition during the training phase to provide a differential privacy guarantee.\nWe extensively test our method on different facial datasets with diverse face\nattributes using various deep learning architectures, showcasing its\noutstanding performance. We also demonstrate the effectiveness of privacy\npreservation through successful defenses against different white-box and deep\nreconstruction attacks.\n'] , ['  Explainable Artificial Intelligence (XAI) aims to uncover the decision-making\nprocesses of AI models. However, the data used for such explanations can pose\nsecurity and privacy risks. Existing literature identifies attacks on machine\nlearning models, including membership inference, model inversion, and model\nextraction attacks. These attacks target either the model or the training data,\ndepending on the settings and parties involved.\n  XAI tools can increase the vulnerability of model extraction attacks, which\nis a concern when model owners prefer black-box access, thereby keeping model\nparameters and architecture private. To exploit this risk, we propose\nAUTOLYCUS, a novel retraining (learning) based model extraction attack\nframework against interpretable models under black-box settings. As XAI tools,\nwe exploit Local Interpretable Model-Agnostic Explanations (LIME) and Shapley\nvalues (SHAP) to infer decision boundaries and create surrogate models that\nreplicate the functionality of the target model. LIME and SHAP are mainly\nchosen for their realistic yet information-rich explanations, coupled with\ntheir extensive adoption, simplicity, and usability.\n  We evaluate AUTOLYCUS on six machine learning datasets, measuring the\naccuracy and similarity of the surrogate model to the target model. The results\nshow that AUTOLYCUS is highly effective, requiring significantly fewer queries\ncompared to state-of-the-art attacks, while maintaining comparable accuracy and\nsimilarity. We validate its performance and transferability on multiple\ninterpretable ML models, including decision trees, logistic regression, naive\nbayes, and k-nearest neighbor. Additionally, we show the resilience of\nAUTOLYCUS against proposed countermeasures.\n', ""  The field of Explainable Artificial Intelligence (XAI) focuses on techniques\nfor providing explanations to end-users about the decision-making processes\nthat underlie modern-day machine learning (ML) models. Within the vast universe\nof XAI techniques, counterfactual (CF) explanations are often preferred by\nend-users as they help explain the predictions of ML models by providing an\neasy-to-understand & actionable recourse (or contrastive) case to individual\nend-users who are adversely impacted by predicted outcomes. However, recent\nstudies have shown significant security concerns with using CF explanations in\nreal-world applications; in particular, malicious adversaries can exploit CF\nexplanations to perform query-efficient model extraction attacks on proprietary\nML models. In this paper, we propose a model-agnostic watermarking framework\n(for adding watermarks to CF explanations) that can be leveraged to detect\nunauthorized model extraction attacks (which rely on the watermarked CF\nexplanations). Our novel framework solves a bi-level optimization problem to\nembed an indistinguishable watermark into the generated CF explanation such\nthat any future model extraction attacks that rely on these watermarked CF\nexplanations can be detected using a null hypothesis significance testing\n(NHST) scheme, while ensuring that these embedded watermarks do not compromise\nthe quality of the generated CF explanations. We evaluate this framework's\nperformance across a diverse set of real-world datasets, CF explanation\nmethods, and model extraction techniques, and show that our watermarking\ndetection system can be used to accurately identify extracted ML models that\nare trained using the watermarked CF explanations. Our work paves the way for\nthe secure adoption of CF explanations in real-world applications.\n"", '  Machine learning (ML) models, demonstrably powerful, suffer from a lack of\ninterpretability. The absence of transparency, often referred to as the black\nbox nature of ML models, undermines trust and urges the need for efforts to\nenhance their explainability. Explainable AI (XAI) techniques address this\nchallenge by providing frameworks and methods to explain the internal\ndecision-making processes of these complex models. Techniques like\nCounterfactual Explanations (CF) and Feature Importance play a crucial role in\nachieving this goal. Furthermore, high-quality and diverse data remains the\nfoundational element for robust and trustworthy ML applications. In many\napplications, the data used to train ML and XAI explainers contain sensitive\ninformation. In this context, numerous privacy-preserving techniques can be\nemployed to safeguard sensitive information in the data, such as differential\nprivacy. Subsequently, a conflict between XAI and privacy solutions emerges due\nto their opposing goals. Since XAI techniques provide reasoning for the model\nbehavior, they reveal information relative to ML models, such as their decision\nboundaries, the values of features, or the gradients of deep learning models\nwhen explanations are exposed to a third entity. Attackers can initiate privacy\nbreaching attacks using these explanations, to perform model extraction,\ninference, and membership attacks. This dilemma underscores the challenge of\nfinding the right equilibrium between understanding ML decision-making and\nsafeguarding privacy.\n']",Security and Privacy in Artificial Intelligence Systems,Explainable AI Security and Privacy Risks
67,"""Digital Twins and AI in Cybersecurity and Networking"" , ""Smart Grid Security and Cyber Attack Detection"" , ""AI-Powered Cybersecurity and Threat Detection""","['twins', 'twinning', 'twin', 'digital', 'cyber', 'ai', 'cybersecurity', 'network', 'replicas', 'technologies'] , ['adversarial', 'attacks', 'cyberattacks', 'attack', 'security', 'supervised', 'mitigation', 'threats', 'vulnerability', 'grid'] , ['adversarial', 'cybersecurity', 'security', 'ai', 'attacks', 'threats', 'intrusion', 'attack', 'threat', 'defense']","['  Digital twin, which enables emulation, evaluation, and optimization of\nphysical entities through synchronized digital replicas, has gained increasing\nattention as a promising technology for intricate wireless networks. For 6G,\nnumerous innovative wireless technologies and network architectures have posed\nnew challenges in establishing wireless network digital twins. To tackle these\nchallenges, artificial intelligence (AI), particularly the flourishing\ngenerative AI, emerges as a potential solution. In this article, we discuss\nemerging prerequisites for wireless network digital twins considering the\ncomplicated network architecture, tremendous network scale, extensive coverage,\nand diversified application scenarios in the 6G era. We further explore the\napplications of generative AI, such as Transformer and diffusion model, to\nempower the 6G digital twin from multiple perspectives including\nphysical-digital modeling, synchronization, and slicing capability.\nSubsequently, we propose a hierarchical generative AI-enabled wireless network\ndigital twin at both the message-level and policy-level, and provide a typical\nuse case with numerical results to validate the effectiveness and efficiency.\nFinally, open research issues for wireless network digital twins in the 6G era\nare discussed.\n', ""  The potential of digital twin technology is yet to be fully realized due to\nits diversity and untapped potential. Digital twins enable systems' analysis,\ndesign, optimization, and evolution to be performed digitally or in conjunction\nwith a cyber-physical approach to improve speed, accuracy, and efficiency over\ntraditional engineering methods. Industry 4.0, factories of the future, and\ndigital twins continue to benefit from the technology and provide enhanced\nefficiency within existing systems. Due to the lack of information and security\nstandards associated with the transition to cyber digitization, cybercriminals\nhave been able to take advantage of the situation. Access to a digital twin of\na product or service is equivalent to threatening the entire collection. There\nis a robust interaction between digital twins and artificial intelligence\ntools, which leads to strong interaction between these technologies, so it can\nbe used to improve the cybersecurity of these digital platforms based on their\nintegration with these technologies. This study aims to investigate the role of\nartificial intelligence in providing cybersecurity for digital twin versions of\nvarious industries, as well as the risks associated with these versions. In\naddition, this research serves as a road map for researchers and others\ninterested in cybersecurity and digital security.\n"", '  In recent years, digital twins have been proposed and implemented in various\nfields with potential applications ranging from prototyping to maintenance.\nGoing forward, they are to enable numerous efficient and sustainable\ntechnologies, among them autonomous cars. However, despite a large body of\nresearch in many fields, academics have yet to agree on what exactly a digital\ntwin is -- and as a result, what its capabilities and limitations might be. To\nfurther our understanding, we explore the capabilities of digital twins\nconcerning diagnosis in the field of transportation. We conduct a systematic\nmapping study including digital twins of vehicles and their components, as well\nas transportation infrastructure. We discovered that few papers on digital\ntwins describe any diagnostic process. Furthermore, most existing approaches\nappear limited to system monitoring or fault detection. These findings suggest\nthat we need more research for diagnostic reasoning utilizing digital twins.\n'] , ['  Detection of cyber attacks in smart power distribution grids with unbalanced\nconfigurations poses challenges due to the inherent nonlinear nature of these\nuncertain and stochastic systems. It originates from the intermittent\ncharacteristics of the distributed energy resources (DERs) generation and load\nvariations. Moreover, the unknown behavior of cyber attacks, especially false\ndata injection attacks (FDIAs) in the distribution grids with complex temporal\ncorrelations and the limited amount of labeled data increases the vulnerability\nof the grids and imposes a high risk in the secure and reliable operation of\nthe grids. To address these challenges, this paper proposes an unsupervised\nadversarial autoencoder (AAE) model to detect FDIAs in unbalanced power\ndistribution grids integrated with DERs, i.e., PV systems and wind generation.\nThe proposed method utilizes long short-term memory (LSTM) in the structure of\nthe autoencoder to capture the temporal dependencies in the time-series\nmeasurements and leverages the power of generative adversarial networks (GANs)\nfor better reconstruction of the input data. The advantage of the proposed\ndata-driven model is that it can detect anomalous points for the system\noperation without reliance on abstract models or mathematical representations.\nTo evaluate the efficacy of the approach, it is tested on IEEE 13-bus and\n123-bus systems with historical meteorological data (wind speed, ambient\ntemperature, and solar irradiance) as well as historical real-world load data\nunder three types of data falsification functions. The comparison of the\ndetection results of the proposed model with other unsupervised learning\nmethods verifies its superior performance in detecting cyber attacks in\nunbalanced power distribution grids.\n', '  In smart electrical grids, fault detection tasks may have a high impact on\nsociety due to their economic and critical implications. In the recent years,\nnumerous smart grid applications, such as defect detection and load\nforecasting, have embraced data-driven methodologies. The purpose of this study\nis to investigate the challenges associated with the security of machine\nlearning (ML) applications in the smart grid scenario. Indeed, the robustness\nand security of these data-driven algorithms have not been extensively studied\nin relation to all power grid applications. We demonstrate first that the deep\nneural network method used in the smart grid is susceptible to adversarial\nperturbation. Then, we highlight how studies on fault localization and type\nclassification illustrate the weaknesses of present ML algorithms in smart\ngrids to various adversarial attacks\n', '  In this study, we conduct a comprehensive review of smart grid security,\nexploring system architectures, attack methodologies, defense strategies, and\nfuture research opportunities. We provide an in-depth analysis of various\nattack vectors, focusing on new attack surfaces introduced by advanced\ncomponents in smart grids. The review particularly includes an extensive\nanalysis of coordinated attacks that incorporate multiple attack strategies and\nexploit vulnerabilities across various smart grid components to increase their\nadverse impact, demonstrating the complexity and potential severity of these\nthreats. Following this, we examine innovative detection and mitigation\nstrategies, including game theory, graph theory, blockchain, and machine\nlearning, discussing their advancements in counteracting evolving threats and\nassociated research challenges. In particular, our review covers a thorough\nexamination of widely used machine learning-based mitigation strategies,\nanalyzing their applications and research challenges spanning across\nsupervised, unsupervised, semi-supervised, ensemble, and reinforcement\nlearning. Further, we outline future research directions and explore new\ntechniques and concerns. We first discuss the research opportunities for\nexisting and emerging strategies, and then explore the potential role of new\ntechniques, such as large language models (LLMs), and the emerging threat of\nadversarial machine learning in the future of smart grid security.\n'] , ['  The last decades have been characterized by unprecedented technological\nadvances, many of them powered by modern technologies such as Artificial\nIntelligence (AI) and Machine Learning (ML). The world has become more\ndigitally connected than ever, but we face major challenges. One of the most\nsignificant is cybercrime, which has emerged as a global threat to governments,\nbusinesses, and civil societies. The pervasiveness of digital technologies\ncombined with a constantly shifting technological foundation has created a\ncomplex and powerful playground for cybercriminals, which triggered a surge in\ndemand for intelligent threat detection systems based on machine and deep\nlearning. This paper investigates AI-based cyber threat detection to protect\nour modern digital ecosystems. The primary focus is on evaluating ML-based\nclassifiers and ensembles for anomaly-based malware detection and network\nintrusion detection and how to integrate those models in the context of network\nsecurity, mobile security, and IoT security. The discussion highlights the\nchallenges when deploying and integrating AI-enabled cybersecurity solutions\ninto existing enterprise systems and IT infrastructures, including options to\novercome those challenges. Finally, the paper provides future research\ndirections to further increase the security and resilience of our modern\ndigital industries, infrastructures, and ecosystems.\n', '  The 2nd International Workshop on Adaptive Cyber Defense was held at the\nFlorida Institute of Technology, Florida. This workshop was organized to share\nresearch that explores unique applications of Artificial Intelligence (AI) and\nMachine Learning (ML) as foundational capabilities for the pursuit of adaptive\ncyber defense. The cyber domain cannot currently be reliably and effectively\ndefended without extensive reliance on human experts. Skilled cyber defenders\nare in short supply and often cannot respond fast enough to cyber threats.\n  Building on recent advances in AI and ML the Cyber defense research community\nhas been motivated to develop new dynamic and sustainable defenses through the\nadoption of AI and ML techniques to cyber settings. Bridging critical gaps\nbetween AI and Cyber researchers and practitioners can accelerate efforts to\ncreate semi-autonomous cyber defenses that can learn to recognize and respond\nto cyber attacks or discover and mitigate weaknesses in cooperation with other\ncyber operation systems and human experts. Furthermore, these defenses are\nexpected to be adaptive and able to evolve over time to thwart changes in\nattacker behavior, changes in the system health and readiness, and natural\nshifts in user behavior over time.\n  The workshop was comprised of invited keynote talks, technical presentations\nand a panel discussion about how AI/ML can enable autonomous mitigation of\ncurrent and future cyber attacks. Workshop submissions were peer reviewed by a\npanel of domain experts with a proceedings consisting of six technical articles\nexploring challenging problems of critical importance to national and global\nsecurity. Participation in this workshop offered new opportunities to stimulate\nresearch and innovation in the emerging domain of adaptive and autonomous cyber\ndefense.\n', '  We introduce the AI Security Pyramid of Pain, a framework that adapts the\ncybersecurity Pyramid of Pain to categorize and prioritize AI-specific threats.\nThis framework provides a structured approach to understanding and addressing\nvarious levels of AI threats. Starting at the base, the pyramid emphasizes Data\nIntegrity, which is essential for the accuracy and reliability of datasets and\nAI models, including their weights and parameters. Ensuring data integrity is\ncrucial, as it underpins the effectiveness of all AI-driven decisions and\noperations. The next level, AI System Performance, focuses on MLOps-driven\nmetrics such as model drift, accuracy, and false positive rates. These metrics\nare crucial for detecting potential security breaches, allowing for early\nintervention and maintenance of AI system integrity. Advancing further, the\npyramid addresses the threat posed by Adversarial Tools, identifying and\nneutralizing tools used by adversaries to target AI systems. This layer is key\nto staying ahead of evolving attack methodologies. At the Adversarial Input\nlayer, the framework addresses the detection and mitigation of inputs designed\nto deceive or exploit AI models. This includes techniques like adversarial\npatterns and prompt injection attacks, which are increasingly used in\nsophisticated attacks on AI systems. Data Provenance is the next critical\nlayer, ensuring the authenticity and lineage of data and models. This layer is\npivotal in preventing the use of compromised or biased data in AI systems. At\nthe apex is the tactics, techniques, and procedures (TTPs) layer, dealing with\nthe most complex and challenging aspects of AI security. This involves a deep\nunderstanding and strategic approach to counter advanced AI-targeted attacks,\nrequiring comprehensive knowledge and planning.\n']",Cybersecurity and Artificial Intelligence in Emerging Technologies,"""AI-Powered Cybersecurity and Threat Detection"""
68,"Intrusion Detection Systems for Cybersecurity , Intrusion Detection and Defense Strategies","['intrusions', 'intrusion', 'cybersecurity', 'iot', 'botnet', 'attacks', 'ddos', 'ids2018', 'security', 'attack'] , ['intrusions', 'attacks', 'intrusion', 'security', 'defenders', 'reinforcement', 'defence', 'defense', 'attack', 'attacker']","['  Network Intrusion Detection Systems (IDS) aim to detect the presence of an\nintruder by analyzing network packets arriving at an internet connected device.\nData-driven deep learning systems, popular due to their superior performance\ncompared to traditional IDS, depend on availability of high quality training\ndata for diverse intrusion classes. A way to overcome this limitation is\nthrough transferable learning, where training for one intrusion class can lead\nto detection of unseen intrusion classes after deployment. In this paper, we\nprovide a detailed study on the transferability of intrusion detection. We\ninvestigate practical federated learning configurations to enhance the\ntransferability of intrusion detection. We propose two techniques to\nsignificantly improve the transferability of a federated intrusion detection\nsystem. The code for this work can be found at\nhttps://github.com/ghosh64/transferability.\n', '  Intrusion Detection Systems (IDS) play a crucial role in ensuring the\nsecurity of computer networks. Machine learning has emerged as a popular\napproach for intrusion detection due to its ability to analyze and detect\npatterns in large volumes of data. However, current ML-based IDS solutions\noften struggle to keep pace with the ever-changing nature of attack patterns\nand the emergence of new attack types. Additionally, these solutions face\nchallenges related to class imbalance, where the number of instances belonging\nto different classes (normal and intrusions) is significantly imbalanced, which\nhinders their ability to effectively detect minor classes. In this paper, we\npropose a novel multi-agent reinforcement learning (RL) architecture, enabling\nautomatic, efficient, and robust network intrusion detection. To enhance the\ncapabilities of the proposed model, we have improved the DQN algorithm by\nimplementing the weighted mean square loss function and employing\ncost-sensitive learning techniques. Our solution introduces a resilient\narchitecture designed to accommodate the addition of new attacks and\neffectively adapt to changes in existing attack patterns. Experimental results\nrealized using CIC-IDS-2017 dataset, demonstrate that our approach can\neffectively handle the class imbalance problem and provide a fine grained\nclassification of attacks with a very low false positive rate. In comparison to\nthe current state-of-the-art works, our solution demonstrates a significant\nsuperiority in both detection rate and false positive rate.\n', '  The integration of Internet of Things (IoT) applications in our daily lives\nhas led to a surge in data traffic, posing significant security challenges. IoT\napplications using cloud and edge computing are at higher risk of cyberattacks\nbecause of the expanded attack surface from distributed edge and cloud\nservices, the vulnerability of IoT devices, and challenges in managing security\nacross interconnected systems leading to oversights. This led to the rise of\nML-based solutions for intrusion detection systems (IDSs), which have proven\neffective in enhancing network security and defending against diverse threats.\nHowever, ML-based IDS in IoT systems encounters challenges, particularly from\nnoisy, redundant, and irrelevant features in varied IoT datasets, potentially\nimpacting its performance. Therefore, reducing such features becomes crucial to\nenhance system performance and minimize computational costs. This paper focuses\non improving the effectiveness of ML-based IDS at the edge level by introducing\na novel method to find a balanced trade-off between cost and accuracy through\nthe creation of informative features in a two-tier edge-user IoT environment. A\nhybrid Binary Quantum-inspired Artificial Bee Colony and Genetic Programming\nalgorithm is utilized for this purpose. Three IoT intrusion detection datasets,\nnamely NSL-KDD, UNSW-NB15, and BoT-IoT, are used for the evaluation of the\nproposed approach.\n'] , [""  This paper addresses a significant gap in Autonomous Cyber Operations (ACO)\nliterature: the absence of effective edge-blocking ACO strategies in dynamic,\nreal-world networks. It specifically targets the cybersecurity vulnerabilities\nof organizational Active Directory (AD) systems. Unlike the existing literature\non edge-blocking defenses which considers AD systems as static entities, our\nstudy counters this by recognizing their dynamic nature and developing advanced\nedge-blocking defenses through a Stackelberg game model between attacker and\ndefender. We devise a Reinforcement Learning (RL)-based attack strategy and an\nRL-assisted Evolutionary Diversity Optimization-based defense strategy, where\nthe attacker and defender improve each other strategy via parallel gameplay. To\naddress the computational challenges of training attacker-defender strategies\non numerous dynamic AD graphs, we propose an RL Training Facilitator that\nprunes environments and neural networks to eliminate irrelevant elements,\nenabling efficient and scalable training for large graphs. We extensively train\nthe attacker strategy, as a sophisticated attacker model is essential for a\nrobust defense. Our empirical results successfully demonstrate that our\nproposed approach enhances defender's proficiency in hardening dynamic AD\ngraphs while ensuring scalability for large-scale AD.\n"", '  We study automated intrusion prevention using reinforcement learning.\nFollowing a novel approach, we formulate the problem of intrusion prevention as\nan (optimal) multiple stopping problem. This formulation gives us insight into\nthe structure of optimal policies, which we show to have threshold properties.\nFor most practical cases, it is not feasible to obtain an optimal defender\npolicy using dynamic programming. We therefore develop a reinforcement learning\napproach to approximate an optimal threshold policy. We introduce T-SPSA, an\nefficient reinforcement learning algorithm that learns threshold policies\nthrough stochastic approximation. We show that T-SPSA outperforms\nstate-of-the-art algorithms for our use case. Our overall method for learning\nand validating policies includes two systems: a simulation system where\ndefender policies are incrementally learned and an emulation system where\nstatistics are produced that drive simulation runs and where learned policies\nare evaluated. We show that this approach can produce effective defender\npolicies for a practical IT infrastructure.\n', '  We study automated intrusion response and formulate the interaction between\nan attacker and a defender as an optimal stopping game where attack and defense\nstrategies evolve through reinforcement learning and self-play. The\ngame-theoretic modeling enables us to find defender strategies that are\neffective against a dynamic attacker, i.e. an attacker that adapts its strategy\nin response to the defender strategy. Further, the optimal stopping formulation\nallows us to prove that optimal strategies have threshold properties. To obtain\nnear-optimal defender strategies, we develop Threshold Fictitious Self-Play\n(T-FP), a fictitious self-play algorithm that learns Nash equilibria through\nstochastic approximation. We show that T-FP outperforms a state-of-the-art\nalgorithm for our use case. The experimental part of this investigation\nincludes two systems: a simulation system where defender strategies are\nincrementally learned and an emulation system where statistics are collected\nthat drive simulation runs and where learned strategies are evaluated. We argue\nthat this approach can produce effective defender strategies for a practical IT\ninfrastructure.\n']",Intrusion Detection and Defense Strategies for Cybersecurity,Intrusion Detection Systems for Cybersecurity
69,"""Vulnerability Detection in Software Systems"" , ""Adversarial Attacks on Malware Detection in Windows PE Files"" , ""Phishing Email Detection and Prevention in Cybersecurity""","['vulnerabilities', 'vulnerability', 'vulnerable', 'developers', 'security', 'fuzzing', 'bugs', 'code', 'programming', 'malicious'] , ['malware', 'adversarial', 'malicious', 'ransomware', 'antivirus', 'classifiers', 'classifier', 'cybersecurity', 'executable', 'obfuscation'] , ['phishing', 'spam', 'cybersecurity', 'emails', 'malicious', 'phisnet', 'threats', 'phishlang', 'security', 'attackers']","['  The security guarantee of AI-enabled software systems (particularly using\ndeep learning techniques as a functional core) is pivotal against the\nadversarial attacks exploiting software vulnerabilities. However, little\nattention has been paid to a systematic investigation of vulnerabilities in\nsuch systems. A common situation learned from the open source software\ncommunity is that deep learning engineers frequently integrate off-the-shelf or\nopen-source learning frameworks into their ecosystems. In this work, we\nspecifically look into deep learning (DL) framework and perform the first\nsystematic study of vulnerabilities in DL systems through a comprehensive\nanalysis of identified vulnerabilities from Common Vulnerabilities and\nExposures (CVE) and open-source DL tools, including TensorFlow, Caffe, OpenCV,\nKeras, and PyTorch. We propose a two-stream data analysis framework to explore\nvulnerability patterns from various databases. We investigate the unique DL\nframeworks and libraries development ecosystems that appear to be decentralized\nand fragmented. By revisiting the Common Weakness Enumeration (CWE) List, which\nprovides the traditional software vulnerability related practices, we observed\nthat it is more challenging to detect and fix the vulnerabilities throughout\nthe DL systems lifecycle. Moreover, we conducted a large-scale empirical study\nof 3,049 DL vulnerabilities to better understand the patterns of vulnerability\nand the challenges in fixing them. We have released the full replication\npackage at https://github.com/codelzz/Vulnerabilities4DLSystem. We anticipate\nthat our study can advance the development of secure DL systems.\n', '  Despite various approaches being employed to detect vulnerabilities, the\nnumber of reported vulnerabilities shows an upward trend over the years. This\nsuggests the problems are not caught before the code is released, which could\nbe caused by many factors, like lack of awareness, limited efficacy of the\nexisting vulnerability detection tools or the tools not being user-friendly. To\nhelp combat some issues with traditional vulnerability detection tools, we\npropose using large language models (LLMs) to assist in finding vulnerabilities\nin source code. LLMs have shown a remarkable ability to understand and generate\ncode, underlining their potential in code-related tasks. The aim is to test\nmultiple state-of-the-art LLMs and identify the best prompting strategies,\nallowing extraction of the best value from the LLMs. We provide an overview of\nthe strengths and weaknesses of the LLM-based approach and compare the results\nto those of traditional static analysis tools. We find that LLMs can pinpoint\nmany more issues than traditional static analysis tools, outperforming\ntraditional tools in terms of recall and F1 scores. The results should benefit\nsoftware developers and security analysts responsible for ensuring that the\ncode is free of vulnerabilities.\n', '  Software, while beneficial, poses potential cybersecurity risks due to\ninherent vulnerabilities. Detecting these vulnerabilities is crucial, and deep\nlearning has shown promise as an effective tool for this task due to its\nability to perform well without extensive feature engineering. However, a\nchallenge in deploying deep learning for vulnerability detection is the limited\navailability of training data. Recent research highlights the deep learning\nefficacy in diverse tasks. This success is attributed to instruction\nfine-tuning, a technique that remains under-explored in the context of\nvulnerability detection. This paper investigates the capability of models,\nspecifically a recent language model, to generalize beyond the programming\nlanguages used in their training data. It also examines the role of natural\nlanguage instructions in enhancing this generalization. Our study evaluates the\nmodel performance on a real-world dataset to predict vulnerable code. We\npresent key insights and lessons learned, contributing to understanding the\ndeep learning application in software vulnerability detection.\n'] , ['  Malware has been one of the most damaging threats to computers that span\nacross multiple operating systems and various file formats. To defend against\never-increasing and ever-evolving malware, tremendous efforts have been made to\npropose a variety of malware detection that attempt to effectively and\nefficiently detect malware so as to mitigate possible damages as early as\npossible. Recent studies have shown that, on the one hand, existing ML and DL\ntechniques enable superior solutions in detecting newly emerging and previously\nunseen malware. However, on the other hand, ML and DL models are inherently\nvulnerable to adversarial attacks in the form of adversarial examples. In this\npaper, we focus on malware with the file format of portable executable (PE) in\nthe family of Windows operating systems, namely Windows PE malware, as a\nrepresentative case to study the adversarial attack methods in such adversarial\nsettings. To be specific, we start by first outlining the general learning\nframework of Windows PE malware detection based on ML/DL and subsequently\nhighlighting three unique challenges of performing adversarial attacks in the\ncontext of Windows PE malware. Then, we conduct a comprehensive and systematic\nreview to categorize the state-of-the-art adversarial attacks against PE\nmalware detection, as well as corresponding defenses to increase the robustness\nof Windows PE malware detection. Finally, we conclude the paper by first\npresenting other related attacks against Windows PE malware detection beyond\nthe adversarial attacks and then shedding light on future research directions\nand opportunities. In addition, a curated resource list of adversarial attacks\nand defenses for Windows PE malware detection is also available at\nhttps://github.com/ryderling/adversarial-attacks-and-defenses-for-windows-pe-malware-detection.\n', '  Machine learning has proven to be a useful tool for automated malware\ndetection, but machine learning models have also been shown to be vulnerable to\nadversarial attacks. This article addresses the problem of generating\nadversarial malware samples, specifically malicious Windows Portable Executable\nfiles. We summarize and compare work that has focused on adversarial machine\nlearning for malware detection. We use gradient-based, evolutionary\nalgorithm-based, and reinforcement-based methods to generate adversarial\nsamples, and then test the generated samples against selected antivirus\nproducts. We compare the selected methods in terms of accuracy and practical\napplicability. The results show that applying optimized modifications to\npreviously detected malware can lead to incorrect classification of the file as\nbenign. It is also known that generated malware samples can be successfully\nused against detection models other than those used to generate them and that\nusing combinations of generators can create new samples that evade detection.\nExperiments show that the Gym-malware generator, which uses a reinforcement\nlearning approach, has the greatest practical potential. This generator\nachieved an average sample generation time of 5.73 seconds and the highest\naverage evasion rate of 44.11%. Using the Gym-malware generator in combination\nwith itself improved the evasion rate to 58.35%.\n', '  Malware attacks have become significantly more frequent and sophisticated in\nrecent years. Therefore, malware detection and classification are critical\ncomponents of information security. Due to the large amount of malware samples\navailable, it is essential to categorize malware samples according to their\nmalicious characteristics. Clustering algorithms are thus becoming more widely\nused in computer security to analyze the behavior of malware variants and\ndiscover new malware families. Online clustering algorithms help us to\nunderstand malware behavior and produce a quicker response to new threats. This\npaper introduces a novel machine learning-based model for the online clustering\nof malicious samples into malware families. Streaming data is divided according\nto the clustering decision rule into samples from known and new emerging\nmalware families. The streaming data is classified using the weighted k-nearest\nneighbor classifier into known families, and the online k-means algorithm\nclusters the remaining streaming data and achieves a purity of clusters from\n90.20% for four clusters to 93.34% for ten clusters. This work is based on\nstatic analysis of portable executable files for the Windows operating system.\nExperimental results indicate that the proposed online clustering model can\ncreate high-purity clusters corresponding to malware families. This allows\nmalware analysts to receive similar malware samples, speeding up their\nanalysis.\n'] , ['  Phishing email attacks are among the most common and most harmful\ncybersecurity attacks. With the emergence of generative AI, phishing attacks\ncan be based on emails generated automatically, making it more difficult to\ndetect them. That is, instead of a single email format sent to a large number\nof recipients, generative AI can be used to send each potential victim a\ndifferent email, making it more difficult for cybersecurity systems to identify\nthe scam email before it reaches the recipient. Here we describe a corpus of\nAI-generated phishing emails. We also use different machine learning tools to\ntest the ability of automatic text analysis to identify AI-generated phishing\nemails. The results are encouraging, and show that machine learning tools can\nidentify an AI-generated phishing email with high accuracy compared to regular\nemails or human-generated scam email. By applying descriptive analytic, the\nspecific differences between AI-generated emails and manually crafted scam\nemails are profiled, and show that AI-generated emails are different in their\nstyle from human-generated phishing email scams. Therefore, automatic\nidentification tools can be used as a warning for the user. The paper also\ndescribes the corpus of AI-generated phishing emails that is made open to the\npublic, and can be used for consequent studies. While the ability of machine\nlearning to detect AI-generated phishing email is encouraging, AI-generated\nphishing emails are different from regular phishing emails, and therefore it is\nimportant to train machine learning systems also with AI-generated emails in\norder to repel future phishing attacks that are powered by generative AI.\n', '  Phishing email is a serious cyber threat that tries to deceive users by\nsending false emails with the intention of stealing confidential information or\ncausing financial harm. Attackers, often posing as trustworthy entities,\nexploit technological advancements and sophistication to make detection and\nprevention of phishing more challenging. Despite extensive academic research,\nphishing detection remains an ongoing and formidable challenge in the\ncybersecurity landscape. Large Language Models (LLMs) and Masked Language\nModels (MLMs) possess immense potential to offer innovative solutions to\naddress long-standing challenges. In this research paper, we present an\noptimized, fine-tuned transformer-based DistilBERT model designed for the\ndetection of phishing emails. In the detection process, we work with a phishing\nemail dataset and utilize the preprocessing techniques to clean and solve the\nimbalance class issues. Through our experiments, we found that our model\neffectively achieves high accuracy, demonstrating its capability to perform\nwell. Finally, we demonstrate our fine-tuned model using Explainable-AI (XAI)\ntechniques such as Local Interpretable Model-Agnostic Explanations (LIME) and\nTransformer Interpret to explain how our model makes predictions in the context\nof text classification for phishing emails.\n', ""  The critical threat of phishing emails has been further exacerbated by the\npotential of LLMs to generate highly targeted, personalized, and automated\nspear phishing attacks. Two critical problems concerning LLM-facilitated\nphishing require further investigation: 1) Existing studies on lateral phishing\nlack specific examination of LLM integration for large-scale attacks targeting\nthe entire organization, and 2) Current anti-phishing infrastructure, despite\nits extensive development, lacks the capability to prevent LLM-generated\nattacks, potentially impacting both employees and IT security incident\nmanagement. However, the execution of such investigative studies necessitates a\nreal-world environment, one that functions during regular business operations\nand mirrors the complexity of a large organizational infrastructure. This\nsetting must also offer the flexibility required to facilitate a diverse array\nof experimental conditions, particularly the incorporation of phishing emails\ncrafted by LLMs. This study is a pioneering exploration into the use of Large\nLanguage Models (LLMs) for the creation of targeted lateral phishing emails,\ntargeting a large tier 1 university's operation and workforce of approximately\n9,000 individuals over an 11-month period. It also evaluates the capability of\nemail filtering infrastructure to detect such LLM-generated phishing attempts,\nproviding insights into their effectiveness and identifying potential areas for\nimprovement. Based on our findings, we propose machine learning-based detection\ntechniques for such emails to detect LLM-generated phishing emails that were\nmissed by the existing infrastructure, with an F1-score of 98.96.\n""]",Cybersecurity Threat Detection and Prevention,"""Vulnerability Detection in Software Systems"""
70,"Differentially Private Machine Learning , Differentially Private Graph Neural Networks , Private In-Context Learning with Differential Privacy , Differentially Private Deep Learning and Synthetic Data Generation","['privacy', 'private', 'privately', 'sgd', 'differentially', 'public', 'gradients', 'leakage', 'protection', 'algorithms'] , ['privacy', 'graphpub', 'graphs', 'private', 'edge', 'graph', 'adjacency', 'gnndelete', 'nodes', 'neighbors'] , ['privacy', 'private', 'privately', 'memorization', 'secure', 'leak', 'protect', 'serializing', 'examples', 'context'] , ['privacy', 'adversarial', 'private', 'confidential', 'obfuscation', 'datasets', 'unlearning', 'deep', 'secure', 'learning']","['  When analysing Differentially Private (DP) machine learning pipelines, the\npotential privacy cost of data-dependent pre-processing is frequently\noverlooked in privacy accounting. In this work, we propose a general framework\nto evaluate the additional privacy cost incurred by non-private data-dependent\npre-processing algorithms. Our framework establishes upper bounds on the\noverall privacy guarantees by utilising two new technical notions: a variant of\nDP termed Smooth DP and the bounded sensitivity of the pre-processing\nalgorithms. In addition to the generic framework, we provide explicit overall\nprivacy guarantees for multiple data-dependent pre-processing algorithms, such\nas data imputation, quantization, deduplication and PCA, when used in\ncombination with several DP algorithms. Notably, this framework is also simple\nto implement, allowing direct integration into existing DP pipelines.\n', '  Differentially private stochastic gradient descent (DP-SGD) is the standard\nalgorithm for training machine learning models under differential privacy (DP).\nThe major drawback of DP-SGD is the drop in utility which prior work has\ncomprehensively studied. However, in practice another major drawback that\nhinders the large-scale deployment is the significantly higher computational\ncost. We conduct a comprehensive empirical study to quantify the computational\ncost of training deep learning models under DP and benchmark methods that aim\nat reducing the cost. Among these are more efficient implementations of DP-SGD\nand training with lower precision. Finally, we study the scaling behaviour\nusing up to 80 GPUs.\n', ""  Machine learning (ML) models have been shown to leak private information from\ntheir training datasets. Differential Privacy (DP), typically implemented\nthrough the differential private stochastic gradient descent algorithm\n(DP-SGD), has become the standard solution to bound leakage from the models.\nDespite recent improvements, DP-SGD-based approaches for private learning still\nusually struggle in the high privacy ($\\varepsilon\\le1)$ and low data regimes,\nand when the private training datasets are imbalanced. To overcome these\nlimitations, we propose Differentially Private Prototype Learning (DPPL) as a\nnew paradigm for private transfer learning. DPPL leverages publicly pre-trained\nencoders to extract features from private data and generates DP prototypes that\nrepresent each private class in the embedding space and can be publicly\nreleased for inference. Since our DP prototypes can be obtained from only a few\nprivate training data points and without iterative noise addition, they offer\nhigh-utility predictions and strong privacy guarantees even under the notion of\npure DP. We additionally show that privacy-utility trade-offs can be further\nimproved when leveraging the public data beyond pre-training of the encoder: in\nparticular, we can privately sample our DP prototypes from the publicly\navailable data points used to train the encoder. Our experimental evaluation\nwith four state-of-the-art encoders, four vision datasets, and under different\ndata and imbalancedness regimes demonstrate DPPL's high performance under\nstrong privacy guarantees in challenging private learning setups.\n""] , ['  Graph neural networks (GNNs) play a key role in learning representations from\ngraph-structured data and are demonstrated to be useful in many applications.\nHowever, the GNN training pipeline has been shown to be vulnerable to node\nfeature leakage and edge extraction attacks. This paper investigates a scenario\nwhere an attacker aims to recover private edge information from a trained GNN\nmodel. Previous studies have employed differential privacy (DP) to add noise\ndirectly to the adjacency matrix or a compact graph representation. The added\nperturbations cause the graph structure to be substantially morphed, reducing\nthe model utility. We propose a new privacy-preserving GNN training algorithm,\nEclipse, that maintains good model utility while providing strong privacy\nprotection on edges. Eclipse is based on two key observations. First, adjacency\nmatrices in graph structures exhibit low-rank behavior. Thus, Eclipse trains\nGNNs with a low-rank format of the graph via singular values decomposition\n(SVD), rather than the original graph. Using the low-rank format, Eclipse\npreserves the primary graph topology and removes the remaining residual edges.\nEclipse adds noise to the low-rank singular values instead of the entire graph,\nthereby preserving the graph privacy while still maintaining enough of the\ngraph structure to maintain model utility. We theoretically show Eclipse\nprovide formal DP guarantee on edges. Experiments on benchmark graph datasets\nshow that Eclipse achieves significantly better privacy-utility tradeoff\ncompared to existing privacy-preserving GNN training methods. In particular,\nunder strong privacy constraints ($\\epsilon$ < 4), Eclipse shows significant\ngains in the model utility by up to 46%. We further demonstrate that Eclipse\nalso has better resilience against common edge attacks (e.g., LPA), lowering\nthe attack AUC by up to 5% compared to other state-of-the-art baselines.\n', '  Graph Neural Networks (GNNs) have achieved great success in learning with\ngraph-structured data. Privacy concerns have also been raised for the trained\nmodels which could expose the sensitive information of graphs including both\nnode features and the structure information. In this paper, we aim to achieve\nnode-level differential privacy (DP) for training GNNs so that a node and its\nedges are protected. Node DP is inherently difficult for GNNs because all\ndirect and multi-hop neighbors participate in the calculation of gradients for\neach node via layer-wise message passing and there is no bound on how many\ndirect and multi-hop neighbors a node can have, so existing DP methods will\nresult in high privacy cost or poor utility due to high node sensitivity. We\npropose a Decoupled GNN with Differentially Private Approximate Personalized\nPageRank (DPAR) for training GNNs with an enhanced privacy-utility tradeoff.\nThe key idea is to decouple the feature projection and message passing via a DP\nPageRank algorithm which learns the structure information and uses the top-$K$\nneighbors determined by the PageRank for feature aggregation. By capturing the\nmost important neighbors for each node and avoiding the layer-wise message\npassing, it bounds the node sensitivity and achieves improved privacy-utility\ntradeoff compared to layer-wise perturbation based methods. We theoretically\nanalyze the node DP guarantee for the two processes combined together and\nempirically demonstrate better utilities of DPAR with the same level of node DP\ncompared with state-of-the-art methods.\n', ""  Differentially private GNNs (Graph Neural Networks) have been recently\nstudied to provide high accuracy in various tasks on graph data while strongly\nprotecting user privacy. In particular, a recent study proposes an algorithm to\nprotect each user's feature vector in an attributed graph, which includes\nfeature vectors along with node IDs and edges, with LDP (Local Differential\nPrivacy), a strong privacy notion without a trusted third party. However, this\nalgorithm does not protect edges (friendships) in a social graph, hence cannot\nprotect user privacy in unattributed graphs, which include only node IDs and\nedges. How to provide strong privacy with high accuracy in unattributed graphs\nremains open. In this paper, we propose a novel LDP algorithm called the DPRR\n(Degree-Preserving Randomized Response) to provide LDP for edges in GNNs. Our\nDPRR preserves each user's degree hence a graph structure while providing edge\nLDP. Technically, our DPRR uses Warner's RR (Randomized Response) and strategic\nedge sampling, where each user's sampling probability is automatically tuned\nusing the Laplacian mechanism to preserve the degree information under edge\nLDP. We also propose a privacy budget allocation method to make the noise in\nboth Warner's RR and the Laplacian mechanism small. We focus on graph\nclassification as a task of GNNs and evaluate the DPRR using three social graph\ndatasets. Our experimental results show that the DPRR significantly outperforms\nthree baselines and provides accuracy close to a non-private algorithm in all\ndatasets with a reasonable privacy budget, e.g., epsilon=1. Finally, we\nintroduce data poisoning attacks to our DPRR and a defense against the attacks.\nWe evaluate them using the three social graph datasets and discuss the\nexperimental results.\n""] , ['  We study the problem of in-context learning (ICL) with large language models\n(LLMs) on private datasets. This scenario poses privacy risks, as LLMs may leak\nor regurgitate the private examples demonstrated in the prompt. We propose a\nnovel algorithm that generates synthetic few-shot demonstrations from the\nprivate dataset with formal differential privacy (DP) guarantees, and show\nempirically that it can achieve effective ICL. We conduct extensive experiments\non standard benchmarks and compare our algorithm with non-private ICL and\nzero-shot solutions. Our results demonstrate that our algorithm can achieve\ncompetitive performance with strong privacy levels. These results open up new\npossibilities for ICL with privacy protection for a broad range of\napplications.\n', ""  Large Language Models (LLMs) have emerged as dominant tools for various\ntasks, particularly when tailored for a specific target by prompt tuning.\nNevertheless, concerns surrounding data privacy present obstacles due to the\ntuned prompts' dependency on sensitive private information. A practical\nsolution is to host a local LLM and optimize a soft prompt privately using\ndata. Yet, hosting a local model becomes problematic when model ownership is\nprotected. Alternative methods, like sending data to the model's provider for\ntraining, intensify these privacy issues facing an untrusted provider. In this\npaper, we present a novel solution called Differentially-Private Offsite Prompt\nTuning (DP-OPT) to address this challenge. Our approach involves tuning a\ndiscrete prompt on the client side and then applying it to the desired cloud\nmodels. We demonstrate that prompts suggested by LLMs themselves can be\ntransferred without compromising performance significantly. To ensure that the\nprompts do not leak private information, we introduce the first private prompt\ngeneration mechanism, by a differentially-private (DP) ensemble of in-context\nlearning with private demonstrations. With DP-OPT, generating\nprivacy-preserving prompts by Vicuna-7b can yield competitive performance\ncompared to non-private in-context learning on GPT3.5 or local private prompt\ntuning. Codes are available at https://github.com/VITA-Group/DP-OPT .\n"", '  In-context learning (ICL) enables large language models (LLMs) to adapt to\nnew tasks by conditioning on demonstrations of question-answer pairs and it has\nbeen shown to have comparable performance to costly model retraining and\nfine-tuning. Recently, ICL has been extended to allow tabular data to be used\nas demonstration examples by serializing individual records into natural\nlanguage formats. However, it has been shown that LLMs can leak information\ncontained in prompts, and since tabular data often contain sensitive\ninformation, understanding how to protect the underlying tabular data used in\nICL is a critical area of research. This work serves as an initial\ninvestigation into how to use differential privacy (DP) -- the long-established\ngold standard for data privacy and anonymization -- to protect tabular data\nused in ICL. Specifically, we investigate the application of DP mechanisms for\nprivate tabular ICL via data privatization prior to serialization and\nprompting. We formulate two private ICL frameworks with provable privacy\nguarantees in both the local (LDP-TabICL) and global (GDP-TabICL) DP scenarios\nvia injecting noise into individual records or group statistics, respectively.\nWe evaluate our DP-based frameworks on eight real-world tabular datasets and\nacross multiple ICL and DP settings. Our evaluations show that DP-based ICL can\nprotect the privacy of the underlying tabular data while achieving comparable\nperformance to non-LLM baselines, especially under high privacy regimes.\n'] , ['  We present an approach for generating differentially private synthetic text\nusing large language models (LLMs), via private prediction. In the private\nprediction framework, we only require the output synthetic data to satisfy\ndifferential privacy guarantees. This is in contrast to approaches that train a\ngenerative model on potentially sensitive user-supplied source data and seek to\nensure the model itself is safe to release.\n  We prompt a pretrained LLM with source data, but ensure that next-token\npredictions are made with differential privacy guarantees. Previous work in\nthis paradigm reported generating a small number of examples (<10) at\nreasonable privacy levels, an amount of data that is useful only for downstream\nin-context learning or prompting. In contrast, we make changes that allow us to\ngenerate thousands of high-quality synthetic data points, greatly expanding the\nset of potential applications. Our improvements come from an improved privacy\nanalysis and a better private selection mechanism, which makes use of the\nequivalence between the softmax layer for sampling tokens in LLMs and the\nexponential mechanism. Furthermore, we introduce a novel use of public\npredictions via the sparse vector technique, in which we do not pay privacy\ncosts for tokens that are predictable without sensitive data; we find this to\nbe particularly effective for structured data.\n', '  Deep learning holds immense promise for aiding radiologists in breast cancer\ndetection. However, achieving optimal model performance is hampered by\nlimitations in availability and sharing of data commonly associated to patient\nprivacy concerns. Such concerns are further exacerbated, as traditional deep\nlearning models can inadvertently leak sensitive training information. This\nwork addresses these challenges exploring and quantifying the utility of\nprivacy-preserving deep learning techniques, concretely, (i) differentially\nprivate stochastic gradient descent (DP-SGD) and (ii) fully synthetic training\ndata generated by our proposed malignancy-conditioned generative adversarial\nnetwork. We assess these methods via downstream malignancy classification of\nmammography masses using a transformer model. Our experimental results depict\nthat synthetic data augmentation can improve privacy-utility tradeoffs in\ndifferentially private model training. Further, model pretraining on synthetic\ndata achieves remarkable performance, which can be further increased with\nDP-SGD fine-tuning across all privacy guarantees. With this first in-depth\nexploration of privacy-preserving deep learning in breast imaging, we address\ncurrent and emerging clinical privacy requirements and pave the way towards the\nadoption of private high-utility deep diagnostic models. Our reproducible\ncodebase is publicly available at https://github.com/RichardObi/mammo_dp.\n', '  Differentially private training algorithms like DP-SGD protect sensitive\ntraining data by ensuring that trained models do not reveal private\ninformation. An alternative approach, which this paper studies, is to use a\nsensitive dataset to generate synthetic data that is differentially private\nwith respect to the original data, and then non-privately training a model on\nthe synthetic data. Doing so has several advantages: synthetic data can be\nreused for other tasks (including for hyper parameter tuning), retained\nindefinitely, and shared with third parties without sacrificing privacy.\nHowever, generating private synthetic data is much harder than training a\nprivate model. To improve performance on text data, recent work has utilized\npublic data by starting with a pre-trained generative language model and\nprivately fine-tuning it on sensitive data. This model can be used to sample a\nDP synthetic dataset. While this strategy seems straightforward, executing it\nhas proven problematic. Previous approaches either show significant performance\nloss, or have, as we show, critical design flaws. In this paper we demonstrate\nthat a proper training objective along with tuning fewer parameters results in\nexcellent DP synthetic data quality. Our approach is competitive with direct\nDP-training of downstream classifiers in terms of performance on downstream\ntasks. Further, we demonstrate that our DP synthetic data is not only useful\nfor downstream classifier training, but also to tune those same models.\n']",Differential Privacy in Machine Learning,Differentially Private Machine Learning
71,"""Synthetic Data Generation and Privacy"" , ""Synthetic Electronic Health Records Generation"" , Synthetic Data Generation and Applications","['adversarial', 'privacy', 'datasets', 'generative', 'gans', 'anonymization', 'data', 'anonymized', 'gan', 'synthetic'] , ['gans', 'generative', 'gan', 'autoencoders', 'adversarial', 'data', 'ehrs', 'healthcare', 'ehr', 'generate'] , ['generative', 'synthetic', 'datasets', 'models', 'learning', 'classification', 'data', 'trained', 'unlearning', 'artificial']","['  Synthetic data generation, a cornerstone of Generative Artificial\nIntelligence, promotes a paradigm shift in data science by addressing data\nscarcity and privacy while enabling unprecedented performance. As synthetic\ndata becomes more prevalent, concerns emerge regarding the accuracy of\nstatistical methods when applied to synthetic data in contrast to raw data.\nThis article explores the effectiveness of statistical methods on synthetic\ndata and the privacy risks of synthetic data. Regarding effectiveness, we\npresent the Synthetic Data Generation for Analytics framework. This framework\napplies statistical approaches to high-quality synthetic data produced by\ngenerative models like tabular diffusion models, which, initially trained on\nraw data, benefit from insights from pertinent studies through transfer\nlearning. A key finding within this framework is the generational effect, which\nreveals that the error rate of statistical methods on synthetic data decreases\nwith the addition of more synthetic data but may eventually rise or stabilize.\nThis phenomenon, stemming from the challenge of accurately mirroring raw data\ndistributions, highlights a ""reflection point""-an ideal volume of synthetic\ndata defined by specific error metrics. Through three case studies, sentiment\nanalysis, predictive modeling of structured data, and inference in tabular\ndata, we validate the superior performance of this framework compared to\nconventional approaches. On privacy, synthetic data imposes lower risks while\nsupporting the differential privacy standard. These studies underscore\nsynthetic data\'s untapped potential in redefining data science\'s landscape.\n', '  Data serves as the fundamental foundation for advancing deep learning,\nparticularly tabular data presented in a structured format, which is highly\nconducive to modeling. However, even in the era of LLM, obtaining tabular data\nfrom sensitive domains remains a challenge due to privacy or copyright\nconcerns. Hence, exploring how to effectively use models like LLMs to generate\nrealistic and privacy-preserving synthetic tabular data is urgent. In this\npaper, we take a step forward to explore LLMs for tabular data synthesis and\nprivacy protection, by introducing a new framework HARMONIC for tabular data\ngeneration and evaluation. In the tabular data generation of our framework,\nunlike previous small-scale LLM-based methods that rely on continued\npre-training, we explore the larger-scale LLMs with fine-tuning to generate\ntabular data and enhance privacy. Based on idea of the k-nearest neighbors\nalgorithm, an instruction fine-tuning dataset is constructed to inspire LLMs to\ndiscover inter-row relationships. Then, with fine-tuning, LLMs are trained to\nremember the format and connections of the data rather than the data itself,\nwhich reduces the risk of privacy leakage. In the evaluation part of our\nframework, we develop specific privacy risk metrics DLT for LLM synthetic data\ngeneration, as well as performance evaluation metrics LLE for downstream LLM\ntasks. Our experiments find that this tabular data generation framework\nachieves equivalent performance to existing methods with better privacy, which\nalso demonstrates our evaluation framework for the effectiveness of synthetic\ndata and privacy risks in LLM scenarios.\n', '  Synthetic data from generative models emerges as the privacy-preserving\ndata-sharing solution. Such a synthetic data set shall resemble the original\ndata without revealing identifiable private information. The backbone\ntechnology of tabular synthesizers is rooted in image generative models,\nranging from Generative Adversarial Networks (GANs) to recent diffusion models.\nRecent prior work sheds light on the utility-privacy tradeoff on tabular data,\nrevealing and quantifying privacy risks on synthetic data. We first conduct an\nexhaustive empirical analysis, highlighting the utility-privacy tradeoff of\nfive state-of-the-art tabular synthesizers, against eight privacy attacks, with\na special focus on membership inference attacks. Motivated by the observation\nof high data quality but also high privacy risk in tabular diffusion, we\npropose DP-TLDM, Differentially Private Tabular Latent Diffusion Model, which\nis composed of an autoencoder network to encode the tabular data and a latent\ndiffusion model to synthesize the latent tables. Following the emerging f-DP\nframework, we apply DP-SGD to train the auto-encoder in combination with batch\nclipping and use the separation value as the privacy metric to better capture\nthe privacy gain from DP algorithms. Our empirical evaluation demonstrates that\nDP-TLDM is capable of achieving a meaningful theoretical privacy guarantee\nwhile also significantly enhancing the utility of synthetic data. Specifically,\ncompared to other DP-protected tabular generative models, DP-TLDM improves the\nsynthetic quality by an average of 35% in data resemblance, 15% in the utility\nfor downstream tasks, and 50% in data discriminability, all while preserving a\ncomparable level of privacy risk.\n'] , ['  Synthesizing electronic health records (EHR) data has become a preferred\nstrategy to address data scarcity, improve data quality, and model fairness in\nhealthcare. However, existing approaches for EHR data generation predominantly\nrely on state-of-the-art generative techniques like generative adversarial\nnetworks, variational autoencoders, and language models. These methods\ntypically replicate input visits, resulting in inadequate modeling of temporal\ndependencies between visits and overlooking the generation of time information,\na crucial element in EHR data. Moreover, their ability to learn visit\nrepresentations is limited due to simple linear mapping functions, thus\ncompromising generation quality. To address these limitations, we propose a\nnovel EHR data generation model called EHRPD. It is a diffusion-based model\ndesigned to predict the next visit based on the current one while also\nincorporating time interval estimation. To enhance generation quality and\ndiversity, we introduce a novel time-aware visit embedding module and a\npioneering predictive denoising diffusion probabilistic model (PDDPM).\nAdditionally, we devise a predictive U-Net (PU-Net) to optimize P-DDPM.We\nconduct experiments on two public datasets and evaluate EHRPD from fidelity,\nprivacy, and utility perspectives. The experimental results demonstrate the\nefficacy and utility of the proposed EHRPD in addressing the aforementioned\nlimitations and advancing EHR data generation.\n', '  Electronic health records (EHRs) are a pivotal data source that enables\nnumerous applications in computational medicine, e.g., disease progression\nprediction, clinical trial design, and health economics and outcomes research.\nDespite wide usability, their sensitive nature raises privacy and\nconfidentially concerns, which limit potential use cases. To tackle these\nchallenges, we explore the use of generative models to synthesize artificial,\nyet realistic EHRs. While diffusion-based methods have recently demonstrated\nstate-of-the-art performance in generating other data modalities and overcome\nthe training instability and mode collapse issues that plague previous\nGAN-based approaches, their applications in EHR generation remain\nunderexplored. The discrete nature of tabular medical code data in EHRs poses\nchallenges for high-quality data generation, especially for continuous\ndiffusion models. To this end, we introduce a novel tabular EHR generation\nmethod, EHR-D3PM, which enables both unconditional and conditional generation\nusing the discrete diffusion model. Our experiments demonstrate that EHR-D3PM\nsignificantly outperforms existing generative baselines on comprehensive\nfidelity and utility metrics while maintaining less attribute and membership\nvulnerability risks. Furthermore, we show EHR-D3PM is effective as a data\naugmentation method and enhances performance on downstream tasks when combined\nwith real data.\n', '  Electronic health records (EHR) contain a wealth of biomedical information,\nserving as valuable resources for the development of precision medicine\nsystems. However, privacy concerns have resulted in limited access to\nhigh-quality and large-scale EHR data for researchers, impeding progress in\nmethodological development. Recent research has delved into synthesizing\nrealistic EHR data through generative modeling techniques, where a majority of\nproposed methods relied on generative adversarial networks (GAN) and their\nvariants for EHR synthesis. Despite GAN-based methods attaining\nstate-of-the-art performance in generating EHR data, these approaches are\ndifficult to train and prone to mode collapse. Recently introduced in\ngenerative modeling, diffusion models have established cutting-edge performance\nin image generation, but their efficacy in EHR data synthesis remains largely\nunexplored. In this study, we investigate the potential of diffusion models for\nEHR data synthesis and introduce a novel method, EHRDiff. Through extensive\nexperiments, EHRDiff establishes new state-of-the-art quality for synthetic EHR\ndata, protecting private information in the meanwhile.\n'] , ['  This paper provides a detailed survey of synthetic data techniques. We first\ndiscuss the expected goals of using synthetic data in data augmentation, which\ncan be divided into four parts: 1) Improving Diversity, 2) Data Balancing, 3)\nAddressing Domain Shift, and 4) Resolving Edge Cases. Synthesizing data are\nclosely related to the prevailing machine learning techniques at the time,\ntherefore, we summarize the domain of synthetic data techniques into four\ncategories: 1) Expert-knowledge, 2) Direct Training, 3) Pre-train then\nFine-tune, and 4) Foundation Models without Fine-tuning. Next, we categorize\nthe goals of synthetic data filtering into four types for discussion: 1) Basic\nQuality, 2) Label Consistency, and 3) Data Distribution. In section 5 of this\npaper, we also discuss the future directions of synthetic data and state three\ndirection that we believe is important: 1) focus more on quality, 2) the\nevaluation of synthetic data, and 3) multi-model data augmentation.\n', '  Synthetic data algorithms are widely employed in industries to generate\nartificial data for downstream learning tasks. While existing research\nprimarily focuses on empirically evaluating utility of synthetic data, its\ntheoretical understanding is largely lacking. This paper bridges the\npractice-theory gap by establishing relevant utility theory in a statistical\nlearning framework. It considers two utility metrics: generalization and\nranking of models trained on synthetic data. The former is defined as the\ngeneralization difference between models trained on synthetic and on real data.\nBy deriving analytical bounds for this utility metric, we demonstrate that the\nsynthetic feature distribution does not need to be similar as that of real data\nfor ensuring comparable generalization of synthetic models, provided proper\nmodel specifications in downstream learning tasks. The latter utility metric\nstudies the relative performance of models trained on synthetic data. In\nparticular, we discover that the distribution of synthetic data is not\nnecessarily similar as the real one to ensure consistent model comparison.\nInterestingly, consistent model comparison is still achievable even when\nsynthetic responses are not well generated, as long as downstream models are\nseparable by a generalization gap. Finally, extensive experiments on\nnon-parametric models and deep neural networks have been conducted to validate\nthese theoretical findings.\n', '  We study, from an empirical standpoint, the efficacy of synthetic data in\nreal-world scenarios. Leveraging synthetic data for training perception models\nhas become a key strategy embraced by the community due to its efficiency,\nscalability, perfect annotations, and low costs. Despite proven advantages, few\nstudies put their stress on how to efficiently generate synthetic datasets to\nsolve real-world problems and to what extent synthetic data can reduce the\neffort for real-world data collection. To answer the questions, we\nsystematically investigate several interesting properties of synthetic data --\nthe equivalency of synthetic data to real-world data, the substitutability of\nsynthetic data for real data, and the flexibility of synthetic data generators\nto close up domain gaps. Leveraging the M3Act synthetic data generator, we\nconduct experiments on DanceTrack and MOT17. Our results suggest that synthetic\ndata not only enhances model performance but also demonstrates substitutability\nfor real data, with 60% to 80% replacement without performance loss. In\naddition, our study of the impact of synthetic data distributions on downstream\nperformance reveals the importance of flexible data generators in narrowing\ndomain gaps for improved model adaptability.\n']",Synthetic Data Generation and Applications,"""Synthetic Data Generation and Privacy"""
72,"Detecting Ponzi Schemes on Ethereum Blockchain , Blockchain Security and Privacy Solutions","['blockchain', 'cryptocurrencies', 'bitcoin', 'ethereum', 'fraud', 'frauds', 'cryptocurrency', 'ponzi', 'vulnerabilities', 'phishing'] , ['blockchain', 'blockchains', 'blockchained', 'mainchain', 'federated', 'ledger', 'decentralized', 'privacy', 'ipfs', 'security']","[""  As blockchain technology becomes more and more popular, a typical financial\nscam, the Ponzi scheme, has also emerged in the blockchain platform Ethereum.\nThis Ponzi scheme deployed through smart contracts, also known as the smart\nPonzi scheme, has caused a lot of economic losses and negative impacts.\nExisting methods for detecting smart Ponzi schemes on Ethereum mainly rely on\nbytecode features, opcode features, account features, and transaction behavior\nfeatures of smart contracts, which are unable to truly characterize the\nbehavioral features of Ponzi schemes, and thus generally perform poorly in\nterms of detection accuracy and false alarm rates. In this paper, we propose\nSourceP, a method to detect smart Ponzi schemes on the Ethereum platform using\npre-trained models and data flow, which only requires using the source code of\nsmart contracts as features. SourceP reduces the difficulty of data acquisition\nand feature extraction of existing detection methods. Specifically, we first\nconvert the source code of a smart contract into a data flow graph and then\nintroduce a pre-trained model based on learning code representations to build a\nclassification model to identify Ponzi schemes in smart contracts. The\nexperimental results show that SourceP achieves 87.2% recall and 90.7% F-score\nfor detecting smart Ponzi schemes within Ethereum's smart contract dataset,\noutperforming state-of-the-art methods in terms of performance and\nsustainability. We also demonstrate through additional experiments that\npre-trained models and data flow play an important contribution to SourceP, as\nwell as proving that SourceP has a good generalization ability.\n"", ""  The rampant fraudulent activities on Ethereum hinder the healthy development\nof the blockchain ecosystem, necessitating the reinforcement of regulations.\nHowever, multiple imbalances involving account interaction frequencies and\ninteraction types in the Ethereum transaction environment pose significant\nchallenges to data mining-based fraud detection research. To address this, we\nfirst propose the concept of meta-interactions to refine interaction behaviors\nin Ethereum, and based on this, we present a dual self-supervision enhanced\nEthereum fraud detection framework, named Meta-IFD. This framework initially\nintroduces a generative self-supervision mechanism to augment the interaction\nfeatures of accounts, followed by a contrastive self-supervision mechanism to\ndifferentiate various behavior patterns, and ultimately characterizes the\nbehavioral representations of accounts and mines potential fraud risks through\nmulti-view interaction feature learning. Extensive experiments on real Ethereum\ndatasets demonstrate the effectiveness and superiority of our framework in\ndetecting common Ethereum fraud behaviors such as Ponzi schemes and phishing\nscams. Additionally, the generative module can effectively alleviate the\ninteraction distribution imbalance in Ethereum data, while the contrastive\nmodule significantly enhances the framework's ability to distinguish different\nbehavior patterns. The source code will be released on GitHub soon.\n"", '  The Ponzi scheme, an old-fashioned fraud, is now popular on the Ethereum\nblockchain, causing considerable financial losses to many crypto investors. A\nfew Ponzi detection methods have been proposed in the literature, most of which\ndetect a Ponzi scheme based on its smart contract source code. This\ncontract-code-based approach, while achieving very high accuracy, is not robust\nbecause a Ponzi developer can fool a detection model by obfuscating the opcode\nor inventing a new profit distribution logic that cannot be detected. On the\ncontrary, a transaction-based approach could improve the robustness of\ndetection because transactions, unlike smart contracts, are harder to be\nmanipulated. However, the current transaction-based detection models achieve\nfairly low accuracy. In this paper, we aim to improve the accuracy of the\ntransaction-based models by employing time-series features, which turn out to\nbe crucial in capturing the life-time behaviour a Ponzi application but were\ncompletely overlooked in previous works. We propose a new set of 85 features\n(22 known account-based and 63 new time-series features), which allows\noff-the-shelf machine learning algorithms to achieve up to 30% higher F1-scores\ncompared to existing works.\n'] , ['  Generative Artificial Intelligence (GAI) has recently emerged as a promising\nsolution to address critical challenges of blockchain technology, including\nscalability, security, privacy, and interoperability. In this paper, we first\nintroduce GAI techniques, outline their applications, and discuss existing\nsolutions for integrating GAI into blockchains. Then, we discuss emerging\nsolutions that demonstrate the effectiveness of GAI in addressing various\nchallenges of blockchain, such as detecting unknown blockchain attacks and\nsmart contract vulnerabilities, designing key secret sharing schemes, and\nenhancing privacy. Moreover, we present a case study to demonstrate that GAI,\nspecifically the generative diffusion model, can be employed to optimize\nblockchain network performance metrics. Experimental results clearly show that,\ncompared to a baseline traditional AI approach, the proposed generative\ndiffusion model approach can converge faster, achieve higher rewards, and\nsignificantly improve the throughput and latency of the blockchain network.\nAdditionally, we highlight future research directions for GAI in blockchain\napplications, including personalized GAI-enabled blockchains, GAI-blockchain\nsynergy, and privacy and security considerations within blockchain ecosystems.\n', ""  This article aims to study intrusion attacks and then develop a novel\ncyberattack detection framework to detect cyberattacks at the network layer\n(e.g., Brute Password and Flooding of Transactions) of blockchain networks.\nSpecifically, we first design and implement a blockchain network in our\nlaboratory. This blockchain network will serve two purposes, i.e., to generate\nthe real traffic data (including both normal data and attack data) for our\nlearning models and to implement real-time experiments to evaluate the\nperformance of our proposed intrusion detection framework. To the best of our\nknowledge, this is the first dataset that is synthesized in a laboratory for\ncyberattacks in a blockchain network. We then propose a novel collaborative\nlearning model that allows efficient deployment in the blockchain network to\ndetect attacks. The main idea of the proposed learning model is to enable\nblockchain nodes to actively collect data, learn the knowledge from data using\nthe Deep Belief Network, and then share the knowledge learned from its data\nwith other blockchain nodes in the network. In this way, we can not only\nleverage the knowledge from all the nodes in the network but also do not need\nto gather all raw data for training at a centralized node like conventional\ncentralized learning solutions. Such a framework can also avoid the risk of\nexposing local data's privacy as well as excessive network overhead/congestion.\nBoth intensive simulations and real-time experiments clearly show that our\nproposed intrusion detection framework can achieve an accuracy of up to 98.6%\nin detecting attacks.\n"", '  Federated Learning (FL) has recently arisen as a revolutionary approach to\ncollaborative training Machine Learning models. According to this novel\nframework, multiple participants train a global model collaboratively,\ncoordinating with a central aggregator without sharing their local data. As FL\ngains popularity in diverse domains, security, and privacy concerns arise due\nto the distributed nature of this solution. Therefore, integrating this\nstrategy with Blockchain technology has been consolidated as a preferred choice\nto ensure the privacy and security of participants.\n  This paper explores the research efforts carried out by the scientific\ncommunity to define privacy solutions in scenarios adopting Blockchain-Enabled\nFL. It comprehensively summarizes the background related to FL and Blockchain,\nevaluates existing architectures for their integration, and the primary attacks\nand possible countermeasures to guarantee privacy in this setting. Finally, it\nreviews the main application scenarios where Blockchain-Enabled FL approaches\nhave been proficiently applied. This survey can help academia and industry\npractitioners understand which theories and techniques exist to improve the\nperformance of FL through Blockchain to preserve privacy and which are the main\nchallenges and future directions in this novel and still under-explored\ncontext. We believe this work provides a novel contribution respect to the\nprevious surveys and is a valuable tool to explore the current landscape,\nunderstand perspectives, and pave the way for advancements or improvements in\nthis amalgamation of Blockchain and Federated Learning.\n']",Blockchain Security and Fraud Detection,Blockchain Security and Privacy Solutions
73,Byzantine Fault Tolerance in Distributed ML,"['byzantine', 'distributed', 'adversarial', 'mlmc', 'decentralized', 'robustscgmm', 'robust', 'federated', 'aggregators', 'adversaries']","['  This paper proposes two split-and-conquer (SC) learning estimators for finite\nmixture models that are tolerant to Byzantine failures. In SC learning,\nindividual machines obtain local estimates, which are then transmitted to a\ncentral server for aggregation. During this communication, the server may\nreceive malicious or incorrect information from some local machines, a scenario\nknown as Byzantine failures. While SC learning approaches have been devised to\nmitigate Byzantine failures in statistical models with Euclidean parameters,\ndeveloping Byzantine-tolerant methods for finite mixture models with\nnon-Euclidean parameters requires a distinct strategy. Our proposed\ndistance-based methods are hyperparameter tuning free, unlike existing methods,\nand are resilient to Byzantine failures while achieving high statistical\nefficiency. We validate the effectiveness of our methods both theoretically and\nempirically via experiments on simulated and real data from machine learning\napplications for digit recognition. The code for the experiment can be found at\nhttps://github.com/SarahQiong/RobustSCGMM.\n', '  In Federated Reinforcement Learning (FRL), agents aim to collaboratively\nlearn a common task, while each agent is acting in its local environment\nwithout exchanging raw trajectories. Existing approaches for FRL either (a) do\nnot provide any fault-tolerance guarantees (against misbehaving agents), or (b)\nrely on a trusted central agent (a single point of failure) for aggregating\nupdates. We provide the first decentralized Byzantine fault-tolerant FRL\nmethod. Towards this end, we first propose a new centralized Byzantine\nfault-tolerant policy gradient (PG) algorithm that improves over existing\nmethods by relying only on assumptions standard for non-fault-tolerant PG.\nThen, as our main contribution, we show how a combination of robust aggregation\nand Byzantine-resilient agreement methods can be leveraged in order to\neliminate the need for a trusted central entity. Since our results represent\nthe first sample complexity analysis for Byzantine fault-tolerant decentralized\nfederated non-convex optimization, our technical contributions may be of\nindependent interest. Finally, we corroborate our theoretical results\nexperimentally for common RL environments, demonstrating the speed-up of\ndecentralized federations w.r.t. the number of participating agents and\nresilience against various Byzantine attacks.\n', '  Distributed learning has emerged as a leading paradigm for training large\nmachine learning models. However, in real-world scenarios, participants may be\nunreliable or malicious, posing a significant challenge to the integrity and\naccuracy of the trained models. Byzantine fault tolerance mechanisms have been\nproposed to address these issues, but they often assume full participation from\nall clients, which is not always practical due to the unavailability of some\nclients or communication constraints. In our work, we propose the first\ndistributed method with client sampling and provable tolerance to Byzantine\nworkers. The key idea behind the developed method is the use of gradient\nclipping to control stochastic gradient differences in recursive variance\nreduction. This allows us to bound the potential harm caused by Byzantine\nworkers, even during iterations when all sampled clients are Byzantine.\nFurthermore, we incorporate communication compression into the method to\nenhance communication efficiency. Under general assumptions, we prove\nconvergence rates for the proposed method that match the existing\nstate-of-the-art (SOTA) theoretical results. We also propose a heuristic on\nadjusting any Byzantine-robust method to a partial participation scenario via\nclipping.\n']",Byzantine Fault Tolerance in Distributed Machine Learning,Byzantine Fault Tolerance in Distributed ML
74,"Machine Unlearning for Data Privacy , Homomorphic Encryption for Privacy-Preserving Machine Learning , Encrypted Traffic Classification","['unlearning', 'privacy', 'adversary', 'obfuscation', 'security', 'attacks', 'attacker', 'defenses', 'adversaries', 'trained'] , ['homomorphic', 'homomorphically', 'privacy', 'encryption', 'cryptographic', 'cryptonets', 'encrypted', 'confidentiality', 'confidential', 'secure'] , ['traffic', 'trafficgpt', 'packet', 'encryption', 'bytes', 'byte', 'classification', 'classifying', 'streams', 'encrypted']","['  With the continued advancement and widespread adoption of machine learning\n(ML) models across various domains, ensuring user privacy and data security has\nbecome a paramount concern. In compliance with data privacy regulations, such\nas GDPR, a secure machine learning framework should not only grant users the\nright to request the removal of their contributed data used for model training\nbut also facilitates the elimination of sensitive data fingerprints within\nmachine learning models to mitigate potential attack - a process referred to as\nmachine unlearning. In this study, we present a novel unlearning mechanism\ndesigned to effectively remove the impact of specific data samples from a\nneural network while considering the performance of the unlearned model on the\nprimary task. In achieving this goal, we crafted a novel loss function tailored\nto eliminate privacy-sensitive information from weights and activation values\nof the target model by combining target classification loss and membership\ninference loss. Our adaptable framework can easily incorporate various privacy\nleakage approximation mechanisms to guide the unlearning process. We provide\nempirical evidence of the effectiveness of our unlearning approach with a\ntheoretical upper-bound analysis through a membership inference mechanism as a\nproof of concept. Our results showcase the superior performance of our approach\nin terms of unlearning efficacy and latency as well as the fidelity of the\nprimary task, across four datasets and four deep learning architectures.\n', ""  This paper focuses on the challenge of machine unlearning, aiming to remove\nthe influence of specific training data on machine learning models.\nTraditionally, the development of unlearning algorithms runs parallel with that\nof membership inference attacks (MIA), a type of privacy threat to determine\nwhether a data instance was used for training. However, the two strands are\nintimately connected: one can view machine unlearning through the lens of MIA\nsuccess with respect to removed data. Recognizing this connection, we propose a\ngame-theoretic framework that integrates MIAs into the design of unlearning\nalgorithms. Specifically, we model the unlearning problem as a Stackelberg game\nin which an unlearner strives to unlearn specific training data from a model,\nwhile an auditor employs MIAs to detect the traces of the ostensibly removed\ndata. Adopting this adversarial perspective allows the utilization of new\nattack advancements, facilitating the design of unlearning algorithms. Our\nframework stands out in two ways. First, it takes an adversarial approach and\nproactively incorporates the attacks into the design of unlearning algorithms.\nSecondly, it uses implicit differentiation to obtain the gradients that limit\nthe attacker's success, thus benefiting the process of unlearning. We present\nempirical results to demonstrate the effectiveness of the proposed approach for\nmachine unlearning.\n"", '  The high cost of model training makes it increasingly desirable to develop\ntechniques for unlearning. These techniques seek to remove the influence of a\ntraining example without having to retrain the model from scratch. Intuitively,\nonce a model has unlearned, an adversary that interacts with the model should\nno longer be able to tell whether the unlearned example was included in the\nmodel\'s training set or not. In the privacy literature, this is known as\nmembership inference. In this work, we discuss adaptations of Membership\nInference Attacks (MIAs) to the setting of unlearning (leading to their ""U-MIA""\ncounterparts). We propose a categorization of existing U-MIAs into ""population\nU-MIAs"", where the same attacker is instantiated for all examples, and\n""per-example U-MIAs"", where a dedicated attacker is instantiated for each\nexample. We show that the latter category, wherein the attacker tailors its\nmembership prediction to each example under attack, is significantly stronger.\nIndeed, our results show that the commonly used U-MIAs in the unlearning\nliterature overestimate the privacy protection afforded by existing unlearning\ntechniques on both vision and language models. Our investigation reveals a\nlarge variance in the vulnerability of different examples to per-example\nU-MIAs. In fact, several unlearning algorithms lead to a reduced vulnerability\nfor some, but not all, examples that we wish to unlearn, at the expense of\nincreasing it for other examples. Notably, we find that the privacy protection\nfor the remaining training examples may worsen as a consequence of unlearning.\nWe also discuss the fundamental difficulty of equally protecting all examples\nusing existing unlearning schemes, due to the different rates at which examples\nare unlearned. We demonstrate that naive attempts at tailoring unlearning\nstopping criteria to different examples fail to alleviate these issues.\n'] , [""  Transfer learning is a de facto standard method for efficiently training\nmachine learning models for data-scarce problems by adding and fine-tuning new\nclassification layers to a model pre-trained on large datasets. Although\nnumerous previous studies proposed to use homomorphic encryption to resolve the\ndata privacy issue in transfer learning in the machine learning as a service\nsetting, most of them only focused on encrypted inference. In this study, we\npresent HETAL, an efficient Homomorphic Encryption based Transfer Learning\nalgorithm, that protects the client's privacy in training tasks by encrypting\nthe client data using the CKKS homomorphic encryption scheme. HETAL is the\nfirst practical scheme that strictly provides encrypted training, adopting\nvalidation-based early stopping and achieving the accuracy of nonencrypted\ntraining. We propose an efficient encrypted matrix multiplication algorithm,\nwhich is 1.8 to 323 times faster than prior methods, and a highly precise\nsoftmax approximation algorithm with increased coverage. The experimental\nresults for five well-known benchmark datasets show total training times of\n567-3442 seconds, which is less than an hour.\n"", ""  In this paper, we introduce a privacy-preserving stable diffusion framework\nleveraging homomorphic encryption, called HE-Diffusion, which primarily focuses\non protecting the denoising phase of the diffusion process. HE-Diffusion is a\ntailored encryption framework specifically designed to align with the unique\narchitecture of stable diffusion, ensuring both privacy and functionality. To\naddress the inherent computational challenges, we propose a novel\nmin-distortion method that enables efficient partial image encryption,\nsignificantly reducing the overhead without compromising the model's output\nquality. Furthermore, we adopt a sparse tensor representation to expedite\ncomputational operations, enhancing the overall efficiency of the\nprivacy-preserving diffusion process. We successfully implement HE-based\nprivacy-preserving stable diffusion inference. The experimental results show\nthat HE-Diffusion achieves 500 times speedup compared with the baseline method,\nand reduces time cost of the homomorphically encrypted inference to the minute\nlevel. Both the performance and accuracy of the HE-Diffusion are on par with\nthe plaintext counterpart. Our approach marks a significant step towards\nintegrating advanced cryptographic techniques with state-of-the-art generative\nmodels, paving the way for privacy-preserving and efficient image generation in\ncritical applications.\n"", '  Advancements in machine learning (ML) have significantly revolutionized\nmedical image analysis, prompting hospitals to rely on external ML services.\nHowever, the exchange of sensitive patient data, such as chest X-rays, poses\ninherent privacy risks when shared with third parties. Addressing this concern,\nwe propose MedBlindTuner, a privacy-preserving framework leveraging fully\nhomomorphic encryption (FHE) and a data-efficient image transformer (DEiT).\nMedBlindTuner enables the training of ML models exclusively on FHE-encrypted\nmedical images. Our experimental evaluation demonstrates that MedBlindTuner\nachieves comparable accuracy to models trained on non-encrypted images,\noffering a secure solution for outsourcing ML computations while preserving\npatient data privacy. To the best of our knowledge, this is the first work that\nuses data-efficient image transformers and fully homomorphic encryption in this\ndomain.\n'] , ['  With 95% of Internet traffic now encrypted, an effective approach to\nclassifying this traffic is crucial for network security and management. This\npaper introduces ECHO -- a novel optimization process for ML/DL-based encrypted\ntraffic classification. ECHO targets both classification time and memory\nutilization and incorporates two innovative techniques.\n  The first component, HO (Hyperparameter Optimization of binnings), aims at\ncreating efficient traffic representations. While previous research often uses\nrepresentations that map packet sizes and packet arrival times to fixed-sized\nbins, we show that non-uniform binnings are significantly more efficient. These\nnon-uniform binnings are derived by employing a hyperparameter optimization\nalgorithm in the training stage. HO significantly improves accuracy given a\nrequired representation size, or, equivalently, achieves comparable accuracy\nusing smaller representations.\n  Then, we introduce EC (Early Classification of traffic), which enables faster\nclassification using a cascade of classifiers adapted for different exit times,\nwhere classification is based on the level of confidence. EC reduces the\naverage classification latency by up to 90\\%. Remarkably, this method not only\nmaintains classification accuracy but also, in certain cases, improves it.\n  Using three publicly available datasets, we demonstrate that the combined\nmethod, Early Classification with Hyperparameter Optimization (ECHO), leads to\na significant improvement in classification efficiency.\n', '  Network traffic refers to the amount of data being sent and received over the\ninternet or any system that connects computers. Analyzing and understanding\nnetwork traffic is vital for improving network security and management.\nHowever, the analysis of network traffic is challenging due to the diverse\nnature of data packets, which often feature heterogeneous headers and encrypted\npayloads lacking semantics. To capture the latent semantics of traffic, a few\nstudies have adopted pre-training techniques based on the Transformer encoder\nor decoder to learn the representations from massive traffic data. However,\nthese methods typically excel in traffic understanding (classification) or\ntraffic generation tasks. To address this issue, we develop Lens, a foundation\nmodel for network traffic that leverages the T5 architecture to learn the\npre-trained representations from large-scale unlabeled data. Harnessing the\nstrength of the encoder-decoder framework, which captures the global\ninformation while preserving the generative ability, our model can better learn\nthe representations from raw data. To further enhance pre-training\neffectiveness, we design a novel loss that combines three distinct tasks:\nMasked Span Prediction (MSP), Packet Order Prediction (POP), and Homologous\nTraffic Prediction (HTP). Evaluation results across various benchmark datasets\ndemonstrate that the proposed Lens outperforms the baselines in most downstream\ntasks related to both traffic understanding and generation. Notably, it also\nrequires much less labeled data for fine-tuning compared to current methods.\n', ""  Encrypted traffic classification is the task of identifying the application\nor service associated with encrypted network traffic. One effective approach\nfor this task is to use deep learning methods to encode the raw traffic bytes\ndirectly and automatically extract features for classification (byte-based\nmodels). However, current byte-based models input raw traffic bytes, whether\nplaintext or encrypted text, for automated feature extraction, neglecting the\ndistinct impacts of plaintext and encrypted text on downstream tasks.\nAdditionally, these models primarily focus on improving classification\naccuracy, with little emphasis on the efficiency of models. In this paper, for\nthe first time, we analyze the impact of plaintext and encrypted text on the\nmodel's effectiveness and efficiency. Based on our observations and findings,\nwe propose a two-phase approach to balance the trade-off between plaintext and\nencrypted text in traffic classification. Specifically, Stage one is to\nDetermine whether the Plain text is enough to be accurately Classified (DPC)\nusing the proposed DPC Selector. This stage quickly identifies samples that can\nbe classified using plaintext, leveraging explicit byte features in plaintext\nto enhance model's efficiency. Stage two aims to adaptively make a\nclassification with the result from stage one. This stage incorporates\nencrypted text information for samples that cannot be classified using\nplaintext alone, ensuring the model's effectiveness on traffic classification\ntasks. Experiments on two datasets demonstrate that our proposed model achieves\nstate-of-the-art results in both effectiveness and efficiency.\n""]",Machine Learning for Data Privacy and Security,Homomorphic Encryption for Privacy-Preserving Machine Learning
75,"""Speaker Anonymization and Spoofing Countermeasures""","['anonymization', 'spoofing', 'hiddenspeaker', 'anonymized', 'adversarial', 'voice', 'spoof', 'spoofed', 'speaker', 'audioseal']","['  It is now well-known that automatic speaker verification (ASV) systems can be\nspoofed using various types of adversaries. The usual approach to counteract\nASV systems against such attacks is to develop a separate spoofing\ncountermeasure (CM) module to classify speech input either as a bonafide, or a\nspoofed utterance. Nevertheless, such a design requires additional computation\nand utilization efforts at the authentication stage. An alternative strategy\ninvolves a single monolithic ASV system designed to handle both zero-effort\nimposter (non-targets) and spoofing attacks. Such spoof-aware ASV systems have\nthe potential to provide stronger protections and more economic computations.\nTo this end, we propose to generalize the standalone ASV (G-SASV) against\nspoofing attacks, where we leverage limited training data from CM to enhance a\nsimple backend in the embedding space, without the involvement of a separate CM\nmodule during the test (authentication) phase. We propose a novel yet simple\nbackend classifier based on deep neural networks and conduct the study via\ndomain adaptation and multi-task integration of spoof embeddings at the\ntraining stage. Experiments are conducted on the ASVspoof 2019 logical access\ndataset, where we improve the performance of statistical ASV backends on the\njoint (bonafide and spoofed) and spoofed conditions by a maximum of 36.2% and\n49.8% in terms of equal error rates, respectively.\n', ""  The growing use of voice user interfaces has led to a surge in the collection\nand storage of speech data. While data collection allows for the development of\nefficient tools powering most speech services, it also poses serious privacy\nissues for users as centralized storage makes private personal speech data\nvulnerable to cyber threats. With the increasing use of voice-based digital\nassistants like Amazon's Alexa, Google's Home, and Apple's Siri, and with the\nincreasing ease with which personal speech data can be collected, the risk of\nmalicious use of voice-cloning and speaker/gender/pathological/etc. recognition\nhas increased.\n  This thesis proposes solutions for anonymizing speech and evaluating the\ndegree of the anonymization. In this work, anonymization refers to making\npersonal speech data unlinkable to an identity while maintaining the usefulness\n(utility) of the speech signal (e.g., access to linguistic content). We start\nby identifying several challenges that evaluation protocols need to consider to\nevaluate the degree of privacy protection properly. We clarify how\nanonymization systems must be configured for evaluation purposes and highlight\nthat many practical deployment configurations do not permit privacy evaluation.\nFurthermore, we study and examine the most common voice conversion-based\nanonymization system and identify its weak points before suggesting new methods\nto overcome some limitations. We isolate all components of the anonymization\nsystem to evaluate the degree of speaker PPI associated with each of them.\nThen, we propose several transformation methods for each component to reduce as\nmuch as possible speaker PPI while maintaining utility. We promote\nanonymization algorithms based on quantization-based transformation as an\nalternative to the most-used and well-known noise-based approach. Finally, we\nendeavor a new attack method to invert anonymization.\n"", '  Privacy-preserving voice protection approaches primarily suppress\nprivacy-related information derived from paralinguistic attributes while\npreserving the linguistic content. Existing solutions focus on single-speaker\nscenarios. However, they lack practicality for real-world applications, i.e.,\nmulti-speaker scenarios. In this paper, we present an initial attempt to\nprovide a multi-speaker anonymization benchmark by defining the task and\nevaluation protocol, proposing benchmarking solutions, and discussing the\nprivacy leakage of overlapping conversations. Specifically, ideal multi-speaker\nanonymization should preserve the number of speakers and the turn-taking\nstructure of the conversation, ensuring accurate context conveyance while\nmaintaining privacy. To achieve that, a cascaded system uses speaker\ndiarization to aggregate the speech of each speaker and speaker anonymization\nto conceal speaker privacy and preserve speech content. Additionally, we\npropose two conversation-level speaker vector anonymization methods to improve\nthe utility further. Both methods aim to make the original and corresponding\npseudo-speaker identities of each speaker unlinkable while preserving or even\nimproving the distinguishability among pseudo-speakers in a conversation. The\nfirst method minimizes the differential similarity across speaker pairs in the\noriginal and anonymized conversations to maintain original speaker\nrelationships in the anonymized version. The other method minimizes the\naggregated similarity across anonymized speakers to achieve better\ndifferentiation between speakers. Experiments conducted on both non-overlap\nsimulated and real-world datasets demonstrate the effectiveness of the\nmulti-speaker anonymization system with the proposed speaker anonymizers.\nAdditionally, we analyzed overlapping speech regarding privacy leakage and\nprovide potential solutions.\n']",Speaker Privacy and Security in Voice Systems,"""Speaker Anonymization and Spoofing Countermeasures"""
76,Steganography and Steganalysis Techniques,"['steganography', 'steganographic', 'steganalysis', 'watermarking', 'stegano', 'stegotexts', 'stegotext', 'watermark', 'covert', 'hide']","['  This study discusses a new method combining image steganography technology\nwith Natural Language Processing (NLP) large models, aimed at improving the\naccuracy and robustness of extracting steganographic text. Traditional Least\nSignificant Bit (LSB) steganography techniques face challenges in accuracy and\nrobustness of information extraction when dealing with complex character\nencoding, such as Chinese characters. To address this issue, this study\nproposes an innovative LSB-NLP hybrid framework. This framework integrates the\nadvanced capabilities of NLP large models, such as error detection, correction,\nand semantic consistency analysis, as well as information reconstruction\ntechniques, thereby significantly enhancing the robustness of steganographic\ntext extraction. Experimental results show that the LSB-NLP hybrid framework\nexcels in improving the extraction accuracy of steganographic text, especially\nin handling Chinese characters. The findings of this study not only confirm the\neffectiveness of combining image steganography technology and NLP large models\nbut also propose new ideas for research and application in the field of\ninformation hiding. The successful implementation of this interdisciplinary\napproach demonstrates the great potential of integrating image steganography\ntechnology with natural language processing technology in solving complex\ninformation processing problems.\n', '  To detect stego (steganographic text) in complex scenarios, linguistic\nsteganalysis (LS) with various motivations has been proposed and achieved\nexcellent performance. However, with the development of generative\nsteganography, some stegos have strong concealment, especially after the\nemergence of LLMs-based steganography, the existing LS has low detection or\ncannot detect them. We designed a novel LS with two modes called LSGC. In the\ngeneration mode, we created an LS-task ""description"" and used the generation\nability of LLM to explain whether texts to be detected are stegos. On this\nbasis, we rethought the principle of LS and LLMs, and proposed the\nclassification mode. In this mode, LSGC deleted the LS-task ""description"" and\nused the ""causalLM"" LLMs to extract steganographic features. The LS features\ncan be extracted by only one pass of the model, and a linear layer with\ninitialization weights is added to obtain the classification probability.\nExperiments on strongly concealed stegos show that LSGC significantly improves\ndetection and reaches SOTA performance. Additionally, LSGC in classification\nmode greatly reduces training time while maintaining high performance.\n', '  Linguistic steganography (LS) tasks aim to generate steganographic text\n(stego) based on secret. Only authorized receivers can perceive and extract\nsecrets, thereby protecting privacy. However, existing generative LS schemes\noften do not consider the conditional probability of tokens in the candidate\npool, and allocate one or the same number of codings to all tokens. The tokens\nwith lower probabilities are selected to embed secrets that will affect the\nquality of stegos. As a result, the stegos are easy to perceive and detect.\nThis paper proposes the LS scheme based on dynamically allocated intervals,\ncalled DAIRstega. DAIRstega uses the idea of the roulette wheel and takes the\nconditional probabilities of tokens as the main basis for allocating the\nroulette area (i.e., the interval length). Thus, the token with a larger\nconditional probability is allocated more. The secret will be more likely to\nselect the tokens with larger probabilities. In the allocation process, we\ndesign some functions between probability and allocated interval length. Based\non the invisible characteristics of LS, we give three constraints that need to\nbe met to design the function. To simplify the form, the expression of the\nallocation function is condensed. Furthermore, DAIRstega can receive additional\ninstruction and controllably generate stegos. Rich experiments show that the\nproposed embedding way and DAIRstega perform superior to the existing ways and\nLS schemes, which shows strong perceptual, statistical, and semantic\nconcealment and anti-steganalysis ability. This scheme can also generate\nhigh-quality longer stegos, improving the deficiencies in this task. The\nexperiment also verified that DAIRstega can be used as a secure watermarking\nscheme, providing some ideas for its development.\n']",Information Hiding Techniques,Steganography and Steganalysis Techniques
77,"Explainability in NLP Models , Search Engine Transparency and Explanation Systems","['interpretability', 'explainability', 'interpretable', 'nlp', 'explanations', 'contextual', 'ai', 'explaining', 'text', 'syntaxshap'] , ['search', 'retrieval', 'crowdsourced', 'google', 'bing', 'snippets', 'bingchat', 'responses', 'relevance', 'generative']","[""  Explainable AI (XAI) aids in deciphering 'black-box' models. While several\nmethods have been proposed and evaluated primarily in the image domain, the\nexploration of explainability in the text domain remains a growing research\narea. In this paper, we delve into the applicability of XAI methods for the\ntext domain. In this context, the 'Similarity Difference and Uniqueness' (SIDU)\nXAI method, recognized for its superior capability in localizing entire salient\nregions in image-based classification is extended to textual data. The extended\nmethod, SIDU-TXT, utilizes feature activation maps from 'black-box' models to\ngenerate heatmaps at a granular, word-based level, thereby providing\nexplanations that highlight contextually significant textual elements crucial\nfor model predictions. Given the absence of a unified standard for assessing\nXAI methods, this study applies a holistic three-tiered comprehensive\nevaluation framework: Functionally-Grounded, Human-Grounded and\nApplication-Grounded, to assess the effectiveness of the proposed SIDU-TXT\nacross various experiments. We find that, in sentiment analysis task of a movie\nreview dataset, SIDU-TXT excels in both functionally and human-grounded\nevaluations, demonstrating superior performance through quantitative and\nqualitative analyses compared to benchmarks like Grad-CAM and LIME. In the\napplication-grounded evaluation within the sensitive and complex legal domain\nof asylum decision-making, SIDU-TXT and Grad-CAM demonstrate comparable\nperformances, each with its own set of strengths and weaknesses. However, both\nmethods fall short of entirely fulfilling the sophisticated criteria of expert\nexpectations, highlighting the imperative need for additional research in XAI\nmethods suitable for such domains.\n"", '  The necessity for interpretability in natural language processing (NLP) has\nrisen alongside the growing prominence of large language models. Among the\nmyriad tasks within NLP, text generation stands out as a primary objective of\nautoregressive models. The NLP community has begun to take a keen interest in\ngaining a deeper understanding of text generation, leading to the development\nof model-agnostic explainable artificial intelligence (xAI) methods tailored to\nthis task. The design and evaluation of explainability methods are non-trivial\nsince they depend on many factors involved in the text generation process,\ne.g., the autoregressive model and its stochastic nature. This paper outlines\n17 challenges categorized into three groups that arise during the development\nand assessment of attribution-based explainability methods. These challenges\nencompass issues concerning tokenization, defining explanation similarity,\ndetermining token importance and prediction change metrics, the level of human\nintervention required, and the creation of suitable test datasets. The paper\nillustrates how these challenges can be intertwined, showcasing new\nopportunities for the community. These include developing probabilistic\nword-level explainability methods and engaging humans in the explainability\npipeline, from the data design to the final evaluation, to draw robust\nconclusions on xAI methods.\n', ""  Saliency post-hoc explainability methods are important tools for\nunderstanding increasingly complex NLP models. While these methods can reflect\nthe model's reasoning, they may not align with human intuition, making the\nexplanations not plausible. In this work, we present a methodology for\nincorporating rationales, which are text annotations explaining human\ndecisions, into text classification models. This incorporation enhances the\nplausibility of post-hoc explanations while preserving their faithfulness. Our\napproach is agnostic to model architectures and explainability methods. We\nintroduce the rationales during model training by augmenting the standard\ncross-entropy loss with a novel loss function inspired by contrastive learning.\nBy leveraging a multi-objective optimization algorithm, we explore the\ntrade-off between the two loss functions and generate a Pareto-optimal frontier\nof models that balance performance and plausibility. Through extensive\nexperiments involving diverse models, datasets, and explainability methods, we\ndemonstrate that our approach significantly enhances the quality of model\nexplanations without causing substantial (sometimes negligible) degradation in\nthe original model's performance.\n""] , [""  There is a growing demand for transparency in search engines to understand\nhow search results are curated and to enhance users' trust. Prior research has\nintroduced search result explanations with a focus on how to explain, assuming\nexplanations are beneficial. Our study takes a step back to examine if search\nexplanations are needed and when they are likely to provide benefits.\nAdditionally, we summarize key characteristics of helpful explanations and\nshare users' perspectives on explanation features provided by Google and Bing.\nInterviews with non-technical individuals reveal that users do not always seek\nor understand search explanations and mostly desire them for complex and\ncritical tasks. They find Google's search explanations too obvious but\nappreciate the ability to contest search results. Based on our findings, we\noffer design recommendations for search engines and explanations to help users\nbetter evaluate search results and enhance their search experience.\n"", '  The advent of large language models (LLMs) has ushered in a new paradigm of\nsearch engines that use generative models to gather and summarize information\nto answer user queries. This emerging technology, which we formalize under the\nunified framework of generative engines (GEs), can generate accurate and\npersonalized responses, rapidly replacing traditional search engines like\nGoogle and Bing. Generative Engines typically satisfy queries by synthesizing\ninformation from multiple sources and summarizing them using LLMs. While this\nshift significantly improves $\\textit{user}$ utility and $\\textit{generative\nsearch engine}$ traffic, it poses a huge challenge for the third stakeholder --\nwebsite and content creators. Given the black-box and fast-moving nature of\ngenerative engines, content creators have little to no control over\n$\\textit{when}$ and $\\textit{how}$ their content is displayed. With generative\nengines here to stay, we must ensure the creator economy is not disadvantaged.\nTo address this, we introduce Generative Engine Optimization (GEO), the first\nnovel paradigm to aid content creators in improving their content visibility in\ngenerative engine responses through a flexible black-box optimization framework\nfor optimizing and defining visibility metrics. We facilitate systematic\nevaluation by introducing GEO-bench, a large-scale benchmark of diverse user\nqueries across multiple domains, along with relevant web sources to answer\nthese queries. Through rigorous evaluation, we demonstrate that GEO can boost\nvisibility by up to $40\\%$ in generative engine responses. Moreover, we show\nthe efficacy of these strategies varies across domains, underscoring the need\nfor domain-specific optimization methods. Our work opens a new frontier in\ninformation discovery systems, with profound implications for both developers\nof generative engines and content creators.\n', ""  Large Language Models (LLMs) are increasingly used for accessing information\non the web. Their truthfulness and factuality are thus of great interest. To\nhelp users make the right decisions about the information they get, LLMs should\nnot only provide information but also help users fact-check it. Our experiments\nwith 80 crowdworkers compare language models with search engines (information\nretrieval systems) at facilitating fact-checking. We prompt LLMs to validate a\ngiven claim and provide corresponding explanations. Users reading LLM\nexplanations are significantly more efficient than those using search engines\nwhile achieving similar accuracy. However, they over-rely on the LLMs when the\nexplanation is wrong. To reduce over-reliance on LLMs, we ask LLMs to provide\ncontrastive information - explain both why the claim is true and false, and\nthen we present both sides of the explanation to users. This contrastive\nexplanation mitigates users' over-reliance on LLMs, but cannot significantly\noutperform search engines. Further, showing both search engine results and LLM\nexplanations offers no complementary benefits compared to search engines alone.\nTaken together, our study highlights that natural language explanations by LLMs\nmay not be a reliable replacement for reading the retrieved passages,\nespecially in high-stakes settings where over-relying on wrong AI explanations\ncould lead to critical consequences.\n""]",Transparency and Explainability in AI Systems,Search Engine Transparency and Explanation Systems
78,"Explainable Artificial Intelligence (XAI) Methods Evaluation , Explainable AI (XAI) and Conversational Explanations , Explainable AI (XAI) Techniques and Applications , Mechanistic Interpretability in AI Systems , Explainable AI with Shapley Values , Explainability in AI Decision-Making Systems","['explainability', 'explanations', 'interpretability', 'ai', 'explainers', 'interpretable', 'explaining', 'explainer', 'predictive', 'predictors'] , ['ai', 'explainers', 'explanations', 'explainability', 'understandability', 'dialogue', 'conversational', 'explainer', 'conversation', 'dialogues'] , ['ai', 'explainability', 'xai', 'explanations', 'xaiport', 'intelligence', 'understandable', 'explainable', 'insights', 'understanding'] , ['interpretability', 'neural', 'ai', 'abstraction', 'neuron', 'intelligence', 'explanations', 'brain', 'cognition', 'neurons'] , ['ai', 'shapley', 'feature', 'shapg', 'shap', 'features', 'predictive', 'interpretability', 'explanations', 'generalizability'] , ['expertise', 'explanations', 'automated', 'experts', 'understandability', 'expert', 'prediction', 'information', 'interactive', 'predictions']","['  eXplainable Artificial Intelligence (XAI) aims at providing understandable\nexplanations of black box models. In this paper, we evaluate current XAI\nmethods by scoring them based on ground truth simulations and sensitivity\nanalysis. To this end, we used an Electric Arc Furnace (EAF) model to better\nunderstand the limits and robustness characteristics of XAI methods such as\nSHapley Additive exPlanations (SHAP), Local Interpretable Model-agnostic\nExplanations (LIME), as well as Averaged Local Effects (ALE) or Smooth\nGradients (SG) in a highly topical setting. These XAI methods were applied to\nvarious types of black-box models and then scored based on their correctness\ncompared to the ground-truth sensitivity of the data-generating processes using\na novel scoring evaluation methodology over a range of simulated additive\nnoise. The resulting evaluation shows that the capability of the Machine\nLearning (ML) models to capture the process accurately is, indeed, coupled with\nthe correctness of the explainability of the underlying data-generating\nprocess. We furthermore show the differences between XAI methods in their\nability to correctly predict the true sensitivity of the modeled industrial\nprocess.\n', ""  Artificial Intelligence (AI) is often an integral part of modern decision\nsupport systems. The best-performing predictive models used in AI-based\ndecision support systems lack transparency. Explainable Artificial Intelligence\n(XAI) aims to create AI systems that can explain their rationale to human\nusers. Local explanations in XAI can provide information about the causes of\nindividual predictions in terms of feature importance. However, a critical\ndrawback of existing local explanation methods is their inability to quantify\nthe uncertainty associated with a feature's importance. This paper introduces\nan extension of a feature importance explanation method, Calibrated\nExplanations, previously only supporting classification, with support for\nstandard regression and probabilistic regression, i.e., the probability that\nthe target is above an arbitrary threshold. The extension for regression keeps\nall the benefits of Calibrated Explanations, such as calibration of the\nprediction from the underlying model with confidence intervals, uncertainty\nquantification of feature importance, and allows both factual and\ncounterfactual explanations. Calibrated Explanations for standard regression\nprovides fast, reliable, stable, and robust explanations. Calibrated\nExplanations for probabilistic regression provides an entirely new way of\ncreating probabilistic explanations from any ordinary regression model,\nallowing dynamic selection of thresholds. The method is model agnostic with\neasily understood conditional rules. An implementation in Python is freely\navailable on GitHub and for installation using both pip and conda, making the\nresults in this paper easily replicable.\n"", '  Strategies based on Explainable Artificial Intelligence (XAI) have promoted\nbetter human interpretability of the results of black box models. This opens up\nthe possibility of questioning whether explanations created by XAI methods meet\nhuman expectations. The XAI methods being currently used (Ciu, Dalex, Eli5,\nLofo, Shap, and Skater) provide various forms of explanations, including global\nrankings of relevance of features, which allow for an overview of how the model\nis explained as a result of its inputs and outputs. These methods provide for\nan increase in the explainability of the model and a greater interpretability\ngrounded on the context of the problem. Intending to shed light on the\nexplanations generated by XAI methods and their interpretations, this research\naddresses a real-world classification problem related to homicide prediction,\nalready peer-validated, replicated its proposed black box model and used 6\ndifferent XAI methods to generate explanations and 6 different human experts.\nThe results were generated through calculations of correlations, comparative\nanalysis and identification of relationships between all ranks of features\nproduced. It was found that even though it is a model that is difficult to\nexplain, 75\\% of the expectations of human experts were met, with approximately\n48\\% agreement between results from XAI methods and human experts. The results\nallow for answering questions such as: ""Are the Expectation of Interpretation\ngenerated among different human experts similar?"", ""Do the different XAI\nmethods generate similar explanations for the proposed problem?"", ""Can\nexplanations generated by XAI methods meet human expectation of\nInterpretations?"", and ""Can Explanations and Expectations of Interpretation\nwork together?"".\n'] , ['  Explainable Artificial Intelligence (XAI) aims to improve the transparency of\nautonomous decision-making through explanations. Recent literature has\nemphasised users\' need for holistic ""multi-shot"" explanations and the ability\nto personalise their engagement with XAI systems. We refer to this user-centred\ninteraction as an XAI Experience. Despite advances in creating XAI experiences,\nevaluating them in a user-centred manner has remained challenging. To address\nthis, we introduce the XAI Experience Quality (XEQ) Scale (pronounced ""Seek""\nScale), for evaluating the user-centred quality of XAI experiences.\nFurthermore, XEQ quantifies the quality of experiences across four evaluation\ndimensions: learning, utility, fulfilment and engagement. These contributions\nextend the state-of-the-art of XAI evaluation, moving beyond the\none-dimensional metrics frequently developed to assess single-shot\nexplanations. In this paper, we present the XEQ scale development and\nvalidation process, including content validation with XAI experts as well as\ndiscriminant and construct validation through a large-scale pilot study. Out\npilot study results offer strong evidence that establishes the XEQ Scale as a\ncomprehensive framework for evaluating user-centred XAI experiences.\n', ""  The goal of Explainable AI (XAI) is to design methods to provide insights\ninto the reasoning process of black-box models, such as deep neural networks,\nin order to explain them to humans. Social science research states that such\nexplanations should be conversational, similar to human-to-human explanations.\nIn this work, we show how to incorporate XAI in a conversational agent, using a\nstandard design for the agent comprising natural language understanding and\ngeneration components. We build upon an XAI question bank, which we extend by\nquality-controlled paraphrases, to understand the user's information needs. We\nfurther systematically survey the literature for suitable explanation methods\nthat provide the information to answer those questions, and present a\ncomprehensive list of suggestions. Our work is the first step towards truly\nnatural conversations about machine learning models with an explanation agent.\nThe comprehensive list of XAI questions and the corresponding explanation\nmethods may support other researchers in providing the necessary information to\naddress users' demands. To facilitate future work, we release our source code\nand data.\n"", ""  The field of eXplainable Artificial Intelligence (XAI) is increasingly\nrecognizing the need to personalize and/or interactively adapt the explanation\nto better reflect users' explanation needs. While dialogue-based approaches to\nXAI have been proposed recently, the state-of-the-art in XAI is still\ncharacterized by what we call one-shot, non-personalized and one-way\nexplanations. In contrast, dialogue-based systems that can adapt explanations\nthrough interaction with a user promise to be superior to GUI-based or\ndashboard explanations as they offer a more intuitive way of requesting\ninformation. In general, while interactive XAI systems are often evaluated in\nterms of user satisfaction, there are limited studies that access user's\nobjective model understanding. This is in particular the case for\ndialogue-based XAI approaches. In this paper, we close this gap by carrying out\ncontrolled experiments within a dialogue framework in which we measure\nunderstanding of users in three phases by asking them to simulate the\npredictions of the model they are learning about. By this, we can quantify the\nlevel of (improved) understanding w.r.t. how the model works, comparing the\nstate prior, and after the interaction. We further analyze the data to reveal\npatterns of how the interaction between groups with high vs. low understanding\ngain differ. Overall, our work thus contributes to our understanding about the\neffectiveness of XAI approaches.\n""] , ['  In this study, we propose the early adoption of Explainable AI (XAI) with a\nfocus on three properties: Quality of explanation, the explanation summaries\nshould be consistent across multiple XAI methods; Architectural Compatibility,\nfor effective integration in XAI, the architecture styles of both the XAI\nmethods and the models to be explained must be compatible with the framework;\nConfigurable operations, XAI explanations are operable, akin to machine\nlearning operations. Thus, an explanation for AI models should be reproducible\nand tractable to be trustworthy. We present XAIport, a framework of XAI\nmicroservices encapsulated into Open APIs to deliver early explanations as\nobservation for learning model quality assurance. XAIport enables configurable\nXAI operations along with machine learning development. We quantify the\noperational costs of incorporating XAI with three cloud computer vision\nservices on Microsoft Azure Cognitive Services, Google Cloud Vertex AI, and\nAmazon Rekognition. Our findings show comparable operational costs between XAI\nand traditional machine learning, with XAIport significantly improving both\ncloud AI model performance and explanation stability.\n', '  The increasing complexity of AI systems has led to the growth of the field of\nExplainable Artificial Intelligence (XAI), which aims to provide explanations\nand justifications for the outputs of AI algorithms. While there is\nconsiderable demand for XAI, there remains a scarcity of studies aimed at\ncomprehensively understanding the practical distinctions among different\nmethods and effectively aligning each method with users individual needs, and\nideally, offer a mapping function which can map each user with its specific\nneeds to a method of explainability. This study endeavors to bridge this gap by\nconducting a thorough review of extant research in XAI, with a specific focus\non Explainable Machine Learning (XML), and a keen eye on user needs. Our main\nobjective is to offer a classification of XAI methods within the realm of XML,\ncategorizing current works into three distinct domains: philosophy, theory, and\npractice, and providing a critical review for each category. Moreover, our\nstudy seeks to facilitate the connection between XAI users and the most\nsuitable methods for them and tailor explanations to meet their specific needs\nby proposing a mapping function that take to account users and their desired\nproperties and suggest an XAI method to them. This entails an examination of\nprevalent XAI approaches and an evaluation of their properties. The primary\noutcome of this study is the formulation of a clear and concise strategy for\nselecting the optimal XAI method to achieve a given goal, all while delivering\npersonalized explanations tailored to individual users.\n', '  Explainable AI (XAI) refers to techniques that provide human-understandable\ninsights into the workings of AI models. Recently, the focus of XAI is being\nextended towards Large Language Models (LLMs) which are often criticized for\ntheir lack of transparency. This extension calls for a significant\ntransformation in XAI methodologies because of two reasons. First, many\nexisting XAI methods cannot be directly applied to LLMs due to their complexity\nadvanced capabilities. Second, as LLMs are increasingly deployed across diverse\nindustry applications, the role of XAI shifts from merely opening the ""black\nbox"" to actively enhancing the productivity and applicability of LLMs in\nreal-world settings. Meanwhile, unlike traditional machine learning models that\nare passive recipients of XAI insights, the distinct abilities of LLMs can\nreciprocally enhance XAI. Therefore, in this paper, we introduce Usable XAI in\nthe context of LLMs by analyzing (1) how XAI can benefit LLMs and AI systems,\nand (2) how LLMs can contribute to the advancement of XAI. We introduce 10\nstrategies, introducing the key techniques for each and discussing their\nassociated challenges. We also provide case studies to demonstrate how to\nobtain and leverage explanations. The code used in this paper can be found at:\nhttps://github.com/JacksonWuxs/UsableXAI_LLM.\n'] , ['  Mechanistic Interpretability (MI) promises a path toward fully understanding\nhow neural networks make their predictions. Prior work demonstrates that even\nwhen trained to perform simple arithmetic, models can implement a variety of\nalgorithms (sometimes concurrently) depending on initialization and\nhyperparameters. Does this mean neuron-level interpretability techniques have\nlimited applicability? We argue that high-dimensional neural networks can learn\nlow-dimensional representations of their training data that are useful beyond\nsimply making good predictions. Such representations can be understood through\nthe mechanistic interpretability lens and provide insights that are\nsurprisingly faithful to human-derived domain knowledge. This indicates that\nsuch approaches to interpretability can be useful for deriving a new\nunderstanding of a problem from models trained to solve it. As a case study, we\nextract nuclear physics concepts by studying models trained to reproduce\nnuclear data.\n', '  Human cognition operates on a ""Global-first"" cognitive mechanism,\nprioritizing information processing based on coarse-grained details. This\nmechanism inherently possesses an adaptive multi-granularity description\ncapacity, resulting in computational traits such as efficiency, robustness, and\ninterpretability. The analysis pattern reliance on the finest granularity and\nsingle-granularity makes most existing computational methods less efficient,\nrobust, and interpretable, which is an important reason for the current lack of\ninterpretability in neural networks. Multi-granularity granular-ball computing\nemploys granular-balls of varying sizes to daptively represent and envelop the\nsample space, facilitating learning based on these granular-balls. Given that\nthe number of coarse-grained ""granular-balls"" is fewer than sample points,\ngranular-ball computing proves more efficient. Moreover, the inherent\ncoarse-grained nature of granular-balls reduces susceptibility to fine-grained\nsample disturbances, enhancing robustness. The multi-granularity construct of\ngranular-balls generates topological structures and coarse-grained\ndescriptions, naturally augmenting interpretability. Granular-ball computing\nhas successfully ventured into diverse AI domains, fostering the development of\ninnovative theoretical methods, including granular-ball classifiers, clustering\ntechniques, neural networks, rough sets, and evolutionary computing. This has\nnotably ameliorated the efficiency, noise robustness, and interpretability of\ntraditional methods. Overall, granular-ball computing is a rare and innovative\ntheoretical approach in AI that can adaptively and simultaneously enhance\nefficiency, robustness, and interpretability. This article delves into the main\napplication landscapes for granular-ball computing, aiming to equip future\nresearchers with references and insights to refine and expand this promising\ntheory.\n', ""  Understanding AI systems' inner workings is critical for ensuring value\nalignment and safety. This review explores mechanistic interpretability:\nreverse-engineering the computational mechanisms and representations learned by\nneural networks into human-understandable algorithms and concepts to provide a\ngranular, causal understanding. We establish foundational concepts such as\nfeatures encoding knowledge within neural activations and hypotheses about\ntheir representation and computation. We survey methodologies for causally\ndissecting model behaviors and assess the relevance of mechanistic\ninterpretability to AI safety. We investigate challenges surrounding\nscalability, automation, and comprehensive interpretation. We advocate for\nclarifying concepts, setting standards, and scaling techniques to handle\ncomplex models and behaviors and expand to domains such as vision and\nreinforcement learning. Mechanistic interpretability could help prevent\ncatastrophic outcomes as AI systems become more powerful and inscrutable.\n""] , ['  With wide application of Artificial Intelligence (AI), it has become\nparticularly important to make decisions of AI systems explainable and\ntransparent. In this paper, we proposed a new Explainable Artificial\nIntelligence (XAI) method called ShapG (Explanations based on Shapley value for\nGraphs) for measuring feature importance. ShapG is a model-agnostic global\nexplanation method. At the first stage, it defines an undirected graph based on\nthe dataset, where nodes represent features and edges are added based on\ncalculation of correlation coefficients between features. At the second stage,\nit calculates an approximated Shapley value by sampling the data taking into\naccount this graph structure. The sampling approach of ShapG allows to\ncalculate the importance of features efficiently, i.e. to reduce computational\ncomplexity. Comparison of ShapG with other existing XAI methods shows that it\nprovides more accurate explanations for two examined datasets. We also compared\nother XAI methods developed based on cooperative game theory with ShapG in\nrunning time, and the results show that ShapG exhibits obvious advantages in\nits running time, which further proves efficiency of ShapG. In addition,\nextensive experiments demonstrate a wide range of applicability of the ShapG\nmethod for explaining complex models. We find ShapG an important tool in\nimproving explainability and transparency of AI systems and believe it can be\nwidely used in various fields.\n', '  Shapley values originated in cooperative game theory but are extensively used\ntoday as a model-agnostic explanation framework to explain predictions made by\ncomplex machine learning models in the industry and academia. There are several\nalgorithmic approaches for computing different versions of Shapley value\nexplanations. Here, we focus on conditional Shapley values for predictive\nmodels fitted to tabular data. Estimating precise conditional Shapley values is\ndifficult as they require the estimation of non-trivial conditional\nexpectations. In this article, we develop new methods, extend earlier proposed\napproaches, and systematize the new refined and existing methods into different\nmethod classes for comparison and evaluation. The method classes use either\nMonte Carlo integration or regression to model the conditional expectations. We\nconduct extensive simulation studies to evaluate how precisely the different\nmethod classes estimate the conditional expectations, and thereby the\nconditional Shapley values, for different setups. We also apply the methods to\nseveral real-world data experiments and provide recommendations for when to use\nthe different method classes and approaches. Roughly speaking, we recommend\nusing parametric methods when we can specify the data distribution almost\ncorrectly, as they generally produce the most accurate Shapley value\nexplanations. When the distribution is unknown, both generative methods and\nregression models with a similar form as the underlying predictive model are\ngood and stable options. Regression-based methods are often slow to train but\nproduce the Shapley value explanations quickly once trained. The vice versa is\ntrue for Monte Carlo-based methods, making the different methods appropriate in\ndifferent practical situations.\n', '  As Artificial Intelligence (AI) is having more influence on our everyday\nlives, it becomes important that AI-based decisions are transparent and\nexplainable. As a consequence, the field of eXplainable AI (or XAI) has become\npopular in recent years. One way to explain AI models is to elucidate the\npredictive importance of the input features for the AI model in general, also\nreferred to as global explanations. Inspired by cooperative game theory,\nShapley values offer a convenient way for quantifying the feature importance as\nexplanations. However many methods based on Shapley values are built on the\nassumption of feature independence and often overlook causal relations of the\nfeatures which could impact their importance for the ML model. Inspired by\nstudies of explanations at the local level, we propose CAGE (Causally-Aware\nShapley Values for Global Explanations). In particular, we introduce a novel\nsampling procedure for out-coalition features that respects the causal\nrelations of the input features. We derive a practical approach that\nincorporates causal knowledge into global explanation and offers the\npossibility to interpret the predictive feature importance considering their\ncausal relation. We evaluate our method on synthetic data and real-world data.\nThe explanations from our approach suggest that they are not only more\nintuitive but also more faithful compared to previous global explanation\nmethods.\n'] , ['  Explanations of AI systems rarely address the information needs of people\naffected by algorithmic decision-making (ADM). This gap between conveyed\ninformation and information that matters to affected stakeholders can impede\nunderstanding and adherence to regulatory frameworks such as the AI Act. To\naddress this gap, we present the ""XAI Novice Question Bank"": A catalog of\naffected stakeholders\' information needs in two ADM use cases (employment\nprediction and health monitoring), covering the categories data, system\ncontext, system usage, and system specifications. Information needs were\ngathered in an interview study where participants received explanations in\nresponse to their inquiries. Participants further reported their understanding\nand decision confidence, showing that while confidence tended to increase after\nreceiving explanations, participants also met understanding challenges, such as\nbeing unable to tell why their understanding felt incomplete. Explanations\nfurther influenced participants\' perceptions of the systems\' risks and\nbenefits, which they confirmed or changed depending on the use case. When risks\nwere perceived as high, participants expressed particular interest in\nexplanations about intention, such as why and to what end a system was put in\nplace. With this work, we aim to support the inclusion of affected stakeholders\ninto explainability by contributing an overview of information and challenges\nrelevant to them when deciding on the adoption of ADM systems. We close by\nsummarizing our findings in a list of six key implications that inform the\ndesign of future explanations for affected stakeholder audiences.\n', '  In the realm of interactive machine-learning systems, the provision of\nexplanations serves as a vital aid in the processes of debugging and enhancing\nprediction models. However, the extent to which various global model-centric\nand data-centric explanations can effectively assist domain experts in\ndetecting and resolving potential data-related issues for the purpose of model\nimprovement has remained largely unexplored. In this technical report, we\nsummarise the key findings of our two user studies. Our research involved a\ncomprehensive examination of the impact of global explanations rooted in both\ndata-centric and model-centric perspectives within systems designed to support\nhealthcare experts in optimising machine learning models through both automated\nand manual data configurations. To empirically investigate these dynamics, we\nconducted two user studies, comprising quantitative analysis involving a sample\nsize of 70 healthcare experts and qualitative assessments involving 30\nhealthcare experts. These studies were aimed at illuminating the influence of\ndifferent explanation types on three key dimensions: trust, understandability,\nand model improvement. Results show that global model-centric explanations\nalone are insufficient for effectively guiding users during the intricate\nprocess of data configuration. In contrast, data-centric explanations exhibited\ntheir potential by enhancing the understanding of system changes that occur\npost-configuration. However, a combination of both showed the highest level of\nefficacy for fostering trust, improving understandability, and facilitating\nmodel enhancement among healthcare experts. We also present essential\nimplications for developing interactive machine-learning systems driven by\nexplanations. These insights can guide the creation of more effective systems\nthat empower domain experts to harness the full potential of machine learning\n', '  Explanations in interactive machine-learning systems facilitate debugging and\nimproving prediction models. However, the effectiveness of various global\nmodel-centric and data-centric explanations in aiding domain experts to detect\nand resolve potential data issues for model improvement remains unexplored.\nThis research investigates the influence of data-centric and model-centric\nglobal explanations in systems that support healthcare experts in optimising\nmodels through automated and manual data configurations. We conducted\nquantitative (n=70) and qualitative (n=30) studies with healthcare experts to\nexplore the impact of different explanations on trust, understandability and\nmodel improvement. Our results reveal the insufficiency of global model-centric\nexplanations for guiding users during data configuration. Although data-centric\nexplanations enhanced understanding of post-configuration system changes, a\nhybrid fusion of both explanation types demonstrated the highest effectiveness.\nBased on our study results, we also present design implications for effective\nexplanation-driven interactive machine-learning systems.\n']",Explainable Artificial Intelligence (XAI),Explainable AI (XAI) Techniques and Applications
79,"Interpretable Neural Networks with Sparse Autoencoders , Concept Bottleneck Models for Interpretable Classification , Deep Learning for Interpretable Neural Networks","['autoencoders', 'autoencoder', 'transcoders', 'sparse', 'attention', 'features', 'interpretable', 'interpretability', 'activations', 'circuits'] , ['bottleneck', 'concepts', 'classification', 'concept', 'annotations', 'features', 'compositional_concepts', 'models', 'concept_realignment', 'representations'] , ['imagenet', 'cnn', 'autoencoder', 'cnns', 'neural', 'backpropagation', 'networks', 'supervised', 'deep', 'learning']","[""  Identifying the features learned by neural networks is a core challenge in\nmechanistic interpretability. Sparse autoencoders (SAEs), which learn a sparse,\novercomplete dictionary that reconstructs a network's internal activations,\nhave been used to identify these features. However, SAEs may learn more about\nthe structure of the datatset than the computational structure of the network.\nThere is therefore only indirect reason to believe that the directions found in\nthese dictionaries are functionally important to the network. We propose\nend-to-end (e2e) sparse dictionary learning, a method for training SAEs that\nensures the features learned are functionally important by minimizing the KL\ndivergence between the output distributions of the original model and the model\nwith SAE activations inserted. Compared to standard SAEs, e2e SAEs offer a\nPareto improvement: They explain more network performance, require fewer total\nfeatures, and require fewer simultaneously active features per datapoint, all\nwith no cost to interpretability. We explore geometric and qualitative\ndifferences between e2e SAE features and standard SAE features. E2e dictionary\nlearning brings us closer to methods that can explain network behavior\nconcisely and accurately. We release our library for training e2e SAEs and\nreproducing our analysis at https://github.com/ApolloResearch/e2e_sae\n"", ""  Circuit analysis of any certain model behavior is a central task in\nmechanistic interpretability. We introduce our circuit discovery pipeline with\nSparse Autoencoders (SAEs) and a variant called Transcoders. With these two\nmodules inserted into the model, the model's computation graph with respect to\nOV and MLP circuits becomes strictly linear. Our methods do not require linear\napproximation to compute the causal effect of each node. This fine-grained\ngraph identifies both end-to-end and local circuits accounting for either\nlogits or intermediate features. We can scalably apply this pipeline with a\ntechnique called Hierarchical Attribution. We analyze three kinds of circuits\nin GPT-2 Small: bracket, induction, and Indirect Object Identification\ncircuits. Our results reveal new findings underlying existing discoveries.\n"", '  Decomposing model activations into interpretable components is a key open\nproblem in mechanistic interpretability. Sparse autoencoders (SAEs) are a\npopular method for decomposing the internal activations of trained transformers\ninto sparse, interpretable features, and have been applied to MLP layers and\nthe residual stream. In this work we train SAEs on attention layer outputs and\nshow that also here SAEs find a sparse, interpretable decomposition. We\ndemonstrate this on transformers from several model families and up to 2B\nparameters.\n  We perform a qualitative study of the features computed by attention layers,\nand find multiple families: long-range context, short-range context and\ninduction features. We qualitatively study the role of every head in GPT-2\nSmall, and estimate that at least 90% of the heads are polysemantic, i.e. have\nmultiple unrelated roles.\n  Further, we show that Sparse Autoencoders are a useful tool that enable\nresearchers to explain model behavior in greater detail than prior work. For\nexample, we explore the mystery of why models have so many seemingly redundant\ninduction heads, use SAEs to motivate the hypothesis that some are long-prefix\nwhereas others are short-prefix, and confirm this with more rigorous analysis.\nWe use our SAEs to analyze the computation performed by the Indirect Object\nIdentification circuit (Wang et al.), validating that the SAEs find causally\nmeaningful intermediate variables, and deepening our understanding of the\nsemantics of the circuit. We open-source the trained SAEs and a tool for\nexploring arbitrary prompts through the lens of Attention Output SAEs.\n'] , ['  There has been considerable recent interest in interpretable concept-based\nmodels such as Concept Bottleneck Models (CBMs), which first predict\nhuman-interpretable concepts and then map them to output classes. To reduce\nreliance on human-annotated concepts, recent works have converted pretrained\nblack-box models into interpretable CBMs post-hoc. However, these approaches\npredefine a set of concepts, assuming which concepts a black-box model encodes\nin its representations. In this work, we eliminate this assumption by\nleveraging unsupervised concept discovery to automatically extract concepts\nwithout human annotations or a predefined set of concepts. We further introduce\nan input-dependent concept selection mechanism that ensures only a small subset\nof concepts is used across all classes. We show that our approach improves\ndownstream performance and narrows the performance gap to black-box models,\nwhile using significantly fewer concepts in the classification. Finally, we\ndemonstrate how large vision-language models can intervene on the final model\nweights to correct model errors.\n', ""  Concept Bottleneck Models (CBMs) are regarded as inherently interpretable\nbecause they first predict a set of human-defined concepts which are used to\npredict a task label. For inherent interpretability to be fully realised, and\nensure trust in a model's output, it's desirable for concept predictions to use\nsemantically meaningful input features. For instance, in an image, pixels\nrepresenting a broken bone should contribute to predicting a fracture. However,\ncurrent literature suggests that concept predictions often rely on irrelevant\ninput features. We hypothesise that this occurs when dataset labels include\ninaccurate concept annotations, or the relationship between input features and\nconcepts is unclear. In general, the effect of dataset labelling on concept\nrepresentations remains an understudied area. In this paper, we demonstrate\nthat CBMs can learn to map concepts to semantically meaningful input features,\nby utilising datasets with a clear link between the input features and the\ndesired concept predictions. This is achieved, for instance, by ensuring\nmultiple concepts do not always co-occur and, therefore provide a clear\ntraining signal for the CBM to distinguish the relevant input features for each\nconcept. We validate our hypothesis on both synthetic and real-world image\ndatasets, and demonstrate under the correct conditions, CBMs can learn to\nattribute semantically meaningful input features to the correct concept\npredictions.\n"", '  Concept Bottleneck Models (CBMs) map the black-box visual representations\nextracted by deep neural networks onto a set of interpretable concepts and use\nthe concepts to make predictions, enhancing the transparency of the\ndecision-making process. Multimodal pre-trained models can match visual\nrepresentations with textual concept embeddings, allowing for obtaining the\ninterpretable concept bottleneck without the expertise concept annotations.\nRecent research has focused on the concept bank establishment and the\nhigh-quality concept selection. However, it is challenging to construct a\ncomprehensive concept bank through humans or large language models, which\nseverely limits the performance of CBMs. In this work, we propose the\nIncremental Residual Concept Bottleneck Model (Res-CBM) to address the\nchallenge of concept completeness. Specifically, the residual concept\nbottleneck model employs a set of optimizable vectors to complete missing\nconcepts, then the incremental concept discovery module converts the\ncomplemented vectors with unclear meanings into potential concepts in the\ncandidate concept bank. Our approach can be applied to any user-defined concept\nbank, as a post-hoc processing method to enhance the performance of any CBMs.\nFurthermore, to measure the descriptive efficiency of CBMs, the Concept\nUtilization Efficiency (CUE) metric is proposed. Experiments show that the\nRes-CBM outperforms the current state-of-the-art methods in terms of both\naccuracy and efficiency and achieves comparable performance to black-box models\nacross multiple datasets.\n'] , ['  The current deep neural network algorithm still stays in the end-to-end\ntraining supervision method like Image-Label pairs, which makes traditional\nalgorithm is difficult to explain the reason for the results, and the\nprediction logic is difficult to understand and analyze. The current algorithm\ndoes not use the existing human knowledge information, which makes the model\nnot in line with the human cognition model and makes the model not suitable for\nhuman use. In order to solve the above problems, the present invention provides\na deep neural network training method based on the human knowledge, which uses\nthe human cognition model to construct the deep neural network training model,\nand uses the existing human knowledge information to construct the deep neural\nnetwork training model. This paper proposes a multi-level hierarchical deep\nlearning algorithm, which is composed of multi-level hierarchical deep neural\nnetwork architecture and multi-level hierarchical deep learning framework. The\nexperimental results show that the proposed algorithm can effectively explain\nthe hidden information of the neural network. The goal of our study is to\nimprove the interpretability of deep neural networks (DNNs) by providing an\nanalysis of the impact of knowledge injection on the classification task. We\nconstructed a knowledge injection dataset with matching knowledge data and\nimage classification data. The knowledge injection dataset is the benchmark\ndataset for the experiments in the paper. Our model expresses the improvement\nin interpretability and classification task performance of hidden layers at\ndifferent scales.\n', '  In this work, we explore the intersection of sparse coding theory and deep\nlearning to enhance our understanding of feature extraction capabilities in\nadvanced neural network architectures. We begin by introducing a novel class of\nDeep Sparse Coding (DSC) models and establish a thorough theoretical analysis\nof their uniqueness and stability properties. By applying iterative algorithms\nto these DSC models, we derive convergence rates for convolutional neural\nnetworks (CNNs) in their ability to extract sparse features. This provides a\nstrong theoretical foundation for the use of CNNs in sparse feature learning\ntasks. We additionally extend this convergence analysis to more general neural\nnetwork architectures, including those with diverse activation functions, as\nwell as self-attention and transformer-based models. This broadens the\napplicability of our findings to a wide range of deep learning methods for deep\nsparse feature extraction. Inspired by the strong connection between sparse\ncoding and CNNs, we also explore training strategies to encourage neural\nnetworks to learn more sparse features. Through numerical experiments, we\ndemonstrate the effectiveness of these approaches, providing valuable insights\nfor the design of efficient and interpretable deep learning models.\n', '  Distributed sparse block codes (SBCs) exhibit compact representations for\nencoding and manipulating symbolic data structures using fixed-width vectors.\nOne major challenge however is to disentangle, or factorize, the distributed\nrepresentation of data structures into their constituent elements without\nhaving to search through all possible combinations. This factorization becomes\nmore challenging when SBCs vectors are noisy due to perceptual uncertainty and\napproximations made by modern neural networks to generate the query SBCs\nvectors. To address these challenges, we first propose a fast and highly\naccurate method for factorizing a more flexible and hence generalized form of\nSBCs, dubbed GSBCs. Our iterative factorizer introduces a threshold-based\nnonlinear activation, conditional random sampling, and an $\\ell_\\infty$-based\nsimilarity metric. Secondly, the proposed factorizer maintains a high accuracy\nwhen queried by noisy product vectors generated using deep convolutional neural\nnetworks (CNNs). This facilitates its application in replacing the large fully\nconnected layer (FCL) in CNNs, whereby $C$ trainable class vectors, or\nattribute combinations, can be implicitly represented by our factorizer having\n$F$-factor codebooks, each with $\\sqrt[\\leftroot{-2}\\uproot{2}F]{C}$ fixed\ncodevectors. We provide a methodology to flexibly integrate our factorizer in\nthe classification layer of CNNs with a novel loss function. With this\nintegration, the convolutional layers can generate a noisy product vector that\nour factorizer can still decode, whereby the decoded factors can have different\ninterpretations based on downstream tasks. We demonstrate the feasibility of\nour method on four deep CNN architectures over CIFAR-100, ImageNet-1K, and\nRAVEN datasets. In all use cases, the number of parameters and operations are\nnotably reduced compared to the FCL.\n']",Interpretable Deep Learning Models,Deep Learning for Interpretable Neural Networks
80,"Graph Neural Network Explainability , ""Robustness Verification of Neural Networks"" , Concept Learning and Explainability in Neural Networks , Explainable AI for Neural Networks , ""Explainability and Applications of Complex-Valued Neural Networks""","['explanations', 'networks', 'explainers', 'attention', 'subgraph', 'explainability', 'graphs', 'explaining', 'neural', 'explainer'] , ['neural', 'networks', 'computability', 'deepinfer', 'neuralsat', 'robustness', 'neurons', 'dnns', 'dnn', 'deep'] , ['concepts', 'cnn', 'imagenet', 'cnns', 'neural', 'concept', 'explainability', 'explanations', 'classification', 'interpretability'] , ['explanations', 'explainability', 'ai', 'interpretability', 'neural', 'cnn', 'explaining', 'classifiers', 'explainer', 'interpretable'] , ['neural', 'cnns', 'tensorflow', 'networks', 'backpropagation', 'cvnn', 'cvnns', 'hypercomplex', 'deepshap', 'multidimensional']","['  Graph Neural Networks (GNNs) are effective for node classification in\ngraph-structured data, but they lack explainability, especially at the global\nlevel. Current research mainly utilizes subgraphs of the input as local\nexplanations or generates new graphs as global explanations. However, these\ngraph-based methods are limited in their ability to explain classes with\nmultiple sufficient explanations. To provide more expressive explanations, we\npropose utilizing class expressions (CEs) from the field of description logic\n(DL). Our approach explains heterogeneous graphs with different types of nodes\nusing CEs in the EL description logic. To identify the best explanation among\nmultiple candidate explanations, we employ and compare two different scoring\nfunctions: (1) For a given CE, we construct multiple graphs, have the GNN make\na prediction for each graph, and aggregate the predicted scores. (2) We score\nthe CE in terms of fidelity, i.e., we compare the predictions of the GNN to the\npredictions by the CE on a separate validation set. Instead of subgraph-based\nexplanations, we offer CE-based explanations.\n', '  Aside from graph neural networks (GNNs) attracting significant attention as a\npowerful framework revolutionizing graph representation learning, there has\nbeen an increasing demand for explaining GNN models. Although various\nexplanation methods for GNNs have been developed, most studies have focused on\ninstance-level explanations, which produce explanations tailored to a given\ngraph instance. In our study, we propose Prototype-bAsed GNN-Explainer (PAGE),\na novel model-level GNN explanation method that explains what the underlying\nGNN model has learned for graph classification by discovering\nhuman-interpretable prototype graphs. Our method produces explanations for a\ngiven class, thus being capable of offering more concise and comprehensive\nexplanations than those of instance-level explanations. First, PAGE selects\nembeddings of class-discriminative input graphs on the graph-level embedding\nspace after clustering them. Then, PAGE discovers a common subgraph pattern by\niteratively searching for high matching node tuples using node-level embeddings\nvia a prototype scoring function, thereby yielding a prototype graph as our\nexplanation. Using six graph classification datasets, we demonstrate that PAGE\nqualitatively and quantitatively outperforms the state-of-the-art model-level\nexplanation method. We also carry out systematic experimental studies by\ndemonstrating the relationship between PAGE and instance-level explanation\nmethods, the robustness of PAGE to input data scarce environments, and the\ncomputational efficiency of the proposed prototype scoring function in PAGE.\n', ""  Graph Neural Networks (GNNs) have gained considerable traction for their\ncapability to effectively process topological data, yet their interpretability\nremains a critical concern. Current interpretation methods are dominated by\npost-hoc explanations to provide a transparent and intuitive understanding of\nGNNs. However, they have limited performance in interpreting complicated\nsubgraphs and can't utilize the explanation to advance GNN predictions. On the\nother hand, transparent GNN models are proposed to capture critical subgraphs.\nWhile such methods could improve GNN predictions, they usually don't perform\nwell on explanations. Thus, it is desired for a new strategy to better couple\nGNN explanation and prediction. In this study, we have developed a novel\ninterpretable causal GNN framework that incorporates retrieval-based causal\nlearning with Graph Information Bottleneck (GIB) theory. The framework could\nsemi-parametrically retrieve crucial subgraphs detected by GIB and compress the\nexplanatory subgraphs via a causal module. The framework was demonstrated to\nconsistently outperform state-of-the-art methods, and to achieve 32.71\\% higher\nprecision on real-world explanation scenarios with diverse explanation types.\nMore importantly, the learned explanations were shown able to also improve GNN\nprediction performance.\n""] , ['  The ubiquity of deep learning algorithms in various applications has\namplified the need for assuring their robustness against small input\nperturbations such as those occurring in adversarial attacks. Existing complete\nverification techniques offer provable guarantees for all robustness queries\nbut struggle to scale beyond small neural networks. To overcome this\ncomputational intractability, incomplete verification methods often rely on\nconvex relaxation to over-approximate the nonlinearities in neural networks.\nProgress in tighter approximations has been achieved for piecewise linear\nfunctions. However, robustness verification of neural networks for general\nactivation functions (e.g., Sigmoid, Tanh) remains under-explored and poses new\nchallenges. Typically, these networks are verified using convex relaxation\ntechniques, which involve computing linear upper and lower bounds of the\nnonlinear activation functions. In this work, we propose a novel parameter\nsearch method to improve the quality of these linear approximations.\nSpecifically, we show that using a simple search method, carefully adapted to\nthe given verification problem through state-of-the-art algorithm configuration\ntechniques, improves the average global lower bound by 25% on average over the\ncurrent state of the art on several commonly used local robustness verification\nbenchmarks.\n', '  Probabilistic verification of neural networks is concerned with formally\nanalysing the output distribution of a neural network under a probability\ndistribution of the inputs. Examples of probabilistic verification include\nverifying the demographic parity fairness notion or quantifying the safety of a\nneural network. We present a new algorithm for the probabilistic verification\nof neural networks based on an algorithm for computing and iteratively refining\nlower and upper bounds on probabilities over the outputs of a neural network.\nBy applying state-of-the-art bound propagation and branch and bound techniques\nfrom non-probabilistic neural network verification, our algorithm significantly\noutpaces existing probabilistic verification algorithms, reducing solving times\nfor various benchmarks from the literature from tens of minutes to tens of\nseconds. Furthermore, our algorithm compares favourably even to dedicated\nalgorithms for restricted subsets of probabilistic verification. We complement\nour empirical evaluation with a theoretical analysis, proving that our\nalgorithm is sound and, under mildly restrictive conditions, also complete when\nusing a suitable set of heuristics.\n', '  Formal verification of neural networks is essential before their deployment\nin safety-critical applications. However, existing methods for formally\nverifying neural networks are not yet scalable enough to handle practical\nproblems involving a large number of neurons. We address this challenge by\nintroducing a fully automatic and sound reduction of neural networks using\nreachability analysis. The soundness ensures that the verification of the\nreduced network entails the verification of the original network. To the best\nof our knowledge, we present the first sound reduction approach that is\napplicable to neural networks with any type of element-wise activation\nfunction, such as ReLU, sigmoid, and tanh. The network reduction is computed on\nthe fly while simultaneously verifying the original network and its\nspecifications. All parameters are automatically tuned to minimize the network\nsize without compromising verifiability. We further show the applicability of\nour approach to convolutional neural networks by explicitly exploiting similar\nneighboring pixels. Our evaluation shows that our approach can reduce the\nnumber of neurons to a fraction of the original number of neurons with minor\nouter-approximation and thus reduce the verification time to a similar degree.\n'] , [""  With the wide proliferation of Deep Neural Networks in high-stake\napplications, there is a growing demand for explainability behind their\ndecision-making process. Concept learning models attempt to learn high-level\n'concepts' - abstract entities that align with human understanding, and thus\nprovide interpretability to DNN architectures. However, in this paper, we\ndemonstrate that present SOTA concept learning approaches suffer from two major\nproblems - lack of concept fidelity wherein the models fail to learn consistent\nconcepts among similar classes and limited concept interoperability wherein the\nmodels fail to generalize learned concepts to new domains for the same task.\nKeeping these in mind, we propose a novel self-explaining architecture for\nconcept learning across domains which - i) incorporates a new concept saliency\nnetwork for representative concept selection, ii) utilizes contrastive learning\nto capture representative domain invariant concepts, and iii) uses a novel\nprototype-based concept grounding regularization to improve concept alignment\nacross domains. We demonstrate the efficacy of our proposed approach over\ncurrent SOTA concept learning approaches on four widely used real-world\ndatasets. Empirical results show that our method improves both concept fidelity\nmeasured through concept overlap and concept interoperability measured through\ndomain adaptation performance.\n"", '  Analysis of how semantic concepts are represented within Convolutional Neural\nNetworks (CNNs) is a widely used approach in Explainable Artificial\nIntelligence (XAI) for interpreting CNNs. A motivation is the need for\ntransparency in safety-critical AI-based systems, as mandated in various\ndomains like automated driving. However, to use the concept representations for\nsafety-relevant purposes, like inspection or error retrieval, these must be of\nhigh quality and, in particular, stable. This paper focuses on two stability\ngoals when working with concept representations in computer vision CNNs:\nstability of concept retrieval and of concept attribution. The guiding use-case\nis a post-hoc explainability framework for object detection (OD) CNNs, towards\nwhich existing concept analysis (CA) methods are successfully adapted. To\naddress concept retrieval stability, we propose a novel metric that considers\nboth concept separation and consistency, and is agnostic to layer and concept\nrepresentation dimensionality. We then investigate impacts of concept\nabstraction level, number of concept training samples, CNN size, and concept\nrepresentation dimensionality on stability. For concept attribution stability\nwe explore the effect of gradient instability on gradient-based explainability\nmethods. The results on various CNNs for classification and object detection\nyield the main findings that (1) the stability of concept retrieval can be\nenhanced through dimensionality reduction via data aggregation, and (2) in\nshallow layers where gradient instability is more pronounced, gradient\nsmoothing techniques are advised. Finally, our approach provides valuable\ninsights into selecting the appropriate layer and concept representation\ndimensionality, paving the way towards CA in safety-critical XAI applications.\n', ""  The focus of recent research has shifted from merely improving the metrics\nbased performance of Deep Neural Networks (DNNs) to DNNs which are more\ninterpretable to humans. The field of eXplainable Artificial Intelligence (XAI)\nhas observed various techniques, including saliency-based and concept-based\napproaches. These approaches explain the model's decisions in simple human\nunderstandable terms called Concepts. Concepts are known to be the thinking\nground of humans}. Explanations in terms of concepts enable detecting spurious\ncorrelations, inherent biases, or clever-hans. With the advent of concept-based\nexplanations, a range of concept representation methods and automatic concept\ndiscovery algorithms have been introduced. Some recent works also use concepts\nfor model improvement in terms of interpretability and generalization. We\nprovide a systematic review and taxonomy of various concept representations and\ntheir discovery algorithms in DNNs, specifically in vision. We also provide\ndetails on concept-based model improvement literature marking the first\ncomprehensive survey of these methods.\n""] , [""  Challenges persist in providing interpretable explanations for neural network\nreasoning in explainable AI (xAI). Existing methods like Integrated Gradients\nproduce noisy maps, and LIME, while intuitive, may deviate from the model's\nreasoning. We introduce a framework that uses hierarchical segmentation\ntechniques for faithful and interpretable explanations of Convolutional Neural\nNetworks (CNNs). Our method constructs model-based hierarchical segmentations\nthat maintain the model's reasoning fidelity and allows both human-centric and\nmodel-centric segmentation. This approach offers multiscale explanations,\naiding bias identification and enhancing understanding of neural network\ndecision-making. Experiments show that our framework, xAiTrees, delivers highly\ninterpretable and faithful model explanations, not only surpassing traditional\nxAI methods but shedding new light on a novel approach to enhancing xAI\ninterpretability. Code at: https://github.com/CarolMazini/reasoning_with_trees .\n"", ""  Concept-based explainable AI is promising as a tool to improve the\nunderstanding of complex models at the premises of a given user, viz.\\ as a\ntool for personalized explainability. An important class of concept-based\nexplainability methods is constructed with empirically defined concepts,\nindirectly defined through a set of positive and negative examples, as in the\nTCAV approach (Kim et al., 2018). While it is appealing to the user to avoid\nformal definitions of concepts and their operationalization, it can be\nchallenging to establish relevant concept datasets. Here, we address this\nchallenge using general knowledge graphs (such as, e.g., Wikidata or WordNet)\nfor comprehensive concept definition and present a workflow for user-driven\ndata collection in both text and image domains. The concepts derived from\nknowledge graphs are defined interactively, providing an opportunity for\npersonalization and ensuring that the concepts reflect the user's intentions.\nWe test the retrieved concept datasets on two concept-based explainability\nmethods, namely concept activation vectors (CAVs) and concept activation\nregions (CARs) (Crabbe and van der Schaar, 2022). We show that CAVs and CARs\nbased on these empirical concept datasets provide robust and accurate\nexplanations. Importantly, we also find good alignment between the models'\nrepresentations of concepts and the structure of knowledge graphs, i.e., human\nrepresentations. This supports our conclusion that knowledge graph-based\nconcepts are relevant for XAI.\n"", '  A major challenge in Explainable AI is in correctly interpreting activations\nof hidden neurons: accurate interpretations would help answer the question of\nwhat a deep learning system internally detects as relevant in the input,\ndemystifying the otherwise black-box nature of deep learning systems. The state\nof the art indicates that hidden node activations can, in some cases, be\ninterpretable in a way that makes sense to humans, but systematic automated\nmethods that would be able to hypothesize and verify interpretations of hidden\nneuron activations are underexplored. This is particularly the case for\napproaches that can both draw explanations from substantial background\nknowledge, and that are based on inherently explainable (symbolic) methods.\n  In this paper, we introduce a novel model-agnostic post-hoc Explainable AI\nmethod demonstrating that it provides meaningful interpretations. Our approach\nis based on using a Wikipedia-derived concept hierarchy with approximately 2\nmillion classes as background knowledge, and utilizes OWL-reasoning-based\nConcept Induction for explanation generation. Additionally, we explore and\ncompare the capabilities of off-the-shelf pre-trained multimodal-based\nexplainable methods.\n  Our results indicate that our approach can automatically attach meaningful\nclass expressions as explanations to individual neurons in the dense layer of a\nConvolutional Neural Network. Evaluation through statistical analysis and\ndegree of concept activation in the hidden layer show that our method provides\na competitive edge in both quantitative and qualitative aspects compared to\nprior work.\n'] , ['  Deep Neural Networks are widely used in academy as well as corporate and\npublic applications, including safety critical applications such as health care\nand autonomous driving. The ability to explain their output is critical for\nsafety reasons as well as acceptance among applicants. A multitude of methods\nhave been proposed to explain real-valued neural networks. Recently,\ncomplex-valued neural networks have emerged as a new class of neural networks\ndealing with complex-valued input data without the necessity of projecting them\nonto $\\mathbb{R}^2$. This brings up the need to develop explanation algorithms\nfor this kind of neural networks. In this paper we provide these developments.\nWhile we focus on adapting the widely used DeepSHAP algorithm to the complex\ndomain, we also present versions of four gradient based explanation methods\nsuitable for use in complex-valued neural networks. We evaluate the explanation\nquality of all presented algorithms and provide all of them as an open source\nlibrary adaptable to most recent complex-valued neural network architectures.\n', '  The universal approximation theorem states that a neural network with one\nhidden layer can approximate continuous functions on compact sets with any\ndesired precision. This theorem supports using neural networks for various\napplications, including regression and classification tasks. Furthermore, it is\nvalid for real-valued neural networks and some hypercomplex-valued neural\nnetworks such as complex-, quaternion-, tessarine-, and Clifford-valued neural\nnetworks. However, hypercomplex-valued neural networks are a type of\nvector-valued neural network defined on an algebra with additional algebraic or\ngeometric properties. This paper extends the universal approximation theorem\nfor a wide range of vector-valued neural networks, including\nhypercomplex-valued models as particular instances. Precisely, we introduce the\nconcept of non-degenerate algebra and state the universal approximation theorem\nfor neural networks defined on such algebras.\n', '  Despite the many successful applications of deep learning models for\nmultidimensional signal and image processing, most traditional neural networks\nprocess data represented by (multidimensional) arrays of real numbers. The\nintercorrelation between feature channels is usually expected to be learned\nfrom the training data, requiring numerous parameters and careful training. In\ncontrast, vector-valued neural networks are conceived to process arrays of\nvectors and naturally consider the intercorrelation between feature channels.\nConsequently, they usually have fewer parameters and often undergo more robust\ntraining than traditional neural networks. This paper aims to present a broad\nframework for vector-valued neural networks, referred to as V-nets. In this\ncontext, hypercomplex-valued neural networks are regarded as vector-valued\nmodels with additional algebraic properties. Furthermore, this paper explains\nthe relationship between vector-valued and traditional neural networks.\nPrecisely, a vector-valued neural network can be obtained by placing\nrestrictions on a real-valued model to consider the intercorrelation between\nfeature channels. Finally, we show how V-nets, including hypercomplex-valued\nneural networks, can be implemented in current deep-learning libraries as\nreal-valued networks.\n']",Explainability and Interpretability in Neural Networks,Explainable AI for Neural Networks
81,"Reducing Carbon Footprint in AI and ML , Agricultural AI for Farming Efficiency","['emissions', 'carbon', 'energy', 'co2', 'efficiency', 'efficient', 'models', 'emission', 'environmentally', 'footprint'] , ['agricultural', 'agriculture', 'ai', 'farming', 'farmers', 'automating', 'gardening', 'crop', 'machinery', 'expertise']","['  DNN inference, known for its significant energy consumption and the resulting\nhigh carbon footprint, can be made more sustainable by adapting model size and\naccuracy to the varying carbon intensity throughout the day. Our heuristic\nalgorithm uses larger, high-accuracy models during low-intensity periods and\nsmaller, lower-accuracy ones during high-intensity periods. We also introduce a\nmetric, carbon-emission efficiency, which quantitatively measures the efficacy\nof adaptive model selection in terms of carbon footprint. The evaluation showed\nthat the proposed approach could improve the carbon emission efficiency in\nimproving the accuracy of vision recognition services by up to 80%.\n', '  By integrating Artificial Intelligence (AI) with the Internet of Things\n(IoT), Artificial Intelligence of Things (AIoT) has revolutionized many fields.\nHowever, AIoT is facing the challenges of energy consumption and carbon\nemissions due to the continuous advancement of mobile technology. Fortunately,\nGenerative AI (GAI) holds immense potential to reduce carbon emissions of AIoT\ndue to its excellent reasoning and generation capabilities. In this article, we\nexplore the potential of GAI for carbon emissions reduction and propose a novel\nGAI-enabled solution for low-carbon AIoT. Specifically, we first study the main\nimpacts that cause carbon emissions in AIoT, and then introduce GAI techniques\nand their relations to carbon emissions. We then explore the application\nprospects of GAI in low-carbon AIoT, focusing on how GAI can reduce carbon\nemissions of network components. Subsequently, we propose a Large Language\nModel (LLM)-enabled carbon emission optimization framework, in which we design\npluggable LLM and Retrieval Augmented Generation (RAG) modules to generate more\naccurate and reliable optimization problems. Furthermore, we utilize Generative\nDiffusion Models (GDMs) to identify optimal strategies for carbon emission\nreduction. Numerical results demonstrate the effectiveness of the proposed\nframework. Finally, we insightfully provide open research directions for\nlow-carbon AIoT.\n', '  Machine learning (ML) has seen tremendous advancements, but its environmental\nfootprint remains a concern. Acknowledging the growing environmental impact of\nML this paper investigates Green ML, examining various model architectures and\nhyperparameters in both training and inference phases to identify\nenergy-efficient practices. Our study leverages software-based power\nmeasurements for ease of replication across diverse configurations, models and\ndatasets. In this paper, we examine multiple models and hardware configurations\nto identify correlations across the various measurements and metrics and key\ncontributors to energy reduction. Our analysis offers practical guidelines for\nconstructing sustainable ML operations, emphasising energy consumption and\ncarbon footprint reductions while maintaining performance. As identified,\nshort-lived profiling can quantify the long-term expected energy consumption.\nMoreover, model parameters can also be used to accurately estimate the expected\ntotal energy without the need for extensive experimentation.\n'] , ['  Agriculture, vital for global sustenance, necessitates innovative solutions\ndue to a lack of organized domain experts, particularly in developing countries\nwhere many farmers are impoverished and cannot afford expert consulting.\nInitiatives like Farmers Helpline play a crucial role in such countries, yet\nchallenges such as high operational costs persist. Automating query resolution\ncan alleviate the burden on traditional call centers, providing farmers with\nimmediate and contextually relevant information. The integration of Agriculture\nand Artificial Intelligence (AI) offers a transformative opportunity to empower\nfarmers and bridge information gaps. Language models like transformers, the\nrising stars of AI, possess remarkable language understanding capabilities,\nmaking them ideal for addressing information gaps in agriculture. This work\nexplores and demonstrates the transformative potential of Large Language Models\n(LLMs) in automating query resolution for agricultural farmers, leveraging\ntheir expertise in deciphering natural language and understanding context.\nUsing a subset of a vast dataset of real-world farmer queries collected in\nIndia, our study focuses on approximately 4 million queries from the state of\nTamil Nadu, spanning various sectors, seasonal crops, and query types.\n', '  The past decade has witnessed the rapid development and adoption of ML & DL\nmethodologies in agricultural systems, showcased by great successes in\nagricultural applications. However, these conventional ML/DL models have\ncertain limitations: they heavily rely on large, costly-to-acquire labeled\ndatasets for training, require specialized expertise for development and\nmaintenance, and are mostly tailored for specific tasks, thus lacking\ngeneralizability. Recently, large pre-trained models, also known as FMs, have\ndemonstrated remarkable successes in language, vision, and decision-making\ntasks across various domains. These models are trained on a large amount of\ndata from multiple domains and modalities. Once trained, they can accomplish\nversatile tasks with just minor fine-tuning and minimal task-specific labeled\ndata. Despite their proven effectiveness and huge potential, there has been\nlittle exploration of applying FMs to agriculture AI. Thus, this study aims to\nexplore the potential of FMs in the field of smart agriculture. In particular,\nconceptual tools and technical background are presented to help the\nunderstanding of the problem space and uncover new research directions. To this\nend, recent FMs in the general CS domain are reviewed, and the models are\ncategorized into four categories: language FMs, vision FMs, multimodal FMs, and\nreinforcement learning FMs. Then, the steps of developing agriculture FMs\n(AFMs) are outlined and potential applications in smart agriculture are\ndiscussed. Moreover, challenges and risks associated with developing AFMs are\ndiscussed, including model training, validation, and deployment. In summary,\nthe advancement of AI in agriculture is explored by introducing AFMs as a\npromising paradigm that can significantly mitigate the reliance on extensive\nlabeled datasets and enhance the efficiency, effectiveness, and generalization\nof agricultural AI systems.\n', '  Food production is a vital global concern and the potential for an agritech\nrevolution through artificial intelligence (AI) remains largely unexplored.\nThis paper presents a comprehensive review focused on the application of\nmachine learning (ML) in agriculture, aiming to explore its transformative\npotential in farming practices and efficiency enhancement. To understand the\nextent of research activity in this field, statistical data have been gathered,\nrevealing a substantial growth trend in recent years. This indicates that it\nstands out as one of the most dynamic and vibrant research domains. By\nintroducing the concept of ML and delving into the realm of smart agriculture,\nincluding Precision Agriculture, Smart Farming, Digital Agriculture, and\nAgriculture 4.0, we investigate how AI can optimize crop output and minimize\nenvironmental impact. We highlight the capacity of ML to analyze and classify\nagricultural data, providing examples of improved productivity and\nprofitability on farms. Furthermore, we discuss prominent ML models and their\nunique features that have shown promising results in agricultural applications.\nThrough a systematic review of the literature, this paper addresses the\nexisting literature gap on AI in agriculture and offers valuable information to\nnewcomers and researchers. By shedding light on unexplored areas within this\nemerging field, our objective is to facilitate a deeper understanding of the\nsignificant contributions and potential of AI in agriculture, ultimately\nbenefiting the research community.\n']",Sustainable AI and Digital Agriculture Innovations,Reducing Carbon Footprint in AI and ML
82,"Artificial Consciousness and Cognitive AI , Neuro-Symbolic AI and Semantic Integration , Human-AI Decision Making and Assistance , ""Reasoning and Lateral Thinking in AI Models"" , Human-AI Collaboration and Teamwork","['consciousness', 'conscious', 'cognition', 'ai', 'brain', 'cognitive', 'philosophical', 'artificial', 'neuroscience', 'intelligence'] , ['symbolic', 'ai', 'neural', 'neuro', 'symbols', 'neurosymbolic', 'semantic', 'semantics', 'knowledge', 'deductive'] , ['ai', 'intelligence', 'decisions', 'explanations', 'assisted', 'misrepresentations', 'behavior', 'strategies', 'predictions', 'artificial'] , ['ai', 'thinking', 'brainteaser', 'tasks', 'subtasks', 'cognitive', 'interactive', 'reasoning', 'planning', 'cognition'] , ['ai', 'agent', 'agents', 'cooperation', 'intelligence', 'warfare', 'autonomous', 'robot', 'intelligent', 'combat']","['  We here analyse the question of developing artificial consciousness from an\nevolutionary perspective, taking the evolution of the human brain and its\nrelation with consciousness as a reference model. This kind of analysis reveals\nseveral structural and functional features of the human brain that appear to be\nkey for reaching human-like complex conscious experience and that current\nresearch on Artificial Intelligence (AI) should take into account in its\nattempt to develop systems capable of conscious processing. We argue that, even\nif AI is limited in its ability to emulate human consciousness for both\nintrinsic (structural and architectural) and extrinsic (related to the current\nstage of scientific and technological knowledge) reasons, taking inspiration\nfrom those characteristics of the brain that make conscious processing possible\nand/or modulate it, is a potentially promising strategy towards developing\nconscious AI. Also, it is theoretically possible that AI research can develop\npartial or potentially alternative forms of consciousness that is qualitatively\ndifferent from the human, and that may be either more or less sophisticated\ndepending on the perspectives. Therefore, we recommend neuroscience-inspired\ncaution in talking about artificial consciousness: since the use of the same\nword consciousness for humans and AI becomes ambiguous and potentially\nmisleading, we propose to clearly specify what is common and what differs in AI\nconscious processing from full human conscious experience.\n', '  Is artificial consciousness theoretically possible? Is it plausible? If so,\nis it technically feasible? To make progress on these questions, it is\nnecessary to lay some groundwork clarifying the logical and empirical\nconditions for artificial consciousness to arise and the meaning of relevant\nterms involved. Consciousness is a polysemic word: researchers from different\nfields, including neuroscience, Artificial Intelligence, robotics, and\nphilosophy, among others, sometimes use different terms in order to refer to\nthe same phenomena or the same terms to refer to different phenomena. In fact,\nif we want to pursue artificial consciousness, a proper definition of the key\nconcepts is required. Here, after some logical and conceptual preliminaries, we\nargue for the necessity of using dimensions and profiles of consciousness for a\nbalanced discussion about their possible instantiation or realisation in\nartificial systems. Our primary goal in this paper is to review the main\ntheoretical questions that arise in the domain of artificial consciousness. On\nthe basis of this review, we propose to assess the issue of artificial\nconsciousness within a multidimensional account. The theoretical possibility of\nartificial consciousness is already presumed within some theoretical\nframeworks; however, empirical possibility cannot simply be deduced from these\nframeworks but needs independent empirical validation. We break down the\ncomplexity of consciousness by identifying constituents, components, and\ndimensions, and reflect pragmatically about the general challenges confronting\nthe creation of artificial consciousness. Despite these challenges, we outline\na research strategy for showing how ""awareness"" as we propose to understand it\ncould plausibly be realised in artificial systems.\n', '  Consciousness is notoriously hard to define with objective terms. An\nobjective definition of consciousness is critically needed so that we might\naccurately understand how consciousness and resultant choice behaviour may\narise in biological or artificial systems. Many theories have integrated\nneurobiological and psychological research to explain how consciousness might\narise, but few, if any, outline what is fundamentally required to generate\nconsciousness. To identify such requirements, I examine current theories of\nconsciousness and corresponding scientific research to generate a new\ndefinition of consciousness from first principles. Critically, consciousness is\nthe apparatus that provides the ability to make decisions, but it is not\ndefined by the decision itself. As such, a definition of consciousness does not\nrequire choice behaviour or an explicit awareness of temporality despite both\nbeing well-characterised outcomes of conscious thought. Rather, requirements\nfor consciousness include: at least some capability for perception, a memory\nfor the storage of such perceptual information which in turn provides a\nframework for an imagination with which a sense of self can be capable of\nmaking decisions based on possible and desired futures. Thought experiments and\nobservable neurological phenomena demonstrate that these components are\nfundamentally required of consciousness, whereby the loss of any one component\nremoves the capability for conscious thought. Identifying these requirements\nprovides a new definition for consciousness by which we can objectively\ndetermine consciousness in any conceivable agent, such as non-human animals and\nartificially intelligent systems.\n'] , ['  Neuro-Symbolic (NeSy) integration combines symbolic reasoning with Neural\nNetworks (NNs) for tasks requiring perception and reasoning. Most NeSy systems\nrely on continuous relaxation of logical knowledge, and no discrete decisions\nare made within the model pipeline. Furthermore, these methods assume that the\nsymbolic rules are given. In this paper, we propose Deep Symbolic Learning\n(DSL), a NeSy system that learns NeSy-functions, i.e., the composition of a\n(set of) perception functions which map continuous data to discrete symbols,\nand a symbolic function over the set of symbols. DSL learns simultaneously the\nperception and symbolic functions while being trained only on their composition\n(NeSy-function). The key novelty of DSL is that it can create internal\n(interpretable) symbolic representations and map them to perception inputs\nwithin a differentiable NN learning pipeline. The created symbols are\nautomatically selected to generate symbolic functions that best explain the\ndata. We provide experimental analysis to substantiate the efficacy of DSL in\nsimultaneously learning perception and symbolic functions.\n', '  The field of neuro-symbolic AI aims to benefit from the combination of neural\nnetworks and symbolic systems. A cornerstone of the field is the translation or\nencoding of symbolic knowledge into neural networks. Although many\nneuro-symbolic methods and approaches have been proposed throughout the years,\nand with an large increase in recent years, no common definition of encoding\nexists that can enable a precise, theoretical comparison of neuro-symbolic\nmethods. This paper addresses this problem by introducing a semantic framework\nfor neuro-symbolic AI. We start by providing a formal definition of semantic\nencoding, specifying the components and conditions under which a knowledge-base\ncan be encoded correctly by a neural network. We then show that many\nneuro-symbolic approaches are accounted for by this definition. We provide a\nnumber of examples and correspondence proofs of the application of the proposed\nframework to the neural encoding of various forms of knowledge representation.\nMany, at first sight disparate, neuro-symbolic methods, are shown to fall\nwithin the proposed formalization. This is expected to provide a guidance to\nfuture neuro-symbolic encodings by placing them in the broader context of the\nsemantic encoding of entire families of existing neuro-symbolic systems. The\npaper is hoped to help initiate a discussion around the provision of a theory\nfor neuro-symbolic AI and a semantics for deep learning.\n', '  Neurosymbolic artificial intelligence (AI) is an emerging branch of AI that\ncombines the strengths of symbolic AI and sub-symbolic AI. A major drawback of\nsub-symbolic AI is that it acts as a ""black box"", meaning that predictions are\ndifficult to explain, making the testing & evaluation (T&E) and validation &\nverification (V&V) processes of a system that uses sub-symbolic AI a challenge.\nSince neurosymbolic AI combines the advantages of both symbolic and\nsub-symbolic AI, this survey explores how neurosymbolic applications can ease\nthe V&V process. This survey considers two taxonomies of neurosymbolic AI,\nevaluates them, and analyzes which algorithms are commonly used as the symbolic\nand sub-symbolic components in current applications. Additionally, an overview\nof current techniques for the T&E and V&V processes of these components is\nprovided. Furthermore, it is investigated how the symbolic part is used for T&E\nand V&V purposes in current neurosymbolic applications. Our research shows that\nneurosymbolic AI as great potential to ease the T&E and V&V processes of\nsub-symbolic AI by leveraging the possibilities of symbolic AI. Additionally,\nthe applicability of current T&E and V&V methods to neurosymbolic AI is\nassessed, and how different neurosymbolic architectures can impact these\nmethods is explored. It is found that current T&E and V&V techniques are partly\nsufficient to test, evaluate, verify, or validate the symbolic and sub-symbolic\npart of neurosymbolic applications independently, while some of them use\napproaches where current T&E and V&V methods are not applicable by default, and\nadjustments or even new approaches are needed. Our research shows that there is\ngreat potential in using symbolic AI to test, evaluate, verify, or validate the\npredictions of a sub-symbolic model, making neurosymbolic AI an interesting\nresearch direction for safe, secure, and trustworthy AI.\n'] , [""  Humans frequently make decisions with the aid of artificially intelligent\n(AI) systems. A common pattern is for the AI to recommend an action to the\nhuman who retains control over the final decision. Researchers have identified\nensuring that a human has appropriate reliance on an AI as a critical component\nof achieving complementary performance. We argue that the current definition of\nappropriate reliance used in such research lacks formal statistical grounding\nand can lead to contradictions. We propose a formal definition of reliance,\nbased on statistical decision theory, which separates the concepts of reliance\nas the probability the decision-maker follows the AI's recommendation from\nchallenges a human may face in differentiating the signals and forming accurate\nbeliefs about the situation. Our definition gives rise to a framework that can\nbe used to guide the design and interpretation of studies on human-AI\ncomplementarity and reliance. Using recent AI-advised decision making studies\nfrom literature, we demonstrate how our framework can be used to separate the\nloss due to mis-reliance from the loss due to not accurately differentiating\nthe signals. We evaluate these losses by comparing to a baseline and a\nbenchmark for complementary performance defined by the expected payoff achieved\nby a rational decision-maker facing the same decision task as the behavioral\ndecision-makers.\n"", '  Imagine if AI decision-support tools not only complemented our ability to\nmake accurate decisions, but also improved our skills, boosted collaboration,\nand elevated the joy we derive from our tasks. Despite the potential to\noptimize a broad spectrum of such human-centric objectives, the design of\ncurrent AI tools remains focused on decision accuracy alone. We propose offline\nreinforcement learning (RL) as a general approach for modeling human-AI\ndecision-making to optimize human-AI interaction for diverse objectives. RL can\noptimize such objectives by tailoring decision support, providing the right\ntype of assistance to the right person at the right time. We instantiated our\napproach with two objectives: human-AI accuracy on the decision-making task and\nhuman learning about the task and learned decision support policies from\nprevious human-AI interaction data. We compared the optimized policies against\nseveral baselines in AI-assisted decision-making. Across two experiments (N=316\nand N=964), our results demonstrated that people interacting with policies\noptimized for accuracy achieve significantly better accuracy -- and even\nhuman-AI complementarity -- compared to those interacting with any other type\nof AI support. Our results further indicated that human learning was more\ndifficult to optimize than accuracy, with participants who interacted with\nlearning-optimized policies showing significant learning improvement only at\ntimes. Our research (1) demonstrates offline RL to be a promising approach to\nmodel human-AI decision-making, leading to policies that may optimize\nhuman-centric objectives and provide novel insights about the AI-assisted\ndecision-making space, and (2) emphasizes the importance of considering\nhuman-centric objectives beyond decision accuracy in AI-assisted\ndecision-making, opening up the novel research challenge of optimizing human-AI\ninteraction for such objectives.\n', ""  With the rapid development of AI-based decision aids, different forms of AI\nassistance have been increasingly integrated into the human decision making\nprocesses. To best support humans in decision making, it is essential to\nquantitatively understand how diverse forms of AI assistance influence humans'\ndecision making behavior. To this end, much of the current research focuses on\nthe end-to-end prediction of human behavior using ``black-box'' models, often\nlacking interpretations of the nuanced ways in which AI assistance impacts the\nhuman decision making process. Meanwhile, methods that prioritize the\ninterpretability of human behavior predictions are often tailored for one\nspecific form of AI assistance, making adaptations to other forms of assistance\ndifficult. In this paper, we propose a computational framework that can provide\nan interpretable characterization of the influence of different forms of AI\nassistance on decision makers in AI-assisted decision making. By\nconceptualizing AI assistance as the ``{\\em nudge}'' in human decision making\nprocesses, our approach centers around modelling how different forms of AI\nassistance modify humans' strategy in weighing different information in making\ntheir decisions. Evaluations on behavior data collected from real human\ndecision makers show that the proposed framework outperforms various baselines\nin accurately predicting human behavior in AI-assisted decision making. Based\non the proposed framework, we further provide insights into how individuals\nwith different cognitive styles are nudged by AI assistance differently.\n""] , ['  To achieve faithful reasoning that aligns with human expectations, large\nlanguage models (LLMs) need to ground their reasoning to real-world knowledge\n(e.g., web facts, math and physical rules). Tools help LLMs access this\nexternal knowledge, but there remains challenges for fine-tuning LLM agents\n(e.g., Toolformer) to invoke tools in multi-step reasoning problems, where\ninter-connected tool calls require holistic and efficient tool usage planning.\n  In this work, we propose a new method for LLMs to better leverage tools in\nmulti-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to\nfirst decode reasoning chains with abstract placeholders, and then call domain\ntools to reify each reasoning chain by filling in specific knowledge. This\nplanning with abstract chains enables LLMs to learn more general reasoning\nstrategies, which are robust to shifts of domain knowledge (e.g., math results)\nrelevant to different reasoning questions. It also allows LLMs to perform\ndecoding and calling of external tools in parallel, which avoids the inference\ndelay caused by waiting for tool responses. In mathematical reasoning and Wiki\nQA domains, we show that our method consistently outperforms previous\nchain-of-thought and tool-augmented baselines on both in-distribution and\nout-of-distribution test sets, with an average ~6% absolute QA accuracy\nimprovement. LLM agents trained with our method also show more efficient tool\nuse, with inference speed being on average ~1.4x faster than baseline\ntool-augmented LLMs.\n', ""  While vertical thinking relies on logical and commonsense reasoning, lateral\nthinking requires systems to defy commonsense associations and overwrite them\nthrough unconventional thinking. Lateral thinking has been shown to be\nchallenging for current models but has received little attention. A recent\nbenchmark, BRAINTEASER, aims to evaluate current models' lateral thinking\nability in a zero-shot setting. In this paper, we split the original benchmark\nto also support fine-tuning setting and present SemEval Task 9:\nBRAIN-TEASER(S), the first task at this competition designed to test the\nsystem's reasoning and lateral thinking ability. As a popular task,\nBRAINTEASER(S)'s two subtasks receive 483 team submissions from 182\nparticipants during the competition. This paper provides a fine-grained system\nanalysis of the competition results, together with a reflection on what this\nmeans for the ability of the systems to reason laterally. We hope that the\nBRAINTEASER(S) subtasks and findings in this paper can stimulate future work on\nlateral thinking and robust reasoning by computational models.\n"", ""  With the continuous evolution and refinement of LLMs, they are endowed with\nimpressive logical reasoning or vertical thinking capabilities. But can they\nthink out of the box? Do they possess proficient lateral thinking abilities?\nFollowing the setup of Lateral Thinking Puzzles, we propose a novel evaluation\nbenchmark, LatEval, which assesses the model's lateral thinking within an\ninteractive framework. In our benchmark, we challenge LLMs with 2 aspects: the\nquality of questions posed by the model and the model's capability to integrate\ninformation for problem-solving. We find that nearly all LLMs struggle with\nemploying lateral thinking during interactions. For example, even the most\nadvanced model, GPT-4, exhibits the advantage to some extent, yet still\nmaintain a noticeable gap when compared to human. This evaluation benchmark\nprovides LLMs with a highly challenging and distinctive task that is crucial to\nan effective AI assistant.\n""] , [""  We anticipate increased instances of humans and AI systems working together\nin what we refer to as a hybrid team. The increase in collaboration is expected\nas AI systems gain proficiency and their adoption becomes more widespread.\nHowever, their behavior is not error-free, making hybrid teams a very suitable\nsolution. As such, we consider methods for improving performance for these\nteams of humans and AI systems. For hybrid teams, we will refer to both the\nhumans and AI systems as agents. To improve team performance over that seen for\nagents operating individually, we propose a manager which learns, through a\nstandard Reinforcement Learning scheme, how to best delegate, over time, the\nresponsibility of taking a decision to any of the agents. We further guide the\nmanager's learning so they also minimize how many changes in delegation are\nmade resulting from undesirable team behavior. We demonstrate the optimality of\nour manager's performance in several grid environments which include failure\nstates which terminate an episode and should be avoided. We perform our\nexperiments with teams of agents with varying degrees of acceptable risk, in\nthe form of proximity to a failure state, and measure the manager's ability to\nmake effective delegation decisions with respect to its own risk-based\nconstraints, then compare these to the optimal decisions. Our results show our\nmanager can successfully learn desirable delegations which result in team paths\nnear/exactly optimal with respect to path length and number of delegations.\n"", ""  Defining and measuring trust in dynamic, multiagent teams is important in a\nrange of contexts, particularly in defense and security domains. Team members\nshould be trusted to work towards agreed goals and in accordance with shared\nvalues. In this paper, our concern is with the definition of goals and values\nsuch that it is possible to define 'trust' in a way that is interpretable, and\nhence usable, by both humans and robots. We argue that the outcome of team\nactivity can be considered in terms of 'goal', 'individual/team values', and\n'legal principles'. We question whether alignment is possible at the level of\n'individual/team values', or only at the 'goal' and 'legal principles' levels.\nWe argue for a set of metrics to define trust in human-robot teams that are\ninterpretable by human or robot team members, and consider an experiment that\ncould demonstrate the notion of 'satisficing trust' over the course of a\nsimulated mission.\n"", '  Future warfare will require Command and Control (C2) personnel to make\ndecisions at shrinking timescales in complex and potentially ill-defined\nsituations. Given the need for robust decision-making processes and\ndecision-support tools, integration of artificial and human intelligence holds\nthe potential to revolutionize the C2 operations process to ensure adaptability\nand efficiency in rapidly changing operational environments. We propose to\nleverage recent promising breakthroughs in interactive machine learning, in\nwhich humans can cooperate with machine learning algorithms to guide machine\nlearning algorithm behavior. This paper identifies several gaps in\nstate-of-the-art science and technology that future work should address to\nextend these approaches to function in complex C2 contexts. In particular, we\ndescribe three research focus areas that together, aim to enable scalable\ninteractive machine learning (SIML): 1) developing human-AI interaction\nalgorithms to enable planning in complex, dynamic situations; 2) fostering\nresilient human-AI teams through optimizing roles, configurations, and trust;\nand 3) scaling algorithms and human-AI teams for flexibility across a range of\npotential contexts and situations.\n']",Artificial Intelligence and Cognitive Systems,Human-AI Decision Making and Assistance
83,"""AI-Driven Wireless Networks and 6G Communications"" , ""AI-Driven Software Development and Engineering"" , ""Automating Workflows with AI and ML"" , Artificial Intelligence in Industrial Automation , ""Intent-Based Network Automation with AI""","['wirelessllm', 'generative', 'ai', 'wireless', 'networks', 'network', 'mobile', 'communications', '6g', 'telecom'] , ['ai', 'automation', 'software', 'developers', 'programmers', 'development', 'architectural', 'automl', 'engineering', 'prototyping'] , ['workflows', 'workflow', 'automate', 'automation', 'apis', 'tools', 'interpreter', 'flowmind', 'software', 'pipelines'] , ['automation', 'ai', 'architecture', 'agent', 'agents', 'software', 'industrial', 'engineering', 'intelligence', 'artificial'] , ['intents', 'intent', 'automation', 'automate', 'automated', 'semantic', 'ai', 'orchestration', 'networks', 'network']","['  Intelligent communications have played a pivotal role in shaping the\nevolution of 6G networks. Native artificial intelligence (AI) within green\ncommunication systems must meet stringent real-time requirements. To achieve\nthis, deploying lightweight and resource-efficient AI models is necessary.\nHowever, as wireless networks generate a multitude of data fields and\nindicators during operation, only a fraction of them imposes significant impact\non the network AI models. Therefore, real-time intelligence of communication\nsystems heavily relies on a small but critical set of the data that profoundly\ninfluences the performance of network AI models. These challenges underscore\nthe need for innovative architectures and solutions. In this paper, we propose\na solution, termed the pervasive multi-level (PML) native AI architecture,\nwhich integrates the concept of knowledge graph (KG) into the intelligent\noperational manipulations of mobile networks, resulting in the establishment of\na wireless data KG. Leveraging the wireless data KG, we characterize the\nmassive and complex data collected from wireless communication networks and\nanalyze the relationships among various data fields. The obtained graph of data\nfield relations enables the on-demand generation of minimal and effective\ndatasets, referred to as feature datasets, tailored to specific application\nrequirements. Consequently, this architecture not only enhances AI training,\ninference, and validation processes but also significantly reduces resource\nwastage and overhead for communication networks. To implement this\narchitecture, we have developed a specific solution comprising a\nspatio-temporal heterogeneous graph attention neural network model (STREAM) as\nwell as a feature dataset generation algorithm. Experiments are conducted to\nvalidate the effectiveness of the proposed architecture.\n', '  Generative artificial intelligence (GenAI) and communication networks are\nexpected to have groundbreaking synergies in 6G. Connecting GenAI agents over a\nwireless network can potentially unleash the power of collective intelligence\nand pave the way for artificial general intelligence (AGI). However, current\nwireless networks are designed as a ""data pipe"" and are not suited to\naccommodate and leverage the power of GenAI. In this paper, we propose the\nGenAINet framework in which distributed GenAI agents communicate knowledge\n(high-level concepts or abstracts) to accomplish arbitrary tasks. We first\nprovide a network architecture integrating GenAI capabilities to manage both\nnetwork protocols and applications. Building on this, we investigate effective\ncommunication and reasoning problems by proposing a semantic-native GenAINet.\nSpecifically, GenAI agents extract semantic concepts from multi-modal raw data,\nbuild a knowledgebase representing their semantic relations, which is retrieved\nby GenAI models for planning and reasoning. Under this paradigm, an agent can\nlearn fast from other agents\' experience for making better decisions with\nefficient communications. Furthermore, we conduct two case studies where in\nwireless device query, we show that extracting and transferring knowledge can\nimprove query accuracy with reduced communication; and in wireless power\ncontrol, we show that distributed agents can improve decisions via\ncollaborative reasoning. Finally, we address that developing a hierarchical\nsemantic level Telecom world model is a key path towards network of collective\nintelligence.\n', '  Wireless communications advance hand-in-hand with artificial intelligence\n(AI), indicating an interconnected advancement where each facilitates and\nbenefits from the other. This synergy is particularly evident in the\ndevelopment of the sixth-generation technology standard for mobile networks\n(6G), envisioned to be AI-native. Generative-AI (GenAI), a novel technology\ncapable of producing various types of outputs, including text, images, and\nvideos, offers significant potential for wireless communications, with its\ndistinctive features. Traditionally, conventional AI techniques have been\nemployed for predictions, classifications, and optimization, while GenAI has\nmore to offer. This article introduces the concept of strategic demand-planning\nthrough demand-labeling, demand-shaping, and demand-rescheduling. Accordingly,\nGenAI is proposed as a powerful tool to facilitate demand-shaping in wireless\nnetworks. More specifically, GenAI is used to compress and convert the content\nof various kind (e.g., from a higher bandwidth mode to a lower one, such as\nfrom a video to text), which subsequently enhances performance of wireless\nnetworks in various usage scenarios such as cell-switching, user association\nand load balancing, interference management, and disaster scenarios management.\nTherefore, GenAI can serve a function in saving energy and spectrum in wireless\nnetworks. With recent advancements in AI, including sophisticated algorithms\nlike large-language-models and the development of more powerful hardware built\nexclusively for AI tasks, such as AI accelerators, the concept of\ndemand-planning, particularly demand-shaping through GenAI, becomes\nincreasingly relevant. Furthermore, recent efforts to make GenAI accessible on\ndevices, such as user terminals, make the implementation of this concept even\nmore straightforward and feasible.\n'] , ['  A paradigm shift is underway in Software Engineering, with AI systems such as\nLLMs gaining increasing importance for improving software development\nproductivity. This trend is anticipated to persist. In the next five years, we\nwill likely see an increasing symbiotic partnership between human developers\nand AI. The Software Engineering research community cannot afford to overlook\nthis trend; we must address the key research challenges posed by the\nintegration of AI into the software development process. In this paper, we\npresent our vision of the future of software development in an AI-Driven world\nand explore the key challenges that our research community should address to\nrealize this vision.\n', ""  Background:Technical systems are growing in complexity with more components\nand functions across various disciplines. Model-Driven Engineering (MDE) helps\nmanage this complexity by using models as key artifacts. Domain-Specific\nLanguages (DSL) supported by MDE facilitate modeling. As data generation in\nproduct development increases, there's a growing demand for AI algorithms,\nwhich can be challenging to implement. Integrating AI algorithms with DSL and\nMDE can streamline this process. Objective:This study aims to investigate the\nexisting model-driven approaches relying on DSL in support of the engineering\nof AI software systems to sharpen future research further and define the\ncurrent state of the art. Method:We conducted a Systemic Literature Review\n(SLR), collecting papers from five major databases resulting in 1335 candidate\nstudies, eventually retaining 18 primary studies. Each primary study will be\nevaluated and discussed with respect to the adoption of MDE principles and\npractices and the phases of AI development support aligned with the stages of\nthe CRISP-DM methodology. Results:The study's findings show that language\nworkbenches are of paramount importance in dealing with all aspects of modeling\nlanguage development and are leveraged to define DSL explicitly addressing AI\nconcerns. The most prominent AI-related concerns are training and modeling of\nthe AI algorithm, while minor emphasis is given to the time-consuming\npreparation of the data. Early project phases that support interdisciplinary\ncommunication of requirements, e.g., CRISP-DM Business Understanding phase, are\nrarely reflected. Conclusion:The study found that the use of MDE for AI is\nstill in its early stages, and there is no single tool or method that is widely\nused. Additionally, current approaches tend to focus on specific stages of\ndevelopment rather than providing support for the entire development process.\n"", '  Across the dynamic business landscape today, enterprises face an\never-increasing range of challenges. These include the constantly evolving\nregulatory environment, the growing demand for personalization within software\napplications, and the heightened emphasis on governance. In response to these\nmultifaceted demands, large enterprises have been adopting automation that\nspans from the optimization of core business processes to the enhancement of\ncustomer experiences. Indeed, Artificial Intelligence (AI) has emerged as a\npivotal element of modern software systems. In this context, data plays an\nindispensable role. AI-centric software systems based on supervised learning\nand operating at an industrial scale require large volumes of training data to\nperform effectively. Moreover, the incorporation of generative AI has led to a\ngrowing demand for adequate evaluation benchmarks. Our experience in this field\nhas revealed that the requirement for large datasets for training and\nevaluation introduces a host of intricate challenges. This book chapter\nexplores the evolving landscape of Software Engineering (SE) in general, and\nRequirements Engineering (RE) in particular, in this era marked by AI\nintegration. We discuss challenges that arise while integrating Natural\nLanguage Processing (NLP) and generative AI into enterprise-critical software\nsystems. The chapter provides practical insights, solutions, and examples to\nequip readers with the knowledge and tools necessary for effectively building\nsolutions with NLP at their cores. We also reflect on how these text\ndata-centric tasks sit together with the traditional RE process. We also\nhighlight new RE tasks that may be necessary for handling the increasingly\nimportant text data-centricity involved in developing software systems.\n'] , ['  Automating enterprise workflows could unlock $4 trillion/year in productivity\ngains. Despite being of interest to the data management community for decades,\nthe ultimate vision of end-to-end workflow automation has remained elusive.\nCurrent solutions rely on process mining and robotic process automation (RPA),\nin which a bot is hard-coded to follow a set of predefined rules for completing\na workflow. Through case studies of a hospital and large B2B enterprise, we\nfind that the adoption of RPA has been inhibited by high set-up costs (12-18\nmonths), unreliable execution (60% initial accuracy), and burdensome\nmaintenance (requiring multiple FTEs). Multimodal foundation models (FMs) such\nas GPT-4 offer a promising new approach for end-to-end workflow automation\ngiven their generalized reasoning and planning abilities. To study these\ncapabilities we propose ECLAIR, a system to automate enterprise workflows with\nminimal human supervision. We conduct initial experiments showing that\nmultimodal FMs can address the limitations of traditional RPA with (1)\nnear-human-level understanding of workflows (93% accuracy on a workflow\nunderstanding task) and (2) instant set-up with minimal technical barrier\n(based solely on a natural language description of a workflow, ECLAIR achieves\nend-to-end completion rates of 40%). We identify human-AI collaboration,\nvalidation, and self-improvement as open challenges, and suggest ways they can\nbe solved with data management techniques. Code is available at:\nhttps://github.com/HazyResearch/eclair-agents\n', '  Data science and engineering workflows often span multiple stages, from\nwarehousing to orchestration, using tools like BigQuery, dbt, and Airbyte. As\nvision language models (VLMs) advance in multimodal understanding and code\ngeneration, VLM-based agents could potentially automate these workflows by\ngenerating SQL queries, Python code, and GUI operations. This automation can\nimprove the productivity of experts while democratizing access to large-scale\ndata analysis. In this paper, we introduce Spider2-V, the first multimodal\nagent benchmark focusing on professional data science and engineering\nworkflows, featuring 494 real-world tasks in authentic computer environments\nand incorporating 20 enterprise-level professional applications. These tasks,\nderived from real-world use cases, evaluate the ability of a multimodal agent\nto perform data-related tasks by writing code and managing the GUI in\nenterprise data software systems. To balance realistic simulation with\nevaluation simplicity, we devote significant effort to developing automatic\nconfigurations for task setup and carefully crafting evaluation metrics for\neach task. Furthermore, we supplement multimodal agents with comprehensive\ndocuments of these enterprise data software systems. Our empirical evaluation\nreveals that existing state-of-the-art LLM/VLM-based agents do not reliably\nautomate full data workflows (14.0% success). Even with step-by-step guidance,\nthese agents still underperform in tasks that require fine-grained,\nknowledge-intensive GUI actions (16.2%) and involve remote cloud-hosted\nworkspaces (10.6%). We hope that Spider2-V paves the way for autonomous\nmultimodal agents to transform the automation of data science and engineering\nworkflow. Our code and data are available at https://spider2-v.github.io.\n', ""  Machine Learning (ML) has become ubiquitous, fueling data-driven applications\nacross various organizations. Contrary to the traditional perception of ML in\nresearch, ML workflows can be complex, resource-intensive, and time-consuming.\nExpanding an ML workflow to encompass a wider range of data infrastructure and\ndata types may lead to larger workloads and increased deployment costs.\nCurrently, numerous workflow engines are available (with over ten being widely\nrecognized). This variety poses a challenge for end-users in terms of mastering\ndifferent engine APIs. While efforts have primarily focused on optimizing ML\nOperations (MLOps) for a specific workflow engine, current methods largely\noverlook workflow optimization across different engines.\n  In this work, we design and implement Couler, a system designed for unified\nML workflow optimization in the cloud. Our main insight lies in the ability to\ngenerate an ML workflow using natural language (NL) descriptions. We integrate\nLarge Language Models (LLMs) into workflow generation, and provide a unified\nprogramming interface for various workflow engines. This approach alleviates\nthe need to understand various workflow engines' APIs. Moreover, Couler\nenhances workflow computation efficiency by introducing automated caching at\nmultiple stages, enabling large workflow auto-parallelization and automatic\nhyperparameters tuning. These enhancements minimize redundant computational\ncosts and improve fault tolerance during deep learning workflow training.\nCouler is extensively deployed in real-world production scenarios at Ant Group,\nhandling approximately 22k workflows daily, and has successfully improved the\nCPU/Memory utilization by more than 15% and the workflow completion rate by\naround 17%.\n""] , ['  In this paper, we explore the transformative impact of Artificial\nIntelligence (AI) in the manufacturing sector, highlighting its potential to\nrevolutionize industry practices and enhance operational efficiency. We delve\ninto various applications of AI in manufacturing, with a particular emphasis on\nhuman-machine interfaces (HMI) and AI-powered milling machines, showcasing how\nthese technologies contribute to more intuitive operations and precision in\nproduction processes. Through rigorous market analysis, the paper presents\ninsightful data on AI adoption rates among German manufacturers, comparing\nthese figures with global trends and exploring the specific uses of AI in\nproduction, maintenance, customer service, and more. In addition, the paper\nexamines the emerging field of Generative AI and the potential applications of\nlarge language models in manufacturing processes. The findings indicate a\nsignificant increase in AI adoption from 6% in 2020 to 13.3% in 2023 among\nGerman companies, with a projection of substantial economic impact by 2030. The\nstudy also addresses the challenges faced by companies, such as data quality\nand integration hurdles, providing a balanced view of the opportunities and\nobstacles in AI implementation.\n', ""  Smart home systems are gaining popularity as homeowners strive to enhance\ntheir living and working environments while minimizing energy consumption.\nHowever, the adoption of artificial intelligence (AI)-enabled decision-making\nmodels in smart home systems faces challenges due to the complexity and\nblack-box nature of these systems, leading to concerns about explainability,\ntrust, transparency, accountability, and fairness. The emerging field of\nexplainable artificial intelligence (XAI) addresses these issues by providing\nexplanations for the models' decisions and actions. While state-of-the-art XAI\nmethods are beneficial for AI developers and practitioners, they may not be\neasily understood by general users, particularly household members. This paper\nadvocates for human-centered XAI methods, emphasizing the importance of\ndelivering readily comprehensible explanations to enhance user satisfaction and\ndrive the adoption of smart home systems. We review state-of-the-art XAI\nmethods and prior studies focusing on human-centered explanations for general\nusers in the context of smart home applications. Through experiments on two\nsmart home application scenarios, we demonstrate that explanations generated by\nprominent XAI techniques might not be effective in helping users understand and\nmake decisions. We thus argue for the necessity of a human-centric approach in\nrepresenting explanations in smart home systems and highlight relevant\nhuman-computer interaction (HCI) methodologies, including user studies,\nprototyping, technology probes analysis, and heuristic evaluation, that can be\nemployed to generate and present human-centered explanations to users.\n"", '  The rapid advancement of AI technology has led to widespread applications of\nagent systems across various domains. However, the need for detailed\narchitecture design poses significant challenges in designing and operating\nthese systems. This paper introduces a taxonomy focused on the architectures of\nfoundation-model-based agents, addressing critical aspects such as functional\ncapabilities and non-functional qualities. We also discuss the operations\ninvolved in both design-time and run-time phases, providing a comprehensive\nview of architectural design and operational characteristics. By unifying and\ndetailing these classifications, our taxonomy aims to improve the design of\nfoundation-model-based agents. Additionally, the paper establishes a decision\nmodel that guides critical design and runtime decisions, offering a structured\napproach to enhance the development of foundation-model-based agents. Our\ncontributions include providing a structured architecture design option and\nguiding the development process of foundation-model-based agents, thereby\naddressing current fragmentation in the field.\n'] , ['  Automated management requires decomposing high-level user requests, such as\nintents, to an abstraction that the system can understand and execute. This is\nchallenging because even a simple intent requires performing a number of\nordered steps. And the task of identifying and adapting these steps (as\nconditions change) requires a decomposition approach that cannot be exactly\npre-defined beforehand. To tackle these challenges and support automated intent\ndecomposition and execution, we explore the few-shot capability of Large\nLanguage Models (LLMs). We propose a pipeline that progressively decomposes\nintents by generating the required actions using a policy-based abstraction.\nThis allows us to automate the policy execution by creating a closed control\nloop for the intent deployment. To do so, we generate and map the policies to\nAPIs and form application management loops that perform the necessary\nmonitoring, analysis, planning and execution. We evaluate our proposal with a\nuse-case to fulfill and assure an application service chain of virtual network\nfunctions. Using our approach, we can generalize and generate the necessary\nsteps to realize intents, thereby enabling intent automation for application\nmanagement.\n', ""  Large language models (LLMs) are rapidly emerging in Artificial Intelligence\n(AI) applications, especially in the fields of natural language processing and\ngenerative AI. Not limited to text generation applications, these models\ninherently possess the opportunity to leverage prompt engineering, where the\ninputs of such models can be appropriately structured to articulate a model's\npurpose explicitly. A prominent example of this is intent-based networking, an\nemerging approach for automating and maintaining network operations and\nmanagement. This paper presents semantic routing to achieve enhanced\nperformance in LLM-assisted intent-based management and orchestration of 5G\ncore networks. This work establishes an end-to-end intent extraction framework\nand presents a diverse dataset of sample user intents accompanied by a thorough\nanalysis of the effects of encoders and quantization on overall system\nperformance. The results show that using a semantic router improves the\naccuracy and efficiency of the LLM deployment compared to stand-alone LLMs with\nprompting architectures.\n"", ""  To effectively express and satisfy network application requirements,\nintent-based network management has emerged as a promising solution. In\nintent-based methods, users and applications express their intent in a\nhigh-level abstract language to the network. Although this abstraction\nsimplifies network operation, it induces many challenges to efficiently express\napplications' intents and map them to different network capabilities.\nTherefore, in this work, we propose an AI-based framework for intent profiling\nand translation. We consider a scenario where applications interacting with the\nnetwork express their needs for network services in their domain language. The\nmachine-to-machine communication (i.e., between applications and the network)\nis complex since it requires networks to learn how to understand the domain\nlanguages of each application, which is neither practical nor scalable.\nInstead, a framework based on emergent communication is proposed for intent\nprofiling, in which applications express their abstract quality-of-experience\n(QoE) intents to the network through emergent communication messages.\nSubsequently, the network learns how to interpret these communication messages\nand map them to network capabilities (i.e., slices) to guarantee the requested\nQuality-of-Service (QoS). Simulation results show that the proposed method\noutperforms self-learning slicing and other baselines, and achieves a\nperformance close to the perfect knowledge baseline.\n""]",Artificial Intelligence in Technology and Engineering,"""Intent-Based Network Automation with AI"""
84,"""AI Governance and Accountability"" , ""AI in Healthcare and Medical Education"" , ""AI Fairness and Bias in Education and Applications"" , Human-AI Alignment and Ethics , ""Human Trust in Artificial Intelligence (AI) Systems"" , ""Ethics in Artificial Intelligence and Autonomous Agents"" , ""AI Ethics in Healthcare: Bias and Fairness""","['ai', 'accountability', 'ethics', 'ethical', 'intelligence', 'aia', 'governance', 'oversight', 'regulations', 'responsibility'] , ['ai', 'healthcare', 'medical', 'biomedical', 'medicine', 'med', 'clinical', 'trustworthy', 'patients', 'mllms'] , ['fairness', 'discrimination', 'unfairness', 'ai', 'discriminatory', 'ethical', 'bias', 'ethics', 'unfair', 'biases'] , ['ai', 'alignmentsurvey', 'alignment', 'intelligence', 'humanity', 'agent', 'agents', 'aligned', 'ethical', 'ethics'] , ['trust', 'trustworthiness', 'trusting', 'distrust', 'trustworthy', 'trustors', 'ai', 'confidence', 'faith', 'validated'] , ['ethical', 'ethics', 'ai', 'moral', 'agent', 'intelligence', 'algorithmic', 'agents', 'humanity', 'cooperation'] , ['ai', 'healthcare', 'bias', 'ethical', 'ethics', 'fairness', 'outcomes', 'biases', 'intelligence', 'medical']","[""  Auditing of AI systems is a promising way to understand and manage ethical\nproblems and societal risks associated with contemporary AI systems, as well as\nsome anticipated future risks. Efforts to develop standards for auditing\nArtificial Intelligence (AI) systems have therefore understandably gained\nmomentum. However, we argue that creating auditing standards is not just\ninsufficient, but actively harmful by proliferating unheeded and inconsistent\nstandards, especially in light of the rapid evolution and ethical and safety\nchallenges of AI. Instead, the paper proposes the establishment of an AI Audit\nStandards Board, responsible for developing and updating auditing methods and\nstandards in line with the evolving nature of AI technologies. Such a body\nwould ensure that auditing practices remain relevant, robust, and responsive to\nthe rapid advancements in AI. The paper argues that such a governance structure\nwould also be helpful for maintaining public trust in AI and for promoting a\nculture of safety and ethical responsibility within the AI industry.\n  Throughout the paper, we draw parallels with other industries, including\nsafety-critical industries like aviation and nuclear energy, as well as more\nprosaic ones such as financial accounting and pharmaceuticals. AI auditing\nshould emulate those fields, and extend beyond technical assessments to include\nethical considerations and stakeholder engagement, but we explain that this is\nnot enough; emulating other fields' governance mechanisms for these processes,\nand for audit standards creation, is a necessity. We also emphasize the\nimportance of auditing the entire development process of AI systems, not just\nthe final products...\n"", '  The evolution of AI is set to profoundly reshape the future. The European\nUnion, recognizing this impending prominence, has enacted the AI Act,\nregulating market access for AI-based systems. A salient feature of the Act is\nto guard democratic and humanistic values by focusing regulation on\ntransparency, explainability, and the human ability to understand and control\nAI systems. Hereby, the EU AI Act does not merely specify technological\nrequirements for AI systems. The EU issues a democratic call for human-centered\nAI systems and, in turn, an interdisciplinary research agenda for\nhuman-centered innovation in AI development. Without robust methods to assess\nAI systems and their effect on individuals and society, the EU AI Act may lead\nto repeating the mistakes of the General Data Protection Regulation of the EU\nand to rushed, chaotic, ad-hoc, and ambiguous implementation, causing more\nconfusion than lending guidance. Moreover, determined research activities in\nHuman-AI interaction will be pivotal for both regulatory compliance and the\nadvancement of AI in a manner that is both ethical and effective. Such an\napproach will ensure that AI development aligns with human values and needs,\nfostering a technology landscape that is innovative, responsible, and an\nintegral part of our society.\n', ""  As AI systems become increasingly prevalent and impactful, the need for\neffective AI governance and accountability measures is paramount. This paper\nexamines the AI governance landscape, focusing on Anthropic's Claude, a\nfoundational AI model. We analyze Claude through the lens of the NIST AI Risk\nManagement Framework and the EU AI Act, identifying potential threats and\nproposing mitigation strategies. The paper highlights the importance of\ntransparency, rigorous benchmarking, and comprehensive data handling processes\nin ensuring the responsible development and deployment of AI systems. We\nconclude by discussing the social impact of AI governance and the ethical\nconsiderations surrounding AI accountability.\n""] , ['  As more clinical workflows continue to be augmented by artificial\nintelligence (AI), AI literacy among physicians will become a critical\nrequirement for ensuring safe and ethical AI-enabled patient care. Despite the\nevolving importance of AI in healthcare, the extent to which it has been\nadopted into traditional and often-overloaded medical curricula is currently\nunknown. In a scoping review of 1,699 articles published between January 2016\nand June 2024, we identified 18 studies which propose guiding frameworks, and\n11 studies documenting real-world instruction, centered around the integration\nof AI into medical education. We found that comprehensive guidelines will\nrequire greater clinical relevance and personalization to suit medical student\ninterests and career trajectories. Current efforts highlight discrepancies in\nthe teaching guidelines, emphasizing AI evaluation and ethics over technical\ntopics such as data science and coding. Additionally, we identified several\nchallenges associated with integrating AI training into the medical education\nprogram, including a lack of guidelines to define medical students AI literacy,\na perceived lack of proven clinical value, and a scarcity of qualified\ninstructors. With this knowledge, we propose an AI literacy framework to define\ncompetencies for medical students. To prioritize relevant and personalized AI\neducation, we categorize literacy into four dimensions: Foundational,\nPractical, Experimental, and Ethical, with tailored learning objectives to the\npre-clinical, clinical, and clinical research stages of medical education. This\nreview provides a road map for developing practical and relevant education\nstrategies for building an AI-competent healthcare workforce.\n', ""  The recent advancements in artificial intelligence (AI) combined with the\nextensive amount of data generated by today's clinical systems, has led to the\ndevelopment of imaging AI solutions across the whole value chain of medical\nimaging, including image reconstruction, medical image segmentation,\nimage-based diagnosis and treatment planning. Notwithstanding the successes and\nfuture potential of AI in medical imaging, many stakeholders are concerned of\nthe potential risks and ethical implications of imaging AI solutions, which are\nperceived as complex, opaque, and difficult to comprehend, utilise, and trust\nin critical clinical applications. Addressing these concerns and risks, the\nFUTURE-AI framework has been proposed, which, sourced from a global\nmulti-domain expert consensus, comprises guiding principles for increased\ntrust, safety, and adoption for AI in healthcare. In this paper, we transform\nthe general FUTURE-AI healthcare principles to a concise and specific AI\nimplementation guide tailored to the needs of the medical imaging community. To\nthis end, we carefully assess each building block of the FUTURE-AI framework\nconsisting of (i) Fairness, (ii) Universality, (iii) Traceability, (iv)\nUsability, (v) Robustness and (vi) Explainability, and respectively define\nconcrete best practices based on accumulated AI implementation experiences from\nfive large European projects on AI in Health Imaging. We accompany our concrete\nstep-by-step medical imaging development guide with a practical AI solution\nmaturity checklist, thus enabling AI development teams to design, evaluate,\nmaintain, and deploy technically, clinically and ethically trustworthy imaging\nAI solutions into clinical practice.\n"", '  Despite major advances in artificial intelligence (AI) for medicine and\nhealthcare, the deployment and adoption of AI technologies remain limited in\nreal-world clinical practice. In recent years, concerns have been raised about\nthe technical, clinical, ethical and legal risks associated with medical AI. To\nincrease real world adoption, it is essential that medical AI tools are trusted\nand accepted by patients, clinicians, health organisations and authorities.\nThis work describes the FUTURE-AI guideline as the first international\nconsensus framework for guiding the development and deployment of trustworthy\nAI tools in healthcare. The FUTURE-AI consortium was founded in 2021 and\ncurrently comprises 118 inter-disciplinary experts from 51 countries\nrepresenting all continents, including AI scientists, clinicians, ethicists,\nand social scientists. Over a two-year period, the consortium defined guiding\nprinciples and best practices for trustworthy AI through an iterative process\ncomprising an in-depth literature review, a modified Delphi survey, and online\nconsensus meetings. The FUTURE-AI framework was established based on 6 guiding\nprinciples for trustworthy AI in healthcare, i.e. Fairness, Universality,\nTraceability, Usability, Robustness and Explainability. Through consensus, a\nset of 28 best practices were defined, addressing technical, clinical, legal\nand socio-ethical dimensions. The recommendations cover the entire lifecycle of\nmedical AI, from design, development and validation to regulation, deployment,\nand monitoring. FUTURE-AI is a risk-informed, assumption-free guideline which\nprovides a structured approach for constructing medical AI tools that will be\ntrusted, deployed and adopted in real-world practice. Researchers are\nencouraged to take the recommendations into account in proof-of-concept stages\nto facilitate future translation towards clinical practice of medical AI.\n'] , [""  The integration of Artificial Intelligence (AI) into education has\ntransformative potential, providing tailored learning experiences and creative\ninstructional approaches. However, the inherent biases in AI algorithms hinder\nthis improvement by unintentionally perpetuating prejudice against specific\ndemographics, especially in human-centered applications like education. This\nsurvey delves deeply into the developing topic of algorithmic fairness in\neducational contexts, providing a comprehensive evaluation of the diverse\nliterature on fairness, bias, and ethics in AI-driven educational applications.\nIt identifies the common forms of biases, such as data-related, algorithmic,\nand user-interaction, that fundamentally undermine the accomplishment of\nfairness in AI teaching aids. By outlining existing techniques for mitigating\nthese biases, ranging from varied data gathering to algorithmic fairness\ninterventions, the survey emphasizes the critical role of ethical\nconsiderations and legal frameworks in shaping a more equitable educational\nenvironment. Furthermore, it guides readers through the complexities of\nfairness measurements, methods, and datasets, shedding light on the way to bias\nreduction. Despite these gains, this survey highlights long-standing issues,\nsuch as achieving a balance between fairness and accuracy, as well as the need\nfor diverse datasets. Overcoming these challenges and ensuring the ethical and\nfair use of AI's promise in education call for a collaborative,\ninterdisciplinary approach.\n"", '  Reaching consensus on a commonly accepted definition of AI Fairness has long\nbeen a central challenge in AI ethics and governance. There is a broad spectrum\nof views across society on what the concept of fairness means and how it should\nbest be put to practice. In this workbook, we tackle this challenge by\nexploring how a context-based and society-centred approach to understanding AI\nFairness can help project teams better identify, mitigate, and manage the many\nways that unfair bias and discrimination can crop up across the AI project\nworkflow.\n  We begin by exploring how, despite the plurality of understandings about the\nmeaning of fairness, priorities of equality and non-discrimination have come to\nconstitute the broadly accepted core of its application as a practical\nprinciple. We focus on how these priorities manifest in the form of equal\nprotection from direct and indirect discrimination and from discriminatory\nharassment. These elements form ethical and legal criteria based upon which\ninstances of unfair bias and discrimination can be identified and mitigated\nacross the AI project workflow.\n  We then take a deeper dive into how the different contexts of the AI project\nlifecycle give rise to different fairness concerns. This allows us to identify\nseveral types of AI Fairness (Data Fairness, Application Fairness, Model Design\nand Development Fairness, Metric-Based Fairness, System Implementation\nFairness, and Ecosystem Fairness) that form the basis of a multi-lens approach\nto bias identification, mitigation, and management. Building on this, we\ndiscuss how to put the principle of AI Fairness into practice across the AI\nproject workflow through Bias Self-Assessment and Bias Risk Management as well\nas through the documentation of metric-based fairness criteria in a Fairness\nPosition Statement.\n', ""  The rise in the use of AI/ML applications across industries has sparked more\ndiscussions about the fairness of AI/ML in recent times. While prior research\non the fairness of AI/ML exists, there is a lack of empirical studies focused\non understanding the perspectives and experiences of AI practitioners in\ndeveloping a fair AI/ML system. Understanding AI practitioners' perspectives\nand experiences on the fairness of AI/ML systems are important because they are\ndirectly involved in its development and deployment and their insights can\noffer valuable real-world perspectives on the challenges associated with\nensuring fairness in AI/ML systems. We conducted semi-structured interviews\nwith 22 AI practitioners to investigate their understanding of what a 'fair\nAI/ML' is, the challenges they face in developing a fair AI/ML system, the\nconsequences of developing an unfair AI/ML system, and the strategies they\nemploy to ensure AI/ML system fairness. We developed a framework showcasing the\nrelationship between AI practitioners' understanding of 'fair AI/ML' system and\n(i) their challenges in its development, (ii) the consequences of developing an\nunfair AI/ML system, and (iii) strategies used to ensure AI/ML system fairness.\nBy exploring AI practitioners' perspectives and experiences, this study\nprovides actionable insights to enhance AI/ML fairness, which may promote\nfairer systems, reduce bias, and foster public trust in AI technologies.\nAdditionally, we also identify areas for further investigation and offer\nrecommendations to aid AI practitioners and AI companies in navigating\nfairness.\n""] , ['  Discussion of AI alignment (alignment between humans and AI systems) has\nfocused on value alignment, broadly referring to creating AI systems that share\nhuman values. We argue that before we can even attempt to align values, it is\nimperative that AI systems and humans align the concepts they use to understand\nthe world. We integrate ideas from philosophy, cognitive science, and deep\nlearning to explain the need for concept alignment, not just value alignment,\nbetween humans and machines. We summarize existing accounts of how humans and\nmachines currently learn concepts, and we outline opportunities and challenges\nin the path towards shared concepts. Finally, we explain how we can leverage\nthe tools already being developed in cognitive science and AI research to\naccelerate progress towards concept alignment.\n', ""  How can we build AI systems that are aligned with human values to avoid\ncausing harm or violating societal standards for acceptable behavior? We argue\nthat representational alignment between humans and AI agents facilitates value\nalignment. Making AI systems learn human-like representations of the world has\nmany known benefits, including improving generalization, robustness to domain\nshifts, and few-shot learning performance. We propose that this kind of\nrepresentational alignment between machine learning (ML) models and humans can\nalso support value alignment, allowing ML systems to conform to human values\nand societal norms. We focus on ethics as one aspect of value alignment and\ntrain ML agents using a variety of methods in a multi-armed bandit setting,\nwhere rewards reflect the moral acceptability of the chosen action. We use a\nsynthetic experiment to demonstrate that agents' representational alignment\nwith the environment bounds their learning performance. We then repeat this\nprocedure in a realistic setting, using textual action descriptions and\nsimilarity judgments collected from humans and a variety of language models, to\nshow that the results generalize and are model-agnostic when grounded in an\nethically relevant context.\n"", '  Recent advancements in general-purpose AI have highlighted the importance of\nguiding AI systems towards the intended goals, ethical principles, and values\nof individuals and groups, a concept broadly recognized as alignment. However,\nthe lack of clarified definitions and scopes of human-AI alignment poses a\nsignificant obstacle, hampering collaborative efforts across research domains\nto achieve this alignment. In particular, ML- and philosophy-oriented alignment\nresearch often views AI alignment as a static, unidirectional process (i.e.,\naiming to ensure that AI systems\' objectives match humans) rather than an\nongoing, mutual alignment problem. This perspective largely neglects the\nlong-term interaction and dynamic changes of alignment. To understand these\ngaps, we introduce a systematic review of over 400 papers published between\n2019 and January 2024, spanning multiple domains such as Human-Computer\nInteraction (HCI), Natural Language Processing (NLP), Machine Learning (ML). We\ncharacterize, define and scope human-AI alignment. From this, we present a\nconceptual framework of ""Bidirectional Human-AI Alignment"" to organize the\nliterature from a human-centered perspective. This framework encompasses both\n1) conventional studies of aligning AI to humans that ensures AI produces the\nintended outcomes determined by humans, and 2) a proposed concept of aligning\nhumans to AI, which aims to help individuals and society adjust to AI\nadvancements both cognitively and behaviorally. Additionally, we articulate the\nkey findings derived from literature analysis, including literature gaps and\ntrends, human values, and interaction techniques. To pave the way for future\nstudies, we envision three key challenges and give recommendations for future\nresearch.\n'] , [""  The rationale of this work is based on the current user trust discourse of\nArtificial Intelligence (AI). We aim to produce novel HCI approaches that use\ntrust as a facilitator for the uptake (or appropriation) of current\ntechnologies. We propose a framework (HCTFrame) to guide non-experts to unlock\nthe full potential of user trust in AI design. Results derived from a data\ntriangulation of findings from three literature reviews demystify some\nmisconceptions of user trust in computer science and AI discourse, and three\ncase studies are conducted to assess the effectiveness of a psychometric scale\nin mapping potential users' trust breakdowns and concerns. This work primarily\ncontributes to the fight against the tendency to design technical-centered\nvulnerable interactions, which can eventually lead to additional real and\nperceived breaches of trust. The proposed framework can be used to guide system\ndesigners on how to map and define user trust and the socioethical and\norganisational needs and characteristics of AI system design. It can also guide\nAI system designers on how to develop a prototype and operationalise a solution\nthat meets user trust requirements. The article ends by providing some user\nresearch tools that can be employed to measure users' trust intentions and\nbehaviours towards a proposed solution.\n"", ""  Humans' trust in AI constitutes a pivotal element in fostering a synergistic\nrelationship between humans and AI. This is particularly significant in the\ncontext of systems that leverage AI technology, such as autonomous driving\nsystems and human-robot interaction. Trust facilitates appropriate utilization\nof these systems, thereby optimizing their potential benefits. If humans\nover-trust or under-trust an AI, serious problems such as misuse and accidents\noccur. To prevent over/under-trust, it is necessary to predict trust dynamics.\nHowever, trust is an internal state of humans and hard to directly observe.\nTherefore, we propose a prediction model for trust dynamics using dynamic\nstructure equation modeling, which extends SEM that can handle time-series\ndata. A path diagram, which shows causalities between variables, is developed\nin an exploratory way and the resultant path diagram is optimized for effective\npath structures. Over/under-trust was predicted with 90\\% accuracy in a drone\nsimulator task,, and it was predicted with 99\\% accuracy in an autonomous\ndriving task. These results show that our proposed method outperformed the\nconventional method including an auto regression family.\n"", ""  Trust is not just a cognitive issue but also an emotional one, yet the\nresearch in human-AI interactions has primarily focused on the cognitive route\nof trust development. Recent work has highlighted the importance of studying\naffective trust towards AI, especially in the context of emerging human-like\nLLMs-powered conversational agents. However, there is a lack of validated and\ngeneralizable measures for the two-dimensional construct of trust in AI agents.\nTo address this gap, we developed and validated a set of 27-item semantic\ndifferential scales for affective and cognitive trust through a scenario-based\nsurvey study. We then further validated and applied the scale through an\nexperiment study. Our empirical findings showed how the emotional and cognitive\naspects of trust interact with each other and collectively shape a person's\noverall trust in AI agents. Our study methodology and findings also provide\ninsights into the capability of the state-of-art LLMs to foster trust through\ndifferent routes.\n""] , ['  Increasing interest in ensuring safety of next-generation Artificial\nIntelligence (AI) systems calls for novel approaches to embedding morality into\nautonomous agents. Traditionally, this has been done by imposing explicit\ntop-down rules or hard constraints on systems, for example by filtering system\noutputs through pre-defined ethical rules. Recently, instead, entirely\nbottom-up methods for learning implicit preferences from human behavior have\nbecome increasingly popular, such as those for training and fine-tuning Large\nLanguage Models. In this paper, we provide a systematization of existing\napproaches to the problem of introducing morality in machines - modeled as a\ncontinuum, and argue that the majority of popular techniques lie at the\nextremes - either being fully hard-coded, or entirely learned, where no\nexplicit statement of any moral principle is required. Given the relative\nstrengths and weaknesses of each type of methodology, we argue that more hybrid\nsolutions are needed to create adaptable and robust, yet more controllable and\ninterpretable agents.\n  In particular, we present three case studies of recent works which use\nlearning from experience (i.e., Reinforcement Learning) to explicitly provide\nmoral principles to learning agents - either as intrinsic rewards, moral\nlogical constraints or textual principles for language models. For example,\nusing intrinsic rewards in Social Dilemma games, we demonstrate how it is\npossible to represent classical moral frameworks for agents. We also present an\noverview of the existing work in this area in order to provide empirical\nevidence for the potential of this hybrid approach. We then discuss strategies\nfor evaluating the effectiveness of moral learning agents. Finally, we present\nopen research questions and implications for the future of AI safety and ethics\nwhich are emerging from this framework.\n', '  With the rise of individual and collaborative networks of autonomous agents,\nAI is deployed in more key reasoning and decision-making roles. For this\nreason, ethics-based audits play a pivotal role in the rapidly growing fields\nof AI safety and regulation. This paper undertakes an ethics-based audit to\nprobe the 8 leading commercial and open-source Large Language Models including\nGPT-4. We assess explicability and trustworthiness by a) establishing how well\ndifferent models engage in moral reasoning and b) comparing normative values\nunderlying models as ethical frameworks. We employ an experimental,\nevidence-based approach that challenges the models with ethical dilemmas in\norder to probe human-AI alignment. The ethical scenarios are designed to\nrequire a decision in which the particulars of the situation may or may not\nnecessitate deviating from normative ethical principles. A sophisticated\nethical framework was consistently elicited in one model, GPT-4. Nonetheless,\ntroubling findings include underlying normative frameworks with clear bias\ntowards particular cultural norms. Many models also exhibit disturbing\nauthoritarian tendencies. Code is available at\nhttps://github.com/jonchun/llm-sota-chatbots-ethics-based-audit.\n', '  This study addresses ethical issues surrounding Large Language Models (LLMs)\nwithin the field of artificial intelligence. It explores the common ethical\nchallenges posed by both LLMs and other AI systems, such as privacy and\nfairness, as well as ethical challenges uniquely arising from LLMs. It\nhighlights challenges such as hallucination, verifiable accountability, and\ndecoding censorship complexity, which are unique to LLMs and distinct from\nthose encountered in traditional AI systems. The study underscores the need to\ntackle these complexities to ensure accountability, reduce biases, and enhance\ntransparency in the influential role that LLMs play in shaping information\ndissemination. It proposes mitigation strategies and future directions for LLM\nethics, advocating for interdisciplinary collaboration. It recommends ethical\nframeworks tailored to specific domains and dynamic auditing systems adapted to\ndiverse contexts. This roadmap aims to guide responsible development and\nintegration of LLMs, envisioning a future where ethical considerations govern\nAI advancements in society.\n'] , ['  Artificial intelligence (AI) is rapidly advancing in healthcare, enhancing\nthe efficiency and effectiveness of services across various specialties,\nincluding cardiology, ophthalmology, dermatology, emergency medicine, etc. AI\napplications have significantly improved diagnostic accuracy, treatment\npersonalization, and patient outcome predictions by leveraging technologies\nsuch as machine learning, neural networks, and natural language processing.\nHowever, these advancements also introduce substantial ethical and fairness\nchallenges, particularly related to biases in data and algorithms. These biases\ncan lead to disparities in healthcare delivery, affecting diagnostic accuracy\nand treatment outcomes across different demographic groups. This survey paper\nexamines the integration of AI in healthcare, highlighting critical challenges\nrelated to bias and exploring strategies for mitigation. We emphasize the\nnecessity of diverse datasets, fairness-aware algorithms, and regulatory\nframeworks to ensure equitable healthcare delivery. The paper concludes with\nrecommendations for future research, advocating for interdisciplinary\napproaches, transparency in AI decision-making, and the development of\ninnovative and inclusive AI applications.\n', '  Objectives: Leveraging artificial intelligence (AI) in conjunction with\nelectronic health records (EHRs) holds transformative potential to improve\nhealthcare. Yet, addressing bias in AI, which risks worsening healthcare\ndisparities, cannot be overlooked. This study reviews methods to detect and\nmitigate diverse forms of bias in AI models developed using EHR data. Methods:\nWe conducted a systematic review following the Preferred Reporting Items for\nSystematic Reviews and Meta-analyses (PRISMA) guidelines, analyzing articles\nfrom PubMed, Web of Science, and IEEE published between January 1, 2010, and\nDec 17, 2023. The review identified key biases, outlined strategies for\ndetecting and mitigating bias throughout the AI model development process, and\nanalyzed metrics for bias assessment. Results: Of the 450 articles retrieved,\n20 met our criteria, revealing six major bias types: algorithmic, confounding,\nimplicit, measurement, selection, and temporal. The AI models were primarily\ndeveloped for predictive tasks in healthcare settings. Four studies\nconcentrated on the detection of implicit and algorithmic biases employing\nfairness metrics like statistical parity, equal opportunity, and predictive\nequity. Sixty proposed various strategies for mitigating biases, especially\ntargeting implicit and selection biases. These strategies, evaluated through\nboth performance (e.g., accuracy, AUROC) and fairness metrics, predominantly\ninvolved data collection and preprocessing techniques like resampling,\nreweighting, and transformation. Discussion: This review highlights the varied\nand evolving nature of strategies to address bias in EHR-based AI models,\nemphasizing the urgent needs for the establishment of standardized,\ngeneralizable, and interpretable methodologies to foster the creation of\nethical AI systems that promote fairness and equity in healthcare.\n', '  The ethical integration of Artificial Intelligence (AI) in healthcare\nnecessitates addressing fairness-a concept that is highly context-specific\nacross medical fields. Extensive studies have been conducted to expand the\ntechnical components of AI fairness, while tremendous calls for AI fairness\nhave been raised from healthcare. Despite this, a significant disconnect\npersists between technical advancements and their practical clinical\napplications, resulting in a lack of contextualized discussion of AI fairness\nin clinical settings. Through a detailed evidence gap analysis, our review\nsystematically pinpoints several deficiencies concerning both healthcare data\nand the provided AI fairness solutions. We highlight the scarcity of research\non AI fairness in many medical domains where AI technology is increasingly\nutilized. Additionally, our analysis highlights a substantial reliance on group\nfairness, aiming to ensure equality among demographic groups from a macro\nhealthcare system perspective; in contrast, individual fairness, focusing on\nequity at a more granular level, is frequently overlooked. To bridge these\ngaps, our review advances actionable strategies for both the healthcare and AI\nresearch communities. Beyond applying existing AI fairness methods in\nhealthcare, we further emphasize the importance of involving healthcare\nprofessionals to refine AI fairness concepts and methods to ensure contextually\nrelevant and ethically sound AI applications in healthcare.\n']",Artificial Intelligence Ethics and Governance,"""Ethics in Artificial Intelligence and Autonomous Agents"""
85,"""Legal Case Retrieval and AI Applications"" , ""Patent Analysis and Innovation with AI and NLP"" , ""Autism Spectrum Disorder Diagnosis using AI""","['courts', 'judicial', 'court', 'legalsemi', 'lawyers', 'lawyer', 'law', 'legalai', 'retrieval', 'jurisdiction'] , ['patents', 'patenting', 'patent', 'patenteval', 'inventors', 'invention', 'semantic', 'inventions', 'citation', 'texts'] , ['autism', 'autistic', 'asd', 'speech', 'voice', 'neurodevelopment', 'diagnosing', 'developmental', 'linguistic', 'diagnostics']","['  Legal case retrieval aims to help legal workers find relevant cases related\nto their cases at hand, which is important for the guarantee of fairness and\njustice in legal judgments. While recent advances in neural retrieval methods\nhave significantly improved the performance of open-domain retrieval tasks\n(e.g., Web search), their advantages have not been observed in legal case\nretrieval due to their thirst for annotated data. As annotating large-scale\ntraining data in legal domains is prohibitive due to the need for domain\nexpertise, traditional search techniques based on lexical matching such as\nTF-IDF, BM25, and Query Likelihood are still prevalent in legal case retrieval\nsystems. While previous studies have designed several pre-training methods for\nIR models in open-domain tasks, these methods are usually suboptimal in legal\ncase retrieval because they cannot understand and capture the key knowledge and\ndata structures in the legal corpus. To this end, we propose a novel\npre-training framework named Caseformer that enables the pre-trained models to\nlearn legal knowledge and domain-specific relevance information in legal case\nretrieval without any human-labeled data. Through three unsupervised learning\ntasks, Caseformer is able to capture the special language, document structure,\nand relevance patterns of legal case documents, making it a strong backbone for\ndownstream legal case retrieval tasks. Experimental results show that our model\nhas achieved state-of-the-art performance in both zero-shot and full-data\nfine-tuning settings. Also, experiments on both Chinese and English legal\ndatasets demonstrate that the effectiveness of Caseformer is\nlanguage-independent in legal case retrieval.\n', '  General and legal domain LLMs have demonstrated strong performance in various\ntasks of LegalAI. However, the current evaluations of these LLMs in LegalAI are\ndefined by the experts of computer science, lacking consistency with the logic\nof legal practice, making it difficult to judge their practical capabilities.\nTo address this challenge, we are the first to build the Chinese legal LLMs\nbenchmark LAiW, based on the logic of legal practice. To align with the\nthinking process of legal experts and legal practice (syllogism), we divide the\nlegal capabilities of LLMs from easy to difficult into three levels: basic\ninformation retrieval, legal foundation inference, and complex legal\napplication. Each level contains multiple tasks to ensure a comprehensive\nevaluation. Through automated evaluation of current general and legal domain\nLLMs on our benchmark, we indicate that these LLMs may not align with the logic\nof legal practice. LLMs seem to be able to directly acquire complex legal\napplication capabilities but perform poorly in some basic tasks, which may pose\nobstacles to their practical application and acceptance by legal experts. To\nfurther confirm the complex legal application capabilities of current LLMs in\nlegal application scenarios, we also incorporate human evaluation with legal\nexperts. The results indicate that while LLMs may demonstrate strong\nperformance, they still require reinforcement of legal logic.\n', '  Recent advances in Large Language Models (LLMs) have significantly shaped the\napplications of AI in multiple fields, including the studies of legal\nintelligence. Trained on extensive legal texts, including statutes and legal\ndocuments, the legal LLMs can capture important legal knowledge/concepts\neffectively and provide important support for downstream legal applications\nsuch as legal consultancy. Yet, the dynamic nature of legal statutes and\ninterpretations also poses new challenges to the use of LLMs in legal\napplications. Particularly, how to update the legal knowledge of LLMs\neffectively and efficiently has become an important research problem in\npractice. Existing benchmarks for evaluating knowledge update methods are\nmostly designed for the open domain and cannot address the specific challenges\nof the legal domain, such as the nuanced application of new legal knowledge,\nthe complexity and lengthiness of legal regulations, and the intricate nature\nof legal reasoning. To address this gap, we introduce the Legal Knowledge\nUpdate BEnchmark, i.e. LeKUBE, which evaluates knowledge update methods for\nlegal LLMs across five dimensions. Specifically, we categorize the needs of\nknowledge updates in the legal domain with the help of legal professionals, and\nthen hire annotators from law schools to create synthetic updates to the\nChinese Criminal and Civil Code as well as sets of questions of which the\nanswers would change after the updates. Through a comprehensive evaluation of\nstate-of-the-art knowledge update methods, we reveal a notable gap between\nexisting knowledge update methods and the unique needs of the legal domain,\nemphasizing the need for further research and development of knowledge update\nmechanisms tailored for legal LLMs.\n'] , ['  Recent advancements in Artificial Intelligence (AI) and machine learning have\ndemonstrated transformative capabilities across diverse domains. This progress\nextends to the field of patent analysis and innovation, where AI-based tools\npresent opportunities to streamline and enhance important tasks in the patent\ncycle such as classification, retrieval, and valuation prediction. This not\nonly accelerates the efficiency of patent researchers and applicants but also\nopens new avenues for technological innovation and discovery. Our survey\nprovides a comprehensive summary of recent AI tools in patent analysis from\nmore than 40 papers from 26 venues between 2017 and 2023. Unlike existing\nsurveys, we include methods that work for patent image and text data.\nFurthermore, we introduce a novel taxonomy for the categorization based on the\ntasks in the patent life cycle as well as the specifics of the AI methods. This\ninterdisciplinary survey aims to serve as a resource for researchers and\npractitioners who are working at the intersection of AI and patent analysis as\nwell as the patent offices that are aiming to build efficient patent systems.\n', ""  This paper makes two contributions to the field of text-based patent\nsimilarity. First, it compares the performance of different kinds of\npatent-specific pretrained embedding models, namely static word embeddings\n(such as word2vec and doc2vec models) and contextual word embeddings (such as\ntransformers based models), on the task of patent similarity calculation.\nSecond, it compares specifically the performance of Sentence Transformers\n(SBERT) architectures with different training phases on the patent similarity\ntask. To assess the models' performance, we use information about patent\ninterferences, a phenomenon in which two or more patent claims belonging to\ndifferent patent applications are proven to be overlapping by patent examiners.\nTherefore, we use these interferences cases as a proxy for maximum similarity\nbetween two patents, treating them as ground-truth to evaluate the performance\nof the different embedding models. Our results point out that, first, Patent\nSBERT-adapt-ub, the domain adaptation of the pretrained Sentence Transformer\narchitecture proposed in this research, outperforms the current\nstate-of-the-art in patent similarity. Second, they show that, in some cases,\nlarge static models performances are still comparable to contextual ones when\ntrained on extensive data; thus, we believe that the superiority in the\nperformance of contextual embeddings may not be related to the actual\narchitecture but rather to the way the training phase is performed.\n"", '  Patents, encapsulating crucial technical and legal information, present a\nrich domain for natural language processing (NLP) applications. As NLP\ntechnologies evolve, large language models (LLMs) have demonstrated outstanding\ncapabilities in general text processing and generation tasks. However, the\napplication of LLMs in the patent domain remains under-explored and\nunder-developed due to the complexity of patent processing. Understanding the\nunique characteristics of patent documents and related research in the patent\ndomain becomes essential for researchers to apply these tools effectively.\nTherefore, this paper aims to equip NLP researchers with the essential\nknowledge to navigate this complex domain efficiently. We introduce the\nrelevant fundamental aspects of patents to provide solid background\ninformation, particularly for readers unfamiliar with the patent system. In\naddition, we systematically break down the structural and linguistic\ncharacteristics unique to patents and map out how NLP can be leveraged for\npatent analysis and generation. Moreover, we demonstrate the spectrum of\ntext-based patent-related tasks, including nine patent analysis and four patent\ngeneration tasks.\n'] , ['  Autism Spectrum Disorder (ASD) represents a multifaceted neurodevelopmental\ncondition marked by difficulties in social interaction, communication\nimpediments, and repetitive behaviors. Despite progress in understanding ASD,\nits diagnosis and treatment continue to pose significant challenges due to the\nvariability in symptomatology and the necessity for multidisciplinary care\napproaches. This paper investigates the potential of Artificial Intelligence\n(AI) to augment the capabilities of healthcare professionals and caregivers in\nmanaging ASD. We have developed a sophisticated algorithm designed to analyze\nfacial and bodily expressions during daily activities of both autistic and\nnon-autistic children, leading to the development of a powerful deep\nlearning-based autism detection system. Our study demonstrated that AI models,\nspecifically the Xception and ResNet50V2 architectures, achieved high accuracy\nin diagnosing Autism Spectrum Disorder (ASD). This research highlights the\ntransformative potential of AI in improving the diagnosis, treatment, and\ncomprehensive management of ASD. Our study revealed that AI models, notably the\nXception and ResNet50V2 architectures, demonstrated high accuracy in diagnosing\nASD.\n', ""  Purpose: Our study explored the use of artificial intelligence (AI) to\ndiagnose autism spectrum disorder (ASD). It focused on machine learning (ML)\nand deep learning (DL) to detect ASD from text inputs on social media,\naddressing challenges in traditional ASD diagnosis.\n  Methods: We used natural language processing (NLP), ML, and DL models\n(including decision trees, XGB, KNN, RNN, LSTM, Bi-LSTM, BERT, and BERTweet) to\nanalyze 404,627 tweets, classifying them based on ASD or non-ASD authors. A\nsubset of 90,000 tweets was used for model training and testing.\n  Results: Our AI models showed high accuracy, with an 88% success rate in\nidentifying texts from individuals with ASD.\n  Conclusion: The study demonstrates AI's potential in improving ASD diagnosis,\nespecially in children, highlighting the importance of early detection.\n"", ""  The diagnosis of autism spectrum disorder (ASD) is a complex, challenging\ntask as it depends on the analysis of interactional behaviors by psychologists\nrather than the use of biochemical diagnostics. In this paper, we present a\nmodeling approach to ASD diagnosis by analyzing acoustic/prosodic and\nlinguistic features extracted from diagnostic conversations between a\npsychologist and children who either are typically developing (TD) or have ASD.\nWe compare the contributions of different features across a range of\nconversation tasks. We focus on finding a minimal set of parameters that\ncharacterize conversational behaviors of children with ASD. Because ASD is\ndiagnosed through conversational interaction, in addition to analyzing the\nbehavior of the children, we also investigate whether the psychologist's\nconversational behaviors vary across diagnostic groups. Our results can\nfacilitate fine-grained analysis of conversation data for children with ASD to\nsupport diagnosis and intervention.\n""]","Applications of Artificial Intelligence in Law, Patent Analysis, and Healthcare","""Patent Analysis and Innovation with AI and NLP"""
86,"""Music Generation and Editing with AI"" , ""Assessing Creativity in Artificial Intelligence"" , ""Copyright Infringement in Generative AI"" , ""Mitigating Unsafe Content in AI-Generated Media"" , ""AI-Generated Media and Its Societal Impact""","['musicgen', 'music', 'midi', 'musicrl', 'songs', 'musical', 'genres', 'musicians', 'melodies', 'composers'] , ['creativity', 'creative', 'generative', 'ai', 'ideation', 'storymaking', 'creation', 'creators', 'generated', 'writers'] , ['copyrightability', 'copyright', 'copyrights', 'copyrighted', 'infringing', 'infringement', 'copying', 'creators', 'ai', 'generative'] , ['safegen', 'unsafe', 'malicious', 'safety', 'protecting', 'prevent', 'redacts', 'images', 'videos', 'content'] , ['ai', 'artmaking', 'deviantart', 'creators', 'artistic', 'artists', 'arts', 'copyright', 'imagery', 'media']","['  Recent advances in text-to-music generation models have opened new avenues in\nmusical creativity. However, music generation usually involves iterative\nrefinements, and how to edit the generated music remains a significant\nchallenge. This paper introduces a novel approach to the editing of music\ngenerated by such models, enabling the modification of specific attributes,\nsuch as genre, mood and instrument, while maintaining other aspects unchanged.\nOur method transforms text editing to \\textit{latent space manipulation} while\nadding an extra constraint to enforce consistency. It seamlessly integrates\nwith existing pretrained text-to-music diffusion models without requiring\nadditional training. Experimental results demonstrate superior performance over\nboth zero-shot and certain supervised baselines in style and timbre transfer\nevaluations. Additionally, we showcase the practical applicability of our\napproach in real-world music editing scenarios.\n', ""  Symbolic Music, akin to language, can be encoded in discrete symbols. Recent\nresearch has extended the application of large language models (LLMs) such as\nGPT-4 and Llama2 to the symbolic music domain including understanding and\ngeneration. Yet scant research explores the details of how these LLMs perform\non advanced music understanding and conditioned generation, especially from the\nmulti-step reasoning perspective, which is a critical aspect in the\nconditioned, editable, and interactive human-computer co-creation process. This\nstudy conducts a thorough investigation of LLMs' capability and limitations in\nsymbolic music processing. We identify that current LLMs exhibit poor\nperformance in song-level multi-step music reasoning, and typically fail to\nleverage learned music knowledge when addressing complex musical tasks. An\nanalysis of LLMs' responses highlights distinctly their pros and cons. Our\nfindings suggest achieving advanced musical capability is not intrinsically\nobtained by LLMs, and future research should focus more on bridging the gap\nbetween music knowledge and reasoning, to improve the co-creation experience\nfor musicians.\n"", '  Controllable music generation plays a vital role in human-AI music\nco-creation. While Large Language Models (LLMs) have shown promise in\ngenerating high-quality music, their focus on autoregressive generation limits\ntheir utility in music editing tasks. To address this gap, we propose a novel\napproach leveraging a parameter-efficient heterogeneous adapter combined with a\nmasking training scheme. This approach enables autoregressive language models\nto seamlessly address music inpainting tasks. Additionally, our method\nintegrates frame-level content-based controls, facilitating track-conditioned\nmusic refinement and score-conditioned music arrangement. We apply this method\nto fine-tune MusicGen, a leading autoregressive music generation model. Our\nexperiments demonstrate promising results across multiple music editing tasks,\noffering more flexible controls for future AI-driven music editing tools. The\nsource codes and a demo page showcasing our work are available at\nhttps://kikyo-16.github.io/AIR.\n'] , ['  What constitutes human creativity, and is it possible for computers to\nexhibit genuine creativity? We argue that achieving human-level intelligence in\ncomputers, or so-called Artificial General Intelligence, necessitates attaining\nalso human-level creativity. We contribute to this discussion by developing a\nstatistical representation of human creativity, incorporating prior insights\nfrom stochastic theory, psychology, philosophy, neuroscience, and chaos theory.\nThis highlights the stochastic nature of the human creative process, which\nincludes both a bias guided, random proposal step, and an evaluation step\ndepending on a flexible or transformable bias structure. The acquired\nrepresentation of human creativity is subsequently used to assess the\ncreativity levels of various contemporary AI systems. Our analysis includes\nmodern AI algorithms such as reinforcement learning, diffusion models, and\nlarge language models, addressing to what extent they measure up to human level\ncreativity. We conclude that these technologies currently lack the capability\nfor autonomous creative action at a human level.\n', ""  In the field of natural language processing, the rapid development of large\nlanguage model (LLM) has attracted more and more attention. LLMs have shown a\nhigh level of creativity in various tasks, but the methods for assessing such\ncreativity are inadequate. The assessment of LLM creativity needs to consider\ndifferences from humans, requiring multi-dimensional measurement while\nbalancing accuracy and efficiency. This paper aims to establish an efficient\nframework for assessing the level of creativity in LLMs. By adapting the\nmodified Torrance Tests of Creative Thinking, the research evaluates the\ncreative performance of various LLMs across 7 tasks, emphasizing 4 criteria\nincluding Fluency, Flexibility, Originality, and Elaboration. In this context,\nwe develop a comprehensive dataset of 700 questions for testing and an\nLLM-based evaluation method. In addition, this study presents a novel analysis\nof LLMs' responses to diverse prompts and role-play situations. We found that\nthe creativity of LLMs primarily falls short in originality, while excelling in\nelaboration. Besides, the use of prompts and the role-play settings of the\nmodel significantly influence creativity. Additionally, the experimental\nresults also indicate that collaboration among multiple LLMs can enhance\noriginality. Notably, our findings reveal a consensus between human evaluations\nand LLMs regarding the personality traits that influence creativity. The\nfindings underscore the significant impact of LLM design on creativity and\nbridges artificial intelligence and human creativity, offering insights into\nLLMs' creativity and potential applications.\n"", ""  Creativity serves as a cornerstone for societal progress and innovation. With\nthe rise of advanced generative AI models capable of tasks once reserved for\nhuman creativity, the study of AI's creative potential becomes imperative for\nits responsible development and application. In this paper, we prove in theory\nthat AI can be as creative as humans under the condition that it can properly\nfit the data generated by human creators. Therefore, the debate on AI's\ncreativity is reduced into the question of its ability to fit a sufficient\namount of data. To arrive at this conclusion, this paper first addresses the\ncomplexities in defining creativity by introducing a new concept called\nRelative Creativity. Rather than attempting to define creativity universally,\nwe shift the focus to whether AI can match the creative abilities of a\nhypothetical human. The methodological shift leads to a statistically\nquantifiable assessment of AI's creativity, term Statistical Creativity. This\nconcept, statistically comparing the creative abilities of AI with those of\nspecific human groups, facilitates theoretical exploration of AI's creative\npotential. Our analysis reveals that by fitting extensive conditional data\nwithout marginalizing out the generative conditions, AI can emerge as a\nhypothetical new creator. The creator possesses the same creative abilities on\npar with the human creators it was trained on. Building on theoretical\nfindings, we discuss the application in prompt-conditioned autoregressive\nmodels, providing a practical means for evaluating creative abilities of\ngenerative AI models, such as Large Language Models (LLMs). Additionally, this\nstudy provides an actionable training guideline, bridging the theoretical\nquantification of creativity with practical model training.\n""] , ['  In the rapidly evolving landscape of generative artificial intelligence (AI),\nthe increasingly pertinent issue of copyright infringement arises as AI\nadvances to generate content from scraped copyrighted data, prompting questions\nabout ownership and protection that impact professionals across various\ncareers. With this in mind, this survey provides an extensive examination of\ncopyright infringement as it pertains to generative AI, aiming to stay abreast\nof the latest developments and open problems. Specifically, it will first\noutline methods of detecting copyright infringement in mediums such as text,\nimage, and video. Next, it will delve an exploration of existing techniques\naimed at safeguarding copyrighted works from generative models. Furthermore,\nthis survey will discuss resources and tools for users to evaluate copyright\nviolations. Finally, insights into ongoing regulations and proposals for AI\nwill be explored and compared. Through combining these disciplines, the\nimplications of AI-driven content and copyright are thoroughly illustrated and\nbrought into question.\n', '  This paper addresses the contentious issue of copyright infringement in\nimages generated by text-to-image models, sparking debates among AI developers,\ncontent creators, and legal entities. State-of-the-art models create\nhigh-quality content without crediting original creators, causing concern in\nthe artistic community. To mitigate this, we propose the \\copyright Plug-in\nAuthorization framework, introducing three operations: addition, extraction,\nand combination. Addition involves training a \\copyright plug-in for specific\ncopyright, facilitating proper credit attribution. Extraction allows creators\nto reclaim copyright from infringing models, and combination enables users to\nmerge different \\copyright plug-ins. These operations act as permits,\nincentivizing fair use and providing flexibility in authorization. We present\ninnovative approaches,""Reverse LoRA"" for extraction and ""EasyMerge"" for\nseamless combination. Experiments in artist-style replication and cartoon IP\nrecreation demonstrate \\copyright plug-ins\' effectiveness, offering a valuable\nsolution for human copyright protection in the age of generative AIs.\n', '  The advent of Generative Artificial Intelligence (GenAI) models, including\nGitHub Copilot, OpenAI GPT, and Stable Diffusion, has revolutionized content\ncreation, enabling non-professionals to produce high-quality content across\nvarious domains. This transformative technology has led to a surge of synthetic\ncontent and sparked legal disputes over copyright infringement. To address\nthese challenges, this paper introduces a novel approach that leverages the\nlearning capacity of GenAI models for copyright legal analysis, demonstrated\nwith GPT2 and Stable Diffusion models. Copyright law distinguishes between\noriginal expressions and generic ones (Sc\\`enes \\`a faire), protecting the\nformer and permitting reproduction of the latter. However, this distinction has\nhistorically been challenging to make consistently, leading to over-protection\nof copyrighted works. GenAI offers an unprecedented opportunity to enhance this\nlegal analysis by revealing shared patterns in preexisting works. We propose a\ndata-driven approach to identify the genericity of works created by GenAI,\nemploying ""data-driven bias"" to assess the genericity of expressive\ncompositions. This approach aids in copyright scope determination by utilizing\nthe capabilities of GenAI to identify and prioritize expressive elements and\nrank them according to their frequency in the model\'s dataset. The potential\nimplications of measuring expressive genericity for copyright law are profound.\nSuch scoring could assist courts in determining copyright scope during\nlitigation, inform the registration practices of Copyright Offices, allowing\nregistration of only highly original synthetic works, and help copyright owners\nsignal the value of their works and facilitate fairer licensing deals. More\ngenerally, this approach offers valuable insights to policymakers grappling\nwith adapting copyright law to the challenges posed by the era of GenAI.\n'] , ['  Large-scale vision-and-language models, such as CLIP, are typically trained\non web-scale data, which can introduce inappropriate content and lead to the\ndevelopment of unsafe and biased behavior. This, in turn, hampers their\napplicability in sensitive and trustworthy contexts and could raise significant\nconcerns in their adoption. Our research introduces a novel approach to\nenhancing the safety of vision-and-language models by diminishing their\nsensitivity to NSFW (not safe for work) inputs. In particular, our methodology\nseeks to sever ""toxic"" linguistic and visual concepts, unlearning the linkage\nbetween unsafe linguistic or visual items and unsafe regions of the embedding\nspace. We show how this can be done by fine-tuning a CLIP model on synthetic\ndata obtained from a large language model trained to convert between safe and\nunsafe sentences, and a text-to-image generator. We conduct extensive\nexperiments on the resulting embedding space for cross-modal retrieval,\ntext-to-image, and image-to-text generation, where we show that our model can\nbe remarkably employed with pre-trained generative models. Our source code and\ntrained models are available at: https://github.com/aimagelab/safe-clip.\n', ""  Text-to-image (T2I) models, such as Stable Diffusion, have exhibited\nremarkable performance in generating high-quality images from text descriptions\nin recent years. However, text-to-image models may be tricked into generating\nnot-safe-for-work (NSFW) content, particularly in sexual scenarios. Existing\ncountermeasures mostly focus on filtering inappropriate inputs and outputs, or\nsuppressing improper text embeddings, which can block explicit NSFW-related\ncontent (e.g., naked or sexy) but may still be vulnerable to adversarial\nprompts inputs that appear innocent but are ill-intended. In this paper, we\npresent SafeGen, a framework to mitigate unsafe content generation by\ntext-to-image models in a text-agnostic manner. The key idea is to eliminate\nunsafe visual representations from the model regardless of the text input. In\nthis way, the text-to-image model is resistant to adversarial prompts since\nunsafe visual representations are obstructed from within. Extensive experiments\nconducted on four datasets demonstrate SafeGen's effectiveness in mitigating\nunsafe content generation while preserving the high-fidelity of benign images.\nSafeGen outperforms eight state-of-the-art baseline methods and achieves 99.1%\nsexual content removal performance. Furthermore, our constructed benchmark of\nadversarial prompts provides a basis for future development and evaluation of\nanti-NSFW-generation methods.\n"", ""  Video generation models (VGMs) have demonstrated the capability to synthesize\nhigh-quality output. It is important to understand their potential to produce\nunsafe content, such as violent or terrifying videos. In this work, we provide\na comprehensive understanding of unsafe video generation.\n  First, to confirm the possibility that these models could indeed generate\nunsafe videos, we choose unsafe content generation prompts collected from 4chan\nand Lexica, and three open-source SOTA VGMs to generate unsafe videos. After\nfiltering out duplicates and poorly generated content, we created an initial\nset of 2112 unsafe videos from an original pool of 5607 videos. Through\nclustering and thematic coding analysis of these generated videos, we identify\n5 unsafe video categories: Distorted/Weird, Terrifying, Pornographic,\nViolent/Bloody, and Political. With IRB approval, we then recruit online\nparticipants to help label the generated videos. Based on the annotations\nsubmitted by 403 participants, we identified 937 unsafe videos from the initial\nvideo set. With the labeled information and the corresponding prompts, we\ncreated the first dataset of unsafe videos generated by VGMs.\n  We then study possible defense mechanisms to prevent the generation of unsafe\nvideos. Existing defense methods in image generation focus on filtering either\ninput prompt or output results. We propose a new approach called Latent\nVariable Defense (LVD), which works within the model's internal sampling\nprocess. LVD can achieve 0.90 defense accuracy while reducing time and\ncomputing resources by 10x when sampling a large number of unsafe prompts.\n""] , [""  Text-to-video generative AI models such as Sora OpenAI have the potential to\ndisrupt multiple industries. In this paper, we report a qualitative social\nmedia analysis aiming to uncover people's perceived impact of and concerns\nabout Sora's integration. We collected and analyzed comments (N=292) under\npopular posts about Sora-generated videos, comparison between Sora videos and\nMidjourney images, and artists' complaints about copyright infringement by\nGenerative AI. We found that people were most concerned about Sora's impact on\ncontent creation-related industries. Emerging governance challenges included\nthe for-profit nature of OpenAI, the blurred boundaries between real and fake\ncontent, human autonomy, data privacy, copyright issues, and environmental\nimpact. Potential regulatory solutions proposed by people included law-enforced\nlabeling of AI content and AI literacy education for the public. Based on the\nfindings, we discuss the importance of gauging people's tech perceptions early\nand propose policy recommendations to regulate Sora before its public release.\n"", ""  2023 was the year the world woke up to generative AI, and 2024 is the year\npolicymakers are responding more firmly. Importantly, this policy momentum is\ntaking place alongside real world creation and distribution of synthetic media.\nSocial media platforms, news organizations, dating apps, image generation\ncompanies, and more are already navigating a world of AI-generated visuals and\nsounds, already changing hearts and minds, as policymakers try to catch up.\nHow, then, can AI governance capture the complexity of the synthetic media\nlandscape? How can it attend to synthetic media's myriad uses, ranging from\nstorytelling to privacy preservation, to deception, fraud, and defamation,\ntaking into account the many stakeholders involved in its development,\ncreation, and distribution? And what might it mean to govern synthetic media in\na manner that upholds the truth while bolstering freedom of expression? What\nfollows is the first known collection of diverse examples of the implementation\nof synthetic media governance that responds to these questions, specifically\nthrough Partnership on AI's (PAI) Responsible Practices for Synthetic Media - a\nvoluntary, normative Framework for creating, distributing, and building\ntechnology for synthetic media responsibly, launched in February 2023. In this\npaper, we present a case bank of real world examples that help operationalize\nthe Framework - highlighting areas synthetic media governance can be applied,\naugmented, expanded, and refined for use, in practice. Read together, the cases\nemphasize distinct elements of AI policymaking and seven emergent best\npractices supporting transparency, safety, expression, and digital dignity\nonline: consent, disclosure, and differentiation between harmful and creative\nuse cases.\n"", '  The article presents some current observations (as of April 10, 2024) on the\nintegration of AI-generated images within processes of media convergence. It\ndraws on two different concepts of intermediality. Primary intermediality\nconcepts are motivated by the object when a new type of technology develops the\npotential to become socially relevant as a media form and thus a socially,\npolitically, or culturally important communicative factor. Due to their\nuncertain \'measurements\' within the wider media ecology, however, the new,\nstill potential media form appears hybrid. The ""inter-"" or ""between-"" of this\ninitial intermediality moment thus refers to the questionable ""site"" and the\nquestionable description of the potential media form between already existing\ntechnologies and cultural forms and their conceptual measurements. For\nsecondary concepts of intermediality, in contrast, it can be assumed that the\nboundaries of media forms and their application have already been drawn and are\nreasonably undisputed. This then raises the question of intentional and staged\nreferences to AI imagery within other media forms and pictures. The article\ndiscusses indicators of both intermediality moments using current examples and\ncontroversies surrounding AI images. The thesis is that there can be no talk of\na seamless \'integration\' of AI images into the wider media landscape at the\nmoment (within films, comic books, or video games, for example) - as one of\ncountless other image production techniques - and that the medial \'site\' of AI\nimage circulation - at least where it is not a matter of deception, but rather\ntheir conscious use as AI images - especially in social media communication and\nin fan cultures, but with repercussions for the more general media ecology and\nimage interpretation, insofar as the suspicion that an image could be\nAI-generated is now increasingly present as a ""hermeneutics of suspicion"".\n']",Artificial Intelligence in Creative Industries,"""AI-Generated Media and Its Societal Impact"""
87,"Multimodal Learning and Fusion , Multimodal Medical Image Fusion Techniques , Audio-Visual and Multimodal Learning Models , Multimodal Learning and Fusion","['multimodal', 'modality', 'modal', 'supervised', 'unlearning', 'embeddings', 'unimodal', 'modalities', 'learning', 'learnt'] , ['multimodal', 'fusioninn', 'fusion', 'multinpe', 'cnn', 'fused', 'modality', 'multifix', 'deep', 'saliency'] , ['audiovisual', 'audio', 'audioldm', 'audiocaps', 'generative', 'recordings', 'sound', 'music', 'visual', 'videos'] , ['multimodal', 'modality', 'audio', 'videos', 'nonverbal', 'modal', 'recognition', 'fusionnets', 'supervised', 'decoder']","['  Multimodal fusion focuses on integrating information from multiple modalities\nwith the goal of more accurate prediction, which has achieved remarkable\nprogress in a wide range of scenarios, including autonomous driving and medical\ndiagnosis. However, the reliability of multimodal fusion remains largely\nunexplored especially under low-quality data settings. This paper surveys the\ncommon challenges and recent advances of multimodal fusion in the wild and\npresents them in a comprehensive taxonomy. From a data-centric view, we\nidentify four main challenges that are faced by multimodal fusion on\nlow-quality data, namely (1) noisy multimodal data that are contaminated with\nheterogeneous noises, (2) incomplete multimodal data that some modalities are\nmissing, (3) imbalanced multimodal data that the qualities or properties of\ndifferent modalities are significantly different and (4) quality-varying\nmultimodal data that the quality of each modality dynamically changes with\nrespect to different samples. This new taxonomy will enable researchers to\nunderstand the state of the field and identify several potential directions. We\nalso provide discussion for the open problems in this field together with\ninteresting future research directions.\n', '  Multimodal learning seeks to utilize data from multiple sources to improve\nthe overall performance of downstream tasks. It is desirable for redundancies\nin the data to make multimodal systems robust to missing or corrupted\nobservations in some correlated modalities. However, we observe that the\nperformance of several existing multimodal networks significantly deteriorates\nif one or multiple modalities are absent at test time. To enable robustness to\nmissing modalities, we propose a simple and parameter-efficient adaptation\nprocedure for pretrained multimodal networks. In particular, we exploit\nmodulation of intermediate features to compensate for the missing modalities.\nWe demonstrate that such adaptation can partially bridge performance drop due\nto missing modalities and outperform independent, dedicated networks trained\nfor the available modality combinations in some cases. The proposed adaptation\nrequires extremely small number of parameters (e.g., fewer than 1% of the total\nparameters) and applicable to a wide range of modality combinations and tasks.\nWe conduct a series of experiments to highlight the missing modality robustness\nof our proposed method on five different multimodal tasks across seven\ndatasets. Our proposed method demonstrates versatility across various tasks and\ndatasets, and outperforms existing methods for robust multimodal learning with\nmissing modalities.\n', '  Multimodal learning typically relies on the assumption that all modalities\nare fully available during both the training and inference phases. However, in\nreal-world scenarios, consistently acquiring complete multimodal data presents\nsignificant challenges due to various factors. This often leads to the issue of\nmissing modalities, where data for certain modalities are absent, posing\nconsiderable obstacles not only for the availability of multimodal pretrained\nmodels but also for their fine-tuning and the preservation of robustness in\ndownstream tasks. To address these challenges, we propose a novel framework\nintegrating parameter-efficient fine-tuning of unimodal pretrained models with\na self-supervised joint-embedding learning method. This framework enables the\nmodel to predict the embedding of a missing modality in the representation\nspace during inference. Our method effectively predicts the missing embedding\nthrough prompt tuning, leveraging information from available modalities. We\nevaluate our approach on several multimodal benchmark datasets and demonstrate\nits effectiveness and robustness across various scenarios of missing\nmodalities.\n'] , ['  Multi-modal fusion is crucial in medical data research, enabling a\ncomprehensive understanding of diseases and improving diagnostic performance by\ncombining diverse modalities. However, multi-modal fusion faces challenges,\nincluding capturing interactions between modalities, addressing missing\nmodalities, handling erroneous modal information, and ensuring\ninterpretability. Many existing researchers tend to design different solutions\nfor these problems, often overlooking the commonalities among them. This paper\nproposes a novel multi-modal fusion framework that achieves adaptive adjustment\nover the weights of each modality by introducing the Modal-Domain Attention\n(MDA). It aims to facilitate the fusion of multi-modal information while\nallowing for the inclusion of missing modalities or intrinsic noise, thereby\nenhancing the representation of multi-modal data. We provide visualizations of\naccuracy changes and MDA weights by observing the process of modal fusion,\noffering a comprehensive analysis of its interpretability. Extensive\nexperiments on various gastrointestinal disease benchmarks, the proposed MDA\nmaintains high accuracy even in the presence of missing modalities and\nintrinsic noise. One thing worth mentioning is that the visualization of MDA is\nhighly consistent with the conclusions of existing clinical studies on the\ndependence of different diseases on various modalities. Code and dataset will\nbe made available.\n', '  Image fusion typically employs non-invertible neural networks to merge\nmultiple source images into a single fused image. However, for clinical\nexperts, solely relying on fused images may be insufficient for making\ndiagnostic decisions, as the fusion mechanism blends features from source\nimages, thereby making it difficult to interpret the underlying tumor\npathology. We introduce FusionINN, a novel decomposable image fusion framework,\ncapable of efficiently generating fused images and also decomposing them back\nto the source images. FusionINN is designed to be bijective by including a\nlatent image alongside the fused image, while ensuring minimal transfer of\ninformation from the source images to the latent representation. To the best of\nour knowledge, we are the first to investigate the decomposability of fused\nimages, which is particularly crucial for life-sensitive applications such as\nmedical image fusion compared to other tasks like multi-focus or multi-exposure\nimage fusion. Our extensive experimentation validates FusionINN over existing\ndiscriminative and generative fusion methods, both subjectively and\nobjectively. Moreover, compared to a recent denoising diffusion-based fusion\nmodel, our approach offers faster and qualitatively better fusion results.\n', '  Multimodal medical imaging plays a pivotal role in clinical diagnosis and\nresearch, as it combines information from various imaging modalities to provide\na more comprehensive understanding of the underlying pathology. Recently, deep\nlearning-based multimodal fusion techniques have emerged as powerful tools for\nimproving medical image classification. This review offers a thorough analysis\nof the developments in deep learning-based multimodal fusion for medical\nclassification tasks. We explore the complementary relationships among\nprevalent clinical modalities and outline three main fusion schemes for\nmultimodal classification networks: input fusion, intermediate fusion\n(encompassing single-level fusion, hierarchical fusion, and attention-based\nfusion), and output fusion. By evaluating the performance of these fusion\ntechniques, we provide insight into the suitability of different network\narchitectures for various multimodal fusion scenarios and application domains.\nFurthermore, we delve into challenges related to network architecture\nselection, handling incomplete multimodal data management, and the potential\nlimitations of multimodal fusion. Finally, we spotlight the promising future of\nTransformer-based multimodal fusion techniques and give recommendations for\nfuture research in this rapidly evolving field.\n'] , ['  This technical report details our work towards building an enhanced\naudio-visual sound event localization and detection (SELD) network. We build on\ntop of the audio-only SELDnet23 model and adapt it to be audio-visual by\nmerging both audio and video information prior to the gated recurrent unit\n(GRU) of the audio-only network. Our model leverages YOLO and DETIC object\ndetectors. We also build a framework that implements audio-visual data\naugmentation and audio-visual synthetic data generation. We deliver an\naudio-visual SELDnet system that outperforms the existing audio-visual SELD\nbaseline.\n', '  Generative Pre-trained Transformer (GPT) models have achieved remarkable\nperformance on various natural language processing tasks, and have shown great\npotential as backbones for audio-and-text large language models (LLMs).\nPrevious mainstream audio-and-text LLMs use discrete audio tokens to represent\nboth input and output audio; however, they suffer from performance degradation\non tasks such as automatic speech recognition, speech-to-text translation, and\nspeech enhancement over models using continuous speech features. In this paper,\nwe propose LauraGPT, a novel unified audio-and-text GPT-based LLM for audio\nrecognition, understanding, and generation. LauraGPT is a versatile LLM that\ncan process both audio and text inputs and generate outputs in either\nmodalities. We propose a novel data representation that combines continuous and\ndiscrete features for audio: LauraGPT encodes input audio into continuous\nrepresentations using an audio encoder and generates output audio from discrete\ncodec codes. We propose a one-step codec vocoder to overcome the prediction\nchallenge caused by the multimodal distribution of codec tokens. We fine-tune\nLauraGPT using supervised multi-task learning. Extensive experiments show that\nLauraGPT consistently achieves comparable to superior performance compared to\nstrong baselines on a wide range of audio tasks related to content, semantics,\nparalinguistics, and audio-signal analysis, such as automatic speech\nrecognition, speech-to-text translation, text-to-speech synthesis, speech\nenhancement, automated audio captioning, speech emotion recognition, and spoken\nlanguage understanding.\n', '  We introduce C3LLM (Conditioned-on-Three-Modalities Large Language Models), a\nnovel framework combining three tasks of video-to-audio, audio-to-text, and\ntext-to-audio together. C3LLM adapts the Large Language Model (LLM) structure\nas a bridge for aligning different modalities, synthesizing the given\nconditional information, and making multimodal generation in a discrete manner.\nOur contributions are as follows. First, we adapt a hierarchical structure for\naudio generation tasks with pre-trained audio codebooks. Specifically, we train\nthe LLM to generate audio semantic tokens from the given conditions, and\nfurther use a non-autoregressive transformer to generate different levels of\nacoustic tokens in layers to better enhance the fidelity of the generated\naudio. Second, based on the intuition that LLMs were originally designed for\ndiscrete tasks with the next-word prediction method, we use the discrete\nrepresentation for audio generation and compress their semantic meanings into\nacoustic tokens, similar to adding ""acoustic vocabulary"" to LLM. Third, our\nmethod combines the previous tasks of audio understanding, video-to-audio\ngeneration, and text-to-audio generation together into one unified model,\nproviding more versatility in an end-to-end fashion. Our C3LLM achieves\nimproved results through various automated evaluation metrics, providing better\nsemantic alignment compared to previous methods.\n'] , [""  Multimodal intent recognition aims to leverage diverse modalities such as\nexpressions, body movements and tone of speech to comprehend user's intent,\nconstituting a critical task for understanding human language and behavior in\nreal-world multimodal scenarios. Nevertheless, the majority of existing methods\nignore potential correlations among different modalities and own limitations in\neffectively learning semantic features from nonverbal modalities. In this\npaper, we introduce a token-level contrastive learning method with\nmodality-aware prompting (TCL-MAP) to address the above challenges. To\nestablish an optimal multimodal semantic environment for text modality, we\ndevelop a modality-aware prompting module (MAP), which effectively aligns and\nfuses features from text, video and audio modalities with similarity-based\nmodality alignment and cross-modality attention mechanism. Based on the\nmodality-aware prompt and ground truth labels, the proposed token-level\ncontrastive learning framework (TCL) constructs augmented samples and employs\nNT-Xent loss on the label token. Specifically, TCL capitalizes on the optimal\ntextual semantic insights derived from intent labels to guide the learning\nprocesses of other modalities in return. Extensive experiments show that our\nmethod achieves remarkable improvements compared to state-of-the-art methods.\nAdditionally, ablation analyses demonstrate the superiority of the\nmodality-aware prompt over the handcrafted prompt, which holds substantial\nsignificance for multimodal prompt learning. The codes are released at\nhttps://github.com/thuiar/TCL-MAP.\n"", '  Audio and video are two most common modalities in the mainstream media\nplatforms, e.g., YouTube. To learn from multimodal videos effectively, in this\nwork, we propose a novel audio-video recognition approach termed audio video\nTransformer, AVT, leveraging the effective spatio-temporal representation by\nthe video Transformer to improve action recognition accuracy. For multimodal\nfusion, simply concatenating multimodal tokens in a cross-modal Transformer\nrequires large computational and memory resources, instead we reduce the\ncross-modality complexity through an audio-video bottleneck Transformer. To\nimprove the learning efficiency of multimodal Transformer, we integrate\nself-supervised objectives, i.e., audio-video contrastive learning, audio-video\nmatching, and masked audio and video learning, into AVT training, which maps\ndiverse audio and video representations into a common multimodal representation\nspace. We further propose a masked audio segment loss to learn semantic audio\nactivities in AVT. Extensive experiments and ablation studies on three public\ndatasets and two in-house datasets consistently demonstrate the effectiveness\nof the proposed AVT. Specifically, AVT outperforms its previous\nstate-of-the-art counterparts on Kinetics-Sounds by 8%. AVT also surpasses one\nof the previous state-of-the-art video Transformers [25] by 10% on VGGSound by\nleveraging the audio signal. Compared to one of the previous state-of-the-art\nmultimodal methods, MBT [32], AVT is 1.3% more efficient in terms of FLOPs and\nimproves the accuracy by 3.8% on Epic-Kitchens-100.\n', '  Action quality assessment (AQA) is to assess how well an action is performed.\nPrevious works perform modelling by only the use of visual information,\nignoring audio information. We argue that although AQA is highly dependent on\nvisual information, the audio is useful complementary information for improving\nthe score regression accuracy, especially for sports with background music,\nsuch as figure skating and rhythmic gymnastics. To leverage multimodal\ninformation for AQA, i.e., RGB, optical flow and audio information, we propose\na Progressive Adaptive Multimodal Fusion Network (PAMFN) that separately models\nmodality-specific information and mixed-modality information. Our model\nconsists of with three modality-specific branches that independently explore\nmodality-specific information and a mixed-modality branch that progressively\naggregates the modality-specific information from the modality-specific\nbranches. To build the bridge between modality-specific branches and the\nmixed-modality branch, three novel modules are proposed. First, a\nModality-specific Feature Decoder module is designed to selectively transfer\nmodality-specific information to the mixed-modality branch. Second, when\nexploring the interaction between modality-specific information, we argue that\nusing an invariant multimodal fusion policy may lead to suboptimal results, so\nas to take the potential diversity in different parts of an action into\nconsideration. Therefore, an Adaptive Fusion Module is proposed to learn\nadaptive multimodal fusion policies in different parts of an action. This\nmodule consists of several FusionNets for exploring different multimodal fusion\nstrategies and a PolicyNet for deciding which FusionNets are enabled. Third, a\nmodule called Cross-modal Feature Decoder is designed to transfer cross-modal\nfeatures generated by Adaptive Fusion Module to the mixed-modality branch.\n']",Multimodal Learning and Fusion,Multimodal Learning and Fusion
88,"Model Merging for Multitask Learning , Multimodal EHR Representation Learning","['multitask', 'merging', 'merge', 'combine', 'taskonomy', 'tasking', 'models', 'merged', 'tasks', 'task'] , ['multimodal', 'embedding', 'supervised', 'modality', 'ehr', 'ehrs', 'health', 'medical', 'structured', 'unstructured']","['  Model merging has emerged as an effective approach to combine multiple\nsingle-task models, fine-tuned from the same pre-trained model, into a\nmultitask model. This process typically involves computing a weighted average\nof the model parameters without any additional training. Existing model-merging\nmethods focus on enhancing average task accuracy. However, interference and\nconflicts between the objectives of different tasks can lead to trade-offs\nduring model merging. In real-world applications, a set of solutions with\nvarious trade-offs can be more informative, helping practitioners make\ndecisions based on diverse preferences. In this paper, we introduce a novel\nlow-compute algorithm, Model Merging with Amortized Pareto Front (MAP). MAP\nidentifies a Pareto set of scaling coefficients for merging multiple models to\nreflect the trade-offs. The core component of MAP is approximating the\nevaluation metrics of the various tasks using a quadratic approximation\nsurrogate model derived from a pre-selected set of scaling coefficients,\nenabling amortized inference. Experimental results on vision and natural\nlanguage processing tasks show that MAP can accurately identify the Pareto\nfront. To further reduce the required computation of MAP, we propose (1) a\nBayesian adaptive sampling algorithm and (2) a nested merging scheme with\nmultiple stages.\n', '  Model merging is an efficient empowerment technique in the machine learning\ncommunity that does not require the collection of raw training data and does\nnot require expensive computation. As model merging becomes increasingly\nprevalent across various fields, it is crucial to understand the available\nmodel merging techniques comprehensively. However, there is a significant gap\nin the literature regarding a systematic and thorough review of these\ntechniques. This survey provides a comprehensive overview of model merging\nmethods and theories, their applications in various domains and settings, and\nfuture research directions. Specifically, we first propose a new taxonomic\napproach that exhaustively discusses existing model merging methods. Secondly,\nwe discuss the application of model merging techniques in large language\nmodels, multimodal large language models, and 10+ machine learning subfields,\nincluding continual learning, multi-task learning, few-shot learning, etc.\nFinally, we highlight the remaining challenges of model merging and discuss\nfuture research directions. A comprehensive list of papers about model merging\nis available at\n\\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}.\n', ""  In this paper, we introduce a novel approach for large language model merging\nvia black-box multi-objective optimization algorithms. The goal of model\nmerging is to combine multiple models, each excelling in different tasks, into\na single model that outperforms any of the individual source models. However,\nmodel merging faces two significant challenges: First, existing methods rely\nheavily on human intuition and customized strategies to tackle multiple tasks.\nSecond, it's difficult to search for the great model merging configuration in\nlimited evaluations. To address these challenges, we propose a multi-objective\noptimization based model merging method named MM-MO. The proposed method can\nautomatically search merging configurations for multiple tasks with\nmulti-objective optimization algorithms. Moreover, to obtain high-quality model\nmerging configurations within a limited number of evaluation iterations, we\nhave made several improvements to multi-objective Bayesian optimization\nspecifically for model merging scenarios. First, we introduced a weak-to-strong\nmethod to improve the acquisition strategy. Second, we employed Fisher\ninformation to select configurations, further increasing the chances of\ndiscovering superior model merging configurations. Third, we designed a\nsparsity metric as an additional optimization objective to enhance the model's\ngeneralization performance across different tasks. We conducted comprehensive\nexperiments with other mainstream model merging methods, demonstrating that our\nmethod consistently outperforms them. Moreover, performance improvements are\nobserved even on the tasks not explicitly targeted as optimization objectives,\nindicating that our method enhances the overall potential of the model. ...\n""] , [""  Electronic health record (EHR) systems contain a wealth of multimodal\nclinical data including structured data like clinical codes and unstructured\ndata such as clinical notes. However, many existing EHR-focused studies has\ntraditionally either concentrated on an individual modality or merged different\nmodalities in a rather rudimentary fashion. This approach often results in the\nperception of structured and unstructured data as separate entities, neglecting\nthe inherent synergy between them. Specifically, the two important modalities\ncontain clinically relevant, inextricably linked and complementary health\ninformation. A more complete picture of a patient's medical history is captured\nby the joint analysis of the two modalities of data. Despite the great success\nof multimodal contrastive learning on vision-language, its potential remains\nunder-explored in the realm of multimodal EHR, particularly in terms of its\ntheoretical understanding. To accommodate the statistical analysis of\nmultimodal EHR data, in this paper, we propose a novel multimodal feature\nembedding generative model and design a multimodal contrastive loss to obtain\nthe multimodal EHR feature representation. Our theoretical analysis\ndemonstrates the effectiveness of multimodal learning compared to\nsingle-modality learning and connects the solution of the loss function to the\nsingular value decomposition of a pointwise mutual information matrix. This\nconnection paves the way for a privacy-preserving algorithm tailored for\nmultimodal EHR feature representation learning. Simulation studies show that\nthe proposed algorithm performs well under a variety of configurations. We\nfurther validate the clinical utility of the proposed algorithm in real-world\nEHR data.\n"", '  The integration of multimodal Electronic Health Records (EHR) data has\nsignificantly improved clinical predictive capabilities. Leveraging clinical\nnotes and multivariate time-series EHR, existing models often lack the medical\ncontext relevent to clinical tasks, prompting the incorporation of external\nknowledge, particularly from the knowledge graph (KG). Previous approaches with\nKG knowledge have primarily focused on structured knowledge extraction,\nneglecting unstructured data modalities and semantic high dimensional medical\nknowledge. In response, we propose REALM, a Retrieval-Augmented Generation\n(RAG) driven framework to enhance multimodal EHR representations that address\nthese limitations. Firstly, we apply Large Language Model (LLM) to encode long\ncontext clinical notes and GRU model to encode time-series EHR data. Secondly,\nwe prompt LLM to extract task-relevant medical entities and match entities in\nprofessionally labeled external knowledge graph (PrimeKG) with corresponding\nmedical knowledge. By matching and aligning with clinical standards, our\nframework eliminates hallucinations and ensures consistency. Lastly, we propose\nan adaptive multimodal fusion network to integrate extracted knowledge with\nmultimodal EHR data. Our extensive experiments on MIMIC-III mortality and\nreadmission tasks showcase the superior performance of our REALM framework over\nbaselines, emphasizing the effectiveness of each module. REALM framework\ncontributes to refining the use of multimodal EHR data in healthcare and\nbridging the gap with nuanced medical context essential for informed clinical\npredictions.\n', ""  The integration of multimodal Electronic Health Records (EHR) data has\nnotably advanced clinical predictive capabilities. However, current models that\nutilize clinical notes and multivariate time-series EHR data often lack the\nnecessary medical context for precise clinical tasks. Previous methods using\nknowledge graphs (KGs) primarily focus on structured knowledge extraction. To\naddress this, we propose EMERGE, a Retrieval-Augmented Generation (RAG) driven\nframework aimed at enhancing multimodal EHR predictive modeling. Our approach\nextracts entities from both time-series data and clinical notes by prompting\nLarge Language Models (LLMs) and aligns them with professional PrimeKG to\nensure consistency. Beyond triplet relationships, we include entities'\ndefinitions and descriptions to provide richer semantics. The extracted\nknowledge is then used to generate task-relevant summaries of patients' health\nstatuses. These summaries are fused with other modalities utilizing an adaptive\nmultimodal fusion network with cross-attention. Extensive experiments on the\nMIMIC-III and MIMIC-IV datasets for in-hospital mortality and 30-day\nreadmission tasks demonstrate the superior performance of the EMERGE framework\ncompared to baseline models. Comprehensive ablation studies and analyses\nunderscore the efficacy of each designed module and the framework's robustness\nto data sparsity. EMERGE significantly enhances the use of multimodal EHR data\nin healthcare, bridging the gap with nuanced medical contexts crucial for\ninformed clinical predictions.\n""]",Multimodal Learning and Model Integration for Healthcare and Multitask Applications,Model Merging for Multitask Learning
89,"Multimodal Large Language Models (MLLMs) , Multimodal Large Language Models (MLLMs) , Multimodal Large Language Models (MLLMs) Evaluation","['multimodal', 'visual', 'captioning', 'mllm', 'modality', 'mllms', 'comprehension', 'textual', 'text', 'answering'] , ['multimodal', 'modality', 'dialogues', 'visual', 'models', 'mllm', 'interactive', 'conversations', 'conversational', 'dialog'] , ['multimodal', 'mllm', 'mllms', 'textual', 'detection', 'toolkit', 'benchmarking', 'modal', 'empirically', 'lvlm']","[""  In production, multi-modal large language models (MLLMs) are expected to\nsupport multi-turn queries of interchanging image and text modalities. However,\nthe current MLLMs trained with visual-question-answering (VQA) datasets could\nsuffer from degradation, as VQA datasets lack the diversity and complexity of\nthe original text instruction datasets which the underlying language model had\nbeen trained with. To address this challenging degradation, we first collect a\nlightweight (6k entries) VQA preference dataset where answers were annotated by\nGemini for 5 quality metrics in a granular fashion, and investigate standard\nSupervised Fine-tuning, rejection sampling, Direct Preference Optimization\n(DPO), and SteerLM. Our findings indicate that the with DPO we are able to\nsurpass instruction-following capabilities of the language model, achieving a\n6.73 score on MT-Bench, compared to Vicuna's 6.57 and LLaVA's 5.99 despite\nsmall data scale. This enhancement in textual instruction proficiency\ncorrelates with boosted visual instruction performance (+4.9\\% on MM-Vet, +6\\%\non LLaVA-Bench), with minimal alignment tax on visual knowledge benchmarks\ncompared to previous RLHF approach. In conclusion, we propose a\ndistillation-based multi-modal alignment model with fine-grained annotations on\na small dataset that reconciles the textual and visual performance of MLLMs,\nrestoring and boosting language capability after visual instruction tuning.\n"", '  Connecting text and visual modalities plays an essential role in generative\nintelligence. For this reason, inspired by the success of large language\nmodels, significant research efforts are being devoted to the development of\nMultimodal Large Language Models (MLLMs). These models can seamlessly integrate\nvisual and textual modalities, while providing a dialogue-based interface and\ninstruction-following capabilities. In this paper, we provide a comprehensive\nreview of recent visual-based MLLMs, analyzing their architectural choices,\nmultimodal alignment strategies, and training techniques. We also conduct a\ndetailed analysis of these models across a wide range of tasks, including\nvisual grounding, image generation and editing, visual understanding, and\ndomain-specific applications. Additionally, we compile and describe training\ndatasets and evaluation benchmarks, conducting comparisons among existing\nmodels in terms of performance and computational requirements. Overall, this\nsurvey offers a comprehensive overview of the current state of the art, laying\nthe groundwork for future MLLMs.\n', '  Recent advancements in Chain-of-Thought (CoT) and related rationale-based\nworks have significantly improved the performance of Large Language Models\n(LLMs) in complex reasoning tasks. With the evolution of Multimodal Large\nLanguage Models (MLLMs), enhancing their capability to tackle complex\nmultimodal reasoning problems is a crucial frontier. However, incorporating\nmultimodal rationales in CoT has yet to be thoroughly investigated. We propose\nthe Image-of-Thought (IoT) prompting method, which helps MLLMs to extract\nvisual rationales step-by-step. Specifically, IoT prompting can automatically\ndesign critical visual information extraction operations based on the input\nimages and questions. Each step of visual information refinement identifies\nspecific visual rationales that support answers to complex visual reasoning\nquestions. Beyond the textual CoT, IoT simultaneously utilizes visual and\ntextual rationales to help MLLMs understand complex multimodal information. IoT\nprompting has improved zero-shot visual reasoning performance across various\nvisual understanding tasks in different MLLMs. Moreover, the step-by-step\nvisual feature explanations generated by IoT prompting elucidate the visual\nreasoning process, aiding in analyzing the cognitive processes of large\nmultimodal models\n'] , ['  Multimodal Large Models (MLMs) are becoming a significant research focus,\ncombining powerful large language models with multimodal learning to perform\ncomplex tasks across different data modalities. This review explores the latest\ndevelopments and challenges in MLMs, emphasizing their potential in achieving\nartificial general intelligence and as a pathway to world models. We provide an\noverview of key techniques such as Multimodal Chain of Thought (M-COT),\nMultimodal Instruction Tuning (M-IT), and Multimodal In-Context Learning\n(M-ICL). Additionally, we discuss both the fundamental and specific\ntechnologies of multimodal models, highlighting their applications,\ninput/output modalities, and design characteristics. Despite significant\nadvancements, the development of a unified multimodal model remains elusive. We\ndiscuss the integration of 3D generation and embodied intelligence to enhance\nworld simulation capabilities and propose incorporating external rule systems\nfor improved reasoning and decision-making. Finally, we outline future research\ndirections to address these challenges and advance the field.\n', ""  Multimodal foundation models that can holistically process text alongside\nimages, video, audio, and other sensory modalities are increasingly used in a\nvariety of real-world applications. However, it is challenging to characterize\nand study progress in multimodal foundation models, given the range of possible\nmodeling decisions, tasks, and domains. In this paper, we introduce Holistic\nEvaluation of Multimodal Models (HEMM) to systematically evaluate the\ncapabilities of multimodal foundation models across a set of 3 dimensions:\nbasic skills, information flow, and real-world use cases. Basic multimodal\nskills are internal abilities required to solve problems, such as learning\ninteractions across modalities, fine-grained alignment, multi-step reasoning,\nand the ability to handle external knowledge. Information flow studies how\nmultimodal content changes during a task through querying, translation,\nediting, and fusion. Use cases span domain-specific challenges introduced in\nreal-world multimedia, affective computing, natural sciences, healthcare, and\nhuman-computer interaction applications. Through comprehensive experiments\nacross the 30 tasks in HEMM, we (1) identify key dataset dimensions (e.g.,\nbasic skills, information flows, and use cases) that pose challenges to today's\nmodels, and (2) distill performance trends regarding how different modeling\ndimensions (e.g., scale, pre-training data, multimodal alignment, pre-training,\nand instruction tuning objectives) influence performance. Our conclusions\nregarding challenging multimodal interactions, use cases, and tasks requiring\nreasoning and external knowledge, the benefits of data and model scale, and the\nimpacts of instruction tuning yield actionable insights for future work in\nmultimodal foundation models.\n"", '  With the recent advancement in large language models (LLMs), there is a\ngrowing interest in combining LLMs with multimodal learning. Previous surveys\nof multimodal large language models (MLLMs) mainly focus on multimodal\nunderstanding. This survey elaborates on multimodal generation and editing\nacross various domains, comprising image, video, 3D, and audio. Specifically,\nwe summarize the notable advancements with milestone works in these fields and\ncategorize these studies into LLM-based and CLIP/T5-based methods. Then, we\nsummarize the various roles of LLMs in multimodal generation and exhaustively\ninvestigate the critical technical components behind these methods and the\nmultimodal datasets utilized in these studies. Additionally, we dig into\ntool-augmented multimodal agents that can leverage existing generative models\nfor human-computer interaction. Lastly, we discuss the advancements in the\ngenerative AI safety field, investigate emerging applications, and discuss\nfuture prospects. Our work provides a systematic and insightful overview of\nmultimodal generation and processing, which is expected to advance the\ndevelopment of Artificial Intelligence for Generative Content (AIGC) and world\nmodels. A curated list of all related papers can be found at\nhttps://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation\n'] , [""  Multimodal large language models (MLLMs) (e.g., GPT-4V, LLaVA, and Claude-3)\nhave broadened the scope of AI applications. Yet, evaluating their performance\npresents a significant challenge owing to the inherently subjective nature of\ntasks that do not yield clear-cut solutions especially for those open-ended\nqueries. Existing automatic evaluation methodologies are mainly limited in\nevaluating objective queries without considering real-world user experiences,\ninadequately addressing the nuances of creative and associative multimodal\ntasks. In our paper, we propose a new evaluation paradigm for MLLMs, which is\nevaluating MLLMs with \\textit{per-sample criteria} using potent MLLM as the\njudge. To validate the feasibility and effectiveness of this paradigm, we\ndesign a benchmark, dubbed \\textit{MLLM-Bench}, with the evaluation samples\nacross six critical levels following the revised Bloom's Taxonomy with the\nethical consideration. We benchmark 21 popular MLLMs in a pairwise-comparison\nfashion, showing diverse performance across models. Moreover, the validity of\nour benchmark manifests itself in reaching 88.02\\% agreement with human\nevaluation. We contend that the proposed paradigm explores the potential of\nMLLMs as effective evaluation tools with the help of per-sample criteria, and\nthat MLLM-Bench will serve as a catalyst for encouraging the development of\nuser-centric MLLMs tailored to real-world applications. Our benchmark data,\nonline leaderboard and submission entry are at https://mllm-bench.llmzoo.com.\n"", '  Nowadays, misinformation is widely spreading over various social media\nplatforms and causes extremely negative impacts on society. To combat this\nissue, automatically identifying misinformation, especially those containing\nmultimodal content, has attracted growing attention from the academic and\nindustrial communities, and induced an active research topic named Multimodal\nMisinformation Detection (MMD). Typically, existing MMD methods capture the\nsemantic correlation and inconsistency between multiple modalities, but neglect\nsome potential clues in multimodal content. Recent studies suggest that\nmanipulated traces of the images in articles are non-trivial clues for\ndetecting misinformation. Meanwhile, we find that the underlying intentions\nbehind the manipulation, e.g., harmful and harmless, also matter in MMD.\nAccordingly, in this work, we propose to detect misinformation by learning\nmanipulation features that indicate whether the image has been manipulated, as\nwell as intention features regarding the harmful and harmless intentions of the\nmanipulation. Unfortunately, the manipulation and intention labels that make\nthese features discriminative are unknown. To overcome the problem, we propose\ntwo weakly supervised signals as alternatives by introducing additional\ndatasets on image manipulation detection and formulating two classification\ntasks as positive and unlabeled learning problems. Based on these ideas, we\npropose a novel MMD method, namely Harmfully Manipulated Images Matter in MMD\n(HAMI-M3D). Extensive experiments across three benchmark datasets can\ndemonstrate that HAMI-M3D can consistently improve the performance of any MMD\nbaselines.\n', ""  Humans are prone to cognitive distortions -- biased thinking patterns that\nlead to exaggerated responses to specific stimuli, albeit in very different\ncontexts. This paper demonstrates that advanced Multimodal Large Language\nModels (MLLMs) exhibit similar tendencies. While these models are designed to\nrespond queries under safety mechanism, they sometimes reject harmless queries\nin the presence of certain visual stimuli, disregarding the benign nature of\ntheir contexts. As the initial step in investigating this behavior, we identify\nthree types of stimuli that trigger the oversensitivity of existing MLLMs:\nExaggerated Risk, Negated Harm, and Counterintuitive Interpretation. To\nsystematically evaluate MLLMs' oversensitivity to these stimuli, we propose the\nMultimodal OverSenSitivity Benchmark (MOSSBench). This toolkit consists of 300\nmanually collected benign multimodal queries, cross-verified by third-party\nreviewers (AMT). Empirical studies using MOSSBench on 20 MLLMs reveal several\ninsights: (1). Oversensitivity is prevalent among SOTA MLLMs, with refusal\nrates reaching up to 76% for harmless queries. (2). Safer models are more\noversensitive: increasing safety may inadvertently raise caution and\nconservatism in the model's responses. (3). Different types of stimuli tend to\ncause errors at specific stages -- perception, intent reasoning, and safety\njudgement -- in the response process of MLLMs. These findings highlight the\nneed for refined safety mechanisms that balance caution with contextually\nappropriate responses, improving the reliability of MLLMs in real-world\napplications. We make our project available at\nhttps://turningpoint-ai.github.io/MOSSBench/.\n""]",Multimodal Large Language Models (MLLMs),Multimodal Large Language Models (MLLMs)
90,"Image Captioning Models , ""Contrastive Image-Language Pre-training (CLIP) for Captioning"" , Multimodal Captioning and Translation","['captioning', 'captions', 'caption', 'multimodal', 'alttexts', 'visual', 'coco', 'descriptions', 'images', 'nlg'] , ['captioning', 'embedding', 'captions', 'caption', 'clip', 'visual', 'recognition', 'modality', 'generatively', 'retrieval'] , ['captioning', 'captions', 'multimodal', 'caption', 'multilingual', 'visual', 'translation', 'textual', 'text', 'benchmark']","['  The objective of image captioning models is to bridge the gap between the\nvisual and linguistic modalities by generating natural language descriptions\nthat accurately reflect the content of input images. In recent years,\nresearchers have leveraged deep learning-based models and made advances in the\nextraction of visual features and the design of multimodal connections to\ntackle this task. This work presents a novel approach towards developing image\ncaptioning models that utilize an external kNN memory to improve the generation\nprocess. Specifically, we propose two model variants that incorporate a\nknowledge retriever component that is based on visual similarities, a\ndifferentiable encoder to represent input images, and a kNN-augmented language\nmodel to predict tokens based on contextual cues and text retrieved from the\nexternal memory. We experimentally validate our approach on COCO and nocaps\ndatasets and demonstrate that incorporating an explicit external memory can\nsignificantly enhance the quality of captions, especially with a larger\nretrieval corpus. This work provides valuable insights into retrieval-augmented\ncaptioning models and opens up new avenues for improving image captioning at a\nlarger scale.\n', '  Effectively aligning with human judgment when evaluating machine-generated\nimage captions represents a complex yet intriguing challenge. Existing\nevaluation metrics like CIDEr or CLIP-Score fall short in this regard as they\ndo not take into account the corresponding image or lack the capability of\nencoding fine-grained details and penalizing hallucinations. To overcome these\nissues, in this paper, we propose BRIDGE, a new learnable and reference-free\nimage captioning metric that employs a novel module to map visual features into\ndense vectors and integrates them into multi-modal pseudo-captions which are\nbuilt during the evaluation process. This approach results in a multimodal\nmetric that properly incorporates information from the input image without\nrelying on reference captions, bridging the gap between human judgment and\nmachine-generated image captions. Experiments spanning several datasets\ndemonstrate that our proposal achieves state-of-the-art results compared to\nexisting reference-free evaluation scores. Our source code and trained models\nare publicly available at: https://github.com/aimagelab/bridge-score.\n', '  Training image captioning models using teacher forcing results in very\ngeneric samples, whereas more distinctive captions can be very useful in\nretrieval applications or to produce alternative texts describing images for\naccessibility. Reinforcement Learning (RL) allows to use cross-modal retrieval\nsimilarity score between the generated caption and the input image as reward to\nguide the training, leading to more distinctive captions. Recent studies show\nthat pre-trained cross-modal retrieval models can be used to provide this\nreward, completely eliminating the need for reference captions. However, we\nargue in this paper that Ground Truth (GT) captions can still be useful in this\nRL framework. We propose a new image captioning model training strategy that\nmakes use of GT captions in different ways. Firstly, they can be used to train\na simple MLP discriminator that serves as a regularization to prevent reward\nhacking and ensures the fluency of generated captions, resulting in a textual\nGAN setup extended for multimodal inputs. Secondly, they can serve as\nadditional trajectories in the RL strategy, resulting in a teacher forcing loss\nweighted by the similarity of the GT to the image. This objective acts as an\nadditional learning signal grounded to the distribution of the GT captions.\nThirdly, they can serve as strong baselines when added to the pool of captions\nused to compute the proposed contrastive reward to reduce the variance of\ngradient estimate. Experiments on MS-COCO demonstrate the interest of the\nproposed training strategy to produce highly distinctive captions while\nmaintaining high writing quality.\n'] , [""  Image captioning aims at generating descriptive and meaningful textual\ndescriptions of images, enabling a broad range of vision-language applications.\nPrior works have demonstrated that harnessing the power of Contrastive Image\nLanguage Pre-training (CLIP) offers a promising approach to achieving zero-shot\ncaptioning, eliminating the need for expensive caption annotations. However,\nthe widely observed modality gap in the latent space of CLIP harms the\nperformance of zero-shot captioning by breaking the alignment between paired\nimage-text features. To address this issue, we conduct an analysis on the CLIP\nlatent space which leads to two findings. Firstly, we observe that the CLIP's\nvisual feature of image subregions can achieve closer proximity to the paired\ncaption due to the inherent information loss in text descriptions. In addition,\nwe show that the modality gap between a paired image-text can be empirically\nmodeled as a zero-mean Gaussian distribution. Motivated by the findings, we\npropose a novel zero-shot image captioning framework with text-only training to\nreduce the modality gap. In particular, we introduce a subregion feature\naggregation to leverage local region information, which produces a compact\nvisual representation for matching text representation. Moreover, we\nincorporate a noise injection and CLIP reranking strategy to boost captioning\nperformance. We also extend our framework to build a zero-shot VQA pipeline,\ndemonstrating its generality. Through extensive experiments on common\ncaptioning and VQA datasets such as MSCOCO, Flickr30k and VQAV2, we show that\nour method achieves remarkable performance improvements. Code is available at\nhttps://github.com/Artanic30/MacCap.\n"", '  In the field of vision-language contrastive learning, models such as CLIP\ncapitalize on matched image-caption pairs as positive examples and leverage\nwithin-batch non-matching pairs as negatives. This approach has led to\nremarkable outcomes in zero-shot image classification, cross-modal retrieval,\nand linear evaluation tasks. We show that the zero-shot classification and\nretrieval capabilities of CLIP-like models can be improved significantly\nthrough the introduction of semantically composite examples during pretraining.\nInspired by CutMix in vision categorization, we create semantically composite\nimage-caption pairs by merging elements from two distinct instances in the\ndataset via a novel procedure. Our method fuses the captions and blends 50% of\neach image to form a new composite sample. This simple technique (termed CLIP-C\nfor CLIP Compositions), devoid of any additional computational overhead or\nincrease in model parameters, significantly improves zero-shot image\nclassification and cross-modal retrieval. The benefits of CLIP-C are\nparticularly pronounced in settings with relatively limited pretraining data.\n', '  Multi-modal learning has become increasingly popular due to its ability to\nleverage information from different data sources (e.g., text and images) to\nimprove the model performance. Recently, CLIP has emerged as an effective\napproach that employs vision-language contrastive pretraining to learn joint\nimage and text representations and exhibits remarkable performance in zero-shot\nlearning and text-guided natural image generation. Despite the huge practical\nsuccess of CLIP, its theoretical understanding remains elusive. In this paper,\nwe formally study transferrable representation learning underlying CLIP and\ndemonstrate how features from different modalities get aligned. We also analyze\nits zero-shot transfer performance on the downstream tasks. Inspired by our\nanalysis, we propose a new CLIP-type approach, which achieves better\nperformance than CLIP and other state-of-the-art methods on benchmark datasets.\n'] , ['  We propose Wolf, a WOrLd summarization Framework for accurate video\ncaptioning. Wolf is an automated captioning framework that adopts a\nmixture-of-experts approach, leveraging complementary strengths of Vision\nLanguage Models (VLMs). By utilizing both image and video models, our framework\ncaptures different levels of information and summarizes them efficiently. Our\napproach can be applied to enhance video understanding, auto-labeling, and\ncaptioning. To evaluate caption quality, we introduce CapScore, an LLM-based\nmetric to assess the similarity and quality of generated captions compared to\nthe ground truth captions. We further build four human-annotated datasets in\nthree domains: autonomous driving, general scenes, and robotics, to facilitate\ncomprehensive comparisons. We show that Wolf achieves superior captioning\nperformance compared to state-of-the-art approaches from the research community\n(VILA1.5, CogAgent) and commercial solutions (Gemini-Pro-1.5, GPT-4V). For\ninstance, in comparison with GPT-4V, Wolf improves CapScore both quality-wise\nby 55.6% and similarity-wise by 77.4% on challenging driving videos. Finally,\nwe establish a benchmark for video captioning and introduce a leaderboard,\naiming to accelerate advancements in video understanding, captioning, and data\nalignment. Leaderboard: https://wolfv0.github.io/leaderboard.html.\n', '  A good evaluation framework should evaluate multimodal machine translation\n(MMT) models by measuring 1) their use of visual information to aid in the\ntranslation task and 2) their ability to translate complex sentences such as\ndone for text-only machine translation. However, most current work in MMT is\nevaluated against the Multi30k testing sets, which do not measure these\nproperties. Namely, the use of visual information by the MMT model cannot be\nshown directly from the Multi30k test set results and the sentences in Multi30k\nare are image captions, i.e., short, descriptive sentences, as opposed to\ncomplex sentences that typical text-only machine translation models are\nevaluated against.\n  Therefore, we propose that MMT models be evaluated using 1) the CoMMuTE\nevaluation framework, which measures the use of visual information by MMT\nmodels, 2) the text-only WMT news translation task test sets, which evaluates\ntranslation performance against complex sentences, and 3) the Multi30k test\nsets, for measuring MMT model performance against a real MMT dataset. Finally,\nwe evaluate recent MMT models trained solely against the Multi30k dataset\nagainst our proposed evaluation framework and demonstrate the dramatic drop\nperformance against text-only testing sets compared to recent text-only MT\nmodels.\n', '  News image captioning requires model to generate an informative caption rich\nin entities, with the news image and the associated news article. Though\nMultimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in addressing various vision-language tasks, our research finds\nthat current MLLMs still bear limitations in handling entity information on\nnews image captioning task. Besides, while MLLMs have the ability to process\nlong inputs, generating high-quality news image captions still requires a\ntrade-off between sufficiency and conciseness of textual input information. To\nexplore the potential of MLLMs and address problems we discovered, we propose :\nan Entity-Aware Multimodal Alignment based approach for news image captioning.\nOur approach first aligns the MLLM through Balance Training Strategy with two\nextra alignment tasks: Entity-Aware Sentence Selection task and Entity\nSelection task, together with News Image Captioning task, to enhance its\ncapability in handling multimodal entity information. The aligned MLLM will\nutilizes the additional entity-related information it explicitly extracts to\nsupplement its textual input while generating news image captions. Our approach\nachieves better results than all previous models in CIDEr score on GoodNews\ndataset (72.33 -> 88.39) and NYTimes800k dataset (70.83 -> 85.61).\n']",Multimodal Vision-Language Understanding and Generation,Multimodal Captioning and Translation
91,"Multimodal Recommendation Systems , Multimodal Image Retrieval , Multimodal Image Retrieval and Captioning , Multimodal Semantic Analysis and Linking","['multimodal', 'recommender', 'embeddings', 'modality', 'personalized', 'modal', 'recommendation', 'recommendations', 'collaborative', 'features'] , ['retrieval', 'multimodal', 'embeddings', 'embedding', 'modality', 'modal', 'discriminative', 'search', 'images', 'text'] , ['captioning', 'multimodal', 'embeddings', 'recognition', 'captions', 'encoder', 'embedding', 'retrieval', 'visual', 'text'] , ['multimodal', 'modality', 'attention', 'semantic', 'modal', 'linking', 'semantics', 'visual', 'retrieval', 'linkage']","['  Multimodal recommendation aims to model user and item representations\ncomprehensively with the involvement of multimedia content for effective\nrecommendations. Existing research has shown that it is beneficial for\nrecommendation performance to combine (user- and item-) ID embeddings with\nmultimodal salient features, indicating the value of IDs. However, there is a\nlack of a thorough analysis of the ID embeddings in terms of feature semantics\nin the literature. In this paper, we revisit the value of ID embeddings for\nmultimodal recommendation and conduct a thorough study regarding its semantics,\nwhich we recognize as subtle features of \\emph{content} and \\emph{structure}.\nBased on our findings, we propose a novel recommendation model by incorporating\nID embeddings to enhance the salient features of both content and structure.\nSpecifically, we put forward a hierarchical attention mechanism to incorporate\nID embeddings in modality fusing, coupled with contrastive learning, to enhance\ncontent representations. Meanwhile, we propose a lightweight graph convolution\nnetwork for each modality to amalgamate neighborhood and ID embeddings for\nimproving structural representations. Finally, the content and structure\nrepresentations are combined to form the ultimate item embedding for\nrecommendation. Extensive experiments on three real-world datasets (Baby,\nSports, and Clothing) demonstrate the superiority of our method over\nstate-of-the-art multimodal recommendation methods and the effectiveness of\nfine-grained ID embeddings. Our code is available at\nhttps://anonymous.4open.science/r/IDSF-code/.\n', '  With the increasing multimedia information, multimodal recommendation has\nreceived extensive attention. It utilizes multimodal information to alleviate\nthe data sparsity problem in recommendation systems, thus improving\nrecommendation accuracy. However, the reliance on labeled data severely limits\nthe performance of multimodal recommendation models. Recently, self-supervised\nlearning has been used in multimodal recommendations to mitigate the label\nsparsity problem. Nevertheless, the state-of-the-art methods cannot avoid the\nmodality noise when aligning multimodal information due to the large\ndifferences in the distributions of different modalities. To this end, we\npropose a Multi-level sElf-supervised learNing for mulTimOdal Recommendation\n(MENTOR) method to address the label sparsity problem and the modality\nalignment problem. Specifically, MENTOR first enhances the specific features of\neach modality using the graph convolutional network (GCN) and fuses the visual\nand textual modalities. It then enhances the item representation via the item\nsemantic graph for all modalities, including the fused modality. Then, it\nintroduces two multilevel self-supervised tasks: the multilevel cross-modal\nalignment task and the general feature enhancement task. The multilevel\ncross-modal alignment task aligns each modality under the guidance of the ID\nembedding from multiple levels while maintaining the historical interaction\ninformation. The general feature enhancement task enhances the general feature\nfrom both the graph and feature perspectives to improve the robustness of our\nmodel. Extensive experiments on three publicly available datasets demonstrate\nthe effectiveness of our method. Our code is publicly available at\nhttps://github.com/Jinfeng-Xu/MENTOR.\n', '  Multi-modal recommendation greatly enhances the performance of recommender\nsystems by modeling the auxiliary information from multi-modality contents.\nMost existing multi-modal recommendation models primarily exploit multimedia\ninformation propagation processes to enrich item representations and directly\nutilize modal-specific embedding vectors independently obtained from upstream\npre-trained models. However, this might be inappropriate since the abundant\ntask-specific semantics remain unexplored, and the cross-modality semantic gap\nhinders the recommendation performance.\n  Inspired by the recent progress of the cross-modal alignment model CLIP, in\nthis paper, we propose a novel \\textbf{CLIP} \\textbf{E}nhanced\n\\textbf{R}ecommender (\\textbf{CLIPER}) framework to bridge the semantic gap\nbetween modalities and extract fine-grained multi-view semantic information.\nSpecifically, we introduce a multi-view modality-alignment approach for\nrepresentation extraction and measure the semantic similarity between\nmodalities. Furthermore, we integrate the multi-view multimedia representations\ninto downstream recommendation models. Extensive experiments conducted on three\npublic datasets demonstrate the consistent superiority of our model over\nstate-of-the-art multi-modal recommendation models.\n'] , ['  Composed Image Retrieval (CIR) involves searching for target images based on\nan image-text pair query. While current methods treat this as a query-target\nmatching problem, we argue that CIR triplets contain additional associations\nbeyond this primary relation. In our paper, we identify two new relations\nwithin triplets, treating each triplet as a graph node. Firstly, we introduce\nthe concept of text-bridged image alignment, where the query text serves as a\nbridge between the query image and the target image. We propose a hinge-based\ncross-attention mechanism to incorporate this relation into network learning.\nSecondly, we explore complementary text reasoning, considering CIR as a form of\ncross-modal retrieval where two images compose to reason about complementary\ntext. To integrate these perspectives effectively, we design a twin\nattention-based compositor. By combining these complementary associations with\nthe explicit query pair-target image relation, we establish a comprehensive set\nof constraints for CIR. Our framework, CaLa (Complementary Association Learning\nfor Augmenting Composed Image Retrieval), leverages these insights. We evaluate\nCaLa on CIRR and FashionIQ benchmarks with multiple backbones, demonstrating\nits superiority in composed image retrieval.\n', '  Image-text matching aims to find matched cross-modal pairs accurately. While\ncurrent methods often rely on projecting cross-modal features into a common\nembedding space, they frequently suffer from imbalanced feature representations\nacross different modalities, leading to unreliable retrieval results. To\naddress these limitations, we introduce a novel Feature Enhancement Module that\nadaptively aggregates single-modal features for more balanced and robust\nimage-text retrieval. Additionally, we propose a new loss function that\novercomes the shortcomings of original triplet ranking loss, thereby\nsignificantly improving retrieval performance. The proposed model has been\nevaluated on two public datasets and achieves competitive retrieval performance\nwhen compared with several state-of-the-art models. Implementation codes can be\nfound here.\n', '  Learned Sparse Retrieval (LSR) is a group of neural methods designed to\nencode queries and documents into sparse lexical vectors. These vectors can be\nefficiently indexed and retrieved using an inverted index. While LSR has shown\npromise in text retrieval, its potential in multi-modal retrieval remains\nlargely unexplored. Motivated by this, in this work, we explore the application\nof LSR in the multi-modal domain, i.e., we focus on Multi-Modal Learned Sparse\nRetrieval (MLSR). We conduct experiments using several MLSR model\nconfigurations and evaluate the performance on the image suggestion task. We\nfind that solving the task solely based on the image content is challenging.\nEnriching the image content with its caption improves the model performance\nsignificantly, implying the importance of image captions to provide\nfine-grained concepts and context information of images. Our approach presents\na practical and effective solution for training LSR retrieval models in\nmulti-modal settings.\n'] , ['  Composed Image Retrieval (CIR) aims to retrieve a target image based on a\nreference image and conditioning text, enabling controllable searches. Due to\nthe expensive dataset construction cost for CIR triplets, a zero-shot (ZS) CIR\nsetting has been actively studied to eliminate the need for human-collected\ntriplet datasets. The mainstream of ZS-CIR employs an efficient projection\nmodule that projects a CLIP image embedding to the CLIP text token embedding\nspace, while fixing the CLIP encoders. Using the projected image embedding,\nthese methods generate image-text composed features by using the pre-trained\ntext encoder. However, their CLIP image and text encoders suffer from the task\ndiscrepancy between the pre-training task (text $\\leftrightarrow$ image) and\nthe target CIR task (image + text $\\leftrightarrow$ image). Conceptually, we\nneed expensive triplet samples to reduce the discrepancy, but we use cheap text\ntriplets instead and update the text encoder. To that end, we introduce the\nReducing Task Discrepancy of text encoders for Composed Image Retrieval (RTD),\na plug-and-play training scheme for the text encoder that enhances its\ncapability using a novel target-anchored text contrastive learning. We also\npropose two additional techniques to improve the proposed learning scheme: a\nhard negatives-based refined batch sampling strategy and a sophisticated\nconcatenation scheme. Integrating RTD into the state-of-the-art\nprojection-based ZS-CIR methods significantly improves performance across\nvarious datasets and backbones, demonstrating its efficiency and\ngeneralizability.\n', ""  CLIP models perform remarkably well on zero-shot classification and retrieval\ntasks. But recent studies have shown that learnt representations in CLIP are\nnot well suited for dense prediction tasks like object detection, semantic\nsegmentation or depth estimation. More recently, multi-stage training methods\nfor CLIP models was introduced to mitigate the weak performance of CLIP on\ndownstream tasks. In this work, we find that simply improving the quality of\ncaptions in image-text datasets improves the quality of CLIP's visual\nrepresentations, resulting in significant improvement on downstream dense\nprediction vision tasks. In fact, we find that CLIP pretraining with good\nquality captions can surpass recent supervised, self-supervised and weakly\nsupervised pretraining methods. We show that when CLIP model with ViT-B/16 as\nimage encoder is trained on well aligned image-text pairs it obtains 12.1%\nhigher mIoU and 11.5% lower RMSE on semantic segmentation and depth estimation\ntasks over recent state-of-the-art Masked Image Modeling (MIM) pretraining\nmethods like Masked Autoencoder (MAE). We find that mobile architectures also\nbenefit significantly from CLIP pretraining. A recent mobile vision\narchitecture, MCi2, with CLIP pretraining obtains similar performance as\nSwin-L, pretrained on ImageNet-22k for semantic segmentation task while being\n6.1$\\times$ smaller. Moreover, we show that improving caption quality results\nin $10\\times$ data efficiency when finetuning for dense prediction tasks.\n"", '  Treating texts as images, combining prompts with textual labels for prompt\ntuning, and leveraging the alignment properties of CLIP have been successfully\napplied in zero-shot multi-label image recognition. Nonetheless, relying solely\non textual labels to store visual information is insufficient for representing\nthe diversity of visual objects. In this paper, we propose reversing the\ntraining process of CLIP and introducing the concept of Pseudo Visual Prompts.\nThese prompts are initialized for each object category and pre-trained on\nlarge-scale, low-cost sentence data generated by large language models. This\nprocess mines the aligned visual information in CLIP and stores it in\nclass-specific visual prompts. We then employ contrastive learning to transfer\nthe stored visual information to the textual labels, enhancing their visual\nrepresentation capacity. Additionally, we introduce a dual-adapter module that\nsimultaneously leverages knowledge from the original CLIP and new learning\nknowledge derived from downstream datasets. Benefiting from the pseudo visual\nprompts, our method surpasses the state-of-the-art not only on clean annotated\ntext data but also on pseudo text data generated by large language models.\n'] , ['  Semantic location prediction aims to derive meaningful location insights from\nmultimodal social media posts, offering a more contextual understanding of\ndaily activities than using GPS coordinates. This task faces significant\nchallenges due to the noise and modality heterogeneity in ""text-image"" posts.\nExisting methods are generally constrained by inadequate feature\nrepresentations and modal interaction, struggling to effectively reduce noise\nand modality heterogeneity. To address these challenges, we propose a\nSimilarity-Guided Multimodal Fusion Transformer (SG-MFT) for predicting the\nsemantic locations of users from their multimodal posts. First, we incorporate\nhigh-quality text and image representations by utilizing a pre-trained large\nvision-language model. Then, we devise a Similarity-Guided Interaction Module\n(SIM) to alleviate modality heterogeneity and noise interference by\nincorporating both coarse-grained and fine-grained similarity guidance for\nimproving modality interactions. Specifically, we propose a novel\nsimilarity-aware feature interpolation attention mechanism at the\ncoarse-grained level, leveraging modality-wise similarity to mitigate\nheterogeneity and reduce noise within each modality. At the fine-grained level,\nwe utilize a similarity-aware feed-forward block and element-wise similarity to\nfurther address the issue of modality heterogeneity. Finally, building upon\npre-processed features with minimal noise and modal interference, we devise a\nSimilarity-aware Fusion Module (SFM) to fuse two modalities with a\ncross-attention mechanism. Comprehensive experimental results clearly\ndemonstrate the superior performance of our proposed method.\n', '  Multimodal Entity Linking (MEL) is a crucial task that aims at linking\nambiguous mentions within multimodal contexts to the referent entities in a\nmultimodal knowledge base, such as Wikipedia. Existing methods focus heavily on\nusing complex mechanisms and extensive model tuning methods to model the\nmultimodal interaction on specific datasets. However, these methods\novercomplicate the MEL task and overlook the visual semantic information, which\nmakes them costly and hard to scale. Moreover, these methods can not solve the\nissues like textual ambiguity, redundancy, and noisy images, which severely\ndegrade their performance. Fortunately, the advent of Large Language Models\n(LLMs) with robust capabilities in text understanding and reasoning,\nparticularly Multimodal Large Language Models (MLLMs) that can process\nmultimodal inputs, provides new insights into addressing this challenge.\nHowever, how to design a universally applicable LLMs-based MEL approach remains\na pressing challenge. To this end, we propose UniMEL, a unified framework which\nestablishes a new paradigm to process multimodal entity linking tasks using\nLLMs. In this framework, we employ LLMs to augment the representation of\nmentions and entities individually by integrating textual and visual\ninformation and refining textual information. Subsequently, we employ the\nembedding-based method for retrieving and re-ranking candidate entities. Then,\nwith only ~0.26% of the model parameters fine-tuned, LLMs can make the final\nselection from the candidate entities. Extensive experiments on three public\nbenchmark datasets demonstrate that our solution achieves state-of-the-art\nperformance, and ablation studies verify the effectiveness of all modules. Our\ncode is available at https://anonymous.4open.science/r/UniMEL/.\n', '  Mining structured knowledge from tweets using named entity recognition (NER)\ncan be beneficial for many down stream applications such as recommendation and\nintention understanding. With tweet posts tending to be multimodal, multimodal\nnamed entity recognition (MNER) has attracted more attention. In this paper, we\npropose a novel approach, which can dynamically align the image and text\nsequence and achieve the multi-level cross-modal learning to augment textual\nword representation for MNER improvement. To be specific, our framework can be\nsplit into three main stages: the first stage focuses on intra-modality\nrepresentation learning to derive the implicit global and local knowledge of\neach modality, the second evaluates the relevance between the text and its\naccompanying image and integrates different grained visual information based on\nthe relevance, the third enforces semantic refinement via iterative cross-modal\ninteractions and co-attention. We conduct experiments on two open datasets, and\nthe results and detailed analysis demonstrate the advantage of our model.\n']",Multimodal Learning and Applications,Multimodal Image Retrieval
92,"Predicting Human Gaze Behavior , Eye Movements and Reading Behavior Corpus","['gaze', 'eyedentify', 'eye', 'eyes', 'ocular', 'saliency', 'pupil', 'visual', 'attention', 'viewing'] , ['reading', 'gaze', 'dyslexia', 'corpus', 'attention', 'linguistic', 'texts', 'comprehension', 'eye', 'text']","[""  Predicting human gaze behavior within computer vision is integral for\ndeveloping interactive systems that can anticipate user attention, address\nfundamental questions in cognitive science, and hold implications for fields\nlike human-computer interaction (HCI) and augmented/virtual reality (AR/VR)\nsystems. Despite methodologies introduced for modeling human eye gaze behavior,\napplying these models to medical imaging for scanpath prediction remains\nunexplored. Our proposed system aims to predict eye gaze sequences from\nradiology reports and CXR images, potentially streamlining data collection and\nenhancing AI systems using larger datasets. However, predicting human scanpaths\non medical images presents unique challenges due to the diverse nature of\nabnormal regions. Our model predicts fixation coordinates and durations\ncritical for medical scanpath prediction, outperforming existing models in the\ncomputer vision community. Utilizing a two-stage training process and large\npublicly available datasets, our approach generates static heatmaps and eye\ngaze videos aligned with radiology reports, facilitating comprehensive\nanalysis. We validate our approach by comparing its performance with\nstate-of-the-art methods and assessing its generalizability among different\nradiologists, introducing novel strategies to model radiologists' search\npatterns during CXR image diagnosis. Based on the radiologist's evaluation,\nMedGaze can generate human-like gaze sequences with a high focus on relevant\nregions over the CXR images. It sometimes also outperforms humans in terms of\nredundancy and randomness in the scanpaths.\n"", '  We propose a novel neural pipeline, MSGazeNet, that learns gaze\nrepresentations by taking advantage of the eye anatomy information through a\nmultistream framework. Our proposed solution comprises two components, first a\nnetwork for isolating anatomical eye regions, and a second network for\nmultistream gaze estimation. The eye region isolation is performed with a U-Net\nstyle network which we train using a synthetic dataset that contains eye region\nmasks for the visible eyeball and the iris region. The synthetic dataset used\nin this stage is procured using the UnityEyes simulator, and consists of 80,000\neye images. Successive to training, the eye region isolation network is then\ntransferred to the real domain for generating masks for the real-world eye\nimages. In order to successfully make the transfer, we exploit domain\nrandomization in the training process, which allows for the synthetic images to\nbenefit from a larger variance with the help of augmentations that resemble\nartifacts. The generated eye region masks along with the raw eye images are\nthen used together as a multistream input to our gaze estimation network, which\nconsists of wide residual blocks. The output embeddings from these encoders are\nfused in the channel dimension before feeding into the gaze regression layers.\nWe evaluate our framework on three gaze estimation datasets and achieve strong\nperformances. Our method surpasses the state-of-the-art by 7.57% and 1.85% on\ntwo datasets, and obtains competitive results on the other. We also study the\nrobustness of our method with respect to the noise in the data and demonstrate\nthat our model is less sensitive to noisy data. Lastly, we perform a variety of\nexperiments including ablation studies to evaluate the contribution of\ndifferent components and design choices in our solution.\n', '  Eye-tracking applications that utilize the human gaze in video understanding\ntasks have become increasingly important. To effectively automate the process\nof video analysis based on eye-tracking data, it is important to accurately\nreplicate human gaze behavior. However, this task presents significant\nchallenges due to the inherent complexity and ambiguity of human gaze patterns.\nIn this work, we introduce a novel method for simulating human gaze behavior.\nOur approach uses a transformer-based reinforcement learning algorithm to train\nan agent that acts as a human observer, with the primary role of watching\nvideos and simulating human gaze behavior. We employed an eye-tracking dataset\ngathered from videos generated by the VirtualHome simulator, with a primary\nfocus on activity recognition. Our experimental results demonstrate the\neffectiveness of our gaze prediction method by highlighting its capability to\nreplicate human gaze behavior and its applicability for downstream tasks where\nreal human-gaze is used as input.\n'] , [""  The Eye Movements on Machine-Generated Texts Corpus (EMTeC) is a naturalistic\neye-movements-while-reading corpus of 107 native English speakers reading\nmachine-generated texts. The texts are generated by three large language models\nusing five different decoding strategies, and they fall into six different text\ntype categories. EMTeC entails the eye movement data at all stages of\npre-processing, i.e., the raw coordinate data sampled at 2000 Hz, the fixation\nsequences, and the reading measures. It further provides both the original and\na corrected version of the fixation sequences, accounting for vertical\ncalibration drift. Moreover, the corpus includes the language models' internals\nthat underlie the generation of the stimulus texts: the transition scores, the\nattention scores, and the hidden states. The stimuli are annotated for a range\nof linguistic features both at text and at word level. We anticipate EMTeC to\nbe utilized for a variety of use cases such as, but not restricted to, the\ninvestigation of reading behavior on machine-generated text and the impact of\ndifferent decoding strategies; reading behavior on different text types; the\ndevelopment of new pre-processing, data filtering, and drift correction\nalgorithms; the cognitive interpretability and enhancement of language models;\nand the assessment of the predictive power of surprisal and entropy for human\nreading times. The data at all stages of pre-processing, the model internals,\nand the code to reproduce the stimulus generation, data pre-processing and\nanalyses can be accessed via https://github.com/DiLi-Lab/EMTeC/.\n"", ""  We present a novel computational model employing hierarchical active\ninference to simulate reading and eye movements. The model characterizes\nlinguistic processing as inference over a hierarchical generative model,\nfacilitating predictions and inferences at various levels of granularity, from\nsyllables to sentences.\n  Our approach combines the strengths of large language models for realistic\ntextual predictions and active inference for guiding eye movements to\ninformative textual information, enabling the testing of predictions. The model\nexhibits proficiency in reading both known and unknown words and sentences,\nadhering to the distinction between lexical and nonlexical routes in dual-route\ntheories of reading. Notably, our model permits the exploration of maladaptive\ninference effects on eye movements during reading, such as in dyslexia. To\nsimulate this condition, we attenuate the contribution of priors during the\nreading process, leading to incorrect inferences and a more fragmented reading\nstyle, characterized by a greater number of shorter saccades. This alignment\nwith empirical findings regarding eye movements in dyslexic individuals\nhighlights the model's potential to aid in understanding the cognitive\nprocesses underlying reading and eye movements, as well as how reading deficits\nassociated with dyslexia may emerge from maladaptive predictive processing.\n  In summary, our model represents a significant advancement in comprehending\nthe intricate cognitive processes involved in reading and eye movements, with\npotential implications for understanding and addressing dyslexia through the\nsimulation of maladaptive inference. It may offer valuable insights into this\ncondition and contribute to the development of more effective interventions for\ntreatment.\n"", '  We present WebQAmGaze, a multilingual low-cost eye-tracking-while-reading\ndataset, designed as the first webcam-based eye-tracking corpus of reading to\nsupport the development of explainable computational language processing\nmodels. WebQAmGaze includes webcam eye-tracking data from 600 participants of a\nwide age range naturally reading English, German, Spanish, and Turkish texts.\nEach participant performs two reading tasks composed of five texts each, a\nnormal reading and an information-seeking task, followed by a comprehension\nquestion. We compare the collected webcam data to high-quality eye-tracking\nrecordings. The results show a moderate to strong correlation between the eye\nmovement measures obtained with the webcam compared to those obtained with a\ncommercial eye-tracking device. When validating the data, we find that higher\nfixation duration on relevant text spans accurately indicates correctness when\nanswering the corresponding questions. This dataset advances webcam-based\nreading studies and opens avenues to low-cost and diverse data collection.\nWebQAmGaze is beneficial to learn about the cognitive processes behind\nquestion-answering and to apply these insights to computational models of\nlanguage understanding.\n']",Eye Movement and Gaze in Human Behavior and Cognition,Predicting Human Gaze Behavior
93,"Efficient Attention Mechanisms for Long Sequences , Efficient Attention Mechanisms for Transformers","['attention', 'memory', 'sparse', 'recurrent', 'recall', 'tasks', 'efficient', 'language', 'chunk', 'longlora'] , ['attention', 'memory', 'softmax', 'decoder', 'layers', 'transformers', 'throughput', 'efficient', 'pruning', 'faster']","[""  The transformer architecture has driven breakthroughs in recent years on\ntasks which require modeling pairwise relationships between sequential\nelements, as is the case in natural language understanding. However, long\nseqeuences pose a problem due to the quadratic complexity of the attention\noperation. Previous research has aimed to lower the complexity by sparsifying\nor linearly approximating the attention matrix. Yet, these approaches cannot\nstraightforwardly distill knowledge from a teacher's attention matrix and often\nrequire complete retraining from scratch. Furthermore, previous sparse and\nlinear approaches lose interpretability if they cannot produce full attention\nmatrices. To address these challenges, we propose SEA: Sparse linear attention\nwith an Estimated Attention mask. SEA estimates the attention matrix with\nlinear complexity via kernel-based linear attention, then subsequently creates\na sparse attention matrix with a top-k selection to perform a sparse attention\noperation. For language modeling tasks (Wikitext2), previous linear and sparse\nattention methods show roughly two-fold worse perplexity scores over the\nquadratic OPT-1.3B baseline, while SEA achieves better perplexity than\nOPT-1.3B, using roughly half the memory of OPT-1.3B, providing interpretable\nattention matrix. We believe that our work will have a large practical impact,\nas it opens the possibility of running large transformers on resource-limited\ndevices with less memory.\n"", '  Extending the functionality of the Transformer model to accommodate longer\nsequence lengths has become a critical challenge. This extension is crucial not\nonly for improving tasks such as language translation and long-context\nprocessing but also for enabling novel applications like chatbots, code\ngeneration, and multimedia content creation. The primary obstacle is the\nself-attention mechanism, which scales quadratically with sequence length in\nterms of computation time and memory requirements. LongLoRA proposed shifted\nsparse attention (S\\(^2\\)-Attn), effectively enabling context extension and\nleading to non-trivial computation savings with similar performance to\nfine-tuning with vanilla attention. However, LongLoRA is still not as efficient\nas vanilla attention, reaching only 39\\% of the perplexity improvement compared\nto full attention. This inefficiency is due to the cyclic shift applied within\ndifferent attention head patterns, causing either chaos in the attention head\nstructure or unnecessary information exchange between token groups. To address\nthese issues, We propose \\textbf{SinkLoRA}, which features better work\npartitioning. Specifically, (1) we developed SF-Attn with a segmentation and\nreassembly algorithm to proportionally return cyclically shifted groups of\nattention heads to their un-shifted state together with global attention of\n""sink attention tokens"", achieving 92\\% of the perplexity improvement compared\nto full attention after fine tuning, and (2) applied a SOTA KV cache\ncompression algorithm H$_2$O to accelerate inference. Furthermore, We conducted\nsupervised fine-tuning with SinkLoRA using a self collected LongAlpaca-plus\ndataset. All our code, models, datasets, and demos are available at\n\\url{https://github.com/Dexter-GT-86/SinkLoRA}.\n', ""  In modern large language models (LLMs), increasing sequence lengths is a\ncrucial challenge for enhancing their comprehension and coherence in handling\ncomplex tasks such as multi-modal question answering. However, handling long\ncontext sequences with LLMs is prohibitively costly due to the conventional\nattention mechanism's quadratic time and space complexity, and the context\nwindow size is limited by the GPU memory. Although recent works have proposed\nlinear and sparse attention mechanisms to address this issue, their real-world\napplicability is often limited by the need to re-train pre-trained models. In\nresponse, we propose a novel approach, Hierarchically Pruned Attention (HiP),\nwhich simultaneously reduces the training and inference time complexity from\n$O(T^2)$ to $O(T \\log T)$ and the space complexity from $O(T^2)$ to $O(T)$. To\nthis end, we devise a dynamic sparse attention mechanism that generates an\nattention mask through a novel tree-search-like algorithm for a given query on\nthe fly. HiP is training-free as it only utilizes the pre-trained attention\nscores to spot the positions of the top-$k$ most significant elements for each\nquery. Moreover, it ensures that no token is overlooked, unlike the sliding\nwindow-based sub-quadratic attention methods, such as StreamingLLM. Extensive\nexperiments on diverse real-world benchmarks demonstrate that HiP significantly\nreduces prompt (i.e., prefill) and decoding latency and memory usage while\nmaintaining high generation performance with little or no degradation. As HiP\nallows pretrained LLMs to scale to millions of tokens on commodity GPUs with no\nadditional engineering due to its easy plug-and-play deployment, we believe\nthat our work will have a large practical impact, opening up the possibility to\nmany long-context LLM applications previously infeasible.\n""] , ['  We present Lightning Attention, the first linear attention implementation\nthat maintains a constant training speed for various sequence lengths under\nfixed memory consumption. Due to the issue with cumulative summation operations\n(cumsum), previous linear attention implementations cannot achieve their\ntheoretical advantage in a casual setting. However, this issue can be\neffectively solved by utilizing different attention calculation strategies to\ncompute the different parts of attention. Specifically, we split the attention\ncalculation into intra-blocks and inter-blocks and use conventional attention\ncomputation for intra-blocks and linear attention kernel tricks for\ninter-blocks. This eliminates the need for cumsum in the linear attention\ncalculation. Furthermore, a tiling technique is adopted through both forward\nand backward procedures to take full advantage of the GPU hardware. To enhance\naccuracy while preserving efficacy, we introduce TransNormerLLM (TNL), a new\narchitecture that is tailored to our lightning attention. We conduct rigorous\ntesting on standard and self-collected datasets with varying model sizes and\nsequence lengths. TNL is notably more efficient than other language models. In\naddition, benchmark results indicate that TNL performs on par with\nstate-of-the-art LLMs utilizing conventional transformer structures. The source\ncode is released at github.com/OpenNLPLab/TransnormerLLM.\n', '  Scaling Transformer-based large language models (LLMs) has demonstrated\npromising performance across various tasks. However, it also introduces\nredundant structures, posing challenges for real-world deployment. Despite some\nrecognition of redundancy in LLMs, the variability of redundancy across\ndifferent modules, such as MLP and Attention layers, is under-explored. In this\nwork, we investigate the varying redundancy across different modules within\nTransformers, including Blocks, MLP, and Attention layers, using a\nsimilarity-based metric. This metric operates on the premise that redundant\nstructures produce outputs highly similar to their inputs. Surprisingly, while\nattention layers are essential for transformers and distinguish them from other\nmainstream architectures, we found that a large proportion of attention layers\nexhibit excessively high similarity and can be safely pruned without degrading\nperformance, leading to reduced memory and computation costs. Additionally, we\nfurther propose a method that jointly drops Attention and MLP layers, achieving\nimproved performance and dropping ratios. Extensive experiments demonstrate the\neffectiveness of our methods, e.g., Llama-3-70B maintains comparable\nperformance even after pruning half of the attention layers. Our findings\nprovide valuable insights for future network architecture design. The code is\nreleased at: \\url{https://github.com/Shwai-He/LLM-Drop}.\n', '  Linear attention is an efficient attention mechanism that has recently\nemerged as a promising alternative to conventional softmax attention. With its\nability to process tokens in linear computational complexities, linear\nattention, in theory, can handle sequences of unlimited length without\nsacrificing speed, i.e., maintaining a constant training speed for various\nsequence lengths with a fixed memory consumption. However, due to the issue\nwith cumulative summation (cumsum), current linear attention algorithms cannot\ndemonstrate their theoretical advantage in a causal setting. In this paper, we\npresent Lightning Attention-2, the first linear attention implementation that\nenables linear attention to realize its theoretical computational benefits. To\nachieve this, we leverage the thought of tiling, separately handling the\nintra-block and inter-block components in linear attention calculation.\nSpecifically, we utilize the conventional attention computation mechanism for\nthe intra-blocks and apply linear attention kernel tricks for the inter-blocks.\nA tiling technique is adopted through both forward and backward procedures to\ntake full advantage of the GPU hardware. We implement our algorithm in Triton\nto make it IO-aware and hardware-friendly. Various experiments are conducted on\ndifferent model sizes and sequence lengths. Lightning Attention-2 retains\nconsistent training and inference speed regardless of input sequence length and\nis significantly faster than other attention mechanisms. The source code is\navailable at https://github.com/OpenNLPLab/lightning-attention.\n']",Efficient Attention Mechanisms for Large Language Models,Efficient Attention Mechanisms for Long Sequences
94,"Evaluating Saliency Methods for Explainable AI , Visual Saliency and Representation Learning , Automated Distractor Generation for Math MCQs","['saliency', 'salient', 'cnn', 'imagenet', 'explanations', 'neural', 'recognition', 'ai', 'classificationmetricsforimageexplanations', 'explainability'] , ['saliency', 'neural', 'attention', 'recognition', 'visual', 'attentions', 'vision', 'representations', 'images', 'perception'] , ['quizzes', 'distractors', 'distractor', 'assessments', 'students', 'comprehension', 'exam', 'quiz', 'pedagogical', 'assessment']","['  Deep learning models have performed well on many NLP tasks. However, their\ninternal mechanisms are typically difficult for humans to understand. The\ndevelopment of methods to explain models has become a key issue in the\nreliability of deep learning models in many important applications. Various\nsaliency explanation methods, which give each feature of input a score\nproportional to the contribution of output, have been proposed to determine the\npart of the input which a model values most. Despite a considerable body of\nwork on the evaluation of saliency methods, whether the results of various\nevaluation metrics agree with human cognition remains an open question. In this\nstudy, we propose a new human-based method to evaluate saliency methods in NLP\nby crowdsourcing. We recruited 800 crowd workers and empirically evaluated\nseven saliency methods on two datasets with the proposed method. We analyzed\nthe performance of saliency methods, compared our results with existing\nautomated evaluation methods, and identified notable differences between NLP\nand computer vision (CV) fields when using saliency methods. The instance-level\ndata of our crowdsourced experiments and the code to reproduce the explanations\nare available at https://github.com/xtlu/lreccoling_evaluation.\n', '  Gradient-based saliency maps are widely used to explain deep neural network\ndecisions. However, as models become deeper and more black-box, such as in\nclosed-source APIs like ChatGPT, computing gradients become challenging,\nhindering conventional explanation methods. In this work, we introduce a novel\nunified framework for estimating gradients in black-box settings and generating\nsaliency maps to interpret model decisions. We employ the likelihood ratio\nmethod to estimate output-to-input gradients and utilize them for saliency map\ngeneration. Additionally, we propose blockwise computation techniques to\nenhance estimation accuracy. Extensive experiments in black-box settings\nvalidate the effectiveness of our method, demonstrating accurate gradient\nestimation and explainability of generated saliency maps. Furthermore, we\nshowcase the scalability of our approach by applying it to explain GPT-Vision,\nrevealing the continued relevance of gradient-based explanation methods in the\nera of large, closed-source, and black-box models.\n', '  Input gradients have a pivotal role in a variety of applications, including\nadversarial attack algorithms for evaluating model robustness, explainable AI\ntechniques for generating Saliency Maps, and counterfactual\nexplanations.However, Saliency Maps generated by traditional neural networks\nare often noisy and provide limited insights. In this paper, we demonstrate\nthat, on the contrary, the Saliency Maps of 1-Lipschitz neural networks,\nlearned with the dual loss of an optimal transportation problem, exhibit\ndesirable XAI properties:They are highly concentrated on the essential parts of\nthe image with low noise, significantly outperforming state-of-the-art\nexplanation approaches across various models and metrics. We also prove that\nthese maps align unprecedentedly well with human explanations on ImageNet.To\nexplain the particularly beneficial properties of the Saliency Map for such\nmodels, we prove this gradient encodes both the direction of the transportation\nplan and the direction towards the nearest adversarial attack. Following the\ngradient down to the decision boundary is no longer considered an adversarial\nattack, but rather a counterfactual explanation that explicitly transports the\ninput from one class to another. Thus, Learning with such a loss jointly\noptimizes the classification objective and the alignment of the gradient, i.e.\nthe Saliency Map, to the transportation plan direction.These networks were\npreviously known to be certifiably robust by design, and we demonstrate that\nthey scale well for large problems and models, and are tailored for\nexplainability using a fast and straightforward method.\n'] , [""  Advances in multi-modal embeddings, and in particular CLIP, have recently\ndriven several breakthroughs in Computer Vision (CV). CLIP has shown impressive\nperformance on a variety of tasks, yet, its inherently opaque architecture may\nhinder the application of models employing CLIP as backbone, especially in\nfields where trust and model explainability are imperative, such as in the\nmedical domain. Current explanation methodologies for CV models rely on\nSaliency Maps computed through gradient analysis or input perturbation.\nHowever, these Saliency Maps can only be computed to explain classes relevant\nto the end task, often smaller in scope than the backbone training classes. In\nthe context of models implementing CLIP as their vision backbone, a substantial\nportion of the information embedded within the learned representations is thus\nleft unexplained.\n  In this work, we propose Concept Visualization (ConVis), a novel saliency\nmethodology that explains the CLIP embedding of an image by exploiting the\nmulti-modal nature of the embeddings. ConVis makes use of lexical information\nfrom WordNet to compute task-agnostic Saliency Maps for any concept, not\nlimited to concepts the end model was trained on. We validate our use of\nWordNet via an out of distribution detection experiment, and test ConVis on an\nobject localization benchmark, showing that Concept Visualizations correctly\nidentify and localize the image's semantic content. Additionally, we perform a\nuser study demonstrating that our methodology can give users insight on the\nmodel's functioning.\n"", '  Deep learning algorithms lack human-interpretable accounts of how they\ntransform raw visual input into a robust semantic understanding, which impedes\ncomparisons between different architectures, training objectives, and the human\nbrain. In this work, we take inspiration from neuroscience and employ\nrepresentational approaches to shed light on how neural networks encode\ninformation at low (visual saliency) and high (semantic similarity) levels of\nabstraction. Moreover, we introduce a custom image dataset where we\nsystematically manipulate salient and semantic information. We find that\nResNets are more sensitive to saliency information than ViTs, when trained with\nobject classification objectives. We uncover that networks suppress saliency in\nearly layers, a process enhanced by natural language supervision (CLIP) in\nResNets. CLIP also enhances semantic encoding in both architectures. Finally,\nwe show that semantic encoding is a key factor in aligning AI with human visual\nperception, while saliency suppression is a non-brain-like strategy.\n', '  Humans judge the similarity of two objects not just based on their visual\nappearance but also based on their semantic relatedness. However, it remains\nunclear how humans learn about semantic relationships between objects and\ncategories. One important source of semantic knowledge is that semantically\nrelated objects frequently co-occur in the same context. For instance, forks\nand plates are perceived as similar, at least in part, because they are often\nexperienced together in a ``kitchen"" or ``eating\'\' context. Here, we\ninvestigate whether a bio-inspired learning principle exploiting such\nco-occurrence statistics suffices to learn a semantically structured object\nrepresentation {\\em de novo} from raw visual or combined visual and linguistic\ninput. To this end, we simulate temporal sequences of visual experience by\nbinding together short video clips of real-world scenes showing objects in\ndifferent contexts. A bio-inspired neural network model aligns close-in-time\nvisual representations while also aligning visual and category label\nrepresentations to simulate visuo-language alignment. Our results show that our\nmodel clusters object representations based on their context, e.g. kitchen or\nbedroom, in particular in high-level layers of the network, akin to humans. In\ncontrast, lower-level layers tend to better reflect object identity or\ncategory. To achieve this, the model exploits two distinct strategies: the\nvisuo-language alignment ensures that different objects of the same category\nare represented similarly, whereas the temporal alignment leverages that\nobjects from the same context are frequently seen in succession to make their\nrepresentations more similar. Overall, our work suggests temporal and\nvisuo-language alignment as plausible computational principles for explaining\nthe origins of certain forms of semantic knowledge in humans.\n'] , ['  High-quality distractors are crucial to both the assessment and pedagogical\nvalue of multiple-choice questions (MCQs), where manually crafting ones that\nanticipate knowledge deficiencies or misconceptions among real students is\ndifficult. Meanwhile, automated distractor generation, even with the help of\nlarge language models (LLMs), remains challenging for subjects like math. It is\ncrucial to not only identify plausible distractors but also understand the\nerror behind them. In this paper, we introduce DiVERT (Distractor Generation\nwith Variational Errors Represented as Text), a novel variational approach that\nlearns an interpretable representation of errors behind distractors in math\nMCQs. Through experiments on a real-world math MCQ dataset with 1,434 questions\nused by hundreds of thousands of students, we show that DiVERT, despite using a\nbase open-source LLM with 7B parameters, outperforms state-of-the-art\napproaches using GPT-4o on downstream distractor generation. We also conduct a\nhuman evaluation with math educators and find that DiVERT leads to error labels\nthat are of comparable quality to human-authored ones.\n', '  Multiple-choice questions (MCQs) are ubiquitous in almost all levels of\neducation since they are easy to administer, grade, and are a reliable form of\nassessment. An important aspect of MCQs is the distractors, i.e., incorrect\noptions that are designed to target specific misconceptions or insufficient\nknowledge among students. To date, the task of crafting high-quality\ndistractors has largely remained a labor-intensive process for teachers and\nlearning content designers, which has limited scalability. In this work, we\nexplore the task of automated distractor and corresponding feedback message\ngeneration in math MCQs using large language models. We establish a formulation\nof these two tasks and propose a simple, in-context learning-based solution.\nMoreover, we propose generative AI-based metrics for evaluating the quality of\nthe feedback messages. We conduct extensive experiments on these tasks using a\nreal-world MCQ dataset. Our findings suggest that there is a lot of room for\nimprovement in automated distractor and feedback generation; based on these\nfindings, we outline several directions for future work.\n', '  Multiple-choice questions (MCQs) are ubiquitous in almost all levels of\neducation since they are easy to administer, grade, and are a reliable format\nin assessments and practices. One of the most important aspects of MCQs is the\ndistractors, i.e., incorrect options that are designed to target common errors\nor misconceptions among real students. To date, the task of crafting\nhigh-quality distractors largely remains a labor and time-intensive process for\nteachers and learning content designers, which has limited scalability. In this\nwork, we study the task of automated distractor generation in the domain of\nmath MCQs and explore a wide variety of large language model (LLM)-based\napproaches, from in-context learning to fine-tuning. We conduct extensive\nexperiments using a real-world math MCQ dataset and find that although LLMs can\ngenerate some mathematically valid distractors, they are less adept at\nanticipating common errors or misconceptions among real students.\n']",Explainable AI and Machine Learning,Evaluating Saliency Methods for Explainable AI
95,"Weakly-Supervised Semantic Segmentation , Weakly Supervised 3D Object Detection , Instance Segmentation Models","['supervised', 'segmentation', 'segmenting', 'classifier', 'classes', 'foreground', 'masks', 'coco', 'labels', 'background'] , ['3d', 'points', 'supervised', 'lidar', 'detr3d', 'annotations', 'scenes', 'segmentation', '2d', 'annotation'] , ['cnn', 'segmentation', 'objects', 'instance', 'detection', 'vision', 'images', 'masks', 'detectors', 'mask']","['  Generating reliable pseudo masks from image-level labels is challenging in\nthe weakly supervised semantic segmentation (WSSS) task due to the lack of\nspatial information. Prevalent class activation map (CAM)-based solutions are\nchallenged to discriminate the foreground (FG) objects from the suspicious\nbackground (BG) pixels (a.k.a. co-occurring) and learn the integral object\nregions. This paper proposes a simple fine-grained background representation\n(FBR) method to discover and represent diverse BG semantics and address the\nco-occurring problems. We abandon using the class prototype or pixel-level\nfeatures for BG representation. Instead, we develop a novel primitive, negative\nregion of interest (NROI), to capture the fine-grained BG semantic information\nand conduct the pixel-to-NROI contrast to distinguish the confusing BG pixels.\nWe also present an active sampling strategy to mine the FG negatives\non-the-fly, enabling efficient pixel-to-pixel intra-foreground contrastive\nlearning to activate the entire object region. Thanks to the simplicity of\ndesign and convenience in use, our proposed method can be seamlessly plugged\ninto various models, yielding new state-of-the-art results under various WSSS\nsettings across benchmarks. Leveraging solely image-level (I) labels as\nsupervision, our method achieves 73.2 mIoU and 45.6 mIoU segmentation results\non Pascal Voc and MS COCO test sets, respectively. Furthermore, by\nincorporating saliency maps as an additional supervision signal (I+S), we\nattain 74.9 mIoU on Pascal Voc test set. Concurrently, our FBR approach\ndemonstrates meaningful performance gains in weakly-supervised instance\nsegmentation (WSIS) tasks, showcasing its robustness and strong generalization\ncapabilities across diverse domains.\n', '  Weakly-Supervised Semantic Segmentation (WSSS) aims to train segmentation\nmodels using image data with only image-level supervision. Since precise\npixel-level annotations are not accessible, existing methods typically focus on\nproducing pseudo masks for training segmentation models by refining CAM-like\nheatmaps. However, the produced heatmaps may capture only the discriminative\nimage regions of object categories or the associated co-occurring backgrounds.\nTo address the issues, we propose a Semantic Prompt Learning for WSSS (SemPLeS)\nframework, which learns to effectively prompt the CLIP latent space to enhance\nthe semantic alignment between the segmented regions and the target object\ncategories. More specifically, we propose Contrastive Prompt Learning and\nPrompt-guided Semantic Refinement to learn the prompts that adequately describe\nand suppress the co-occurring backgrounds associated with each target object\ncategory. In this way, SemPLeS can perform better semantic alignment between\nobject regions and the associated class labels, resulting in desired pseudo\nmasks for training the segmentation model. The proposed SemPLeS framework\nachieves SOTA performance on the standard WSSS benchmarks, PASCAL VOC and MS\nCOCO, and shows compatibility with other WSSS methods. The source codes are\nprovided in the supplementary.\n', '  Image-level weakly-supervised semantic segmentation (WSSS) reduces the\nusually vast data annotation cost by surrogate segmentation masks during\ntraining. The typical approach involves training an image classification\nnetwork using global average pooling (GAP) on convolutional feature maps. This\nenables the estimation of object locations based on class activation maps\n(CAMs), which identify the importance of image regions. The CAMs are then used\nto generate pseudo-labels, in the form of segmentation masks, to supervise a\nsegmentation model in the absence of pixel-level ground truth. Our work is\nbased on two techniques for improving CAMs; importance sampling, which is a\nsubstitute for GAP, and the feature similarity loss, which utilizes a heuristic\nthat object contours almost always align with color edges in images. However,\nboth are based on the multinomial posterior with softmax, and implicitly assume\nthat classes are mutually exclusive, which turns out suboptimal in our\nexperiments. Thus, we reformulate both techniques based on binomial posteriors\nof multiple independent binary problems. This has two benefits; their\nperformance is improved and they become more general, resulting in an add-on\nmethod that can boost virtually any WSSS method. This is demonstrated on a wide\nvariety of baselines on the PASCAL VOC dataset, improving the region similarity\nand contour quality of all implemented state-of-the-art methods. Experiments on\nthe MS COCO dataset further show that our proposed add-on is well-suited for\nlarge-scale settings. Our code implementation is available at\nhttps://github.com/arvijj/hfpl.\n'] , [""  Training high-accuracy 3D detectors necessitates massive labeled 3D\nannotations with 7 degree-of-freedom, which is laborious and time-consuming.\nTherefore, the form of point annotations is proposed to offer significant\nprospects for practical applications in 3D detection, which is not only more\naccessible and less expensive but also provides strong spatial information for\nobject localization. In this paper, we empirically discover that it is\nnon-trivial to merely adapt Point-DETR to its 3D form, encountering two main\nbottlenecks: 1) it fails to encode strong 3D prior into the model, and 2) it\ngenerates low-quality pseudo labels in distant regions due to the extreme\nsparsity of LiDAR points. To overcome these challenges, we introduce\nPoint-DETR3D, a teacher-student framework for weakly semi-supervised 3D\ndetection, designed to fully capitalize on point-wise supervision within a\nconstrained instance-wise annotation budget.Different from Point-DETR which\nencodes 3D positional information solely through a point encoder, we propose an\nexplicit positional query initialization strategy to enhance the positional\nprior. Considering the low quality of pseudo labels at distant regions produced\nby the teacher model, we enhance the detector's perception by incorporating\ndense imagery data through a novel Cross-Modal Deformable RoI Fusion\n(D-RoI).Moreover, an innovative point-guided self-supervised learning technique\nis proposed to allow for fully exploiting point priors, even in student\nmodels.Extensive experiments on representative nuScenes dataset demonstrate our\nPoint-DETR3D obtains significant improvements compared to previous works.\nNotably, with only 5% of labeled data, Point-DETR3D achieves over 90%\nperformance of its fully supervised counterpart.\n"", '  3D object detection plays a crucial role in various applications such as\nautonomous vehicles, robotics and augmented reality. However, training 3D\ndetectors requires a costly precise annotation, which is a hindrance to scaling\nannotation to large datasets. To address this challenge, we propose a weakly\nsupervised 3D annotator that relies solely on 2D bounding box annotations from\nimages, along with size priors. One major problem is that supervising a 3D\ndetection model using only 2D boxes is not reliable due to ambiguities between\ndifferent 3D poses and their identical 2D projection. We introduce a simple yet\neffective and generic solution: we build 3D proxy objects with annotations by\nconstruction and add them to the training dataset. Our method requires only\nsize priors to adapt to new classes. To better align 2D supervision with 3D\ndetection, our method ensures depth invariance with a novel expression of the\n2D losses. Finally, to detect more challenging instances, our annotator follows\nan offline pseudo-labelling scheme which gradually improves its 3D\npseudo-labels. Extensive experiments on the KITTI dataset demonstrate that our\nmethod not only performs on-par or above previous works on the Car category,\nbut also achieves performance close to fully supervised methods on more\nchallenging classes. We further demonstrate the effectiveness and robustness of\nour method by being the first to experiment on the more challenging nuScenes\ndataset. We additionally propose a setting where weak labels are obtained from\na 2D detector pre-trained on MS-COCO instead of human annotations.\n', '  We present a Multimodal Interlaced Transformer (MIT) that jointly considers\n2D and 3D data for weakly supervised point cloud segmentation. Research studies\nhave shown that 2D and 3D features are complementary for point cloud\nsegmentation. However, existing methods require extra 2D annotations to achieve\n2D-3D information fusion. Considering the high annotation cost of point clouds,\neffective 2D and 3D feature fusion based on weakly supervised learning is in\ngreat demand. To this end, we propose a transformer model with two encoders and\none decoder for weakly supervised point cloud segmentation using only\nscene-level class tags. Specifically, the two encoders compute the\nself-attended features for 3D point clouds and 2D multi-view images,\nrespectively. The decoder implements interlaced 2D-3D cross-attention and\ncarries out implicit 2D and 3D feature fusion. We alternately switch the roles\nof queries and key-value pairs in the decoder layers. It turns out that the 2D\nand 3D features are iteratively enriched by each other. Experiments show that\nit performs favorably against existing weakly supervised point cloud\nsegmentation methods by a large margin on the S3DIS and ScanNet benchmarks. The\nproject page will be available at https://jimmy15923.github.io/mit_web/.\n'] , ['  Multi-class multi-instance segmentation is the task of identifying masks for\nmultiple object classes and multiple instances of the same class within an\nimage. The foundational Segment Anything Model (SAM) is designed for promptable\nmulti-class multi-instance segmentation but tends to output part or sub-part\nmasks in the ""everything"" mode for various real-world applications. Whole\nobject segmentation masks play a crucial role for indoor scene understanding,\nespecially in robotics applications. We propose a new domain invariant\nReal-to-Simulation (Real-Sim) fine-tuning strategy for SAM. We use object\nimages and ground truth data collected from Ai2Thor simulator during\nfine-tuning (real-to-sim). To allow our Segment Any Object Model (SAOM) to work\nin the ""everything"" mode, we propose the novel nearest neighbour assignment\nmethod, updating point embeddings for each ground-truth mask. SAOM is evaluated\non our own dataset collected from Ai2Thor simulator. SAOM significantly\nimproves on SAM, with a 28% increase in mIoU and a 25% increase in mAcc for 54\nfrequently-seen indoor object classes. Moreover, our Real-to-Simulation\nfine-tuning strategy demonstrates promising generalization performance in real\nenvironments without being trained on the real-world data (sim-to-real). The\ndataset and the code will be released after publication.\n', ""  Instance segmentation is a form of image detection which has a range of\napplications, such as object refinement, medical image analysis, and\nimage/video editing, all of which demand a high degree of accuracy. However,\nthis precision is often beyond the reach of what even state-of-the-art, fully\nautomated instance segmentation algorithms can deliver. The performance gap\nbecomes particularly prohibitive for small and complex objects. Practitioners\ntypically resort to fully manual annotation, which can be a laborious process.\nIn order to overcome this problem, we propose a novel approach to enable more\nprecise predictions and generate higher-quality segmentation masks for\nhigh-curvature, complex and small-scale objects. Our human-assisted\nsegmentation model, HAISTA-NET, augments the existing Strong Mask R-CNN network\nto incorporate human-specified partial boundaries. We also present a dataset of\nhand-drawn partial object boundaries, which we refer to as human attention\nmaps. In addition, the Partial Sketch Object Boundaries (PSOB) dataset contains\nhand-drawn partial object boundaries which represent curvatures of an object's\nground truth mask with several pixels. Through extensive evaluation using the\nPSOB dataset, we show that HAISTA-NET outperforms state-of-the art methods such\nas Mask R-CNN, Strong Mask R-CNN, and Mask2Former, achieving respective\nincreases of +36.7, +29.6, and +26.5 points in AP-Mask metrics for these three\nmodels. We hope that our novel approach will set a baseline for future\nhuman-aided deep learning models by combining fully automated and interactive\ninstance segmentation architectures.\n"", '  Recently, foundation models trained on massive datasets to adapt to a wide\nrange of domains have attracted considerable attention and are actively being\nexplored within the computer vision community. Among these, the Segment\nAnything Model (SAM) stands out for its remarkable progress in generalizability\nand flexibility for image segmentation tasks, achieved through prompt-based\nobject mask generation. However, despite its strength, SAM faces two key\nlimitations when applied to customized instance segmentation that segments\nspecific objects or those in unique environments not typically present in the\ntraining data: 1) the ambiguity inherent in input prompts and 2) the necessity\nfor extensive additional training to achieve optimal segmentation. To address\nthese challenges, we propose a novel method, customized instance segmentation\nvia prompt learning tailored to SAM. Our method involves a prompt learning\nmodule (PLM), which adjusts input prompts into the embedding space to better\nalign with user intentions, thereby enabling more efficient training.\nFurthermore, we introduce a point matching module (PMM) to enhance the feature\nrepresentation for finer segmentation by ensuring detailed alignment with\nground truth boundaries. Experimental results on various customized instance\nsegmentation scenarios demonstrate the effectiveness of the proposed method.\n']",Weakly Supervised Computer Vision,Weakly-Supervised Semantic Segmentation
96,"Agricultural Object Recognition and Monitoring , Object Detection in Aerial Imagery","['crop', 'agriculture', 'agricultural', 'crops', 'cnn', 'farming', 'images', 'livestock', 'vision', 'image'] , ['fusionnet', 'detection', 'images', 'imagery', 'lidar', 'yolov5', 'yolov3', 'yolov8', 'segmentation', 'videos']","[""  Vision is a major component in several digital technologies and tools used in\nagriculture. The object detector, You Look Only Once (YOLO), has gained\npopularity in agriculture in a relatively short span due to its\nstate-of-the-art performance. YOLO offers real-time detection with good\naccuracy and is implemented in various agricultural tasks, including\nmonitoring, surveillance, sensing, automation, and robotics. The research and\napplication of YOLO in agriculture are accelerating rapidly but are fragmented\nand multidisciplinary. Moreover, the performance characteristics (i.e.,\naccuracy, speed, computation) of the object detector influence the rate of\ntechnology implementation and adoption in agriculture. Thus, the study aims to\ncollect extensive literature to document and critically evaluate the advances\nand application of YOLO for agricultural object recognition. First, we\nconducted a bibliometric review of 257 articles to understand the scholarly\nlandscape of YOLO in agricultural domain. Secondly, we conducted a systematic\nreview of 30 articles to identify current knowledge, gaps, and modifications in\nYOLO for specific agricultural tasks. The study critically assesses and\nsummarizes the information on YOLO's end-to-end learning approach, including\ndata acquisition, processing, network modification, integration, and\ndeployment. We also discussed task-specific YOLO algorithm modification and\nintegration to meet the agricultural object or environment-specific challenges.\nIn general, YOLO-integrated digital tools and technologies show the potential\nfor real-time, automated monitoring, surveillance, and object handling to\nreduce labor, production cost, and environmental impact while maximizing\nresource efficiency. The study provides detailed documentation and\nsignificantly advances the existing knowledge on applying YOLO in agriculture,\nwhich can greatly benefit the scientific community.\n"", '  Large models can play important roles in many domains. Agriculture is another\nkey factor affecting the lives of people around the world. It provides food,\nfabric, and coal for humanity. However, facing many challenges such as pests\nand diseases, soil degradation, global warming, and food security, how to\nsteadily increase the yield in the agricultural sector is a problem that humans\nstill need to solve. Large models can help farmers improve production\nefficiency and harvest by detecting a series of agricultural production tasks\nsuch as pests and diseases, soil quality, and seed quality. It can also help\nfarmers make wise decisions through a variety of information, such as images,\ntext, etc. Herein, we delve into the potential applications of large models in\nagriculture, from large language model (LLM) and large vision model (LVM) to\nlarge vision-language models (LVLM). After gaining a deeper understanding of\nmultimodal large language models (MLLM), it can be recognized that problems\nsuch as agricultural image processing, agricultural question answering systems,\nand agricultural machine automation can all be solved by large models. Large\nmodels have great potential in the field of agriculture. We outline the current\napplications of agricultural large models, and aims to emphasize the importance\nof large models in the domain of agriculture. In the end, we envisage a future\nin which famers use MLLM to accomplish many tasks in agriculture, which can\ngreatly improve agricultural production efficiency and yield.\n', ""  We present a specialized procedural model for generating synthetic\nagricultural scenes, focusing on soybean crops, along with various weeds. This\nmodel is capable of simulating distinct growth stages of these plants, diverse\nsoil conditions, and randomized field arrangements under varying lighting\nconditions. The integration of real-world textures and environmental factors\ninto the procedural generation process enhances the photorealism and\napplicability of the synthetic data. Our dataset includes 12,000 images with\nsemantic labels, offering a comprehensive resource for computer vision tasks in\nprecision agriculture, such as semantic segmentation for autonomous weed\ncontrol. We validate our model's effectiveness by comparing the synthetic data\nagainst real agricultural images, demonstrating its potential to significantly\naugment training data for machine learning models in agriculture. This approach\nnot only provides a cost-effective solution for generating high-quality,\ndiverse data but also addresses specific needs in agricultural vision tasks\nthat are not fully covered by general-purpose models.\n""] , [""  One of the most important problems in computer vision and remote sensing is\nobject detection, which identifies particular categories of diverse things in\npictures. Two crucial data sources for public security are the thermal infrared\n(TIR) remote sensing multi-scenario photos and videos produced by unmanned\naerial vehicles (UAVs). Due to the small scale of the target, complex scene\ninformation, low resolution relative to the viewable videos, and dearth of\npublicly available labeled datasets and training models, their object detection\nprocedure is still difficult. A UAV TIR object detection framework for pictures\nand videos is suggested in this study. The Forward-looking Infrared (FLIR)\ncameras used to gather ground-based TIR photos and videos are used to create\nthe ``You Only Look Once'' (YOLO) model, which is based on CNN architecture.\nResults indicated that in the validating task, detecting human object had an\naverage precision at IOU (Intersection over Union) = 0.5, which was 72.5\\%,\nusing YOLOv7 (YOLO version 7) state of the art model \\cite{1}, while the\ndetection speed around 161 frames per second (FPS/second). The usefulness of\nthe YOLO architecture is demonstrated in the application, which evaluates the\ncross-detection performance of people in UAV TIR videos under a YOLOv7 model in\nterms of the various UAVs' observation angles. The qualitative and quantitative\nevaluation of object detection from TIR pictures and videos using deep-learning\nmodels is supported favorably by this work.\n"", '  Robust perception is crucial in autonomous vehicle navigation and\nlocalization. Visual processing tasks, like semantic segmentation, should work\nin varying weather conditions and during different times of day. Semantic\nsegmentation is where each pixel is assigned a class, which is useful for\nlocating overall features (1). Training a segmentation model requires large\namounts of data, and the labeling process for segmentation data is especially\ntedious. Additionally, many large datasets include only images taken in clear\nweather. This is a problem because training a model exclusively on clear\nweather data hinders performance in adverse weather conditions like fog or\nrain. We hypothesize that given a dataset of only clear days images, applying\nimage augmentation (such as random rain, fog, and brightness) during training\nallows for domain adaptation to diverse weather conditions. We used CARLA, a 3D\nrealistic autonomous vehicle simulator, to collect 1200 images in clear weather\ncomposed of 29 classes from 10 different towns (2). We also collected 1200\nimages of random weather effects. We trained encoder-decoder UNet models to\nperform semantic segmentation. Applying augmentations significantly improved\nsegmentation under weathered night conditions (p < 0.001). However, models\ntrained on weather data have significantly lower losses than those trained on\naugmented data in all conditions except for clear days. This shows there is\nroom for improvement in the domain adaptation approach. Future work should test\nmore types of augmentations and also use real-life images instead of CARLA.\nIdeally, the augmented model meets or exceeds the performance of the weather\nmodel.\n', '  Driving is challenging in conditions like night, rain, and snow. The lack of\ngood labeled datasets has hampered progress in scene understanding under such\nconditions. Unsupervised domain adaptation (UDA) using large labeled clear-day\ndatasets is a promising research direction in such cases. Current UDA methods,\nhowever, treat all image pixels uniformly, leading to over-reliance on the\ndominant scene backgrounds (e.g., roads, sky, sidewalks) that appear\ndramatically different across domains. As a result, they struggle to learn\neffective features of smaller and often sparse foreground objects (e.g.,\npeople, vehicles, signs).\n  In this work, we improve UDA training by using in-place image warping to\nfocus on salient object regions. Our insight is that while backgrounds vary\nsignificantly across domains (e.g., snowy night vs. clear day), object\nappearances vary to a lesser extent. Therefore, we design instance-level\nsaliency guidance to adaptively oversample object regions, which reduces\nadverse effects from background context and enhances backbone feature learning.\nWe then unwarp the better learned features while adapting from source to\ntarget. Our approach improves adaptation across geographies, lighting, and\nweather conditions, and is agnostic to the task (segmentation, detection),\ndomain adaptation algorithm, saliency guidance, and underlying model\narchitecture. Result highlights include +6.1 mAP50 for BDD100K Clear\n$\\rightarrow$ DENSE Foggy, +3.7 mAP50 for BDD100K Day $\\rightarrow$ Night, +3.0\nmAP50 for BDD100K Clear $\\rightarrow$ Rainy, and +6.3 mIoU for Cityscapes\n$\\rightarrow$ ACDC. Our method adds minimal training memory and incurs no\nadditional inference latency. Please see Appendix for more results and\nanalysis.\n']",Computer Vision Applications in Agriculture and Aerial Imagery,Object Detection in Aerial Imagery
97,"Visual Object Tracking , Object-Centric Representations and Visual Tokenization , ""Vision Transformers for Object Detection and Recognition"" , Visual Object Representations and Recognition , ""Visual Recognition and Classification via Pre-training""","['tracking', 'tracklets', 'tracker', 'trackers', 'tracklet', 'track', 'tracks', 'frames', 'scenes', 'detections'] , ['attention', 'tokenizers', 'tokenizer', 'tokenization', 'representations', 'recognition', 'imagenet', 'vision', 'visual', 'representation'] , ['recognition', 'vision', 'detection', 'supervised', 'detectors', 'objectness', 'attention', 'yolov8', 'objects', '3d'] , ['recognition', 'imagenet', 'neural', 'vision', 'representations', 'supervised', 'objects', 'features', 'convolutional', 'visual'] , ['supervised', 'classification', 'learning', 'recognition', 'imagenet', 'classes', 'trained', 'pretraining', 'learned', 'training']","['  We observe that the performance of SOTA visual trackers surprisingly strongly\nvaries across different video attributes and datasets. No single tracker\nremains the best performer across all tracking attributes and datasets. To\nbridge this gap, for a given video sequence, we predict the ""Best of the N\nTrackers"", called the BofN meta-tracker. At its core, a Tracking Performance\nPrediction Network (TP2N) selects a predicted best performing visual tracker\nfor the given video sequence using only a few initial frames. We also introduce\na frame-level BofN meta-tracker which keeps predicting best performer after\nregular temporal intervals. The TP2N is based on self-supervised learning\narchitectures MocoV2, SwAv, BT, and DINO; experiments show that the DINO with\nViT-S as a backbone performs the best. The video-level BofN meta-tracker\noutperforms, by a large margin, existing SOTA trackers on nine standard\nbenchmarks - LaSOT, TrackingNet, GOT-10K, VOT2019, VOT2021, VOT2022, UAV123,\nOTB100, and WebUAV-3M. Further improvement is achieved by the frame-level BofN\nmeta-tracker effectively handling variations in the tracking scenarios within\nlong sequences. For instance, on GOT-10k, BofN meta-tracker average overlap is\n88.7% and 91.1% with video and frame-level settings respectively. The best\nperforming tracker, RTS, achieves 85.20% AO. On VOT2022, BofN expected average\noverlap is 67.88% and 70.98% with video and frame level settings, compared to\nthe best performing ARTrack, 64.12%. This work also presents an extensive\nevaluation of competitive tracking methods on all commonly used benchmarks,\nfollowing their protocols. The code, the trained models, and the results will\nsoon be made publicly available on\nhttps://github.com/BasitAlawode/Best_of_N_Trackers.\n', '  Current event-/frame-event based trackers undergo evaluation on short-term\ntracking datasets, however, the tracking of real-world scenarios involves\nlong-term tracking, and the performance of existing tracking algorithms in\nthese scenarios remains unclear. In this paper, we first propose a new\nlong-term and large-scale frame-event single object tracking dataset, termed\nFELT. It contains 742 videos and 1,594,474 RGB frames and event stream pairs\nand has become the largest frame-event tracking dataset to date. We re-train\nand evaluate 15 baseline trackers on our dataset for future works to compare.\nMore importantly, we find that the RGB frames and event streams are naturally\nincomplete due to the influence of challenging factors and spatially sparse\nevent flow. In response to this, we propose a novel associative memory\nTransformer network as a unified backbone by introducing modern Hopfield layers\ninto multi-head self-attention blocks to fuse both RGB and event data.\nExtensive experiments on RGB-Event (FELT), RGB-Thermal (RGBT234, LasHeR), and\nRGB-Depth (DepthTrack) datasets fully validated the effectiveness of our model.\nThe dataset and source code can be found at\n\\url{https://github.com/Event-AHU/FELT_SOT_Benchmark}.\n', '  RGB-Event based tracking is an emerging research topic, focusing on how to\neffectively integrate heterogeneous multi-modal data (synchronized exposure\nvideo frames and asynchronous pulse Event stream). Existing works typically\nemploy Transformer based networks to handle these modalities and achieve decent\naccuracy through input-level or feature-level fusion on multiple datasets.\nHowever, these trackers require significant memory consumption and\ncomputational complexity due to the use of self-attention mechanism. This paper\nproposes a novel RGB-Event tracking framework, Mamba-FETrack, based on the\nState Space Model (SSM) to achieve high-performance tracking while effectively\nreducing computational costs and realizing more efficient tracking.\nSpecifically, we adopt two modality-specific Mamba backbone networks to extract\nthe features of RGB frames and Event streams. Then, we also propose to boost\nthe interactive learning between the RGB and Event features using the Mamba\nnetwork. The fused features will be fed into the tracking head for target\nobject localization. Extensive experiments on FELT and FE108 datasets fully\nvalidated the efficiency and effectiveness of our proposed tracker.\nSpecifically, our Mamba-based tracker achieves 43.5/55.6 on the SR/PR metric,\nwhile the ViT-S based tracker (OSTrack) obtains 40.0/50.9. The GPU memory cost\nof ours and ViT-S based tracker is 13.98GB and 15.44GB, which decreased about\n$9.5\\%$. The FLOPs and parameters of ours/ViT-S based OSTrack are 59GB/1076GB\nand 7MB/60MB, which decreased about $94.5\\%$ and $88.3\\%$, respectively. We\nhope this work can bring some new insights to the tracking field and greatly\npromote the application of the Mamba architecture in tracking. The source code\nof this work will be released on\n\\url{https://github.com/Event-AHU/Mamba_FETrack}.\n'] , ['  The extraction of modular object-centric representations for downstream tasks\nis an emerging area of research. Learning grounded representations of objects\nthat are guaranteed to be stable and invariant promises robust performance\nacross different tasks and environments. Slot Attention (SA) learns\nobject-centric representations by assigning objects to \\textit{slots}, but\npresupposes a \\textit{single} distribution from which all slots are randomly\ninitialised. This results in an inability to learn \\textit{specialized} slots\nwhich bind to specific object types and remain invariant to identity-preserving\nchanges in object appearance. To address this, we present\n\\emph{\\textsc{Co}nditional \\textsc{S}lot \\textsc{A}ttention} (\\textsc{CoSA})\nusing a novel concept of \\emph{Grounded Slot Dictionary} (GSD) inspired by\nvector quantization. Our proposed GSD comprises (i) canonical object-level\nproperty vectors and (ii) parametric Gaussian distributions, which define a\nprior over the slots. We demonstrate the benefits of our method in multiple\ndownstream tasks such as scene generation, composition, and task adaptation,\nwhilst remaining competitive with SA in popular object discovery benchmarks.\n', '  Object-centric methods have seen significant progress in unsupervised\ndecomposition of raw perception into rich object-like abstractions. However,\nlimited ability to ground object semantics of the real world into the learned\nabstractions has hindered their adoption in downstream understanding\napplications. We present the Neural Slot Interpreter (NSI) that learns to\nground and generate object semantics via slot representations. At the core of\nNSI is an XML-like programming language that uses simple syntax rules to\norganize the object semantics of a scene into object-centric program\nprimitives. Then, an alignment model learns to ground program primitives into\nslots through a bi-level contrastive learning objective over a shared embedding\nspace. Finally, we formulate the NSI program generator model to use the dense\nassociations inferred from the alignment model to generate object-centric\nprograms from slots. Experiments on bi-modal retrieval tasks demonstrate the\nefficacy of the learned alignments, surpassing set-matching-based predictors by\na significant margin. Moreover, learning the program generator from grounded\nassociations enhances the predictive power of slots. NSI generated programs\ndemonstrate improved performance of object-centric learners on property\nprediction and object detection, and scale with real-world scene complexity.\n', '  The tokenizer, as one of the fundamental components of large models, has long\nbeen overlooked or even misunderstood in visual tasks. One key factor of the\ngreat comprehension power of the large language model is that natural language\ntokenizers utilize meaningful words or subwords as the basic elements of\nlanguage. In contrast, mainstream visual tokenizers, represented by patch-based\nmethods such as Patch Embed, rely on meaningless rectangular patches as basic\nelements of vision, which cannot serve as effectively as words or subwords in\nlanguage. Starting from the essence of the tokenizer, we defined semantically\nindependent regions (SIRs) for vision. We designed a simple HOmogeneous visual\ntOKenizer: HOOK. HOOK mainly consists of two modules: the Object Perception\nModule (OPM) and the Object Vectorization Module (OVM). To achieve homogeneity,\nthe OPM splits the image into 4*4 pixel seeds and then utilizes the attention\nmechanism to perceive SIRs. The OVM employs cross-attention to merge seeds\nwithin the same SIR. To achieve adaptability, the OVM defines a variable number\nof learnable vectors as cross-attention queries, allowing for the adjustment of\ntoken quantity. We conducted experiments on the NWPU-RESISC45, WHU-RS19\nclassification dataset, and GID5 segmentation dataset for sparse and dense\ntasks. The results demonstrate that the visual tokens obtained by HOOK\ncorrespond to individual objects, which demonstrates homogeneity. HOOK\noutperformed Patch Embed by 6\\% and 10\\% in the two tasks and achieved\nstate-of-the-art performance compared to the baselines used for comparison.\nCompared to Patch Embed, which requires more than one hundred tokens for one\nimage, HOOK requires only 6 and 8 tokens for sparse and dense tasks,\nrespectively, resulting in efficiency improvements of 1.5 to 2.8 times. The\ncode is available at https://github.com/GeoX-Lab/Hook.\n'] , ['  Since the introduction of the self-attention mechanism and the adoption of\nthe Transformer architecture for Computer Vision tasks, the Vision\nTransformer-based architectures gained a lot of popularity in the field, being\nused for tasks such as image classification, object detection and image\nsegmentation. However, efficiently leveraging the attention mechanism in vision\ntransformers for the Monocular 3D Object Detection task remains an open\nquestion. In this paper, we present LAM3D, a framework that Leverages\nself-Attention mechanism for Monocular 3D object Detection. To do so, the\nproposed method is built upon a Pyramid Vision Transformer v2 (PVTv2) as\nfeature extraction backbone and 2D/3D detection machinery. We evaluate the\nproposed method on the KITTI 3D Object Detection Benchmark, proving the\napplicability of the proposed solution in the autonomous driving domain and\noutperforming reference methods. Moreover, due to the usage of self-attention,\nLAM3D is able to systematically outperform the equivalent architecture that\ndoes not employ self-attention.\n', ""  Although accuracy and other common metrics can provide a useful window into\nthe performance of an object detection model, they lack a deeper view of the\nmodel's decision process. Regardless of the quality of the training data and\nprocess, the features that an object detection model learns cannot be\nguaranteed. A model may learn a relationship between certain background\ncontext, i.e., scene level objects, and the presence of the labeled classes.\nFurthermore, standard performance verification and metrics would not identify\nthis phenomenon. This paper presents a new black box explainability method for\nadditional verification of object detection models by finding the impact of\nscene level objects on the identification of the objects within the image. By\ncomparing the accuracies of a model on test data with and without certain scene\nlevel objects, the contributions of these objects to the model's performance\nbecomes clearer. The experiment presented here will assess the impact of\nbuildings and people in image context on the detection of emergency road\nvehicles by a fine-tuned YOLOv8 model. A large increase in accuracy in the\npresence of a scene level object will indicate the model's reliance on that\nobject to make its detections. The results of this research lead to providing a\nquantitative explanation of the object detection model's decision process,\nenabling a deeper understanding of the model's performance.\n"", '  This survey explores the adaptation of visual transformer models in\nAutonomous Driving, a transition inspired by their success in Natural Language\nProcessing. Surpassing traditional Recurrent Neural Networks in tasks like\nsequential image processing and outperforming Convolutional Neural Networks in\nglobal context capture, as evidenced in complex scene recognition, Transformers\nare gaining traction in computer vision. These capabilities are crucial in\nAutonomous Driving for real-time, dynamic visual scene processing. Our survey\nprovides a comprehensive overview of Vision Transformer applications in\nAutonomous Driving, focusing on foundational concepts such as self-attention,\nmulti-head attention, and encoder-decoder architecture. We cover applications\nin object detection, segmentation, pedestrian detection, lane detection, and\nmore, comparing their architectural merits and limitations. The survey\nconcludes with future research directions, highlighting the growing role of\nVision Transformers in Autonomous Driving.\n'] , ['  High-level visual brain regions contain subareas in which neurons appear to\nrespond more strongly to examples of a particular semantic category, like faces\nor bodies, rather than objects. However, recent work has shown that while this\nfinding holds on average, some out-of-category stimuli also activate neurons in\nthese regions. This may be due to visual features common among the preferred\nclass also being present in other images. Here, we propose a\ndeep-learning-based approach for visualizing these features. For each neuron,\nwe identify relevant visual features driving its selectivity by modelling\nresponses to images based on latent activations of a deep neural network. Given\nan out-of-category image which strongly activates the neuron, our method first\nidentifies a reference image from the preferred category yielding a similar\nfeature activation pattern. We then backpropagate latent activations of both\nimages to the pixel level, while enhancing the identified shared dimensions and\nattenuating non-shared features. The procedure highlights image regions\ncontaining shared features driving responses of the model neuron. We apply the\nalgorithm to novel recordings from body-selective regions in macaque IT cortex\nin order to understand why some images of objects excite these neurons.\nVisualizations reveal object parts which resemble parts of a macaque body,\nshedding light on neural preference of these objects.\n', '  Recent work has shown that object-centric representations can greatly help\nimprove the accuracy of learning dynamics while also bringing interpretability.\nIn this work, we take this idea one step further, ask the following question:\n""can learning disentangled representation further improve the accuracy of\nvisual dynamics prediction in object-centric models?"" While there has been some\nattempt to learn such disentangled representations for the case of static\nimages \\citep{nsb}, to the best of our knowledge, ours is the first work which\ntries to do this in a general setting for video, without making any specific\nassumptions about the kind of attributes that an object might have. The key\nbuilding block of our architecture is the notion of a {\\em block}, where\nseveral blocks together constitute an object. Each block is represented as a\nlinear combination of a given number of learnable concept vectors, which is\niteratively refined during the learning process. The blocks in our model are\ndiscovered in an unsupervised manner, by attending over object masks, in a\nstyle similar to discovery of slots \\citep{slot_attention}, for learning a\ndense object-centric representation. We employ self-attention via transformers\nover the discovered blocks to predict the next state resulting in discovery of\nvisual dynamics. We perform a series of experiments on several benchmark 2-D,\nand 3-D datasets demonstrating that our architecture (1) can discover\nsemantically meaningful blocks (2) help improve accuracy of dynamics prediction\ncompared to SOTA object-centric models (3) perform significantly better in OOD\nsetting where the specific attribute combinations are not seen earlier during\ntraining. Our experiments highlight the importance discovery of disentangled\nrepresentation for visual dynamics prediction.\n', ""  We add one more invariance - state invariance - to the more commonly used\nother invariances for learning object representations for recognition and\nretrieval. By state invariance, we mean robust with respect to changes in the\nstructural form of the object, such as when an umbrella is folded, or when an\nitem of clothing is tossed on the floor. Since humans generally have no\ndifficulty in recognizing objects despite such state changes, we are naturally\nfaced with the question of whether it is possible to devise a neural\narchitecture with similar abilities. To that end, we present a novel dataset,\nObjectsWithStateChange, that captures state and pose variations in the object\nimages recorded from arbitrary viewpoints. We believe that this dataset will\nfacilitate research in fine-grained object recognition and retrieval of objects\nthat are capable of state changes. The goal of such research would be to train\nmodels capable of generating object embeddings that remain invariant to state\nchanges while also staying invariant to transformations induced by changes in\nviewpoint, pose, illumination, etc. To demonstrate the usefulness of the\nObjectsWithStateChange dataset, we also propose a curriculum learning strategy\nthat uses the similarity relationships in the learned embedding space after\neach epoch to guide the training process. The model learns discriminative\nfeatures by comparing visually similar objects within and across different\ncategories, encouraging it to differentiate between objects that may be\nchallenging to distinguish due to changes in their state. We believe that this\nstrategy enhances the model's ability to capture discriminative features for\nfine-grained tasks that may involve objects with state changes, leading to\nperformance improvements on object-level tasks not only on our new dataset, but\nalso on two other challenging multi-view datasets such as ModelNet40 and\nObjectPI.\n""] , ['  This paper revisits the standard pretrain-then-finetune paradigm used in\ncomputer vision for visual recognition tasks. Typically, state-of-the-art\nfoundation models are pretrained using large scale (weakly) supervised datasets\nwith billions of images. We introduce an additional pre-pretraining stage that\nis simple and uses the self-supervised MAE technique to initialize the model.\nWhile MAE has only been shown to scale with the size of models, we find that it\nscales with the size of the training dataset as well. Thus, our MAE-based\npre-pretraining scales with both model and data size making it applicable for\ntraining foundation models. Pre-pretraining consistently improves both the\nmodel convergence and the downstream transfer performance across a range of\nmodel scales (millions to billions of parameters), and dataset sizes (millions\nto billions of images). We measure the effectiveness of pre-pretraining on 10\ndifferent visual recognition tasks spanning image classification, video\nrecognition, object detection, low-shot classification and zero-shot\nrecognition. Our largest model achieves new state-of-the-art results on\niNaturalist-18 (91.7%), ImageNet-ReaL (91.1%), 1-shot ImageNet-1k (63.6%), and\nzero-shot transfer on Food-101 (96.2%). Our study reveals that model\ninitialization plays a significant role, even for web-scale pretraining with\nbillions of images, and our models are available publicly.\n', '  Parameter-efficient fine-tuning (PEFT) techniques such as low-rank adaptation\n(LoRA) can effectively adapt large pre-trained foundation models to downstream\ntasks using only a small fraction (0.1%-10%) of the original trainable weights.\nAn under-explored question of PEFT is in extending the pre-training phase\nwithout supervised labels; that is, can we adapt a pre-trained foundation model\nto a new domain via efficient self-supervised pre-training on this new domain?\nIn this work, we introduce ExPLoRA, a highly effective technique to improve\ntransfer learning of pre-trained vision transformers (ViTs) under domain\nshifts. Initializing a ViT with pre-trained weights on large, natural-image\ndatasets such as from DinoV2 or MAE, ExPLoRA continues the unsupervised\npre-training objective on a new domain. In this extended pre-training phase,\nExPLoRA only unfreezes 1-2 pre-trained ViT blocks and all normalization layers,\nand then tunes all other layers with LoRA. Finally, we fine-tune the resulting\nmodel only with LoRA on this new domain for supervised learning. Our\nexperiments demonstrate state-of-the-art results on satellite imagery, even\noutperforming fully pre-training and fine-tuning ViTs. Using the DinoV2\ntraining objective, we demonstrate up to 7% improvement in linear probing top-1\naccuracy on downstream tasks while using <10% of the number of parameters that\nare used in prior fully-tuned state-of-the art approaches. Our ablation studies\nconfirm the efficacy of our approach over other baselines, including PEFT and\nsimply unfreezing more transformer blocks.\n', '  The problem of Novel Class Discovery (NCD) consists in extracting knowledge\nfrom a labeled set of known classes to accurately partition an unlabeled set of\nnovel classes. While NCD has recently received a lot of attention from the\ncommunity, it is often solved on computer vision problems and under unrealistic\nconditions. In particular, the number of novel classes is usually assumed to be\nknown in advance, and their labels are sometimes used to tune hyperparameters.\nMethods that rely on these assumptions are not applicable in real-world\nscenarios. In this work, we focus on solving NCD in tabular data when no prior\nknowledge of the novel classes is available. To this end, we propose to tune\nthe hyperparameters of NCD methods by adapting the $k$-fold cross-validation\nprocess and hiding some of the known classes in each fold. Since we have found\nthat methods with too many hyperparameters are likely to overfit these hidden\nclasses, we define a simple deep NCD model. This method is composed of only the\nessential elements necessary for the NCD problem and performs impressively well\nunder realistic conditions. Furthermore, we find that the latent space of this\nmethod can be used to reliably estimate the number of novel classes.\nAdditionally, we adapt two unsupervised clustering algorithms ($k$-means and\nSpectral Clustering) to leverage the knowledge of the known classes. Extensive\nexperiments are conducted on 7 tabular datasets and demonstrate the\neffectiveness of the proposed method and hyperparameter tuning process, and\nshow that the NCD problem can be solved without relying on knowledge from the\nnovel classes.\n']",Computer Vision and Object Recognition,Visual Object Representations and Recognition
98,"""Vision-Language Models for Image Classification"" , ""3D Object Detection and Vision-Language Models"" , 3D Visual Language Models and Grounding , Vision-Language Models (VLMs) , ""Multimodal Prompt Learning for Vision-Language Tasks"" , Multimodal Language Models for Vision and Language Tasks , ""3D Vision-Language Grounding for Embodied Agents""","['imagenet', 'recognition', 'clips', 'descriptors', 'classification', 'visual', 'trained', 'features', 'learnable', 'datasets'] , ['3d', '3dis', '3dcompat', 'scenes', 'opensun3d', '3ddet', 'visual', 'vision', 'any2point', 'scene'] , ['3d', 'referit3d', 'lv3d', 'scenes', '3ddc', 'visual', 'cube', '3dvg', 'llmi3d', 'annotations'] , ['visual', 'multimodal', 'recognition', 'vision', 'embedding', 'language', 'linguistic', 'concepts', 'semantic', 'vlm'] , ['visual', 'supervised', 'learning', 'multimodal', 'prompts', 'captioning', 'embeddings', 'trained', 'captions', 'prompt'] , ['multimodal', 'videos', 'visual', 'language', 'tasks', 'answering', 'mllm', 'benchmark', 'mind', 'modal'] , ['3d', 'scenecraft', 'scenes', 'uni3d', 'interactive', 'embodied', 'visual', 'robotic', 'scene', 'robot']","['  Recent large vision-language models such as CLIP have shown remarkable\nout-of-distribution (OOD) detection and generalization performance. However,\ntheir zero-shot in-distribution (ID) accuracy is often limited for downstream\ndatasets. Recent CLIP-based fine-tuning methods such as prompt learning have\ndemonstrated significant improvements in ID classification and OOD\ngeneralization where OOD labels are available. Nonetheless, it remains unclear\nwhether the model is reliable to semantic shifts without OOD labels. In this\npaper, we aim to bridge the gap and present a comprehensive study to understand\nhow fine-tuning impact OOD detection for few-shot downstream tasks. By framing\nOOD detection as multi-modal concept matching, we establish a connection\nbetween fine-tuning methods and various OOD scores. Our results suggest that a\nproper choice of OOD scores is essential for CLIP-based fine-tuning. In\nparticular, the maximum concept matching (MCM) score provides a promising\nsolution consistently. We also show that prompt learning demonstrates the\nstate-of-the-art OOD detection performance over the zero-shot counterpart.\n', ""  Foundation models like CLIP are trained on hundreds of millions of samples\nand effortlessly generalize to new tasks and inputs. Out of the box, CLIP shows\nstellar zero-shot and few-shot capabilities on a wide range of\nout-of-distribution (OOD) benchmarks, which prior works attribute mainly to\ntoday's large and comprehensive training dataset (like LAION). However, it is\nquestionable how meaningful terms like out-of-distribution generalization are\nfor CLIP as it seems likely that web-scale datasets like LAION simply contain\nmany samples that are similar to common OOD benchmarks originally designed for\nImageNet. To test this hypothesis, we retrain CLIP on pruned LAION splits that\nreplicate ImageNet's train-test similarity with respect to common OOD\nbenchmarks. While we observe a performance drop on some benchmarks,\nsurprisingly, CLIP's overall performance remains high. This shows that high\ntrain-test similarity is insufficient to explain CLIP's OOD performance, and\nother properties of the training data must drive CLIP to learn more\ngeneralizable representations. Additionally, by pruning data points that are\ndissimilar to the OOD benchmarks, we uncover a 100M split of LAION\n($\\frac{1}{4}$th of its original size) on which CLIP can be trained to match\nits original OOD performance.\n"", ""  The fusion of vision and language has brought about a transformative shift in\ncomputer vision through the emergence of Vision-Language Models (VLMs).\nHowever, the resource-intensive nature of existing VLMs poses a significant\nchallenge. We need an accessible method for developing the next generation of\nVLMs. To address this issue, we propose Zoom-shot, a novel method for\ntransferring the zero-shot capabilities of CLIP to any pre-trained vision\nencoder. We do this by exploiting the multimodal information (i.e. text and\nimage) present in the CLIP latent space through the use of specifically\ndesigned multimodal loss functions. These loss functions are (1)\ncycle-consistency loss and (2) our novel prompt-guided knowledge distillation\nloss (PG-KD). PG-KD combines the concept of knowledge distillation with CLIP's\nzero-shot classification, to capture the interactions between text and image\nfeatures. With our multimodal losses, we train a $\\textbf{linear mapping}$\nbetween the CLIP latent space and the latent space of a pre-trained vision\nencoder, for only a $\\textbf{single epoch}$. Furthermore, Zoom-shot is entirely\nunsupervised and is trained using $\\textbf{unpaired}$ data. We test the\nzero-shot capabilities of a range of vision encoders augmented as new VLMs, on\ncoarse and fine-grained classification datasets, outperforming the previous\nstate-of-the-art in this problem domain. In our ablations, we find Zoom-shot\nallows for a trade-off between data and compute during training; and our\nstate-of-the-art results can be obtained by reducing training from 20% to 1% of\nthe ImageNet training data with 20 epochs. All code and models are available on\nGitHub.\n""] , ['  Open-vocabulary 3D object detection (OV-3DDet) aims to localize and recognize\nboth seen and previously unseen object categories within any new 3D scene.\nWhile language and vision foundation models have achieved success in handling\nvarious open-vocabulary tasks with abundant training data, OV-3DDet faces a\nsignificant challenge due to the limited availability of training data.\nAlthough some pioneering efforts have integrated vision-language models (VLM)\nknowledge into OV-3DDet learning, the full potential of these foundational\nmodels has yet to be fully exploited. In this paper, we unlock the textual and\nvisual wisdom to tackle the open-vocabulary 3D detection task by leveraging the\nlanguage and vision foundation models. We leverage a vision foundation model to\nprovide image-wise guidance for discovering novel classes in 3D scenes.\nSpecifically, we utilize a object detection vision foundation model to enable\nthe zero-shot discovery of objects in images, which serves as the initial seeds\nand filtering guidance to identify novel 3D objects. Additionally, to align the\n3D space with the powerful vision-language space, we introduce a hierarchical\nalignment approach, where the 3D feature space is aligned with the\nvision-language feature space using a pre-trained VLM at the instance,\ncategory, and scene levels. Through extensive experimentation, we demonstrate\nsignificant improvements in accuracy and generalization, highlighting the\npotential of foundation models in advancing open-vocabulary 3D object detection\nin real-world scenarios.\n', ""  While 3D MLLMs have achieved significant progress, they are restricted to\nobject and scene understanding and struggle to understand 3D spatial structures\nat the part level. In this paper, we introduce Kestrel, representing a novel\napproach that empowers 3D MLLMs with part-aware understanding, enabling better\ninterpretation and segmentation grounding of 3D objects at the part level.\nDespite its significance, the current landscape lacks tasks and datasets that\nendow and assess this capability. Therefore, we propose two novel tasks: (1)\nPart-Aware Point Grounding, the model is tasked with directly predicting a\npart-level segmentation mask based on user instructions, and (2) Part-Aware\nPoint Grounded Captioning, the model provides a detailed caption that includes\npart-level descriptions and their corresponding masks. To support learning and\nevaluating for these tasks, we introduce 3DCoMPaT Grounded Instructions Dataset\n(3DCoMPaT-GRIN). 3DCoMPaT-GRIN Vanilla, comprising 789k part-aware point\ncloud-instruction-segmentation mask triplets, is used to evaluate MLLMs'\nability of part-aware segmentation grounding. 3DCoMPaT-GRIN Grounded Caption,\ncontaining 107k part-aware point cloud-instruction-grounded caption triplets,\nassesses both MLLMs' part-aware language comprehension and segmentation\ngrounding capabilities. Our introduced tasks, dataset, and Kestrel represent a\npreliminary effort to bridge the gap between human cognition and 3D MLLMs,\ni.e., the ability to perceive and engage with the environment at both global\nand part levels. Extensive experiments on the 3DCoMPaT-GRIN show that Kestrel\ncan generate user-specified segmentation masks, a capability not present in any\nexisting 3D MLLM. Kestrel thus established a benchmark for evaluating the\npart-aware language comprehension and segmentation grounding of 3D objects.\nProject page at https://feielysia.github.io/Kestrel.github.io/\n"", '  Large 2D vision-language models (2D-LLMs) have gained significant attention\nby bridging Large Language Models (LLMs) with images using a simple projector.\nInspired by their success, large 3D point cloud-language models (3D-LLMs) also\nintegrate point clouds into LLMs. However, directly aligning point clouds with\nLLM requires expensive training costs, typically in hundreds of GPU-hours on\nA100, which hinders the development of 3D-LLMs. In this paper, we introduce\nMiniGPT-3D, an efficient and powerful 3D-LLM that achieves multiple SOTA\nresults while training for only 27 hours on one RTX 3090. Specifically, we\npropose to align 3D point clouds with LLMs using 2D priors from 2D-LLMs, which\ncan leverage the similarity between 2D and 3D visual information. We introduce\na novel four-stage training strategy for modality alignment in a cascaded way,\nand a mixture of query experts module to adaptively aggregate features with\nhigh efficiency. Moreover, we utilize parameter-efficient fine-tuning methods\nLoRA and Norm fine-tuning, resulting in only 47.8M learnable parameters, which\nis up to 260x fewer than existing methods. Extensive experiments show that\nMiniGPT-3D achieves SOTA on 3D object classification and captioning tasks, with\nsignificantly cheaper training costs. Notably, MiniGPT-3D gains an 8.12\nincrease on GPT-4 evaluation score for the challenging object captioning task\ncompared to ShapeLLM-13B, while the latter costs 160 total GPU-hours on 8 A800.\nWe are the first to explore the efficient 3D-LLM, offering new insights to the\ncommunity. Code and weights are available at\nhttps://github.com/TangYuan96/MiniGPT-3D.\n'] , [""  Multi-modal large language models (MLLMs) have shown incredible capabilities\nin a variety of 2D vision and language tasks. We extend MLLMs' perceptual\ncapabilities to ground and reason about images in 3-dimensional space. To that\nend, we first develop a large-scale pre-training dataset for 2D and 3D called\nLV3D by combining multiple existing 2D and 3D recognition datasets under a\ncommon task formulation: as multi-turn question-answering. Next, we introduce a\nnew MLLM named Cube-LLM and pre-train it on LV3D. We show that pure data\nscaling makes a strong 3D perception capability without 3D specific\narchitectural design or training objective. Cube-LLM exhibits intriguing\nproperties similar to LLMs: (1) Cube-LLM can apply chain-of-thought prompting\nto improve 3D understanding from 2D context information. (2) Cube-LLM can\nfollow complex and diverse instructions and adapt to versatile input and output\nformats. (3) Cube-LLM can be visually prompted such as 2D box or a set of\ncandidate 3D boxes from specialists. Our experiments on outdoor benchmarks\ndemonstrate that Cube-LLM significantly outperforms existing baselines by 21.3\npoints of AP-BEV on the Talk2Car dataset for 3D grounded reasoning and 17.7\npoints on the DriveLM dataset for complex reasoning about driving scenarios,\nrespectively. Cube-LLM also shows competitive results in general MLLM\nbenchmarks such as refCOCO for 2D grounding with (87.0) average score, as well\nas visual question answering benchmarks such as VQAv2, GQA, SQA, POPE, etc. for\ncomplex reasoning. Our project is available at\nhttps://janghyuncho.github.io/Cube-LLM.\n"", '  In 3D Visual Question Answering (3D VQA), the scarcity of fully annotated\ndata and limited visual content diversity hampers the generalization to novel\nscenes and 3D concepts (e.g., only around 800 scenes are utilized in ScanQA and\nSQA dataset). Current approaches resort supplement 3D reasoning with 2D\ninformation. However, these methods face challenges: either they use top-down\n2D views that introduce overly complex and sometimes question-irrelevant visual\nclues, or they rely on globally aggregated scene/image-level representations\nfrom 2D VLMs, losing the fine-grained vision-language correlations. To overcome\nthese limitations, our approach utilizes question-conditional 2D view selection\nprocedure, pinpointing semantically relevant 2D inputs for crucial visual\nclues. We then integrate this 2D knowledge into the 3D-VQA system via a\ntwo-branch Transformer structure. This structure, featuring a Twin-Transformer\ndesign, compactly combines 2D and 3D modalities and captures fine-grained\ncorrelations between modalities, allowing them mutually augmenting each other.\nIntegrating proposed mechanisms above, we present BridgeQA, that offers a fresh\nperspective on multi-modal transformer-based architectures for 3D-VQA.\nExperiments validate that BridgeQA achieves state-of-the-art on 3D-VQA datasets\nand significantly outperforms existing solutions. Code is available at\n$\\href{https://github.com/matthewdm0816/BridgeQA}{\\text{this URL}}$.\n', '  Learning to ground natural language queries to target objects or regions in\n3D point clouds is quite essential for 3D scene understanding. Nevertheless,\nexisting 3D visual grounding approaches require a substantial number of\nbounding box annotations for text queries, which is time-consuming and\nlabor-intensive to obtain. In this paper, we propose \\textbf{3D-VLA}, a weakly\nsupervised approach for \\textbf{3D} visual grounding based on \\textbf{V}isual\n\\textbf{L}inguistic \\textbf{A}lignment. Our 3D-VLA exploits the superior\nability of current large-scale vision-language models (VLMs) on aligning the\nsemantics between texts and 2D images, as well as the naturally existing\ncorrespondences between 2D images and 3D point clouds, and thus implicitly\nconstructs correspondences between texts and 3D point clouds with no need for\nfine-grained box annotations in the training procedure. During the inference\nstage, the learned text-3D correspondence will help us ground the text queries\nto the 3D target objects even without 2D images. To the best of our knowledge,\nthis is the first work to investigate 3D visual grounding in a weakly\nsupervised manner by involving large scale vision-language models, and\nextensive experiments on ReferIt3D and ScanRefer datasets demonstrate that our\n3D-VLA achieves comparable and even superior results over the fully supervised\nmethods.\n'] , ['  Following the recent popularity of Large Language Models (LLMs), several\nattempts have been made to extend them to the visual domain. From having a\nvisual assistant that could guide us through unfamiliar environments to\ngenerative models that produce images using only a high-level text description,\nthe vision-language model (VLM) applications will significantly impact our\nrelationship with technology. However, there are many challenges that need to\nbe addressed to improve the reliability of those models. While language is\ndiscrete, vision evolves in a much higher dimensional space in which concepts\ncannot always be easily discretized. To better understand the mechanics behind\nmapping vision to language, we present this introduction to VLMs which we hope\nwill help anyone who would like to enter the field. First, we introduce what\nVLMs are, how they work, and how to train them. Then, we present and discuss\napproaches to evaluate VLMs. Although this work primarily focuses on mapping\nimages to language, we also discuss extending VLMs to videos.\n', ""  Vision-language models (VLMs) excel in zero-shot recognition but their\nperformance varies greatly across different visual concepts. For example,\nalthough CLIP achieves impressive accuracy on ImageNet (60-80%), its\nperformance drops below 10% for more than ten concepts like night snake,\npresumably due to their limited presence in the pretraining data. However,\nmeasuring the frequency of concepts in VLMs' large-scale datasets is\nchallenging. We address this by using large language models (LLMs) to count the\nnumber of pretraining texts that contain synonyms of these concepts. Our\nanalysis confirms that popular datasets, such as LAION, exhibit a long-tailed\nconcept distribution, yielding biased performance in VLMs. We also find that\ndownstream applications of VLMs, including visual chatbots (e.g., GPT-4V) and\ntext-to-image models (e.g., Stable Diffusion), often fail to recognize or\ngenerate images of rare concepts identified by our method. To mitigate the\nimbalanced performance of zero-shot VLMs, we propose REtrieval-Augmented\nLearning (REAL). First, instead of prompting VLMs using the original class\nnames, REAL uses their most frequent synonyms found in pretraining texts. This\nsimple change already outperforms costly human-engineered and LLM-enriched\nprompts over nine benchmark datasets. Second, REAL trains a linear classifier\non a small yet balanced set of pretraining data retrieved using concept\nsynonyms. REAL surpasses the previous zero-shot SOTA, using 400x less storage\nand 10,000x less training time!\n"", '  Vision language models (VLMs) have drastically changed the computer vision\nmodel landscape in only a few years, opening an exciting array of new\napplications from zero-shot image classification, over to image captioning, and\nvisual question answering. Unlike pure vision models, they offer an intuitive\nway to access visual content through language prompting. The wide applicability\nof such models encourages us to ask whether they also align with human vision -\nspecifically, how far they adopt human-induced visual biases through multimodal\nfusion, or whether they simply inherit biases from pure vision models. One\nimportant visual bias is the texture vs. shape bias, or the dominance of local\nover global information. In this paper, we study this bias in a wide range of\npopular VLMs. Interestingly, we find that VLMs are often more shape-biased than\ntheir vision encoders, indicating that visual biases are modulated to some\nextent through text in multimodal models. If text does indeed influence visual\nbiases, this suggests that we may be able to steer visual biases not just\nthrough visual input but also through language: a hypothesis that we confirm\nthrough extensive experiments. For instance, we are able to steer shape bias\nfrom as low as 49% to as high as 72% through prompting alone. For now, the\nstrong human bias towards shape (96%) remains out of reach for all tested VLMs.\n'] , ['  Visual prompt tuning (VPT) is a promising solution incorporating learnable\nprompt tokens to customize pre-trained models for downstream tasks. However,\nVPT and its variants often encounter challenges like prompt initialization,\nprompt length, and subpar performance in self-supervised pretraining, hindering\nsuccessful contextual adaptation. This study commences by exploring the\ncorrelation evolvement between prompts and patch tokens during proficient\ntraining. Inspired by the observation that the prompt tokens tend to share high\nmutual information with patch tokens, we propose initializing prompts with\ndownstream token prototypes. The strategic initialization, a stand-in for the\nprevious initialization, substantially improves performance in fine-tuning. To\nrefine further, we optimize token construction with a streamlined pipeline that\nmaintains excellent performance with almost no increase in computational\nexpenses compared to VPT. Exhaustive experiments show our proposed approach\noutperforms existing methods by a remarkable margin. For instance, it surpasses\nfull fine-tuning in 19 out of 24 tasks, using less than 0.4% of learnable\nparameters on the FGVC and VTAB-1K benchmarks. Notably, our method\nsignificantly advances the adaptation for self-supervised pretraining,\nachieving impressive task performance gains of at least 10% to 30%. Besides,\nthe experimental results demonstrate the proposed SPT is robust to prompt\nlengths and scales well with model capacity and training data size. We finally\nprovide an insightful exploration into the amount of target data facilitating\nthe adaptation of pre-trained models to downstream tasks. The code is available\nat https://github.com/WangYZ1608/Self-Prompt-Tuning.\n', '  Data-Free Knowledge Distillation (DFKD) has shown great potential in creating\na compact student model while alleviating the dependency on real training data\nby synthesizing surrogate data. However, prior arts are seldom discussed under\ndistribution shifts, which may be vulnerable in real-world applications. Recent\nVision-Language Foundation Models, e.g., CLIP, have demonstrated remarkable\nperformance in zero-shot out-of-distribution generalization, yet consuming\nheavy computation resources. In this paper, we discuss the extension of DFKD to\nVision-Language Foundation Models without access to the billion-level\nimage-text datasets. The objective is to customize a student model for\ndistribution-agnostic downstream tasks with given category concepts, inheriting\nthe out-of-distribution generalization capability from the pre-trained\nfoundation models. In order to avoid generalization degradation, the primary\nchallenge of this task lies in synthesizing diverse surrogate images driven by\ntext prompts. Since not only category concepts but also style information are\nencoded in text prompts, we propose three novel Prompt Diversification methods\nto encourage image synthesis with diverse styles, namely Mix-Prompt,\nRandom-Prompt, and Contrastive-Prompt. Experiments on out-of-distribution\ngeneralization datasets demonstrate the effectiveness of the proposed methods,\nwith Contrastive-Prompt performing the best.\n', ""  In recent years, soft prompt learning methods have been proposed to fine-tune\nlarge-scale vision-language pre-trained models for various downstream tasks.\nThese methods typically combine learnable textual tokens with class tokens as\ninput for models with frozen parameters. However, they often employ a single\nprompt to describe class contexts, failing to capture categories' diverse\nattributes adequately. This study introduces the Partitioned Multi-modal Prompt\n(PMPO), a multi-modal prompting technique that extends the soft prompt from a\nsingle learnable prompt to multiple prompts. Our method divides the visual\nencoder depths and connects learnable prompts to the separated visual depths,\nenabling different prompts to capture the hierarchical contextual depths of\nvisual representations. Furthermore, to maximize the advantages of multi-prompt\nlearning, we incorporate prior information from manually designed templates and\nlearnable multi-prompts, thus improving the generalization capabilities of our\napproach. We evaluate the effectiveness of our approach on three challenging\ntasks: new class generalization, cross-dataset evaluation, and domain\ngeneralization. For instance, our method achieves a $79.28$ harmonic mean,\naveraged over 11 diverse image recognition datasets ($+7.62$ compared to CoOp),\ndemonstrating significant competitiveness compared to state-of-the-art\nprompting methods.\n""] , ['  Multimodal Large Language Models (MLLMs) demonstrate exceptional\nproblem-solving capabilities, but there is limited research focusing on their\nability to generate data by converting unlabeled images into visual instruction\ntuning data. To this end, this paper is the first to explore the potential of\nempowering MLLM to generate data rather than prompting GPT-4. We introduce\nGenixer, a holistic data generation pipeline consisting of four key steps: (i)\ninstruction data collection, (ii) instruction template design, (iii) empowering\nMLLMs, and (iv) data generation and filtering. Additionally, we outline two\nmodes of data generation: task-agnostic and task-specific, enabling\ncontrollable output. We demonstrate that a synthetic VQA-like dataset trained\nwith LLaVA1.5 enhances performance on 10 out of 12 multimodal benchmarks.\nAdditionally, the grounding MLLM Shikra, when trained with a REC-like synthetic\ndataset, shows improvements on 7 out of 8 REC datasets. Through experiments and\nsynthetic data analysis, our findings are: (1) current MLLMs can serve as\nrobust data generators without assistance from GPT-4V; (2) MLLMs trained with\ntask-specific datasets can surpass GPT-4V in generating complex instruction\ntuning data; (3) synthetic datasets enhance performance across various\nmultimodal benchmarks and help mitigate model hallucinations. The data, code,\nand models can be found at https://github.com/zhaohengyuan1/Genixer.\n', ""  Can large multimodal models have a human-like ability for emotional and\nsocial reasoning, and if so, how does it work? Recent research has discovered\nemergent theory-of-mind (ToM) reasoning capabilities in large language models\n(LLMs). LLMs can reason about people's mental states by solving various\ntext-based ToM tasks that ask questions about the actors' ToM (e.g., human\nbelief, desire, intention). However, human reasoning in the wild is often\ngrounded in dynamic scenes across time. Thus, we consider videos a new medium\nfor examining spatio-temporal ToM reasoning ability. Specifically, we ask\nexplicit probing questions about videos with abundant social and emotional\nreasoning content. We develop a pipeline for multimodal LLM for ToM reasoning\nusing video and text. We also enable explicit ToM reasoning by retrieving key\nframes for answering a ToM question, which reveals how multimodal LLMs reason\nabout ToM.\n"", '  Multimodal large language models (LLMs) have achieved notable success across\nvarious domains, while research in the medical field has largely focused on\nunimodal images. Meanwhile, current general-domain multimodal models for videos\nstill lack the capabilities to understand and engage in conversations about\nsurgical videos. One major contributing factor is the absence of datasets in\nthe surgical field. In this paper, we create a new dataset, Surg-QA, consisting\nof 102,000 surgical video-instruction pairs, the largest of its kind so far. To\nbuild such a dataset, we propose a novel two-stage question-answer generation\npipeline with LLM to learn surgical knowledge in a structured manner from the\npublicly available surgical lecture videos. The pipeline breaks down the\ngeneration process into two stages to significantly reduce the task complexity,\nallowing us to use a more affordable, locally deployed open-source LLM than the\npremium paid LLM services. It also mitigates the risk of LLM hallucinations\nduring question-answer generation, thereby enhancing the overall quality of the\ngenerated data. We further train LLaVA-Surg, a novel vision-language\nconversational assistant capable of answering open-ended questions about\nsurgical videos, on this Surg-QA dataset, and conduct comprehensive evaluations\non zero-shot surgical video question-answering tasks. We show that LLaVA-Surg\nsignificantly outperforms all previous general-domain models, demonstrating\nexceptional multimodal conversational skills in answering open-ended questions\nabout surgical videos. We will release our code, model, and the\ninstruction-tuning dataset.\n'] , ['  The integration of language and 3D perception is crucial for developing\nembodied agents and robots that comprehend and interact with the physical\nworld. While large language models (LLMs) have demonstrated impressive language\nunderstanding and generation capabilities, their adaptation to 3D environments\n(3D-LLMs) remains in its early stages. A primary challenge is the absence of\nlarge-scale datasets that provide dense grounding between language and 3D\nscenes. In this paper, we introduce 3D-GRAND, a pioneering large-scale dataset\ncomprising 40,087 household scenes paired with 6.2 million densely-grounded\nscene-language instructions. Our results show that instruction tuning with\n3D-GRAND significantly enhances grounding capabilities and reduces\nhallucinations in 3D-LLMs. As part of our contributions, we propose a\ncomprehensive benchmark 3D-POPE to systematically evaluate hallucination in\n3D-LLMs, enabling fair comparisons among future models. Our experiments\nhighlight a scaling effect between dataset size and 3D-LLM performance,\nemphasizing the critical role of large-scale 3D-text datasets in advancing\nembodied AI research. Notably, our results demonstrate early signals for\neffective sim-to-real transfer, indicating that models trained on large\nsynthetic data can perform well on real-world 3D scans. Through 3D-GRAND and\n3D-POPE, we aim to equip the embodied AI community with essential resources and\ninsights, setting the stage for more reliable and better-grounded 3D-LLMs.\nProject website: https://3d-grand.github.io\n', '  3D vision-language grounding, which focuses on aligning language with the 3D\nphysical environment, stands as a cornerstone in the development of embodied\nagents. In comparison to recent advancements in the 2D domain, grounding\nlanguage in 3D scenes faces several significant challenges: (i) the inherent\ncomplexity of 3D scenes due to the diverse object configurations, their rich\nattributes, and intricate relationships; (ii) the scarcity of paired 3D\nvision-language data to support grounded learning; and (iii) the absence of a\nunified learning framework to distill knowledge from grounded 3D data. In this\nwork, we aim to address these three major challenges in 3D vision-language by\nexamining the potential of systematically upscaling 3D vision-language learning\nin indoor environments. We introduce the first million-scale 3D vision-language\ndataset, SceneVerse, encompassing about 68K 3D indoor scenes and comprising\n2.5M vision-language pairs derived from both human annotations and our scalable\nscene-graph-based generation approach. We demonstrate that this scaling allows\nfor a unified pre-training framework, Grounded Pre-training for Scenes (GPS),\nfor 3D vision-language learning. Through extensive experiments, we showcase the\neffectiveness of GPS by achieving state-of-the-art performance on all existing\n3D visual grounding benchmarks. The vast potential of SceneVerse and GPS is\nunveiled through zero-shot transfer experiments in the challenging 3D\nvision-language tasks. Project website: https://scene-verse.github.io.\n', ""  This paper introduces Scene-LLM, a 3D-visual-language model that enhances\nembodied agents' abilities in interactive 3D indoor environments by integrating\nthe reasoning strengths of Large Language Models (LLMs). Scene-LLM adopts a\nhybrid 3D visual feature representation, that incorporates dense spatial\ninformation and supports scene state updates. The model employs a projection\nlayer to efficiently project these features in the pre-trained textual\nembedding space, enabling effective interpretation of 3D visual information.\nUnique to our approach is the integration of both scene-level and ego-centric\n3D information. This combination is pivotal for interactive planning, where\nscene-level data supports global planning and ego-centric data is important for\nlocalization. Notably, we use ego-centric 3D frame features for feature\nalignment, an efficient technique that enhances the model's ability to align\nfeatures of small objects within the scene. Our experiments with Scene-LLM\ndemonstrate its strong capabilities in dense captioning, question answering,\nand interactive planning. We believe Scene-LLM advances the field of 3D visual\nunderstanding and reasoning, offering new possibilities for sophisticated agent\ninteractions in indoor settings.\n""]",Vision-Language Models and Multimodal Learning,Multimodal Language Models for Vision and Language Tasks
99,"Visual Reasoning in Multimodal 3D Spaces , Spatial Reasoning in AI and Multimodal Models , Abstract Visual Reasoning and Intelligence Tests","['visual', '3d', 'multimodal', 'vision', 'spatial', 'interactive', 'reasoning', 'robot', 'tasks', 'robots'] , ['spatial', 'visual', 'planning', 'abstractions', 'reasoning', 'ai', 'language', 'tasks', 'intelligence', 'multimodal'] , ['intelligencetest', 'intelligence', 'visual', 'abstraction', 'iq', 'attributes', 'learner', 'tasks', 'deductive', 'solvers']","[""  Visual Language Models (VLMs) are essential for various tasks, particularly\nvisual reasoning tasks, due to their robust multi-modal information\nintegration, visual reasoning capabilities, and contextual awareness. However,\nexisting \\VLMs{}' visual spatial reasoning capabilities are often inadequate,\nstruggling even with basic tasks such as distinguishing left from right. To\naddress this, we propose the \\ours{} model, designed to enhance the visual\nspatial reasoning abilities of VLMS. ZeroVLM employs Zero-1-to-3, a 3D\nreconstruction model for obtaining different views of the input images and\nincorporates a prompting mechanism to further improve visual spatial reasoning.\nExperimental results on four visual spatial reasoning datasets show that our\n\\ours{} achieves up to 19.48% accuracy improvement, which indicates the\neffectiveness of the 3D reconstruction and prompting mechanisms of our ZeroVLM.\n"", '  Although great progress has been made in 3D visual grounding, current models\nstill rely on explicit textual descriptions for grounding and lack the ability\nto reason human intentions from implicit instructions. We propose a new task\ncalled 3D reasoning grounding and introduce a new benchmark ScanReason which\nprovides over 10K question-answer-location pairs from five reasoning types that\nrequire the synerization of reasoning and grounding. We further design our\napproach, ReGround3D, composed of the visual-centric reasoning module empowered\nby Multi-modal Large Language Model (MLLM) and the 3D grounding module to\nobtain accurate object locations by looking back to the enhanced geometry and\nfine-grained details from the 3D scenes. A chain-of-grounding mechanism is\nproposed to further boost the performance with interleaved reasoning and\ngrounding steps during inference. Extensive experiments on the proposed\nbenchmark validate the effectiveness of our proposed approach.\n', '  Visual representation learning has been a cornerstone in computer vision,\ninvolving typical forms such as visual embeddings, structural symbols, and\ntext-based representations. Despite the success of CLIP-type visual embeddings,\nthey often lack access to world knowledge critical for visual reasoning. In\nthis work, we propose Visual Table, a novel form of visual representation\ntailored for visual reasoning. Visual tables are constructed as hierarchical\ndescriptions of visual scenes, featuring a scene description and multiple\nobject-centric descriptions covering categories, attributes, and knowledge.\nThanks to the structural and textual formats, visual tables offer unique\nadvantages over mere visual embeddings, such as interpretability and\ncontrollable editing. Furthermore, they deliver instance-level world knowledge\nand detailed attributes that are essential for visual reasoning. To create\nvisual tables, we develop a generator trained on the dataset with collected,\nsmall-scale annotations. Extensive results on 11 visual reasoning benchmarks\ndemonstrate that the generated visual tables significantly outperform previous\nstructural and text-based representations. Moreover, they consistently enhance\nstate-of-the-art multimodal large language models across diverse benchmarks,\nshowcasing their potential for advancing visual reasoning tasks. Our code is\navailable at https://github.com/LaVi-Lab/Visual-Table.\n'] , ['  Artificial intelligence (AI) has made remarkable progress across various\ndomains, with large language models like ChatGPT gaining substantial attention\nfor their human-like text-generation capabilities. Despite these achievements,\nspatial reasoning remains a significant challenge for these models. Benchmarks\nlike StepGame evaluate AI spatial reasoning, where ChatGPT has shown\nunsatisfactory performance. However, the presence of template errors in the\nbenchmark has an impact on the evaluation results. Thus there is potential for\nChatGPT to perform better if these template errors are addressed, leading to\nmore accurate assessments of its spatial reasoning capabilities. In this study,\nwe refine the StepGame benchmark, providing a more accurate dataset for model\nevaluation. We analyze GPT\'s spatial reasoning performance on the rectified\nbenchmark, identifying proficiency in mapping natural language text to spatial\nrelations but limitations in multi-hop reasoning. We provide a flawless\nsolution to the benchmark by combining template-to-relation mapping with\nlogic-based reasoning. This combination demonstrates proficiency in performing\nqualitative reasoning on StepGame without encountering any errors. We then\naddress the limitations of GPT models in spatial reasoning. We deploy\nChain-of-thought and Tree-of-thoughts prompting strategies, offering insights\ninto GPT\'s ``cognitive process"", and achieving remarkable improvements in\naccuracy. Our investigation not only sheds light on model deficiencies but also\nproposes enhancements, contributing to the advancement of AI with more robust\nspatial reasoning capabilities.\n', ""  Vision language models (VLMs) are an exciting emerging class of language\nmodels (LMs) that have merged classic LM capabilities with those of image\nprocessing systems. However, the ways that these capabilities combine are not\nalways intuitive and warrant direct investigation. One understudied capability\nin VLMs is visual spatial planning -- the ability to comprehend the spatial\narrangements of objects and devise action plans to achieve desired outcomes in\nvisual scenes. In our study, we introduce VSP, a benchmark that 1) evaluates\nthe spatial planning capability in these models in general, and 2) breaks down\nthe visual planning task into finer-grained sub-tasks, including perception and\nreasoning, and measure the LMs capabilities in these sub-tasks. Our evaluation\nshows that both open-source and private VLMs fail to generate effective plans\nfor even simple spatial planning tasks. Evaluations on the fine-grained\nanalytical tasks further reveal fundamental deficiencies in the models' visual\nperception and bottlenecks in reasoning abilities, explaining their worse\nperformance in the general spatial planning tasks. Our work illuminates future\ndirections for improving VLMs' abilities in spatial planning. Our benchmark is\npublicly available at\nhttps://github.com/UCSB-NLP-Chang/Visual-Spatial-Planning.\n"", '  Large language models (LLMs) and vision-language models (VLMs) have\ndemonstrated remarkable performance across a wide range of tasks and domains.\nDespite this promise, spatial understanding and reasoning -- a fundamental\ncomponent of human cognition -- remains under-explored. We develop novel\nbenchmarks that cover diverse aspects of spatial reasoning such as relationship\nunderstanding, navigation, and counting. We conduct a comprehensive evaluation\nof competitive language and vision-language models. Our findings reveal several\ncounter-intuitive insights that have been overlooked in the literature: (1)\nSpatial reasoning poses significant challenges where competitive models can\nfall behind random guessing; (2) Despite additional visual input, VLMs often\nunder-perform compared to their LLM counterparts; (3) When both textual and\nvisual information is available, multi-modal language models become less\nreliant on visual information if sufficient textual clues are provided.\nAdditionally, we demonstrate that leveraging redundancy between vision and text\ncan significantly enhance model performance. We hope our study will inform the\ndevelopment of multimodal models to improve spatial intelligence and further\nclose the gap with human intelligence.\n'] , [""  The field of Abstract Visual Reasoning (AVR) encompasses a wide range of\nproblems, many of which are inspired by human IQ tests. The variety of AVR\ntasks has resulted in state-of-the-art AVR methods being task-specific\napproaches. Furthermore, contemporary methods consider each AVR problem\ninstance not as a whole, but in the form of a set of individual panels with\nparticular locations and roles (context vs. answer panels) pre-assigned\naccording to the task-specific arrangements. While these highly specialized\napproaches have recently led to significant progress in solving particular AVR\ntasks, considering each task in isolation hinders the development of universal\nlearning systems in this domain. In this paper, we introduce a unified view of\nAVR tasks, where each problem instance is rendered as a single image, with no a\npriori assumptions about the number of panels, their location, or role. The\nmain advantage of the proposed unified view is the ability to develop universal\nlearning models applicable to various AVR tasks. What is more, the proposed\napproach inherently facilitates transfer learning in the AVR domain, as various\ntypes of problems share a common representation. The experiments conducted on\nfour AVR datasets with Raven's Progressive Matrices and Visual Analogy\nProblems, and one real-world visual analogy dataset show that the proposed\nunified representation of AVR tasks poses a challenge to state-of-the-art Deep\nLearning (DL) AVR models and, more broadly, contemporary DL image recognition\nmethods. In order to address this challenge, we introduce the Unified Model for\nAbstract Visual Reasoning (UMAVR) capable of dealing with various types of AVR\nproblems in a unified manner. UMAVR outperforms existing AVR methods in\nselected single-task learning experiments, and demonstrates effective knowledge\nreuse in transfer learning and curriculum learning setups.\n"", ""  A hallmark of human intelligence is the ability to infer abstract rules from\nlimited experience and apply these rules to unfamiliar situations. This\ncapacity is widely studied in the visual domain using the Raven's Progressive\nMatrices. Recent advances in deep learning have led to multiple artificial\nneural network models matching or even surpassing human performance. However,\nwhile humans can identify and express the rule underlying these tasks with\nlittle to no exposure, contemporary neural networks often rely on massive\npattern-based training and cannot express or extrapolate the rule inferred from\nthe task. Furthermore, most Raven's Progressive Matrices or Raven-like tasks\nused for neural network training used symbolic representations, whereas humans\ncan flexibly switch between symbolic and continuous perceptual representations.\nIn this work, we present an algorithmic approach to rule detection and\napplication using feature detection, affine transformation estimation and\nsearch. We applied our model to a simplified Raven's Progressive Matrices task,\npreviously designed for behavioral testing and neuroimaging in humans. The\nmodel exhibited one-shot learning and achieved near human-level performance in\nthe symbolic reasoning condition of the simplified task. Furthermore, the model\ncan express the relationships discovered and generate multi-step predictions in\naccordance with the underlying rule. Finally, the model can reason using\ncontinuous patterns. We discuss our results and their relevance to studying\nabstract reasoning in humans, as well as their implications for improving\nintelligent machines.\n"", ""  We study generalization and knowledge reuse capabilities of deep neural\nnetworks in the domain of abstract visual reasoning (AVR), employing Raven's\nProgressive Matrices (RPMs), a recognized benchmark task for assessing AVR\nabilities. Two knowledge transfer scenarios referring to the I-RAVEN dataset\nare investigated. Firstly, inspired by generalization assessment capabilities\nof the PGM dataset and popularity of I-RAVEN, we introduce\nAttributeless-I-RAVEN, a benchmark with four generalization regimes that allow\nto test generalization of abstract rules applied to held-out attributes.\nSecondly, we construct I-RAVEN-Mesh, a dataset that enriches RPMs with a novel\ncomponent structure comprising line-based patterns, facilitating assessment of\nprogressive knowledge acquisition in transfer learning setting. The developed\nbenchmarks reveal shortcomings of the contemporary deep learning models, which\nwe partly address with Pathways of Normalized Group Convolution (PoNG) model, a\nnovel neural architecture for solving AVR tasks. PoNG excels in both presented\nchallenges, as well as the standard I-RAVEN and PGM setups.\n""]",Visual and Spatial Reasoning in Artificial Intelligence,Visual Reasoning in Multimodal 3D Spaces
100,"Video Understanding and Multimodal Representation , ""Computer Vision for 3D Scene Understanding"" , ""Multimodal Video Understanding and Generation"" , ""Open-Vocabulary Visual Scene Understanding""","['videos', 'captioning', 'captions', 'vid', 'textual', 'videoqa', 'videotree', 'video', 'clips', 'multimodal'] , ['vision', 'camera', 'cameras', '3d', 'rgb', 'recognition', 'scenes', 'view', 'views', 'frames'] , ['videos', 'multimodal', 'visual', 'attention', 'video', 'clips', 'vision', 'frames', 'trained', 'recognition'] , ['attention', 'captioning', 'recognition', 'captions', 'scene', 'visual', 'vision', 'images', 'segmentation', 'coco']","['  Video-text Large Language Models (video-text LLMs) have shown remarkable\nperformance in answering questions and holding conversations on simple videos.\nHowever, they perform almost the same as random on grounding text queries in\nlong and complicated videos, having little ability to understand and reason\nabout temporal information, which is the most fundamental difference between\nvideos and images. In this paper, we propose HawkEye, one of the first\nvideo-text LLMs that can perform temporal video grounding in a fully\ntext-to-text manner. To collect training data that is applicable for temporal\nvideo grounding, we construct InternVid-G, a large-scale video-text corpus with\nsegment-level captions and negative spans, with which we introduce two new\ntime-aware training objectives to video-text LLMs. We also propose a\ncoarse-grained method of representing segments in videos, which is more robust\nand easier for LLMs to learn and follow than other alternatives. Extensive\nexperiments show that HawkEye is better at temporal video grounding and\ncomparable on other video-text tasks with existing video-text LLMs, which\nverifies its superior video-text multi-modal understanding abilities.\n', ""  A more robust and holistic language-video representation is the key to\npushing video understanding forward. Despite the improvement in training\nstrategies, the quality of the language-video dataset is less attention to. The\ncurrent plain and simple text descriptions and the visual-only focus for the\nlanguage-video tasks result in a limited capacity in real-world natural\nlanguage video retrieval tasks where queries are much more complex. This paper\nintroduces a method to automatically enhance video-language datasets, making\nthem more modality and context-aware for more sophisticated representation\nlearning needs, hence helping all downstream tasks. Our multifaceted video\ncaptioning method captures entities, actions, speech transcripts, aesthetics,\nand emotional cues, providing detailed and correlating information from the\ntext side to the video side for training. We also develop an agent-like\nstrategy using language models to generate high-quality, factual textual\ndescriptions, reducing human intervention and enabling scalability. The\nmethod's effectiveness in improving language-video representation is evaluated\nthrough text-video retrieval using the MSR-VTT dataset and several multi-modal\nretrieval models.\n"", '  Recent advancements in image understanding have benefited from the extensive\nuse of web image-text pairs. However, video understanding remains a challenge\ndespite the availability of substantial web video-text data. This difficulty\nprimarily arises from the inherent complexity of videos and the inefficient\nlanguage supervision in recent web-collected video-text datasets. In this\npaper, we introduce Text-Only Pre-Alignment (TOPA), a novel approach to extend\nlarge language models (LLMs) for video understanding, without the need for\npre-training on real video data. Specifically, we first employ an advanced LLM\nto automatically generate Textual Videos comprising continuous textual frames,\nalong with corresponding annotations to simulate real video-text data. Then,\nthese annotated textual videos are used to pre-align a language-only LLM with\nthe video modality. To bridge the gap between textual and real videos, we\nemploy the CLIP model as the feature extractor to align image and text\nmodalities. During text-only pre-alignment, the continuous textual frames,\nencoded as a sequence of CLIP text features, are analogous to continuous CLIP\nimage features, thus aligning the LLM with real video representation. Extensive\nexperiments, including zero-shot evaluation and finetuning on various video\nunderstanding tasks, demonstrate that TOPA is an effective and efficient\nframework for aligning video content with LLMs. In particular, without training\non any video data, the TOPA-Llama2-13B model achieves a Top-1 accuracy of 51.0%\non the challenging long-form video understanding benchmark, Egoschema. This\nperformance surpasses previous video-text pre-training approaches and proves\ncompetitive with recent GPT-3.5-based video agents.\n'] , ['  One of the most critical factors in achieving sharp Novel View Synthesis\n(NVS) using neural field methods like Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS) is the quality of the training images. However,\nConventional RGB cameras are susceptible to motion blur. In contrast,\nneuromorphic cameras like event and spike cameras inherently capture more\ncomprehensive temporal information, which can provide a sharp representation of\nthe scene as additional training data. Recent methods have explored the\nintegration of event cameras to improve the quality of NVS. The event-RGB\napproaches have some limitations, such as high training costs and the inability\nto work effectively in the background. Instead, our study introduces a new\nmethod that uses the spike camera to overcome these limitations. By considering\ntexture reconstruction from spike streams as ground truth, we design the\nTexture from Spike (TfS) loss. Since the spike camera relies on temporal\nintegration instead of temporal differentiation used by event cameras, our\nproposed TfS loss maintains manageable training costs. It handles foreground\nobjects with backgrounds simultaneously. We also provide a real-world dataset\ncaptured with our spike-RGB camera system to facilitate future research\nendeavors. We conduct extensive experiments using synthetic and real-world\ndatasets to demonstrate that our design can enhance novel view synthesis across\nNeRF and 3DGS. The code and dataset will be made available for public access.\n', ""  Semantic segmentation in bird's eye view (BEV) plays a crucial role in\nautonomous driving. Previous methods usually follow an end-to-end pipeline,\ndirectly predicting the BEV segmentation map from monocular RGB inputs.\nHowever, the challenge arises when the RGB inputs and BEV targets from distinct\nperspectives, making the direct point-to-point predicting hard to optimize. In\nthis paper, we decompose the original BEV segmentation task into two stages,\nnamely BEV map reconstruction and RGB-BEV feature alignment. In the first\nstage, we train a BEV autoencoder to reconstruct the BEV segmentation maps\ngiven corrupted noisy latent representation, which urges the decoder to learn\nfundamental knowledge of typical BEV patterns. The second stage involves\nmapping RGB input images into the BEV latent space of the first stage, directly\noptimizing the correlations between the two views at the feature level. Our\napproach simplifies the complexity of combining perception and generation into\ndistinct steps, equipping the model to handle intricate and challenging scenes\neffectively. Besides, we propose to transform the BEV segmentation map from the\nCartesian to the polar coordinate system to establish the column-wise\ncorrespondence between RGB images and BEV maps. Moreover, our method requires\nneither multi-scale features nor camera intrinsic parameters for depth\nestimation and saves computational overhead. Extensive experiments on nuScenes\nand Argoverse show the effectiveness and efficiency of our method. Code is\navailable at https://github.com/happytianhao/TaDe.\n"", ""  Autonomous driving requires an accurate representation of the environment. A\nstrategy toward high accuracy is to fuse data from several sensors. Learned\nBird's-Eye View (BEV) encoders can achieve this by mapping data from individual\nsensors into one joint latent space. For cost-efficient camera-only systems,\nthis provides an effective mechanism to fuse data from multiple cameras with\ndifferent views. Accuracy can further be improved by aggregating sensor\ninformation over time. This is especially important in monocular camera systems\nto account for the lack of explicit depth and velocity measurements. Thereby,\nthe effectiveness of developed BEV encoders crucially depends on the operators\nused to aggregate temporal information and on the used latent representation\nspaces. We analyze BEV encoders proposed in the literature and compare their\neffectiveness, quantifying the effects of aggregation operators and latent\nrepresentations. While most existing approaches aggregate temporal information\neither in image or in BEV latent space, our analyses and performance\ncomparisons suggest that these latent representations exhibit complementary\nstrengths. Therefore, we develop a novel temporal BEV encoder, TempBEV, which\nintegrates aggregated temporal information from both latent spaces. We consider\nsubsequent image frames as stereo through time and leverage methods from\noptical flow estimation for temporal stereo encoding. Empirical evaluation on\nthe NuScenes dataset shows a significant improvement by TempBEV over the\nbaseline for 3D object detection and BEV segmentation. The ablation uncovers a\nstrong synergy of joint temporal aggregation in the image and BEV latent space.\nThese results indicate the overall effectiveness of our approach and make a\nstrong case for aggregating temporal information in both image and BEV latent\nspaces.\n""] , [""  Amidst the advancements in image-based Large Vision-Language Models\n(image-LVLM), the transition to video-based models (video-LVLM) is hindered by\nthe limited availability of quality video data. This paper addresses the\nchallenge by leveraging the visual commonalities between images and videos to\nefficiently evolve image-LVLMs into video-LVLMs. We present a cost-effective\nvideo-LVLM that enhances model architecture, introduces innovative training\nstrategies, and identifies the most effective types of video instruction data.\nOur innovative weighted token sampler significantly compresses the visual token\nnumbers of each video frame, effectively cutting computational expenses. We\nalso find that judiciously using just 10% of the video data, compared to prior\nvideo-LVLMs, yields impressive results during various training phases.\nMoreover, we delve into the influence of video instruction data in\nlimited-resource settings, highlighting the significance of incorporating video\ntraining data that emphasizes temporal understanding to enhance model\nperformance. The resulting Fewer Tokens and Fewer Videos LVLM (FTFV-LVLM)\nexhibits exceptional performance across video and image benchmarks, validating\nour model's design and training approaches.\n"", '  The performance of Large Vision Language Models (LVLMs) is dependent on the\nsize and quality of their training datasets. Existing video instruction tuning\ndatasets lack diversity as they are derived by prompting large language models\nwith video captions to generate question-answer pairs, and are therefore mostly\ndescriptive. Meanwhile, many labeled video datasets with diverse labels and\nsupervision exist - however, we find that their integration into LVLMs is\nnon-trivial. Herein, we present Video Self-Training with augmented Reasoning\n(Video-STaR), the first video self-training approach. Video-STaR allows the\nutilization of any labeled video dataset for video instruction tuning. In\nVideo-STaR, an LVLM cycles between instruction generation and finetuning, which\nwe show (I) improves general video understanding and (II) adapts LVLMs to novel\ndownstream tasks with existing supervision. During generation, an LVLM is\nprompted to propose an answer. The answers are then filtered only to those that\ncontain the original video labels, and the LVLM is then re-trained on the\ngenerated dataset. By only training on generated answers that contain the\ncorrect video labels, Video-STaR utilizes these existing video labels as weak\nsupervision for video instruction tuning. Our results demonstrate that\nVideo-STaR-enhanced LVLMs exhibit improved performance in (I) general video QA,\nwhere TempCompass performance improved by 10%, and (II) on downstream tasks,\nwhere Video-STaR improved Kinetics700-QA accuracy by 20% and action quality\nassessment on FineDiving by 15%.\n', '  In light of recent advances in multimodal Large Language Models (LLMs), there\nis increasing attention to scaling them from image-text data to more\ninformative real-world videos. Compared to static images, video poses unique\nchallenges for effective large-scale pre-training due to the modeling of its\nspatiotemporal dynamics. In this paper, we address such limitations in\nvideo-language pre-training with an efficient video decomposition that\nrepresents each video as keyframes and temporal motions. These are then adapted\nto an LLM using well-designed tokenizers that discretize visual and temporal\ninformation as a few tokens, thus enabling unified generative pre-training of\nvideos, images, and text. At inference, the generated tokens from the LLM are\ncarefully recovered to the original continuous pixel space to create various\nvideo content. Our proposed framework is both capable of comprehending and\ngenerating image and video content, as demonstrated by its competitive\nperformance across 13 multimodal benchmarks in image and video understanding\nand generation. Our code and models are available at\nhttps://video-lavit.github.io.\n'] , ['  In the field of visual scene understanding, deep neural networks have made\nimpressive advancements in various core tasks like segmentation, tracking, and\ndetection. However, most approaches operate on the close-set assumption,\nmeaning that the model can only identify pre-defined categories that are\npresent in the training set. Recently, open vocabulary settings were proposed\ndue to the rapid progress of vision language pre-training. These new approaches\nseek to locate and recognize categories beyond the annotated label space. The\nopen vocabulary approach is more general, practical, and effective compared to\nweakly supervised and zero-shot settings. This paper provides a thorough review\nof open vocabulary learning, summarizing and analyzing recent developments in\nthe field. In particular, we begin by comparing it to related concepts such as\nzero-shot learning, open-set recognition, and out-of-distribution detection.\nThen, we review several closely related tasks in the case of segmentation and\ndetection, including long-tail problems, few-shot, and zero-shot settings. For\nthe method survey, we first present the basic knowledge of detection and\nsegmentation in close-set as the preliminary knowledge. Next, we examine\nvarious scenarios in which open vocabulary learning is used, identifying common\ndesign elements and core ideas. Then, we compare the recent detection and\nsegmentation approaches in commonly used datasets and benchmarks. Finally, we\nconclude with insights, issues, and discussions regarding future research\ndirections. To our knowledge, this is the first comprehensive literature review\nof open vocabulary learning. We keep tracing related works at\nhttps://github.com/jianzongwu/Awesome-Open-Vocabulary.\n', ""  Existing open-vocabulary image segmentation methods require a fine-tuning\nstep on mask labels and/or image-text datasets. Mask labels are\nlabor-intensive, which limits the number of categories in segmentation\ndatasets. Consequently, the vocabulary capacity of pre-trained VLMs is severely\nreduced after fine-tuning. However, without fine-tuning, VLMs trained under\nweak image-text supervision tend to make suboptimal mask predictions. To\nalleviate these issues, we introduce a novel recurrent framework that\nprogressively filters out irrelevant texts and enhances mask quality without\ntraining efforts. The recurrent unit is a two-stage segmenter built upon a\nfrozen VLM. Thus, our model retains the VLM's broad vocabulary space and equips\nit with segmentation ability. Experiments show that our method outperforms not\nonly the training-free counterparts, but also those fine-tuned with millions of\ndata samples, and sets the new state-of-the-art records for both zero-shot\nsemantic and referring segmentation. Concretely, we improve the current record\nby 28.8, 16.0, and 6.9 mIoU on Pascal VOC, COCO Object, and Pascal Context.\n"", '  Slot attention aims to decompose an input image into a set of meaningful\nobject files (slots). These latent object representations enable various\ndownstream tasks. Yet, these slots often bind to object parts, not objects\nthemselves, especially for real-world datasets. To address this, we introduce\nGuided Latent Slot Diffusion - GLASS, an object-centric model that uses\ngenerated captions as a guiding signal to better align slots with objects. Our\nkey insight is to learn the slot-attention module in the space of generated\nimages. This allows us to repurpose the pre-trained diffusion decoder model,\nwhich reconstructs the images from the slots, as a semantic mask generator\nbased on the generated captions. GLASS learns an object-level representation\nsuitable for multiple tasks simultaneously, e.g., segmentation, image\ngeneration, and property prediction, outperforming previous methods. For object\ndiscovery, GLASS achieves approx. a +35% and +10% relative improvement for mIoU\nover the previous state-of-the-art (SOTA) method on the VOC and COCO datasets,\nrespectively, and establishes a new SOTA FID score for conditional image\ngeneration amongst slot-attention-based methods. For the segmentation task,\nGLASS surpasses SOTA weakly-supervised and language-based segmentation models,\nwhich were specifically designed for the task.\n']",Multimodal Video and Image Understanding,Video Understanding and Multimodal Representation
101,"Speech-to-Speech Translation , Text-to-Speech Synthesis Models , Speech-to-Text Processing with Large Language Models","['transcription', 'transcriptions', 'speech', 'voice', 'multilingual', 'decoder', 's2st', 'audio', 'decoding', 'translating'] , ['speechcodes', 'speechx', 'speechgpt', 'voice', 'voicecraft', 'voicebox', 'phonemes', 'speeches', 'speech', 'audio'] , ['utterances', 'utterance', 'transcription', 'speech', 'dialogue', 'voice', 'transcripts', 'multilingual', 'conversational', 'decoder']","['  Speech segmentation is an essential part of speech translation (ST) systems\nin real-world scenarios. Since most ST models are designed to process speech\nsegments, long-form audio must be partitioned into shorter segments before\ntranslation. Recently, data-driven approaches for the speech segmentation task\nhave been developed. Although the approaches improve overall translation\nquality, a performance gap exists due to a mismatch between the models and ST\nsystems. In addition, the prior works require large self-supervised speech\nmodels, which consume significant computational resources. In this work, we\npropose a segmentation model that achieves better speech translation quality\nwith a small model size. We propose an ASR-with-punctuation task as an\neffective pre-training strategy for the segmentation model. We also show that\nproper integration of the speech segmentation model into the underlying ST\nsystem is critical to improve overall translation quality at inference time.\n', '  Existing speech-to-speech translation models fall into two camps: textless\nmodels trained with hundreds of hours of parallel speech data or unsupervised\nmodels that leverage text as an intermediate step. Both approaches limit\nbuilding speech-to-speech translation models for a wide range of languages, as\nthey exclude languages that are primarily spoken and language pairs that lack\nlarge-scale parallel speech data. We present a new framework for training\ntextless low-resource speech-to-speech translation (S2ST) systems that only\nneed dozens of hours of parallel speech data. We reformulate S2ST as a\nunit-to-unit seq2seq translation task, and start by pretraining a model on\nlarge-scale monolingual speech data. Then, we finetune it with a small amount\nof parallel speech data ($20-60$ hours). Lastly, we improve model performance\nthrough an unsupervised backtranslation objective. We train and evaluate our\nmodels for English-to-German, German-to-English and Marathi-to-English\ntranslation on three different domains (European Parliament, Common Voice, and\nAll India Radio) with single-speaker synthesized speech data. Evaluated using\nthe ASR-BLEU metric, our models achieve reasonable performance on all three\ndomains, with some being within 1-2 points of our supervised topline.\n', '  Simultaneous speech-to-speech translation (Simul-S2ST, a.k.a streaming speech\ntranslation) outputs target speech while receiving streaming speech inputs,\nwhich is critical for real-time communication. Beyond accomplishing translation\nbetween speech, Simul-S2ST requires a policy to control the model to generate\ncorresponding target speech at the opportune moment within speech inputs,\nthereby posing a double challenge of translation and policy. In this paper, we\npropose StreamSpeech, a direct Simul-S2ST model that jointly learns translation\nand simultaneous policy in a unified framework of multi-task learning. Adhering\nto a multi-task learning approach, StreamSpeech can perform offline and\nsimultaneous speech recognition, speech translation and speech synthesis via an\n""All-in-One"" seamless model. Experiments on CVSS benchmark demonstrate that\nStreamSpeech achieves state-of-the-art performance in both offline S2ST and\nSimul-S2ST tasks. Besides, StreamSpeech is able to present high-quality\nintermediate results (i.e., ASR or translation results) during simultaneous\ntranslation process, offering a more comprehensive real-time communication\nexperience.\n'] , ['  We propose a novel text-to-speech (TTS) framework centered around a neural\ntransducer. Our approach divides the whole TTS pipeline into semantic-level\nsequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling\nstages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings.\nFor a robust and efficient alignment modeling, we employ a neural transducer\nnamed token transducer for the semantic token prediction, benefiting from its\nhard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR)\nspeech generator efficiently synthesizes waveforms from these semantic tokens.\nAdditionally, a reference speech controls temporal dynamics and acoustic\nconditions at each stage. This decoupled framework reduces the training\ncomplexity of TTS while allowing each stage to focus on semantic and acoustic\nmodeling. Our experimental results on zero-shot adaptive TTS demonstrate that\nour model surpasses the baseline in terms of speech quality and speaker\nsimilarity, both objectively and subjectively. We also delve into the inference\nspeed and prosody control capabilities of our approach, highlighting the\npotential of neural transducers in TTS frameworks.\n', '  Recent years have witnessed a trend that large language model (LLM) based\ntext-to-speech (TTS) emerges into the mainstream due to their high naturalness\nand zero-shot capacity. In this paradigm, speech signals are discretized into\ntoken sequences, which are modeled by an LLM with text as prompts and\nreconstructed by a token-based vocoder to waveforms. Obviously, speech tokens\nplay a critical role in LLM-based TTS models. Current speech tokens are learned\nin an unsupervised manner, which lacks explicit semantic information and\nalignment to the text. In this paper, we propose to represent speech with\nsupervised semantic tokens, which are derived from a multilingual speech\nrecognition model by inserting vector quantization into the encoder. Based on\nthe tokens, we further propose a scalable zero-shot TTS synthesizer, CosyVoice,\nwhich consists of an LLM for text-to-token generation and a conditional flow\nmatching model for token-to-speech synthesis. Experimental results show that\nsupervised semantic tokens significantly outperform existing unsupervised\ntokens in terms of content consistency and speaker similarity for zero-shot\nvoice cloning. Moreover, we find that utilizing large-scale data further\nimproves the synthesis performance, indicating the scalable capacity of\nCosyVoice. To the best of our knowledge, this is the first attempt to involve\nsupervised speech tokens into TTS models.\n', '  The zero-shot text-to-speech (TTS) method, based on speaker embeddings\nextracted from reference speech using self-supervised learning (SSL) speech\nrepresentations, can reproduce speaker characteristics very accurately.\nHowever, this approach suffers from degradation in speech synthesis quality\nwhen the reference speech contains noise. In this paper, we propose a\nnoise-robust zero-shot TTS method. We incorporated adapters into the SSL model,\nwhich we fine-tuned with the TTS model using noisy reference speech. In\naddition, to further improve performance, we adopted a speech enhancement (SE)\nfront-end. With these improvements, our proposed SSL-based zero-shot TTS\nachieved high-quality speech synthesis with noisy reference speech. Through the\nobjective and subjective evaluations, we confirmed that the proposed method is\nhighly robust to noise in reference speech, and effectively works in\ncombination with SE.\n'] , ['  In this work, we introduce a framework for speech summarization that\nleverages the processing and reasoning capabilities of large language models\n(LLMs). We propose an end-to-end system that combines an instruction-tuned LLM\nwith an audio encoder that converts speech into token representations that the\nLLM can interpret. Using a dataset with paired speech-text data, the overall\nsystem is trained to generate consistent responses to prompts with the same\nsemantic information regardless of the input modality. The resulting framework\nallows the LLM to process speech inputs in the same way as text, enabling\nspeech summarization by simply prompting the LLM. Unlike prior approaches, our\nmethod is able to summarize spoken content from any arbitrary domain, and it\ncan produce summaries in different styles by varying the LLM prompting\nstrategy. Experiments demonstrate that our approach outperforms a cascade\nbaseline of speech recognition followed by LLM text processing.\n', '  While recent work shows promising results in expanding the capabilities of\nlarge language models (LLM) to directly understand and synthesize speech, an\nLLM-based strategy for modeling spoken dialogs remains elusive and calls for\nfurther investigation. This work proposes an extensive speech-text LLM\nframework, named the Unified Spoken Dialog Model (USDM), to generate coherent\nspoken responses with organic prosodic features relevant to the given input\nspeech without relying on automatic speech recognition (ASR) or text-to-speech\n(TTS) solutions. Our approach employs a multi-step speech-text inference scheme\nthat leverages chain-of-reasoning capabilities exhibited by the underlying LLM.\nWe also propose a generalized speech-text pretraining scheme that helps with\ncapturing cross-modal semantics. Automatic and human evaluations show that the\nproposed approach is effective in generating natural-sounding spoken responses,\noutperforming both prior and cascaded baselines. Detailed comparative studies\nreveal that, despite the cascaded approach being stronger in individual\ncomponents, the joint speech-text modeling improves robustness against\nrecognition errors and speech quality. Demo is available at\nhttps://unifiedsdm.github.io.\n', '  The emergence of large language models (LLMs) has sparked significant\ninterest in extending their remarkable language capabilities to speech.\nHowever, modality alignment between speech and text still remains an open\nproblem. Current solutions can be categorized into two strategies. One is a\ncascaded approach where outputs (tokens or states) of a separately trained\nspeech recognition system are used as inputs for LLMs, which limits their\npotential in modeling alignment between speech and text. The other is an\nend-to-end approach that relies on speech instruction data, which is very\ndifficult to collect in large quantities. In this paper, we address these\nissues and propose the BLSP approach that Bootstraps Language-Speech\nPre-training via behavior alignment of continuation writing. We achieve this by\nlearning a lightweight modality adapter between a frozen speech encoder and an\nLLM, ensuring that the LLM exhibits the same generation behavior regardless of\nthe modality of input: a speech segment or its transcript. The training process\ncan be divided into two steps. The first step prompts an LLM to generate texts\nwith speech transcripts as prefixes, obtaining text continuations. In the\nsecond step, these continuations are used as supervised signals to train the\nmodality adapter in an end-to-end manner. We demonstrate that this\nstraightforward process can extend the capabilities of LLMs to speech, enabling\nspeech recognition, speech translation, spoken language understanding, and\nspeech conversation, even in zero-shot cross-lingual scenarios.\n']",Speech and Language Processing,Speech-to-Text Processing with Large Language Models
102,"Audio-Driven Talking Face Generation , Neural Vocoders for High-Quality Speech Synthesis , Singing Voice Conversion and Synthesis , Speech Synthesis and Voice Conversion","['facetalk', 'lip', 'styletalker', 'audio', 'portrait', 'animations', 'talkformer', 'audio2rig', 'mouth', 'animation'] , ['vocoders', 'vocoder', 'voice', 'audio', 'vocal', 'audiostylegan', 'gans', 'adversarial', 'generative', 'vocoding'] , ['voices', 'voice', 'vocal', 'vocals', 'singing', 'singers', 'karaoker', 'singer', 'wav2vec', 'transcription'] , ['speechtokenizer', 'voice', 'vocalizations', 'voices', 'utterances', 'audio', 'speaker', 'speech', 'vocal', 'acoustic']","['  We propose StyleTalker, a novel audio-driven talking head generation model\nthat can synthesize a video of a talking person from a single reference image\nwith accurately audio-synced lip shapes, realistic head poses, and eye blinks.\nSpecifically, by leveraging a pretrained image generator and an image encoder,\nwe estimate the latent codes of the talking head video that faithfully reflects\nthe given audio. This is made possible with several newly devised components:\n1) A contrastive lip-sync discriminator for accurate lip synchronization, 2) A\nconditional sequential variational autoencoder that learns the latent motion\nspace disentangled from the lip movements, such that we can independently\nmanipulate the motions and lip movements while preserving the identity. 3) An\nauto-regressive prior augmented with normalizing flow to learn a complex\naudio-to-motion multi-modal latent space. Equipped with these components,\nStyleTalker can generate talking head videos not only in a motion-controllable\nway when another motion source video is given but also in a completely\naudio-driven manner by inferring realistic motions from the input audio.\nThrough extensive experiments and user studies, we show that our model is able\nto synthesize talking head videos with impressive perceptual quality which are\naccurately lip-synced with the input audios, largely outperforming\nstate-of-the-art baselines.\n', '  Audio-driven lip sync has recently drawn significant attention due to its\nwidespread application in the multimedia domain. Individuals exhibit distinct\nlip shapes when speaking the same utterance, attributed to the unique speaking\nstyles of individuals, posing a notable challenge for audio-driven lip sync.\nEarlier methods for such task often bypassed the modeling of personalized\nspeaking styles, resulting in sub-optimal lip sync conforming to the general\nstyles. Recent lip sync techniques attempt to guide the lip sync for arbitrary\naudio by aggregating information from a style reference video, yet they can not\npreserve the speaking styles well due to their inaccuracy in style aggregation.\nThis work proposes an innovative audio-aware style reference scheme that\neffectively leverages the relationships between input audio and reference audio\nfrom style reference video to address the style-preserving audio-driven lip\nsync. Specifically, we first develop an advanced Transformer-based model adept\nat predicting lip motion corresponding to the input audio, augmented by the\nstyle information aggregated through cross-attention layers from style\nreference video. Afterwards, to better render the lip motion into realistic\ntalking face video, we devise a conditional latent diffusion model, integrating\nlip motion through modulated convolutional layers and fusing reference facial\nimages via spatial cross-attention layers. Extensive experiments validate the\nefficacy of the proposed approach in achieving precise lip sync, preserving\nspeaking styles, and generating high-fidelity, realistic talking face videos.\n', '  Speech-driven facial animation methods usually contain two main classes, 3D\nand 2D talking face, both of which attract considerable research attention in\nrecent years. However, to the best of our knowledge, the research on 3D talking\nface does not go deeper as 2D talking face, in the aspect of\nlip-synchronization (lip-sync) and speech perception. To mind the gap between\nthe two sub-fields, we propose a learning framework named Learn2Talk, which can\nconstruct a better 3D talking face network by exploiting two expertise points\nfrom the field of 2D talking face. Firstly, inspired by the audio-video sync\nnetwork, a 3D sync-lip expert model is devised for the pursuit of lip-sync\nbetween audio and 3D facial motion. Secondly, a teacher model selected from 2D\ntalking face methods is used to guide the training of the audio-to-3D motions\nregression network to yield more 3D vertex accuracy. Extensive experiments show\nthe advantages of the proposed framework in terms of lip-sync, vertex accuracy\nand speech perception, compared with state-of-the-arts. Finally, we show two\napplications of the proposed framework: audio-visual speech recognition and\nspeech-driven 3D Gaussian Splatting based avatar animation.\n'] , ['  This paper presents a neural vocoder based on a denoising diffusion\nprobabilistic model (DDPM) incorporating explicit periodic signals as auxiliary\nconditioning signals. Recently, DDPM-based neural vocoders have gained\nprominence as non-autoregressive models that can generate high-quality\nwaveforms. The neural vocoders based on DDPM have the advantage of training\nwith a simple time-domain loss. In practical applications, such as singing\nvoice synthesis, there is a demand for neural vocoders to generate\nhigh-fidelity speech waveforms with flexible pitch control. However,\nconventional DDPM-based neural vocoders struggle to generate speech waveforms\nunder such conditions. Our proposed model aims to accurately capture the\nperiodic structure of speech waveforms by incorporating explicit periodic\nsignals. Experimental results show that our model improves sound quality and\nprovides better pitch control than conventional DDPM-based neural vocoders.\n', '  Neural vocoders model the raw audio waveform and synthesize high-quality\naudio, but even the highly efficient ones, like MB-MelGAN and LPCNet, fail to\nrun real-time on a low-end device like a smartglass. A pure digital signal\nprocessing (DSP) based vocoder can be implemented via lightweight fast Fourier\ntransforms (FFT), and therefore, is a magnitude faster than any neural vocoder.\nA DSP vocoder often gets a lower audio quality due to consuming over-smoothed\nacoustic model predictions of approximate representations for the vocal tract.\nIn this paper, we propose an ultra-lightweight differential DSP (DDSP) vocoder\nthat uses a jointly optimized acoustic model with a DSP vocoder, and learns\nwithout an extracted spectral feature for the vocal tract. The model achieves\naudio quality comparable to neural vocoders with a high average MOS of 4.36\nwhile being efficient as a DSP vocoder. Our C++ implementation, without any\nhardware-specific optimization, is at 15 MFLOPS, surpasses MB-MelGAN by 340\ntimes in terms of FLOPS, and achieves a vocoder-only RTF of 0.003 and overall\nRTF of 0.044 while running single-threaded on a 2GHz Intel Xeon CPU.\n', '  Since the introduction of Generative Adversarial Networks (GANs) in speech\nsynthesis, remarkable achievements have been attained. In a thorough\nexploration of vocoders, it has been discovered that audio waveforms can be\ngenerated at speeds exceeding real-time while maintaining high fidelity,\nachieved through the utilization of GAN-based models. Typically, the inputs to\nthe vocoder consist of band-limited spectral information, which inevitably\nsacrifices high-frequency details. To address this, we adopt the full-band Mel\nspectrogram information as input, aiming to provide the vocoder with the most\ncomprehensive information possible. However, previous studies have revealed\nthat the use of full-band spectral information as input can result in the issue\nof over-smoothing, compromising the naturalness of the synthesized speech. To\ntackle this challenge, we propose VNet, a GAN-based neural vocoder network that\nincorporates full-band spectral information and introduces a Multi-Tier\nDiscriminator (MTD) comprising multiple sub-discriminators to generate\nhigh-resolution signals. Additionally, we introduce an asymptotically\nconstrained method that modifies the adversarial loss of the generator and\ndiscriminator, enhancing the stability of the training process. Through\nrigorous experiments, we demonstrate that the VNet model is capable of\ngenerating high-fidelity speech and significantly improving the performance of\nthe vocoder.\n'] , [""  Singing voice conversion (SVC) automates song covers by converting one\nsinger's singing voice into another target singer's singing voice with the\noriginal lyrics and melody. However, it raises serious concerns about copyright\nand civil right infringements to multiple entities. This work proposes\nSongBsAb, the first proactive approach to mitigate unauthorized SVC-based\nillegal song covers. SongBsAb introduces human-imperceptible perturbations to\nsinging voices before releasing them, so that when they are used, the\ngeneration process of SVC will be interfered, resulting in unexpected singing\nvoices. SongBsAb features a dual prevention effect by causing both (singer)\nidentity disruption and lyric disruption, namely, the SVC-covered singing voice\nneither imitates the target singer nor preserves the original lyrics. To\nimprove the imperceptibility of perturbations, we refine a psychoacoustic\nmodel-based loss with the backing track as an additional masker, a unique\naccompanying element for singing voices compared to ordinary speech voices. To\nenhance the transferability, we propose to utilize a frame-level interaction\nreduction-based loss. We demonstrate the prevention effectiveness, utility, and\nrobustness of SongBsAb on three SVC models and two datasets using both\nobjective and human study-based subjective metrics. Our work fosters an\nemerging research direction for mitigating illegal automated song covers.\n"", '  Significant strides have been made in creating voice identity representations\nusing speech data. However, the same level of progress has not been achieved\nfor singing voices. To bridge this gap, we suggest a framework for training\nsinger identity encoders to extract representations suitable for various\nsinging-related tasks, such as singing voice similarity and synthesis. We\nexplore different self-supervised learning techniques on a large collection of\nisolated vocal tracks and apply data augmentations during training to ensure\nthat the representations are invariant to pitch and content variations. We\nevaluate the quality of the resulting representations on singer similarity and\nidentification tasks across multiple datasets, with a particular emphasis on\nout-of-domain generalization. Our proposed framework produces high-quality\nembeddings that outperform both speaker verification and wav2vec 2.0\npre-trained baselines on singing voice while operating at 44.1 kHz. We release\nour code and trained models to facilitate further research on singing voice and\nrelated areas.\n', '  Style transfer for out-of-domain (OOD) singing voice synthesis (SVS) focuses\non generating high-quality singing voices with unseen styles (such as timbre,\nemotion, pronunciation, and articulation skills) derived from reference singing\nvoice samples. However, the endeavor to model the intricate nuances of singing\nvoice styles is an arduous task, as singing voices possess a remarkable degree\nof expressiveness. Moreover, existing SVS methods encounter a decline in the\nquality of synthesized singing voices in OOD scenarios, as they rest upon the\nassumption that the target vocal attributes are discernible during the training\nphase. To overcome these challenges, we propose StyleSinger, the first singing\nvoice synthesis model for zero-shot style transfer of out-of-domain reference\nsinging voice samples. StyleSinger incorporates two critical approaches for\nenhanced effectiveness: 1) the Residual Style Adaptor (RSA) which employs a\nresidual quantization module to capture diverse style characteristics in\nsinging voices, and 2) the Uncertainty Modeling Layer Normalization (UMLN) to\nperturb the style attributes within the content representation during the\ntraining phase and thus improve the model generalization. Our extensive\nevaluations in zero-shot style transfer undeniably establish that StyleSinger\noutperforms baseline models in both audio quality and similarity to the\nreference singing voice samples. Access to singing voice samples can be found\nat https://stylesinger.github.io/.\n'] , [""  This paper presents a method for end-to-end cross-lingual text-to-speech\n(TTS) which aims to preserve the target language's pronunciation regardless of\nthe original speaker's language. The model used is based on a non-attentive\nTacotron architecture, where the decoder has been replaced with a normalizing\nflow network conditioned on the speaker identity, allowing both TTS and voice\nconversion (VC) to be performed by the same model due to the inherent\nlinguistic content and speaker identity disentanglement. When used in a\ncross-lingual setting, acoustic features are initially produced with a native\nspeaker of the target language and then voice conversion is applied by the same\nmodel in order to convert these features to the target speaker's voice. We\nverify through objective and subjective evaluations that our method can have\nbenefits compared to baseline cross-lingual synthesis. By including speakers\naveraging 7.5 minutes of speech, we also present positive results on\nlow-resource scenarios.\n"", ""  Recently, zero-shot text-to-speech (TTS) systems, capable of synthesizing any\nspeaker's voice from a short audio prompt, have made rapid advancements.\nHowever, the quality of the generated speech significantly deteriorates when\nthe audio prompt contains noise, and limited research has been conducted to\naddress this issue. In this paper, we explored various strategies to enhance\nthe quality of audio generated from noisy audio prompts within the context of\nflow-matching-based zero-shot TTS. Our investigation includes comprehensive\ntraining strategies: unsupervised pre-training with masked speech denoising,\nmulti-speaker detection and DNSMOS-based data filtering on the pre-training\ndata, and fine-tuning with random noise mixing. The results of our experiments\ndemonstrate significant improvements in intelligibility, speaker similarity,\nand overall audio quality compared to the approach of applying speech\nenhancement to the audio prompt.\n"", '  In speech synthesis, modeling of rich emotions and prosodic variations\npresent in human voice are crucial to synthesize natural speech. Although\nspeaker embeddings have been widely used in personalized speech synthesis as\nconditioning inputs, they are designed to lose variation to optimize speaker\nrecognition accuracy. Thus, they are suboptimal for speech synthesis in terms\nof modeling the rich variations at the output speech distribution. In this\nwork, we propose a novel speaker embedding network which utilizes multiple\nclass centers in the speaker classification training rather than a single class\ncenter as traditional embeddings. The proposed approach introduces variations\nin the speaker embedding while retaining the speaker recognition performance\nsince model does not have to map all of the utterances of a speaker into a\nsingle class center. We apply our proposed embedding in voice conversion task\nand show that our method provides better naturalness and prosody in synthesized\nspeech.\n']",Speech and Voice Synthesis,Speech Synthesis and Voice Conversion
103,"Audio Captioning and Speech Recognition , Audio-Visual Segmentation and Understanding , Audio Representation Learning , Speaker Diarization in Audio Files","['audioset', 'captioning', 'captions', 'audiocaps', 'caption', 'audio', 'voice', 'audios', 'speech', 'clips'] , ['audioset', 'audiovisual', 'audio', 'visual', 'videos', 'scenes', 'supervised', 'auditory', 'embeddings', 'encoder'] , ['audio', 'microphone', 'encoder', 'recording', 'acoustic', 'supervised', 'acoustics', 'wavlm', 'recognition', 'auditory'] , ['speaker', 'utterances', 'diarization', 'voice', 'voices', 'speech', 'audio', 'microphone', 'corpus', 'transcription']","['  Multi-modal learning in the audio-language domain has seen significant\nadvancements in recent years. However, audio-language learning faces challenges\ndue to limited and lower-quality data compared to image-language tasks.\nExisting audio-language datasets are notably smaller, and manual labeling is\nhindered by the need to listen to entire audio clips for accurate labeling.\n  Our method systematically generates audio-caption pairs by augmenting audio\nclips with natural language labels and corresponding audio signal processing\noperations. Leveraging a Large Language Model, we generate descriptions of\naugmented audio clips with a prompt template. This scalable method produces\nAudioSetMix, a high-quality training dataset for text-and-audio related models.\n  Integration of our dataset improves models performance on benchmarks by\nproviding diversified and better-aligned examples. Notably, our dataset\naddresses the absence of modifiers (adjectives and adverbs) in existing\ndatasets. By enabling models to learn these concepts, and generating hard\nnegative examples during training, we achieve state-of-the-art performance on\nmultiple benchmarks.\n', '  Humans are adept at leveraging visual cues from lip movements for recognizing\nspeech in adverse listening conditions. Audio-Visual Speech Recognition (AVSR)\nmodels follow similar approach to achieve robust speech recognition in noisy\nconditions. In this work, we present a multilingual AVSR model incorporating\nseveral enhancements to improve performance and audio noise robustness.\nNotably, we adapt the recently proposed Fast Conformer model to process both\naudio and visual modalities using a novel hybrid CTC/RNN-T architecture. We\nincrease the amount of audio-visual training data for six distinct languages,\ngenerating automatic transcriptions of unlabelled multilingual datasets\n(VoxCeleb2 and AVSpeech). Our proposed model achieves new state-of-the-art\nperformance on the LRS3 dataset, reaching WER of 0.8%. On the recently\nintroduced MuAViC benchmark, our model yields an absolute average-WER reduction\nof 11.9% in comparison to the original baseline. Finally, we demonstrate the\nability of the proposed model to perform audio-only, visual-only, and\naudio-visual speech recognition at test time.\n', '  Automated audio captioning is a cross-modal translation task for describing\nthe content of audio clips with natural language sentences. This task has\nattracted increasing attention and substantial progress has been made in recent\nyears. Captions generated by existing models are generally faithful to the\ncontent of audio clips, however, these machine-generated captions are often\ndeterministic (e.g., generating a fixed caption for a given audio clip), simple\n(e.g., using common words and simple grammar), and generic (e.g., generating\nthe same caption for similar audio clips). When people are asked to describe\nthe content of an audio clip, different people tend to focus on different sound\nevents and describe an audio clip diversely from various aspects using distinct\nwords and grammar. We believe that an audio captioning system should have the\nability to generate diverse captions, either for a fixed audio clip, or across\nsimilar audio clips. To this end, we propose an adversarial training framework\nbased on a conditional generative adversarial network (C-GAN) to improve\ndiversity of audio captioning systems. A caption generator and two hybrid\ndiscriminators compete and are learned jointly, where the caption generator can\nbe any standard encoder-decoder captioning model used to generate captions, and\nthe hybrid discriminators assess the generated captions from different\ncriteria, such as their naturalness and semantics. We conduct experiments on\nthe Clotho dataset. The results show that our proposed model can generate\ncaptions with better diversity as compared to state-of-the-art methods.\n'] , ['  Audio-Visual Segmentation (AVS) aims to identify, at the pixel level, the\nobject in a visual scene that produces a given sound. Current AVS methods rely\non costly fine-grained annotations of mask-audio pairs, making them impractical\nfor scalability. To address this, we introduce unsupervised AVS, eliminating\nthe need for such expensive annotation. To tackle this more challenging\nproblem, we propose an unsupervised learning method, named Modality\nCorrespondence Alignment (MoCA), which seamlessly integrates off-the-shelf\nfoundation models like DINO, SAM, and ImageBind. This approach leverages their\nknowledge complementarity and optimizes their joint usage for multi-modality\nassociation. Initially, we estimate positive and negative image pairs in the\nfeature space. For pixel-level association, we introduce an audio-visual\nadapter and a novel pixel matching aggregation strategy within the image-level\ncontrastive learning framework. This allows for a flexible connection between\nobject appearance and audio signal at the pixel level, with tolerance to\nimaging variations such as translation and rotation. Extensive experiments on\nthe AVSBench (single and multi-object splits) and AVSS datasets demonstrate\nthat our MoCA outperforms strongly designed baseline methods and approaches\nsupervised counterparts, particularly in complex scenarios with multiple\nauditory objects. Notably when comparing mIoU, MoCA achieves a substantial\nimprovement over baselines in both the AVSBench (S4: +17.24%; MS3: +67.64%) and\nAVSS (+19.23%) audio-visual segmentation challenges.\n', '  Audio-Visual Segmentation (AVS) aims to achieve pixel-level localization of\nsound sources in videos, while Audio-Visual Semantic Segmentation (AVSS), as an\nextension of AVS, further pursues semantic understanding of audio-visual\nscenes. However, since the AVSS task requires the establishment of audio-visual\ncorrespondence and semantic understanding simultaneously, we observe that\nprevious methods have struggled to handle this mashup of objectives in\nend-to-end training, resulting in insufficient learning and sub-optimization.\nTherefore, we propose a two-stage training strategy called \\textit{Stepping\nStones}, which decomposes the AVSS task into two simple subtasks from\nlocalization to semantic understanding, which are fully optimized in each stage\nto achieve step-by-step global optimization. This training strategy has also\nproved its generalization and effectiveness on existing methods. To further\nimprove the performance of AVS tasks, we propose a novel framework Adaptive\nAudio Visual Segmentation, in which we incorporate an adaptive audio query\ngenerator and integrate masked attention into the transformer decoder,\nfacilitating the adaptive fusion of visual and audio features. Extensive\nexperiments demonstrate that our methods achieve state-of-the-art results on\nall three AVS benchmarks. The project homepage can be accessed at\nhttps://gewu-lab.github.io/stepping_stones/.\n', '  Recent advances in multimodal LLMs, have led to several video-text models\nbeing proposed for critical video-related tasks. However, most of the previous\nworks support visual input only, essentially muting the audio signal in the\nvideo. Few models that support both audio and visual input, are not explicitly\ntrained on audio data. Hence, the effect of audio towards video understanding\nis largely unexplored. To this end, we propose a model architecture that\nhandles audio-visual inputs explicitly. We train our model with both audio and\nvisual data from a video instruction-tuning dataset. Comparison with\nvision-only baselines, and other audio-visual models showcase that training on\naudio data indeed leads to improved grounding of responses. For better\nevaluation of audio-visual models, we also release a human-annotated benchmark\ndataset, with audio-aware question-answer pairs.\n'] , ['  The goal of universal audio representation learning is to obtain foundational\nmodels that can be used for a variety of downstream tasks involving speech,\nmusic and environmental sounds. To approach this problem, methods inspired by\nworks on self-supervised learning for NLP, like BERT, or computer vision, like\nmasked autoencoders (MAE), are often adapted to the audio domain. In this work,\nwe propose masking representations of the audio signal, and training a MAE to\nreconstruct the masked segments. The reconstruction is done by predicting the\ndiscrete units generated by EnCodec, a neural audio codec, from the unmasked\ninputs. We evaluate this approach, which we call EnCodecMAE, on a wide range of\ntasks involving speech, music and environmental sounds. Our best model\noutperforms various state-of-the-art audio representation models in terms of\nglobal performance. Additionally, we evaluate the resulting representations in\nthe challenging task of automatic speech recognition (ASR), obtaining decent\nresults and paving the way for a universal audio representation.\n', '  Audio tagging is an important task of mapping audio samples to their\ncorresponding categories. Recently endeavours that exploit transformer models\nin this field have achieved great success. However, the quadratic\nself-attention cost limits the scaling of audio transformer models and further\nconstrains the development of more universal audio models. In this paper, we\nattempt to solve this problem by proposing Audio Mamba, a self-attention-free\napproach that captures long audio spectrogram dependency with state space\nmodels. Our experimental results on two audio-tagging datasets demonstrate the\nparameter efficiency of Audio Mamba, it achieves comparable results to SOTA\naudio spectrogram transformers with one third parameters.\n', '  Audio self-supervised learning (SSL) pre-training, which aims to learn good\nrepresentations from unlabeled audio, has made remarkable progress. However,\nthe extensive computational demands during pre-training pose a significant\nbarrier to the potential application and optimization of audio SSL models. In\nthis paper, inspired by the success of data2vec 2.0 in image modality and\nAudio-MAE in audio modality, we introduce Efficient Audio Transformer (EAT) to\nfurther improve the effectiveness and efficiency in audio SSL. The proposed EAT\nadopts the bootstrap self-supervised training paradigm to the audio domain. A\nnovel Utterance-Frame Objective (UFO) is designed to enhance the modeling\ncapability of acoustic events. Furthermore, we reveal that the masking strategy\nis critical in audio SSL pre-training, and superior audio representations can\nbe obtained with large inverse block masks. Experiment results demonstrate that\nEAT achieves state-of-the-art (SOTA) performance on a range of audio-related\ntasks, including AudioSet (AS-2M, AS-20K), ESC-50, and SPC-2, along with a\nsignificant pre-training speedup up to ~15x compared to existing audio SSL\nmodels.\n'] , ['  Speaker diarization provides the answer to the question ""who spoke when?"" for\nan audio file. This information can be used to complete audio transcripts for\nfurther processing steps. Most speaker diarization systems assume that the\naudio file is available as a whole. However, there are scenarios in which the\nspeaker labels are needed immediately after the arrival of an audio segment.\nSpeaker diarization with a correspondingly low latency is referred to as online\nspeaker diarization. This paper provides an overview. First the history of\nonline speaker diarization is briefly presented. Next a taxonomy and datasets\nfor training and evaluation are given. In the sections that follow, online\ndiarization methods and systems are discussed in detail. This paper concludes\nwith the presentation of challenges that still need to be solved by future\nresearch in the field of online speaker diarization.\n', '  Speech foundation models, trained on vast datasets, have opened unique\nopportunities in addressing challenging low-resource speech understanding, such\nas child speech. In this work, we explore the capabilities of speech foundation\nmodels on child-adult speaker diarization. We show that exemplary foundation\nmodels can achieve 39.5% and 62.3% relative reductions in Diarization Error\nRate and Speaker Confusion Rate, respectively, compared to previous speaker\ndiarization methods. In addition, we benchmark and evaluate the speaker\ndiarization results of the speech foundation models with varying the input\naudio window size, speaker demographics, and training data ratio. Our results\nhighlight promising pathways for understanding and adopting speech foundation\nmodels to facilitate child speech understanding.\n', ""  Speaker diarization has gained considerable attention within speech\nprocessing research community. Mainstream speaker diarization rely primarily on\nspeakers' voice characteristics extracted from acoustic signals and often\noverlook the potential of semantic information. Considering the fact that\nspeech signals can efficiently convey the content of a speech, it is of our\ninterest to fully exploit these semantic cues utilizing language models. In\nthis work we propose a novel approach to effectively leverage semantic\ninformation in clustering-based speaker diarization systems. Firstly, we\nintroduce spoken language understanding modules to extract speaker-related\nsemantic information and utilize these information to construct pairwise\nconstraints. Secondly, we present a novel framework to integrate these\nconstraints into the speaker diarization pipeline, enhancing the performance of\nthe entire system. Extensive experiments conducted on the public dataset\ndemonstrate the consistent superiority of our proposed approach over\nacoustic-only speaker diarization systems.\n""]",Audio and Speech Processing,Audio Representation Learning
104,"Automatic Speech Recognition (ASR) Systems , Speech Enhancement and Quality Estimation , Dysarthric Speech Recognition and Assessment , Automatic Speech Recognition (ASR) for Multilingual and Child Speech","['transcription', 'voice', 'transcriptions', 'corpus', 'wav2vec', 'speech', 'asr', 'transcribed', 'wav2vec2', 'phonetic'] , ['voice', 'audio', 'vocalizing', 'enhancement', 'speech', 'denoising', 'speaker', 'hearing', 'noisy', 'acoustic'] , ['dysarthric', 'dysarthria', 'speech', 'impaired', 'speaker', 'dsr', 'wav2vec2', 'classification', 'disability', 'assessment'] , ['utterances', 'phonetics', 'phonetic', 'speech', 'phonetically', 'voice', 'corpus', 'asr', 'pronunciation', 'multilingual']","['  Advances in machine learning have made it possible to perform various text\nand speech processing tasks, such as automatic speech recognition (ASR), in an\nend-to-end (E2E) manner. E2E approaches utilizing pre-trained models are\ngaining attention for conserving training data and resources. However, most of\ntheir applications in ASR involve only one of either a pre-trained speech or a\nlanguage model. This paper proposes integrating a pre-trained speech\nrepresentation model and a large language model (LLM) for E2E ASR. The proposed\nmodel enables the optimization of the entire ASR process, including acoustic\nfeature extraction and acoustic and language modeling, by combining pre-trained\nmodels with a bridge network and also enables the application of remarkable\ndevelopments in LLM utilization, such as parameter-efficient domain adaptation\nand inference optimization. Experimental results demonstrate that the proposed\nmodel achieves a performance comparable to that of modern E2E ASR models by\nutilizing powerful pre-training models with the proposed integrated approach.\n', ""  In the realm of automatic speech recognition (ASR), robustness in noisy\nenvironments remains a significant challenge. Recent ASR models, such as\nWhisper, have shown promise, but their efficacy in noisy conditions can be\nfurther enhanced. This study is focused on recovering from packet loss to\nimprove the word error rate (WER) of ASR models. We propose using a front-end\nadaptation network connected to a frozen ASR model. The adaptation network is\ntrained to modify the corrupted input spectrum by minimizing the criteria of\nthe ASR model in addition to an enhancement loss function. Our experiments\ndemonstrate that the adaptation network, trained on Whisper's criteria, notably\nreduces word error rates across domains and languages in packet-loss scenarios.\nThis improvement is achieved with minimal affect to Whisper model's\nfoundational performance, underscoring our method's practicality and potential\nin enhancing ASR models in challenging acoustic environments.\n"", '  Recent advancements in supervised automatic speech recognition (ASR) have\nachieved remarkable performance, largely due to the growing availability of\nlarge transcribed speech corpora. However, most languages lack sufficient\npaired speech and text data to effectively train these systems. In this\narticle, we tackle the challenge of developing ASR systems without paired\nspeech and text corpora by proposing the removal of reliance on a phoneme\nlexicon. We explore a new research direction: word-level unsupervised ASR.\nUsing a curated speech corpus containing only high-frequency English words, our\nsystem achieves a word error rate of nearly 20% without parallel transcripts or\noracle word boundaries. Furthermore, we experimentally demonstrate that an\nunsupervised speech recognizer can emerge from joint speech-to-speech and\ntext-to-text masked token-infilling. This innovative model surpasses the\nperformance of previous unsupervised ASR models trained with direct\ndistribution matching.\n'] , ['  Speech quality estimation has recently undergone a paradigm shift from\nhuman-hearing expert designs to machine-learning models. However, current\nmodels rely mainly on supervised learning, which is time-consuming and\nexpensive for label collection. To solve this problem, we propose VQScore, a\nself-supervised metric for evaluating speech based on the quantization error of\na vector-quantized-variational autoencoder (VQ-VAE). The training of VQ-VAE\nrelies on clean speech; hence, large quantization errors can be expected when\nthe speech is distorted. To further improve correlation with real quality\nscores, domain knowledge of speech processing is incorporated into the model\ndesign. We found that the vector quantization mechanism could also be used for\nself-supervised speech enhancement (SE) model training. To improve the\nrobustness of the encoder for SE, a novel self-distillation mechanism combined\nwith adversarial training is introduced. In summary, the proposed speech\nquality estimation method and enhancement models require only clean speech for\ntraining without any label requirements. Experimental results show that the\nproposed VQScore and enhancement model are competitive with supervised\nbaselines. The code will be released after publication.\n', '  In this paper, we explore a continuous modeling approach for\ndeep-learning-based speech enhancement, focusing on the denoising process. We\nuse a state variable to indicate the denoising process. The starting state is\nnoisy speech and the ending state is clean speech. The noise component in the\nstate variable decreases with the change of the state index until the noise\ncomponent is 0. During training, a UNet-like neural network learns to estimate\nevery state variable sampled from the continuous denoising process. In testing,\nwe introduce a controlling factor as an embedding, ranging from zero to one, to\nthe neural network, allowing us to control the level of noise reduction. This\napproach enables controllable speech enhancement and is adaptable to various\napplication scenarios. Experimental results indicate that preserving a small\namount of noise in the clean target benefits speech enhancement, as evidenced\nby improvements in both objective speech measures and automatic speech\nrecognition performance.\n', '  Speech enhancement systems are typically trained using pairs of clean and\nnoisy speech. In audio-visual speech enhancement (AVSE), there is not as much\nground-truth clean data available; most audio-visual datasets are collected in\nreal-world environments with background noise and reverberation, hampering the\ndevelopment of AVSE. In this work, we introduce AV2Wav, a resynthesis-based\naudio-visual speech enhancement approach that can generate clean speech despite\nthe challenges of real-world training data. We obtain a subset of nearly clean\nspeech from an audio-visual corpus using a neural quality estimator, and then\ntrain a diffusion model on this subset to generate waveforms conditioned on\ncontinuous speech representations from AV-HuBERT with noise-robust training. We\nuse continuous rather than discrete representations to retain prosody and\nspeaker information. With this vocoding task alone, the model can perform\nspeech enhancement better than a masking-based baseline. We further fine-tune\nthe diffusion model on clean/noisy utterance pairs to improve the performance.\nOur approach outperforms a masking-based baseline in terms of both automatic\nmetrics and a human listening test and is close in quality to the target speech\nin the listening test. Audio samples can be found at\nhttps://home.ttic.edu/~jcchou/demo/avse/avse_demo.html.\n'] , ['  Disordered speech recognition profound implications for improving the quality\nof life for individuals afflicted with, for example, dysarthria. Dysarthric\nspeech recognition encounters challenges including limited data, substantial\ndissimilarities between dysarthric and non-dysarthric speakers, and significant\nspeaker variations stemming from the disorder. This paper introduces\nPerceiver-Prompt, a method for speaker adaptation that utilizes P-Tuning on the\nWhisper large-scale model. We first fine-tune Whisper using LoRA and then\nintegrate a trainable Perceiver to generate fixed-length speaker prompts from\nvariable-length inputs, to improve model recognition of Chinese dysarthric\nspeech. Experimental results from our Chinese dysarthric speech dataset\ndemonstrate consistent improvements in recognition performance with\nPerceiver-Prompt. Relative reduction up to 13.04% in CER is obtained over the\nfine-tuned Whisper.\n', ""  Dysarthria is a speech disorder that hinders communication due to\ndifficulties in articulating words. Detection of dysarthria is important for\nseveral reasons as it can be used to develop a treatment plan and help improve\na person's quality of life and ability to communicate effectively. Much of the\nliterature focused on improving ASR systems for dysarthric speech. The\nobjective of the current work is to develop models that can accurately classify\nthe presence of dysarthria and also give information about the intelligibility\nlevel using limited data by employing a few-shot approach using a transformer\nmodel. This work also aims to tackle the data leakage that is present in\nprevious studies. Our whisper-large-v2 transformer model trained on a subset of\nthe UASpeech dataset containing medium intelligibility level patients achieved\nan accuracy of 85%, precision of 0.92, recall of 0.8 F1-score of 0.85, and\nspecificity of 0.91. Experimental results also demonstrate that the model\ntrained using the 'words' dataset performed better compared to the model\ntrained on the 'letters' and 'digits' dataset. Moreover, the multiclass model\nachieved an accuracy of 67%.\n"", '  Automating dysarthria assessments offers the opportunity to develop\npractical, low-cost tools that address the current limitations of manual and\nsubjective assessments. Nonetheless, the small size of most dysarthria datasets\nmakes it challenging to develop automated assessment. Recent research showed\nthat speech representations from models pre-trained on large unlabelled data\ncan enhance Automatic Speech Recognition (ASR) performance for dysarthric\nspeech. We are the first to evaluate the representations from pre-trained\nstate-of-the-art Self-Supervised models across three downstream tasks on\ndysarthric speech: disease classification, word recognition and intelligibility\nclassification, and under three noise scenarios on the UA-Speech dataset. We\nshow that HuBERT is the most versatile feature extractor across dysarthria\nclassification, word recognition, and intelligibility classification, achieving\nrespectively $+24.7\\%, +61\\%, \\text{and} +7.2\\%$ accuracy compared to classical\nacoustic features.\n'] , ['  Whisper is a multitask and multilingual speech model covering 99 languages.\nIt yields commendable automatic speech recognition (ASR) results in a subset of\nits covered languages, but the model still underperforms on a non-negligible\nnumber of under-represented languages, a problem exacerbated in smaller model\nversions. In this work, we examine its limitations, demonstrating the presence\nof speaker-related (gender, age) and model-related (resourcefulness and model\nsize) bias. Despite that, we show that only model-related bias are amplified by\nquantization, impacting more low-resource languages and smaller models.\nSearching for a better compression approach, we propose DistilWhisper, an\napproach that is able to bridge the performance gap in ASR for these languages\nwhile retaining the advantages of multitask and multilingual capabilities. Our\napproach involves two key strategies: lightweight modular ASR fine-tuning of\nwhisper-small using language-specific experts, and knowledge distillation from\nwhisper-large-v2. This dual approach allows us to effectively boost ASR\nperformance while keeping the robustness inherited from the multitask and\nmultilingual pre-training. Results demonstrate that our approach is more\neffective than standard fine-tuning or LoRA adapters, boosting performance in\nthe targeted languages for both in- and out-of-domain test sets, while\nintroducing only a negligible parameter overhead at inference.\n', ""  One of the central skills that language learners need to practice is speaking\nthe language. Currently, students in school do not get enough speaking\nopportunities and lack conversational practice. Recent advances in speech\ntechnology and natural language processing allow for the creation of novel\ntools to practice their speaking skills. In this work, we tackle the first\ncomponent of such a pipeline, namely, the automated speech recognition module\n(ASR), which faces a number of challenges: first, state-of-the-art ASR models\nare often trained on adult read-aloud data by native speakers and do not\ntransfer well to young language learners' speech. Second, most ASR systems\ncontain a powerful language model, which smooths out errors made by the\nspeakers. To give corrective feedback, which is a crucial part of language\nlearning, the ASR systems in our setting need to preserve the errors made by\nthe language learners. In this work, we build an ASR system that satisfies\nthese requirements: it works on spontaneous speech by young language learners\nand preserves their errors. For this, we collected a corpus containing around\n85 hours of English audio spoken by learners in Switzerland from grades 4 to 6\non different language learning tasks, which we used to train an ASR model. Our\nexperiments show that our model benefits from direct fine-tuning on children's\nvoices and has a much higher error preservation rate than other models.\n"", '  Automatic reading diagnosis systems can benefit both teachers for more\nefficient scoring of reading exercises and students for accessing reading\nexercises with feedback more easily. However, there are limited studies on\nAutomatic Speech Recognition (ASR) for child speech in languages other than\nEnglish, and limited research on ASR-based reading diagnosis systems. This\nstudy investigates how efficiently state-of-the-art (SOTA) pretrained ASR\nmodels recognize Dutch native children speech and manage to detect reading\nmiscues. We found that Hubert Large finetuned on Dutch speech achieves SOTA\nphoneme-level child speech recognition (PER at 23.1\\%), while Whisper (Faster\nWhisper Large-v2) achieves SOTA word-level performance (WER at 9.8\\%). Our\nfindings suggest that Wav2Vec2 Large and Whisper are the two best ASR models\nfor reading miscue detection. Specifically, Wav2Vec2 Large shows the highest\nrecall at 0.83, whereas Whisper exhibits the highest precision at 0.52 and an\nF1 score of 0.52.\n']",Speech Processing and Recognition Systems,Automatic Speech Recognition (ASR) Systems
105,"Inverse Problems with Diffusion Models , Diffusion Models for Generative Denoising Tasks , Virtual Try-On with Diffusion Models , Concept Erasure in Diffusion Models , Diffusion Models for Generative Time Series Analysis","['denoising', 'inverse', 'inverses', 'diffusion', 'regularization', 'inversion', 'denoiser', 'regularisation', 'iterative', 'priors'] , ['diffusionmtl', 'denoising', 'diffusion', 'generative', 'models', 'noisy', 'training', 'imagenet', 'noise', 'dtr'] , ['fashionflow', 'garment', 'inception', 'clothing', 'apparel', 'clothes', 'garments', 'cloth', 'fashionformer', 'images'] , ['erasing', 'unlearning', 'erasures', 'erasure', 'relearning', 'unlearn', 'erase', 'memorization', 'eraser', 'erased'] , ['diffusion', 'models', 'denoising', 'generative', 'stochastic', 'denoisers', 'denoiser', 'priors', 'brownian', 'sampling']","['  Inverse problems have many applications in science and engineering. In\nComputer vision, several image restoration tasks such as inpainting,\ndeblurring, and super-resolution can be formally modeled as inverse problems.\nRecently, methods have been developed for solving inverse problems that only\nleverage a pre-trained unconditional diffusion model and do not require\nadditional task-specific training. In such methods, however, the inherent\nintractability of determining the conditional score function during the reverse\ndiffusion process poses a real challenge, leaving the methods to settle with an\napproximation instead, which affects their performance in practice. Here, we\npropose a MAP estimation framework to model the reverse conditional generation\nprocess of a continuous time diffusion model as an optimization process of the\nunderlying MAP objective, whose gradient term is tractable. In theory, the\nproposed framework can be applied to solve general inverse problems using\ngradient-based optimization methods. However, given the highly non-convex\nnature of the loss objective, finding a perfect gradient-based optimization\nalgorithm can be quite challenging, nevertheless, our framework offers several\npotential research directions. We use our proposed formulation and develop\nempirically effective algorithms for solving noiseless and noisy image\ninpainting tasks. We validate our proposed algorithms with extensive\nexperiments across diverse mask settings.\n', ""  Constructing fast samplers for unconditional diffusion and flow-matching\nmodels has received much attention recently; however, existing methods for\nsolving inverse problems, such as super-resolution, inpainting, or deblurring,\nstill require hundreds to thousands of iterative steps to obtain high-quality\nresults. We propose a plug-and-play framework for constructing efficient\nsamplers for inverse problems, requiring only pre-trained diffusion or\nflow-matching models. We present Conditional Conjugate Integrators, which\nleverage the specific form of the inverse problem to project the respective\nconditional diffusion/flow dynamics into a more amenable space for sampling.\nOur method complements popular posterior approximation methods for solving\ninverse problems using diffusion/flow models. We evaluate the proposed method's\nperformance on various linear image restoration tasks across multiple datasets,\nemploying diffusion and flow-matching models. Notably, on challenging inverse\nproblems like 4$\\times$ super-resolution on the ImageNet dataset, our method\ncan generate high-quality samples in as few as 5 conditional sampling steps and\noutperforms competing baselines requiring 20-1000 steps. Our code and models\nwill be publicly available at https://github.com/mandt-lab/CI2RM.\n"", '  Diffusion models have been recently studied as powerful generative inverse\nproblem solvers, owing to their high quality reconstructions and the ease of\ncombining existing iterative solvers. However, most works focus on solving\nsimple linear inverse problems in noiseless settings, which significantly\nunder-represents the complexity of real-world problems. In this work, we extend\ndiffusion solvers to efficiently handle general noisy (non)linear inverse\nproblems via approximation of the posterior sampling. Interestingly, the\nresulting posterior sampling scheme is a blended version of diffusion sampling\nwith the manifold constrained gradient without a strict measurement consistency\nprojection step, yielding a more desirable generative path in noisy settings\ncompared to the previous studies. Our method demonstrates that diffusion models\ncan incorporate various measurement noise statistics such as Gaussian and\nPoisson, and also efficiently handle noisy nonlinear inverse problems such as\nFourier phase retrieval and non-uniform deblurring. Code available at\nhttps://github.com/DPS2022/diffusion-posterior-sampling\n'] , [""  Diffusion models generate highly realistic images by learning a multi-step\ndenoising process, naturally embodying the principles of multi-task learning\n(MTL). Despite the inherent connection between diffusion models and MTL, there\nremains an unexplored area in designing neural architectures that explicitly\nincorporate MTL into the framework of diffusion models. In this paper, we\npresent Denoising Task Routing (DTR), a simple add-on strategy for existing\ndiffusion model architectures to establish distinct information pathways for\nindividual tasks within a single architecture by selectively activating subsets\nof channels in the model. What makes DTR particularly compelling is its\nseamless integration of prior knowledge of denoising tasks into the framework:\n(1) Task Affinity: DTR activates similar channels for tasks at adjacent\ntimesteps and shifts activated channels as sliding windows through timesteps,\ncapitalizing on the inherent strong affinity between tasks at adjacent\ntimesteps. (2) Task Weights: During the early stages (higher timesteps) of the\ndenoising process, DTR assigns a greater number of task-specific channels,\nleveraging the insight that diffusion models prioritize reconstructing global\nstructure and perceptually rich contents in earlier stages, and focus on simple\nnoise removal in later stages. Our experiments reveal that DTR not only\nconsistently boosts diffusion models' performance across different evaluation\nprotocols without adding extra parameters but also accelerates training\nconvergence. Finally, we show the complementarity between our architectural\napproach and existing MTL optimization techniques, providing a more complete\nview of MTL in the context of diffusion training. Significantly, by leveraging\nthis complementarity, we attain matched performance of DiT-XL using the smaller\nDiT-L with a reduction in training iterations from 7M to 2M.\n"", '  Diffusion-based generative models have emerged as powerful tools in the realm\nof generative modeling. Despite extensive research on denoising across various\ntimesteps and noise levels, a conflict persists regarding the relative\ndifficulties of the denoising tasks. While various studies argue that lower\ntimesteps present more challenging tasks, others contend that higher timesteps\nare more difficult. To address this conflict, our study undertakes a\ncomprehensive examination of task difficulties, focusing on convergence\nbehavior and changes in relative entropy between consecutive probability\ndistributions across timesteps. Our observational study reveals that denoising\nat earlier timesteps poses challenges characterized by slower convergence and\nhigher relative entropy, indicating increased task difficulty at these lower\ntimesteps. Building on these observations, we introduce an easy-to-hard\nlearning scheme, drawing from curriculum learning, to enhance the training\nprocess of diffusion models. By organizing timesteps or noise levels into\nclusters and training models with ascending orders of difficulty, we facilitate\nan order-aware training regime, progressing from easier to harder denoising\ntasks, thereby deviating from the conventional approach of training diffusion\nmodels simultaneously across all timesteps. Our approach leads to improved\nperformance and faster convergence by leveraging benefits of curriculum\nlearning, while maintaining orthogonality with existing improvements in\ndiffusion training techniques. We validate these advantages through\ncomprehensive experiments in image generation tasks, including unconditional,\nclass-conditional, and text-to-image generation.\n', '  Diffusion models have demonstrated remarkable efficacy in various generative\ntasks with the predictive prowess of denoising model. Currently, these models\nemploy a uniform denoising approach across all timesteps. However, the inherent\nvariations in noisy latents at each timestep lead to conflicts during training,\nconstraining the potential of diffusion models. To address this challenge, we\npropose a novel two-stage training strategy termed Step-Adaptive Training. In\nthe initial stage, a base denoising model is trained to encompass all\ntimesteps. Subsequently, we partition the timesteps into distinct groups,\nfine-tuning the model within each group to achieve specialized denoising\ncapabilities. Recognizing that the difficulties of predicting noise at\ndifferent timesteps vary, we introduce a diverse model size requirement. We\ndynamically adjust the model size for each timestep by estimating task\ndifficulty based on its signal-to-noise ratio before fine-tuning. This\nadjustment is facilitated by a proxy-based structural importance assessment\nmechanism, enabling precise and efficient pruning of the base denoising model.\nOur experiments validate the effectiveness of the proposed training strategy,\ndemonstrating an improvement in the FID score on CIFAR10 by over 0.3 while\nutilizing only 80\\% of the computational resources. This innovative approach\nnot only enhances model performance but also significantly reduces\ncomputational costs, opening new avenues for the development and application of\ndiffusion models.\n'] , ['  Virtual try-on can significantly improve the garment shopping experiences in\nboth online and in-store scenarios, attracting broad interest in computer\nvision. However, to achieve high-fidelity try-on performance, most\nstate-of-the-art methods still rely on accurate segmentation masks, which are\noften produced by near-perfect parsers or manual labeling. To overcome the\nbottleneck, we propose a parser-free virtual try-on method based on the\ndiffusion model (PFDM). Given two images, PFDM can ""wear"" garments on the\ntarget person seamlessly by implicitly warping without any other information.\nTo learn the model effectively, we synthesize many pseudo-images and construct\nsample pairs by wearing various garments on persons. Supervised by the\nlarge-scale expanded dataset, we fuse the person and garment features using a\nproposed Garment Fusion Attention (GFA) mechanism. Experiments demonstrate that\nour proposed PFDM can successfully handle complex cases, synthesize\nhigh-fidelity images, and outperform both state-of-the-art parser-free and\nparser-based models.\n', '  Virtual Try-on (VTON) involves generating images of a person wearing selected\ngarments. Diffusion-based methods, in particular, can create high-quality\nimages, but they struggle to maintain the identities of the input garments. We\nidentified this problem stems from the specifics in the training formulation\nfor diffusion. To address this, we propose a unique training scheme that limits\nthe scope in which diffusion is trained. We use a control image that perfectly\naligns with the target image during training. In turn, this accurately\npreserves garment details during inference. We demonstrate our method not only\neffectively conserves garment details but also allows for layering, styling,\nand shoe try-on. Our method runs multi-garment try-on in a single inference\ncycle and can support high-quality zoomed-in generations without training in\nhigher resolutions. Finally, we show our method surpasses prior methods in\naccuracy and quality.\n', ""  Image-based virtual try-on is an increasingly important task for online\nshopping. It aims to synthesize images of a specific person wearing a specified\ngarment. Diffusion model-based approaches have recently become popular, as they\nare excellent at image synthesis tasks. However, these approaches usually\nemploy additional image encoders and rely on the cross-attention mechanism for\ntexture transfer from the garment to the person image, which affects the\ntry-on's efficiency and fidelity. To address these issues, we propose an\nTexture-Preserving Diffusion (TPD) model for virtual try-on, which enhances the\nfidelity of the results and introduces no additional image encoders.\nAccordingly, we make contributions from two aspects. First, we propose to\nconcatenate the masked person and reference garment images along the spatial\ndimension and utilize the resulting image as the input for the diffusion\nmodel's denoising UNet. This enables the original self-attention layers\ncontained in the diffusion model to achieve efficient and accurate texture\ntransfer. Second, we propose a novel diffusion-based method that predicts a\nprecise inpainting mask based on the person and reference garment images,\nfurther enhancing the reliability of the try-on results. In addition, we\nintegrate mask prediction and image synthesis into a single compact model. The\nexperimental results show that our approach can be applied to various try-on\ntasks, e.g., garment-to-person and person-to-person try-ons, and significantly\noutperforms state-of-the-art methods on popular VITON, VITON-HD databases.\n""] , ['  Large-scale diffusion models, known for their impressive image generation\ncapabilities, have raised concerns among researchers regarding social impacts,\nsuch as the imitation of copyrighted artistic styles. In response, existing\napproaches turn to machine unlearning techniques to eliminate unsafe concepts\nfrom pre-trained models. However, these methods compromise the generative\nperformance and neglect the coupling among multi-concept erasures, as well as\nthe concept restoration problem. To address these issues, we propose a\nSeparable Multi-concept Eraser (SepME), which mainly includes two parts: the\ngeneration of concept-irrelevant representations and the weight decoupling. The\nformer aims to avoid unlearning substantial information that is irrelevant to\nforgotten concepts. The latter separates optimizable model weights, making each\nweight increment correspond to a specific concept erasure without affecting\ngenerative performance on other concepts. Specifically, the weight increment\nfor erasing a specified concept is formulated as a linear combination of\nsolutions calculated based on other known undesirable concepts. Extensive\nexperiments indicate the efficacy of our approach in eliminating concepts,\npreserving model performance, and offering flexibility in the erasure or\nrecovery of various concepts.\n', '  The rapid expansion of large-scale text-to-image diffusion models has raised\ngrowing concerns regarding their potential misuse in creating harmful or\nmisleading content. In this paper, we introduce MACE, a finetuning framework\nfor the task of mass concept erasure. This task aims to prevent models from\ngenerating images that embody unwanted concepts when prompted. Existing concept\nerasure methods are typically restricted to handling fewer than five concepts\nsimultaneously and struggle to find a balance between erasing concept synonyms\n(generality) and maintaining unrelated concepts (specificity). In contrast,\nMACE differs by successfully scaling the erasure scope up to 100 concepts and\nby achieving an effective balance between generality and specificity. This is\nachieved by leveraging closed-form cross-attention refinement along with LoRA\nfinetuning, collectively eliminating the information of undesirable concepts.\nFurthermore, MACE integrates multiple LoRAs without mutual interference. We\nconduct extensive evaluations of MACE against prior methods across four\ndifferent tasks: object erasure, celebrity erasure, explicit content erasure,\nand artistic style erasure. Our results reveal that MACE surpasses prior\nmethods in all evaluated tasks. Code is available at\nhttps://github.com/Shilin-LU/MACE.\n', '  Generating images from text has become easier because of the scaling of\ndiffusion models and advancements in the field of vision and language. These\nmodels are trained using vast amounts of data from the Internet. Hence, they\noften contain undesirable content such as copyrighted material. As it is\nchallenging to remove such data and retrain the models, methods for erasing\nspecific concepts from pre-trained models have been investigated. We propose a\nnovel concept-erasure method that updates the text encoder using few-shot\nunlearning in which a few real images are used. The discussion regarding the\ngenerated images after erasing a concept has been lacking. While there are\nmethods for specifying the transition destination for concepts, the validity of\nthe specified concepts is unclear. Our method implicitly achieves this by\ntransitioning to the latent concepts inherent in the model or the images. Our\nmethod can erase a concept within 10 s, making concept erasure more accessible\nthan ever before. Implicitly transitioning to related concepts leads to more\nnatural concept erasure. We applied the proposed method to various concepts and\nconfirmed that concept erasure can be achieved tens to hundreds of times faster\nthan with current methods. By varying the parameters to be updated, we obtained\nresults suggesting that, like previous research, knowledge is primarily\naccumulated in the feed-forward networks of the text encoder.\n'] , ['  Fourier analysis has been an instrumental tool in the development of signal\nprocessing. This leads us to wonder whether this framework could similarly\nbenefit generative modelling. In this paper, we explore this question through\nthe scope of time series diffusion models. More specifically, we analyze\nwhether representing time series in the frequency domain is a useful inductive\nbias for score-based diffusion models. By starting from the canonical SDE\nformulation of diffusion in the time domain, we show that a dual diffusion\nprocess occurs in the frequency domain with an important nuance: Brownian\nmotions are replaced by what we call mirrored Brownian motions, characterized\nby mirror symmetries among their components. Building on this insight, we show\nhow to adapt the denoising score matching approach to implement diffusion\nmodels in the frequency domain. This results in frequency diffusion models,\nwhich we compare to canonical time diffusion models. Our empirical evaluation\non real-world datasets, covering various domains like healthcare and finance,\nshows that frequency diffusion models better capture the training distribution\nthan time diffusion models. We explain this observation by showing that time\nseries from these datasets tend to be more localized in the frequency domain\nthan in the time domain, which makes them easier to model in the former case.\nAll our observations point towards impactful synergies between Fourier analysis\nand diffusion models.\n', '  Diffusion models are gaining widespread use in cutting-edge image, video, and\naudio generation. Score-based diffusion models stand out among these methods,\nnecessitating the estimation of score function of the input data distribution.\nIn this study, we present a theoretical framework to analyze two-layer neural\nnetwork-based diffusion models by reframing score matching and denoising score\nmatching as convex optimization. We prove that training shallow neural networks\nfor score prediction can be done by solving a single convex program. Although\nmost analyses of diffusion models operate in the asymptotic setting or rely on\napproximations, we characterize the exact predicted score function and\nestablish convergence results for neural network-based diffusion models with\nfinite data. Our results provide a precise characterization of what neural\nnetwork-based diffusion models learn in non-asymptotic settings.\n', '  Diffusion models, a powerful and universal generative AI technology, have\nachieved tremendous success in computer vision, audio, reinforcement learning,\nand computational biology. In these applications, diffusion models provide\nflexible high-dimensional data modeling, and act as a sampler for generating\nnew samples under active guidance towards task-desired properties. Despite the\nsignificant empirical success, theory of diffusion models is very limited,\npotentially slowing down principled methodological innovations for further\nharnessing and improving diffusion models. In this paper, we review emerging\napplications of diffusion models, understanding their sample generation under\nvarious controls. Next, we overview the existing theories of diffusion models,\ncovering their statistical properties and sampling capabilities. We adopt a\nprogressive routine, beginning with unconditional diffusion models and\nconnecting to conditional counterparts. Further, we review a new avenue in\nhigh-dimensional structured optimization through conditional diffusion models,\nwhere searching for solutions is reformulated as a conditional sampling problem\nand solved by diffusion models. Lastly, we discuss future directions about\ndiffusion models. The purpose of this paper is to provide a well-rounded\ntheoretical exposure for stimulating forward-looking theories and methods of\ndiffusion models.\n']",Diffusion Models for Generative Tasks,Diffusion Models for Generative Denoising Tasks
106,"Diffusion Models for Image Generation with Guidance Techniques , Diffusion Models for Image Generation and Augmentation , Video Diffusion Models for Generation , Diffusion-based Image Editing","['attention', 'generative', 'blurring', 'conditioned', 'images', 'conditioning', 'models', 'guidance', 'denoised', 'diffusion'] , ['imagenet', 'robust_dm_generated_image_detection', 'deepaugment', 'generative', 'imagenet_d', 'augmentation', 'encode', 'gan', 'augment', 'augmentations'] , ['videos', 'video', 'attention', 'memory', 'vid', 'diffusion', 'webvid', 'frames', 'pixels', 'conditioned'] , ['editing', 'gans', 'generative', 'gan', 'blur', 'denoising', 'attention', 'diffusion', 'images', 'pixel']","['  Conditional diffusion models have shown remarkable success in visual content\ngeneration, producing high-quality samples across various domains, largely due\nto classifier-free guidance (CFG). Recent attempts to extend guidance to\nunconditional models have relied on heuristic techniques, resulting in\nsuboptimal generation quality and unintended effects. In this work, we propose\nSmoothed Energy Guidance (SEG), a novel training- and condition-free approach\nthat leverages the energy-based perspective of the self-attention mechanism to\nenhance image generation. By defining the energy of self-attention, we\nintroduce a method to reduce the curvature of the energy landscape of attention\nand use the output as the unconditional prediction. Practically, we control the\ncurvature of the energy landscape by adjusting the Gaussian kernel parameter\nwhile keeping the guidance scale parameter fixed. Additionally, we present a\nquery blurring method that is equivalent to blurring the entire attention\nweights without incurring quadratic complexity in the number of tokens. In our\nexperiments, SEG achieves a Pareto improvement in both quality and the\nreduction of side effects. The code is available at\n\\url{https://github.com/SusungHong/SEG-SDXL}.\n', '  Classifier-free guidance (CFG) is a fundamental tool in modern diffusion\nmodels for text-guided generation. Although effective, CFG has notable\ndrawbacks. For instance, DDIM with CFG lacks invertibility, complicating image\nediting; furthermore, high guidance scales, essential for high-quality outputs,\nfrequently result in issues like mode collapse. Contrary to the widespread\nbelief that these are inherent limitations of diffusion models, this paper\nreveals that the problems actually stem from the off-manifold phenomenon\nassociated with CFG, rather than the diffusion models themselves. More\nspecifically, inspired by the recent advancements of diffusion model-based\ninverse problem solvers (DIS), we reformulate text-guidance as an inverse\nproblem with a text-conditioned score matching loss, and develop CFG++, a novel\napproach that tackles the off-manifold challenges inherent in traditional CFG.\nCFG++ features a surprisingly simple fix to CFG, yet it offers significant\nimprovements, including better sample quality for text-to-image generation,\ninvertibility, smaller guidance scales, reduced mode collapse, etc.\nFurthermore, CFG++ enables seamless interpolation between unconditional and\nconditional sampling at lower guidance scales, consistently outperforming\ntraditional CFG at all scales. Experimental results confirm that our method\nsignificantly enhances performance in text-to-image generation, DDIM inversion,\nediting, and solving inverse problems, suggesting a wide-ranging impact and\npotential applications in various fields that utilize text guidance. Project\nPage: https://cfgpp-diffusion.github.io/.\n', ""  Recent studies have demonstrated that diffusion models are capable of\ngenerating high-quality samples, but their quality heavily depends on sampling\nguidance techniques, such as classifier guidance (CG) and classifier-free\nguidance (CFG). These techniques are often not applicable in unconditional\ngeneration or in various downstream tasks such as image restoration. In this\npaper, we propose a novel sampling guidance, called Perturbed-Attention\nGuidance (PAG), which improves diffusion sample quality across both\nunconditional and conditional settings, achieving this without requiring\nadditional training or the integration of external modules. PAG is designed to\nprogressively enhance the structure of samples throughout the denoising\nprocess. It involves generating intermediate samples with degraded structure by\nsubstituting selected self-attention maps in diffusion U-Net with an identity\nmatrix, by considering the self-attention mechanisms' ability to capture\nstructural information, and guiding the denoising process away from these\ndegraded samples. In both ADM and Stable Diffusion, PAG surprisingly improves\nsample quality in conditional and even unconditional scenarios. Moreover, PAG\nsignificantly improves the baseline performance in various downstream tasks\nwhere existing guidances such as CG or CFG cannot be fully utilized, including\nControlNet with empty prompts and image restoration such as inpainting and\ndeblurring.\n""] , ['  Bagging has achieved great success in the field of machine learning by\nintegrating multiple base classifiers to build a single strong classifier to\nreduce model variance. The performance improvement of bagging mainly relies on\nthe number and diversity of base classifiers. However, traditional deep\nlearning model training methods are expensive to train individually and\ndifficult to train multiple models with low similarity in a restricted dataset.\nRecently, diffusion models, which have been tremendously successful in the\nfields of imaging and vision, have been found to be effective in generating\nneural network model weights and biases with diversity. We creatively propose a\nBagging deep learning training algorithm based on Efficient Neural network\nDiffusion (BEND). The originality of BEND comes from the first use of a neural\nnetwork diffusion model to efficiently build base classifiers for bagging. Our\napproach is simple but effective, first using multiple trained model weights\nand biases as inputs to train autoencoder and latent diffusion model to realize\na diffusion model from noise to valid neural network parameters. Subsequently,\nwe generate several base classifiers using the trained diffusion model.\nFinally, we integrate these ba se classifiers for various inference tasks using\nthe Bagging method. Resulting experiments on multiple models and datasets show\nthat our proposed BEND algorithm can consistently outperform the mean and\nmedian accuracies of both the original trained model and the diffused model. At\nthe same time, new models diffused using the diffusion model have higher\ndiversity and lower cost than multiple models trained using traditional\nmethods. The BEND approach successfully introduces diffusion models into the\nnew deep learning training domain and provides a new paradigm for future deep\nlearning training and inference.\n', '  The evolution of Diffusion Models has dramatically improved image generation\nquality, making it increasingly difficult to differentiate between real and\ngenerated images. This development, while impressive, also raises significant\nprivacy and security concerns. In response to this, we propose a novel Latent\nREconstruction error guided feature REfinement method (LaRE^2) for detecting\nthe diffusion-generated images. We come up with the Latent Reconstruction Error\n(LaRE), the first reconstruction-error based feature in the latent space for\ngenerated image detection. LaRE surpasses existing methods in terms of feature\nextraction efficiency while preserving crucial cues required to differentiate\nbetween the real and the fake. To exploit LaRE, we propose an Error-Guided\nfeature REfinement module (EGRE), which can refine the image feature guided by\nLaRE to enhance the discriminativeness of the feature. Our EGRE utilizes an\nalign-then-refine mechanism, which effectively refines the image feature for\ngenerated-image detection from both spatial and channel perspectives. Extensive\nexperiments on the large-scale GenImage benchmark demonstrate the superiority\nof our LaRE^2, which surpasses the best SoTA method by up to 11.9%/12.1%\naverage ACC/AP across 8 different image generators. LaRE also surpasses\nexisting methods in terms of feature extraction cost, delivering an impressive\nspeed enhancement of 8 times.\n', '  Existing image augmentation methods consist of two categories:\nperturbation-based methods and generative methods. Perturbation-based methods\napply pre-defined perturbations to augment an original image, but only locally\nvary the image, thus lacking image diversity. In contrast, generative methods\nbring more image diversity in the augmented images but may not preserve\nsemantic consistency, thus incorrectly changing the essential semantics of the\noriginal image. To balance image diversity and semantic consistency in\naugmented images, we propose SGID, a Semantic-guided Generative Image\naugmentation method with Diffusion models for image classification.\nSpecifically, SGID employs diffusion models to generate augmented images with\ngood image diversity. More importantly, SGID takes image labels and captions as\nguidance to maintain semantic consistency between the augmented and original\nimages. Experimental results show that SGID outperforms the best augmentation\nbaseline by 1.72% on ResNet-50 (from scratch), 0.33% on ViT (ImageNet-21k), and\n0.14% on CLIP-ViT (LAION-2B). Moreover, SGID can be combined with other image\naugmentation baselines and further improves the overall performance. We\ndemonstrate the semantic consistency and image diversity of SGID through\nquantitative human and automated evaluations, as well as qualitative case\nstudies.\n'] , ['  Diffusion models have obtained substantial progress in image-to-video (I2V)\ngeneration. However, such models are not fully understood. In this paper, we\nreport a significant but previously overlooked issue in I2V diffusion models\n(I2V-DMs), namely, conditional image leakage. I2V-DMs tend to over-rely on the\nconditional image at large time steps, neglecting the crucial task of\npredicting the clean video from noisy inputs, which results in videos lacking\ndynamic and vivid motion. We further address this challenge from both inference\nand training aspects by presenting plug-and-play strategies accordingly. First,\nwe introduce a training-free inference strategy that starts the generation\nprocess from an earlier time step to avoid the unreliable late-time steps of\nI2V-DMs, as well as an initial noise distribution with optimal analytic\nexpressions (Analytic-Init) by minimizing the KL divergence between it and the\nactual marginal distribution to effectively bridge the training-inference gap.\nSecond, to mitigate conditional image leakage during training, we design a\ntime-dependent noise distribution for the conditional image, which favors high\nnoise levels at large time steps to sufficiently interfere with the conditional\nimage. We validate these strategies on various I2V-DMs using our collected\nopen-domain image benchmark and the UCF101 dataset. Extensive results\ndemonstrate that our methods outperform baselines by producing videos with more\ndynamic and natural motion without compromising image alignment and temporal\nconsistency. The project page: \\url{https://cond-image-leak.github.io/}.\n', '  Video diffusion models has been gaining increasing attention for its ability\nto produce videos that are both coherent and of high fidelity. However, the\niterative denoising process makes it computationally intensive and\ntime-consuming, thus limiting its applications. Inspired by the Consistency\nModel (CM) that distills pretrained image diffusion models to accelerate the\nsampling with minimal steps and its successful extension Latent Consistency\nModel (LCM) on conditional image generation, we propose AnimateLCM, allowing\nfor high-fidelity video generation within minimal steps. Instead of directly\nconducting consistency learning on the raw video dataset, we propose a\ndecoupled consistency learning strategy that decouples the distillation of\nimage generation priors and motion generation priors, which improves the\ntraining efficiency and enhance the generation visual quality. Additionally, to\nenable the combination of plug-and-play adapters in stable diffusion community\nto achieve various functions (e.g., ControlNet for controllable generation). we\npropose an efficient strategy to adapt existing adapters to our distilled\ntext-conditioned video consistency model or train adapters from scratch without\nharming the sampling speed. We validate the proposed strategy in\nimage-conditioned video generation and layout-conditioned video generation, all\nachieving top-performing results. Experimental results validate the\neffectiveness of our proposed method. Code and weights will be made public.\nMore details are available at https://github.com/G-U-N/AnimateLCM.\n', '  Video diffusion models have recently made great progress in generation\nquality, but are still limited by the high memory and computational\nrequirements. This is because current video diffusion models often attempt to\nprocess high-dimensional videos directly. To tackle this issue, we propose\ncontent-motion latent diffusion model (CMD), a novel efficient extension of\npretrained image diffusion models for video generation. Specifically, we\npropose an autoencoder that succinctly encodes a video as a combination of a\ncontent frame (like an image) and a low-dimensional motion latent\nrepresentation. The former represents the common content, and the latter\nrepresents the underlying motion in the video, respectively. We generate the\ncontent frame by fine-tuning a pretrained image diffusion model, and we\ngenerate the motion latent representation by training a new lightweight\ndiffusion model. A key innovation here is the design of a compact latent space\nthat can directly utilizes a pretrained image diffusion model, which has not\nbeen done in previous latent video diffusion models. This leads to considerably\nbetter quality generation and reduced computational costs. For instance, CMD\ncan sample a video 7.7$\\times$ faster than prior approaches by generating a\nvideo of 512$\\times$1024 resolution and length 16 in 3.1 seconds. Moreover, CMD\nachieves an FVD score of 212.7 on WebVid-10M, 27.3% better than the previous\nstate-of-the-art of 292.4.\n'] , ['  Recently, there has been a significant advancement in text-to-image diffusion\nmodels, leading to groundbreaking performance in 2D image generation. These\nadvancements have been extended to 3D models, enabling the generation of novel\n3D objects from textual descriptions. This has evolved into NeRF editing\nmethods, which allow the manipulation of existing 3D objects through textual\nconditioning. However, existing NeRF editing techniques have faced limitations\nin their performance due to slow training speeds and the use of loss functions\nthat do not adequately consider editing. To address this, here we present a\nnovel 3D NeRF editing approach dubbed ED-NeRF by successfully embedding\nreal-world scenes into the latent space of the latent diffusion model (LDM)\nthrough a unique refinement layer. This approach enables us to obtain a NeRF\nbackbone that is not only faster but also more amenable to editing compared to\ntraditional image space NeRF editing. Furthermore, we propose an improved loss\nfunction tailored for editing by migrating the delta denoising score (DDS)\ndistillation loss, originally used in 2D image editing to the three-dimensional\ndomain. This novel loss function surpasses the well-known score distillation\nsampling (SDS) loss in terms of suitability for editing purposes. Our\nexperimental results demonstrate that ED-NeRF achieves faster editing speed\nwhile producing improved output quality compared to state-of-the-art 3D editing\nmodels.\n', '  With the remarkable advent of text-to-image diffusion models, image editing\nmethods have become more diverse and continue to evolve. A promising recent\napproach in this realm is Delta Denoising Score (DDS) - an image editing\ntechnique based on Score Distillation Sampling (SDS) framework that leverages\nthe rich generative prior of text-to-image diffusion models. However, relying\nsolely on the difference between scoring functions is insufficient for\npreserving specific structural elements from the original image, a crucial\naspect of image editing. To address this, here we present an embarrassingly\nsimple yet very powerful modification of DDS, called Contrastive Denoising\nScore (CDS), for latent diffusion models (LDM). Inspired by the similarities\nand differences between DDS and the contrastive learning for unpaired\nimage-to-image translation(CUT), we introduce a straightforward approach using\nCUT loss within the DDS framework. Rather than employing auxiliary networks as\nin the original CUT approach, we leverage the intermediate features of LDM,\nspecifically those from the self-attention layers, which possesses rich spatial\ninformation. Our approach enables zero-shot image-to-image translation and\nneural radiance field (NeRF) editing, achieving structural correspondence\nbetween the input and output while maintaining content controllability.\nQualitative results and comparisons demonstrates the effectiveness of our\nproposed method. Project page: https://hyelinnam.github.io/CDS/\n', '  Large-scale Text-to-Image (T2I) diffusion models have revolutionized image\ngeneration over the last few years. Although owning diverse and high-quality\ngeneration capabilities, translating these abilities to fine-grained image\nediting remains challenging. In this paper, we propose DiffEditor to rectify\ntwo weaknesses in existing diffusion-based image editing: (1) in complex\nscenarios, editing results often lack editing accuracy and exhibit unexpected\nartifacts; (2) lack of flexibility to harmonize editing operations, e.g.,\nimagine new content. In our solution, we introduce image prompts in\nfine-grained image editing, cooperating with the text prompt to better describe\nthe editing content. To increase the flexibility while maintaining content\nconsistency, we locally combine stochastic differential equation (SDE) into the\nordinary differential equation (ODE) sampling. In addition, we incorporate\nregional score-based gradient guidance and a time travel strategy into the\ndiffusion sampling, further improving the editing quality. Extensive\nexperiments demonstrate that our method can efficiently achieve\nstate-of-the-art performance on various fine-grained image editing tasks,\nincluding editing within a single image (e.g., object moving, resizing, and\ncontent dragging) and across images (e.g., appearance replacing and object\npasting). Our source code is released at\nhttps://github.com/MC-E/DragonDiffusion.\n']",Diffusion Models for Image and Video Generation and Manipulation,Diffusion Models for Image Generation with Guidance Techniques
107,"""Biases in Text-to-Image Generation Models"" , ""Autoregressive Text Generation with Diffusion Models"" , ""Graph Generation with Diffusion Models"" , ""Copyright Protection in Text-to-Image Diffusion Models""","['biases', 'bias', 'genderbias', 'stereotypes', 'biaspainter', 'stereotype', 'debiasing', 'demographic', 'counterfactual', 'stereotyped'] , ['paragraphs', 'text', 'autoregressively', 'autoregressive', 'decoding', 'writing', 'planner', 'generation', 'language', 'predict'] , ['graphs', 'graph_beta_diffusion', 'graph', 'graphebm', 'nodes', 'diffusion', 'graphaf', 'generative', 'edge', 'models'] , ['adversarial', 'gans', 'watermarking', 'steganalysis', 'copyright', 'steganography', 'deepfacegen', 'watermark', 'spoofing', 'impersonation']","[""  The recent advancement of large and powerful models with Text-to-Image (T2I)\ngeneration abilities -- such as OpenAI's DALLE-3 and Google's Gemini -- enables\nusers to generate high-quality images from textual prompts. However, it has\nbecome increasingly evident that even simple prompts could cause T2I models to\nexhibit conspicuous social bias in generated images. Such bias might lead to\nboth allocational and representational harms in society, further marginalizing\nminority groups. Noting this problem, a large body of recent works has been\ndedicated to investigating different dimensions of bias in T2I systems.\nHowever, an extensive review of these studies is lacking, hindering a\nsystematic understanding of current progress and research gaps. We present the\nfirst extensive survey on bias in T2I generative models. In this survey, we\nreview prior studies on dimensions of bias: Gender, Skintone, and Geo-Culture.\nSpecifically, we discuss how these works define, evaluate, and mitigate\ndifferent aspects of bias. We found that: (1) while gender and skintone biases\nare widely studied, geo-cultural bias remains under-explored; (2) most works on\ngender and skintone bias investigated occupational association, while other\naspects are less frequently studied; (3) almost all gender bias works overlook\nnon-binary identities in their studies; (4) evaluation datasets and metrics are\nscattered, with no unified framework for measuring biases; and (5) current\nmitigation methods fail to resolve biases comprehensively. Based on current\nlimitations, we point out future research directions that contribute to\nhuman-centric definitions, evaluations, and mitigation of biases. We hope to\nhighlight the importance of studying biases in T2I systems, as well as\nencourage future efforts to holistically understand and tackle biases, building\nfair and trustworthy T2I technologies for everyone.\n"", '  While vision-language models (VLMs) have achieved remarkable performance\nimprovements recently, there is growing evidence that these models also posses\nharmful biases with respect to social attributes such as gender and race. Prior\nstudies have primarily focused on probing such bias attributes individually\nwhile ignoring biases associated with intersections between social attributes.\nThis could be due to the difficulty of collecting an exhaustive set of\nimage-text pairs for various combinations of social attributes. To address this\nchallenge, we employ text-to-image diffusion models to produce counterfactual\nexamples for probing intersectional social biases at scale. Our approach\nutilizes Stable Diffusion with cross attention control to produce sets of\ncounterfactual image-text pairs that are highly similar in their depiction of a\nsubject (e.g., a given occupation) while differing only in their depiction of\nintersectional social attributes (e.g., race & gender). Through our\nover-generate-then-filter methodology, we produce SocialCounterfactuals, a\nhigh-quality dataset containing 171k image-text pairs for probing\nintersectional biases related to gender, race, and physical characteristics. We\nconduct extensive experiments to demonstrate the usefulness of our generated\ndataset for probing and mitigating intersectional social biases in\nstate-of-the-art VLMs.\n', '  Large Vision-Language Models (LVLMs) have been widely adopted in various\napplications; however, they exhibit significant gender biases. Existing\nbenchmarks primarily evaluate gender bias at the demographic group level,\nneglecting individual fairness, which emphasizes equal treatment of similar\nindividuals. This research gap limits the detection of discriminatory\nbehaviors, as individual fairness offers a more granular examination of biases\nthat group fairness may overlook. For the first time, this paper introduces the\nGenderBias-\\emph{VL} benchmark to evaluate occupation-related gender bias in\nLVLMs using counterfactual visual questions under individual fairness criteria.\nTo construct this benchmark, we first utilize text-to-image diffusion models to\ngenerate occupation images and their gender counterfactuals. Subsequently, we\ngenerate corresponding textual occupation options by identifying stereotyped\noccupation pairs with high semantic similarity but opposite gender proportions\nin real-world statistics. This method enables the creation of large-scale\nvisual question counterfactuals to expose biases in LVLMs, applicable in both\nmultimodal and unimodal contexts through modifying gender attributes in\nspecific modalities. Overall, our GenderBias-\\emph{VL} benchmark comprises\n34,581 visual question counterfactual pairs, covering 177 occupations. Using\nour benchmark, we extensively evaluate 15 commonly used open-source LVLMs (\\eg,\nLLaVA) and state-of-the-art commercial APIs, including GPT-4o and Gemini-Pro.\nOur findings reveal widespread gender biases in existing LVLMs. Our benchmark\noffers: (1) a comprehensive dataset for occupation-related gender bias\nevaluation; (2) an up-to-date leaderboard on LVLM biases; and (3) a nuanced\nunderstanding of the biases presented by these models. \\footnote{The dataset\nand code are available at the \\href{https://genderbiasvl.github.io/}{website}.}\n'] , ['  Autoregressive models for text sometimes generate repetitive and low-quality\noutput because errors accumulate during the steps of generation. This issue is\noften attributed to exposure bias - the difference between how a model is\ntrained, and how it is used during inference. Denoising diffusion models\nprovide an alternative approach in which a model can revisit and revise its\noutput. However, they can be computationally expensive and prior efforts on\ntext have led to models that produce less fluent output compared to\nautoregressive models, especially for longer text and paragraphs. In this\npaper, we propose PLANNER, a model that combines latent semantic diffusion with\nautoregressive generation, to generate fluent text while exercising global\ncontrol over paragraphs. The model achieves this by combining an autoregressive\n""decoding"" module with a ""planning"" module that uses latent diffusion to\ngenerate semantic paragraph embeddings in a coarse-to-fine manner. The proposed\nmethod is evaluated on various conditional generation tasks, and results on\nsemantic generation, text completion and summarization show its effectiveness\nin generating high-quality long-form text in an efficient manner.\n', '  The modern autoregressive Large Language Models (LLMs) have achieved\noutstanding performance on NLP benchmarks, and they are deployed in the real\nworld. However, they still suffer from limitations of the autoregressive\ntraining paradigm. For example, autoregressive token generation is notably slow\nand can be prone to \\textit{exposure bias}. The diffusion-based language models\nwere proposed as an alternative to autoregressive generation to address some of\nthese limitations. We evaluate the recently proposed Score Entropy Discrete\nDiffusion (SEDD) approach and show it is a promising alternative to\nautoregressive generation but it has some short-comings too. We empirically\ndemonstrate the advantages and challenges of SEDD, and observe that SEDD\ngenerally matches autoregressive models in perplexity and on benchmarks such as\nHellaSwag, Arc or WinoGrande. Additionally, we show that in terms of inference\nlatency, SEDD can be up to 4.5$\\times$ more efficient than GPT-2. While SEDD\nallows conditioning on tokens at abitrary positions, SEDD appears slightly\nweaker than GPT-2 for conditional generation given short prompts. Finally, we\nreproduced the main results from the original SEDD paper.\n', '  Despite their groundbreaking performance for many generative modeling tasks,\ndiffusion models have fallen short on discrete data domains such as natural\nlanguage. Crucially, standard diffusion models rely on the well-established\ntheory of score matching, but efforts to generalize this to discrete structures\nhave not yielded the same empirical gains. In this work, we bridge this gap by\nproposing score entropy, a novel loss that naturally extends score matching to\ndiscrete spaces, integrates seamlessly to build discrete diffusion models, and\nsignificantly boosts performance. Experimentally, we test our Score Entropy\nDiscrete Diffusion models (SEDD) on standard language modeling tasks. For\ncomparable model sizes, SEDD beats existing language diffusion paradigms\n(reducing perplexity by $25$-$75$\\%) and is competitive with autoregressive\nmodels, in particular outperforming GPT-2. Furthermore, compared to\nautoregressive mdoels, SEDD generates faithful text without requiring\ndistribution annealing techniques like temperature scaling (around\n$6$-$8\\times$ better generative perplexity than un-annealed GPT-2), can trade\ncompute and quality (similar quality with $32\\times$ fewer network\nevaluations), and enables controllable infilling (matching nucleus sampling\nquality while enabling other strategies besides left to right prompting).\n'] , ['  Graph is a prevalent discrete data structure, whose generation has wide\napplications such as drug discovery and circuit design. Diffusion generative\nmodels, as an emerging research focus, have been applied to graph generation\ntasks. Overall, according to the space of states and time steps, diffusion\ngenerative models can be categorized into discrete-/continuous-state\ndiscrete-/continuous-time fashions. In this paper, we formulate the graph\ndiffusion generation in a discrete-state continuous-time setting, which has\nnever been studied in previous graph diffusion models. The rationale of such a\nformulation is to preserve the discrete nature of graph-structured data and\nmeanwhile provide flexible sampling trade-offs between sample quality and\nefficiency. Analysis shows that our training objective is closely related to\ngeneration quality, and our proposed generation framework enjoys ideal\ninvariant/equivariant properties concerning the permutation of node ordering.\nOur proposed model shows competitive empirical performance against\nstate-of-the-art graph generation solutions on various benchmarks and, at the\nsame time, can flexibly trade off the generation quality and efficiency in the\nsampling phase.\n', '  Generation of graphs is a major challenge for real-world tasks that require\nunderstanding the complex nature of their non-Euclidean structures. Although\ndiffusion models have achieved notable success in graph generation recently,\nthey are ill-suited for modeling the topological properties of graphs since\nlearning to denoise the noisy samples does not explicitly learn the graph\nstructures to be generated. To tackle this limitation, we propose a generative\nframework that models the topology of graphs by explicitly learning the final\ngraph structures of the diffusion process. Specifically, we design the\ngenerative process as a mixture of endpoint-conditioned diffusion processes\nwhich is driven toward the predicted graph that results in rapid convergence.\nWe further introduce a simple parameterization of the mixture process and\ndevelop an objective for learning the final graph structure, which enables\nmaximum likelihood training. Through extensive experimental validation on\ngeneral graph and 2D/3D molecule generation tasks, we show that our method\noutperforms previous generative models, generating graphs with correct topology\nwith both continuous (e.g. 3D coordinates) and discrete (e.g. atom types)\nfeatures. Our code is available at https://github.com/harryjo97/GruM.\n', '  Diffusion generative models (DMs) have achieved promising results in image\nand graph generation. However, real-world graphs, such as social networks,\nmolecular graphs, and traffic graphs, generally share non-Euclidean topologies\nand hidden hierarchies. For example, the degree distributions of graphs are\nmostly power-law distributions. The current latent diffusion model embeds the\nhierarchical data in a Euclidean space, which leads to distortions and\ninterferes with modeling the distribution. Instead, hyperbolic space has been\nfound to be more suitable for capturing complex hierarchical structures due to\nits exponential growth property. In order to simultaneously utilize the data\ngeneration capabilities of diffusion models and the ability of hyperbolic\nembeddings to extract latent hierarchical distributions, we propose a novel\ngraph generation method called, Hyperbolic Graph Diffusion Model (HGDM), which\nconsists of an auto-encoder to encode nodes into successive hyperbolic\nembeddings, and a DM that operates in the hyperbolic latent space. HGDM\ncaptures the crucial graph structure distributions by constructing a hyperbolic\npotential node space that incorporates edge information. Extensive experiments\nshow that HGDM achieves better performance in generic graph and molecule\ngeneration benchmarks, with a $48\\%$ improvement in the quality of graph\ngeneration with highly hierarchical structures.\n'] , ['  The commercialization of text-to-image diffusion models (DMs) brings forth\npotential copyright concerns. Despite numerous attempts to protect DMs from\ncopyright issues, the vulnerabilities of these solutions are underexplored. In\nthis study, we formalized the Copyright Infringement Attack on generative AI\nmodels and proposed a backdoor attack method, SilentBadDiffusion, to induce\ncopyright infringement without requiring access to or control over training\nprocesses. Our method strategically embeds connections between pieces of\ncopyrighted information and text references in poisoning data while carefully\ndispersing that information, making the poisoning data inconspicuous when\nintegrated into a clean dataset. Our experiments show the stealth and efficacy\nof the poisoning data. When given specific text prompts, DMs trained with a\npoisoning ratio of 0.20% can produce copyrighted images. Additionally, the\nresults reveal that the more sophisticated the DMs are, the easier the success\nof the attack becomes. These findings underline potential pitfalls in the\nprevailing copyright protection strategies and underscore the necessity for\nincreased scrutiny to prevent the misuse of DMs.\n', '  Diffusion Models (DMs) have shown remarkable capabilities in various\nimage-generation tasks. However, there are growing concerns that DMs could be\nused to imitate unauthorized creations and thus raise copyright issues. To\naddress this issue, we propose a novel framework that embeds personal\nwatermarks in the generation of adversarial examples. Such examples can force\nDMs to generate images with visible watermarks and prevent DMs from imitating\nunauthorized images. We construct a generator based on conditional adversarial\nnetworks and design three losses (adversarial loss, GAN loss, and perturbation\nloss) to generate adversarial examples that have subtle perturbation but can\neffectively attack DMs to prevent copyright violations. Training a generator\nfor a personal watermark by our method only requires 5-10 samples within 2-3\nminutes, and once the generator is trained, it can generate adversarial\nexamples with that watermark significantly fast (0.2s per image). We conduct\nextensive experiments in various conditional image-generation scenarios.\nCompared to existing methods that generate images with chaotic textures, our\nmethod adds visible watermarks on the generated images, which is a more\nstraightforward way to indicate copyright violations. We also observe that our\nadversarial examples exhibit good transferability across unknown generative\nmodels. Therefore, this work provides a simple yet powerful way to protect\ncopyright from DM-based imitation.\n', '  Diffusion models excel in many generative modeling tasks, notably in creating\nimages from text prompts, a task referred to as text-to-image (T2I) generation.\nDespite the ability to generate high-quality images, these models often\nreplicate elements from their training data, leading to increasing copyright\nconcerns in real applications in recent years. In response to this raising\nconcern about copyright infringement, recent studies have studied the copyright\nbehavior of diffusion models when using direct, copyrighted prompts. Our\nresearch extends this by examining subtler forms of infringement, where even\nindirect prompts can trigger copyright issues. Specifically, we introduce a\ndata generation pipeline to systematically produce data for studying copyright\nin diffusion models. Our pipeline enables us to investigate copyright\ninfringement in a more practical setting, involving replicating visual features\nrather than entire works using seemingly irrelevant prompts for T2I generation.\nWe generate data using our proposed pipeline to test various diffusion models,\nincluding the latest Stable Diffusion XL. Our findings reveal a widespread\ntendency that these models tend to produce copyright-infringing content,\nhighlighting a significant challenge in this field.\n']","Diffusion Models for Text, Image, and Graph Generation","""Autoregressive Text Generation with Diffusion Models"""
108,"Diffusion-based Image Restoration , Image Denoising Methods , Diffusion Models for Image Restoration","['denoising', 'restoration', 'imaging', 'diffusion', 'inpainting', 'reconstructions', 'generative', 'residual', 'jpeg', 'restored'] , ['denoising', 'denoisers', 'denoiser', 'denoised', 'noise2noise', 'pixels', 'noisy', 'supervised', 'noise', 'images'] , ['diffusion', 'denoising', 'denoiser', 'gans', 'denoised', 'generative', 'imagenet', 'images', 'modeling', 'gan']","['  Diffusion models have demonstrated remarkable efficacy in generating\nhigh-quality samples. Existing diffusion-based image restoration algorithms\nexploit pre-trained diffusion models to leverage data priors, yet they still\npreserve elements inherited from the unconditional generation paradigm. These\nstrategies initiate the denoising process with pure white noise and incorporate\nrandom noise at each generative step, leading to over-smoothed results. In this\npaper, we present a refined paradigm for diffusion-based image restoration.\nSpecifically, we opt for a sample consistent with the measurement identity at\neach generative step, exploiting the sampling selection as an avenue for output\nstability and enhancement. The number of candidate samples used for selection\nis adaptively determined based on the signal-to-noise ratio of the timestep.\nAdditionally, we start the restoration process with an initialization combined\nwith the measurement signal, providing supplementary information to better\nalign the generative process. Extensive experimental results and analyses\nvalidate that our proposed method significantly enhances image restoration\nperformance while consuming negligible additional computational resources.\n', '  We propose residual denoising diffusion models (RDDM), a novel dual diffusion\nprocess that decouples the traditional single denoising diffusion process into\nresidual diffusion and noise diffusion. This dual diffusion framework expands\nthe denoising-based diffusion models, initially uninterpretable for image\nrestoration, into a unified and interpretable model for both image generation\nand restoration by introducing residuals. Specifically, our residual diffusion\nrepresents directional diffusion from the target image to the degraded input\nimage and explicitly guides the reverse generation process for image\nrestoration, while noise diffusion represents random perturbations in the\ndiffusion process. The residual prioritizes certainty, while the noise\nemphasizes diversity, enabling RDDM to effectively unify tasks with varying\ncertainty or diversity requirements, such as image generation and restoration.\nWe demonstrate that our sampling process is consistent with that of DDPM and\nDDIM through coefficient transformation, and propose a partially\npath-independent generation process to better understand the reverse process.\nNotably, our RDDM enables a generic UNet, trained with only an L1 loss and a\nbatch size of 1, to compete with state-of-the-art image restoration methods. We\nprovide code and pre-trained models to encourage further exploration,\napplication, and development of our innovative framework\n(https://github.com/nachifur/RDDM).\n', '  Inversion by Direct Iteration (InDI) is a new formulation for supervised\nimage restoration that avoids the so-called ""regression to the mean"" effect and\nproduces more realistic and detailed images than existing regression-based\nmethods. It does this by gradually improving image quality in small steps,\nsimilar to generative denoising diffusion models. Image restoration is an\nill-posed problem where multiple high-quality images are plausible\nreconstructions of a given low-quality input. Therefore, the outcome of a\nsingle step regression model is typically an aggregate of all possible\nexplanations, therefore lacking details and realism. The main advantage of InDI\nis that it does not try to predict the clean target image in a single step but\ninstead gradually improves the image in small steps, resulting in better\nperceptual quality. While generative denoising diffusion models also work in\nsmall steps, our formulation is distinct in that it does not require knowledge\nof any analytic form of the degradation process. Instead, we directly learn an\niterative restoration process from low-quality and high-quality paired\nexamples. InDI can be applied to virtually any image degradation, given paired\ntraining data. In conditional denoising diffusion image restoration the\ndenoising network generates the restored image by repeatedly denoising an\ninitial image of pure noise, conditioned on the degraded input. Contrary to\nconditional denoising formulations, InDI directly proceeds by iteratively\nrestoring the input low-quality image, producing high-quality results on a\nvariety of image restoration tasks, including motion and out-of-focus\ndeblurring, super-resolution, compression artifact removal, and denoising.\n'] , ['  Recently, the mainstream practice for training low-light raw image denoising\nmethods has shifted towards employing synthetic data. Noise modeling, which\nfocuses on characterizing the noise distribution of real-world sensors,\nprofoundly influences the effectiveness and practicality of synthetic data.\nCurrently, physics-based noise modeling struggles to characterize the entire\nreal noise distribution, while learning-based noise modeling impractically\ndepends on paired real data. In this paper, we propose a novel strategy:\nlearning the noise model from dark frames instead of paired real data, to break\ndown the data dependency. Based on this strategy, we introduce an efficient\nphysics-guided noise neural proxy (PNNP) to approximate the real-world sensor\nnoise model. Specifically, we integrate physical priors into neural proxies and\nintroduce three efficient techniques: physics-guided noise decoupling (PND),\nphysics-guided proxy model (PPM), and differentiable distribution loss (DDL).\nPND decouples the dark frame into different components and handles different\nlevels of noise flexibly, which reduces the complexity of noise modeling. PPM\nincorporates physical priors to constrain the generated noise, which promotes\nthe accuracy of noise modeling. DDL provides explicit and reliable supervision\nfor noise distribution, which promotes the precision of noise modeling. PNNP\nexhibits powerful potential in characterizing the real noise distribution.\nExtensive experiments on public datasets demonstrate superior performance in\npractical low-light raw image denoising. The code will be available at\n\\url{https://github.com/fenghansen/PNNP}.\n', '  Due to the high flexibility and remarkable performance, low-rank\napproximation methods has been widely studied for color image denoising.\nHowever, those methods mostly ignore either the cross-channel difference or the\nspatial variation of noise, which limits their capacity in real world color\nimage denoising. To overcome those drawbacks, this paper is proposed to denoise\ncolor images with a double-weighted truncated nuclear norm minus truncated\nFrobenius norm minimization (DtNFM) method. Through exploiting the nonlocal\nself-similarity of the noisy image, the similar structures are gathered and a\nseries of similar patch matrices are constructed. For each group, the DtNFM\nmodel is conducted for estimating its denoised version. The denoised image\nwould be obtained by concatenating all the denoised patch matrices. The\nproposed DtNFM model has two merits. First, it models and utilizes both the\ncross-channel difference and the spatial variation of noise. This provides\nsufficient flexibility for handling the complex distribution of noise in real\nworld images. Second, the proposed DtNFM model provides a close approximation\nto the underlying clean matrix since it can treat different rank components\nflexibly. To solve the problem resulted from DtNFM model, an accurate and\neffective algorithm is proposed by exploiting the framework of the alternating\ndirection method of multipliers (ADMM). The generated subproblems are discussed\nin detail. And their global optima can be easily obtained in closed-form.\nRigorous mathematical derivation proves that the solution sequences generated\nby the algorithm converge to a single critical point. Extensive experiments on\nsynthetic and real noise datasets demonstrate that the proposed method\noutperforms many state-of-the-art color image denoising methods.\n', ""  Noise is ubiquitous during image acquisition. Sufficient denoising is often\nan important first step for image processing. In recent decades, deep neural\nnetworks (DNNs) have been widely used for image denoising. Most DNN-based image\ndenoising methods require a large-scale dataset or focus on supervised\nsettings, in which single/pairs of clean images or a set of noisy images are\nrequired. This poses a significant burden on the image acquisition process.\nMoreover, denoisers trained on datasets of limited scale may incur\nover-fitting. To mitigate these issues, we introduce a new self-supervised\nframework for image denoising based on the Tucker low-rank tensor\napproximation. With the proposed design, we are able to characterize our\ndenoiser with fewer parameters and train it based on a single image, which\nconsiderably improves the model's generalizability and reduces the cost of data\nacquisition. Extensive experiments on both synthetic and real-world noisy\nimages have been conducted. Empirical results show that our proposed method\noutperforms existing non-learning-based methods (e.g., low-pass filter,\nnon-local mean), single-image unsupervised denoisers (e.g., DIP, NN+BM3D)\nevaluated on both in-sample and out-sample datasets. The proposed method even\nachieves comparable performances with some supervised methods (e.g., DnCNN).\n""] , ['  Diffusion models have recently gained traction as a powerful class of deep\ngenerative priors, excelling in a wide range of image restoration tasks due to\ntheir exceptional ability to model data distributions. To solve image\nrestoration problems, many existing techniques achieve data consistency by\nincorporating additional likelihood gradient steps into the reverse sampling\nprocess of diffusion models. However, the additional gradient steps pose a\nchallenge for real-world practical applications as they incur a large\ncomputational overhead, thereby increasing inference time. They also present\nadditional difficulties when using accelerated diffusion model samplers, as the\nnumber of data consistency steps is limited by the number of reverse sampling\nsteps. In this work, we propose a novel diffusion-based image restoration\nsolver that addresses these issues by decoupling the reverse process from the\ndata consistency steps. Our method involves alternating between a\nreconstruction phase to maintain data consistency and a refinement phase that\nenforces the prior via diffusion purification. Our approach demonstrates\nversatility, making it highly adaptable for efficient problem-solving in latent\nspace. Additionally, it reduces the necessity for numerous sampling steps\nthrough the integration of consistency models. The efficacy of our approach is\nvalidated through comprehensive experiments across various image restoration\ntasks, including image denoising, deblurring, inpainting, and super-resolution.\n', '  Image super-resolution is a fundamentally ill-posed problem because multiple\nvalid high-resolution images exist for one low-resolution image.\nSuper-resolution methods based on diffusion probabilistic models can deal with\nthe ill-posed nature by learning the distribution of high-resolution images\nconditioned on low-resolution images, avoiding the problem of blurry images in\nPSNR-oriented methods. However, existing diffusion-based super-resolution\nmethods have high time consumption with the use of iterative sampling, while\nthe quality and consistency of generated images are less than ideal due to\nproblems like color shifting. In this paper, we propose Efficient Conditional\nDiffusion Model with Probability Flow Sampling (ECDP) for image\nsuper-resolution. To reduce the time consumption, we design a continuous-time\nconditional diffusion model for image super-resolution, which enables the use\nof probability flow sampling for efficient generation. Additionally, to improve\nthe consistency of generated images, we propose a hybrid parametrization for\nthe denoiser network, which interpolates between the data-predicting\nparametrization and the noise-predicting parametrization for different noise\nscales. Moreover, we design an image quality loss as a complement to the score\nmatching loss of diffusion models, further improving the consistency and\nquality of super-resolution. Extensive experiments on DIV2K, ImageNet, and\nCelebA demonstrate that our method achieves higher super-resolution quality\nthan existing diffusion-based image super-resolution methods while having lower\ntime consumption. Our code is available at https://github.com/Yuan-Yutao/ECDP.\n', '  Deep generative models have garnered significant attention in low-level\nvision tasks due to their generative capabilities. Among them, diffusion\nmodel-based solutions, characterized by a forward diffusion process and a\nreverse denoising process, have emerged as widely acclaimed for their ability\nto produce samples of superior quality and diversity. This ensures the\ngeneration of visually compelling results with intricate texture information.\nDespite their remarkable success, a noticeable gap exists in a comprehensive\nsurvey that amalgamates these pioneering diffusion model-based works and\norganizes the corresponding threads. This paper proposes the comprehensive\nreview of diffusion model-based techniques. We present three generic diffusion\nmodeling frameworks and explore their correlations with other deep generative\nmodels, establishing the theoretical foundation. Following this, we introduce a\nmulti-perspective categorization of diffusion models, considering both the\nunderlying framework and the target task. Additionally, we summarize extended\ndiffusion models applied in other tasks, including medical, remote sensing, and\nvideo scenarios. Moreover, we provide an overview of commonly used benchmarks\nand evaluation metrics. We conduct a thorough evaluation, encompassing both\nperformance and efficiency, of diffusion model-based techniques in three\nprominent tasks. Finally, we elucidate the limitations of current diffusion\nmodels and propose seven intriguing directions for future research. This\ncomprehensive examination aims to facilitate a profound understanding of the\nlandscape surrounding denoising diffusion models in the context of low-level\nvision tasks. A curated list of diffusion model-based techniques in over 20\nlow-level vision tasks can be found at\nhttps://github.com/ChunmingHe/awesome-diffusion-models-in-low-level-vision.\n']",Image Restoration and Denoising Techniques,Diffusion-based Image Restoration
109,"Image Enhancement and Camera Processing , Image Dehazing and Super-Resolution , Image Aesthetics Assessment","['rgb', 'camera', 'illumination', 'photography', 'dslr', 'cameras', 'srgb', 'brightness', 'dark', 'reflectance'] , ['dehazing', 'haze', 'blur', 'convolutional', 'encoder', 'vision', 'deblur', 'deep', 'hazy', 'res2net'] , ['aesthetics', 'aesthetic', 'multimodal', 'visual', 'attributes', 'generative', 'creative', 'ratings', 'colors', 'perception']","['  Modern smartphone camera quality heavily relies on the image signal processor\n(ISP) to enhance captured raw images, utilizing carefully designed modules to\nproduce final output images encoded in a standard color space (e.g., sRGB).\nNeural-based end-to-end learnable ISPs offer promising advancements,\npotentially replacing traditional ISPs with their ability to adapt without\nrequiring extensive tuning for each new camera model, as is often the case for\nnearly every module in traditional ISPs. However, the key challenge with the\nrecent learning-based ISPs is the urge to collect large paired datasets for\neach distinct camera model due to the influence of intrinsic camera\ncharacteristics on the formation of input raw images. This paper tackles this\nchallenge by introducing a novel method for unpaired learning of raw-to-raw\ntranslation across diverse cameras. Specifically, we propose Rawformer, an\nunsupervised Transformer-based encoder-decoder method for raw-to-raw\ntranslation. It accurately maps raw images captured by a certain camera to the\ntarget camera, facilitating the generalization of learnable ISPs to new unseen\ncameras. Our method demonstrates superior performance on real camera datasets,\nachieving higher accuracy compared to previous state-of-the-art techniques, and\npreserving a more robust correlation between the original and translated raw\nimages. The codes and the pretrained models are available at\nhttps://github.com/gosha20777/rawformer.\n', '  Dark image enhancement aims at converting dark images to normal-light images.\nExisting dark image enhancement methods take uncompressed dark images as inputs\nand achieve great performance. However, in practice, dark images are often\ncompressed before storage or transmission over the Internet. Current methods\nget poor performance when processing compressed dark images. Artifacts hidden\nin the dark regions are amplified by current methods, which results in\nuncomfortable visual effects for observers. Based on this observation, this\nstudy aims at enhancing compressed dark images while avoiding compression\nartifacts amplification. Since texture details intertwine with compression\nartifacts in compressed dark images, detail enhancement and blocking artifacts\nsuppression contradict each other in image space. Therefore, we handle the task\nin latent space. To this end, we propose a novel latent mapping network based\non variational auto-encoder (VAE). Firstly, different from previous VAE-based\nmethods with single-resolution features only, we exploit multiple latent spaces\nwith multi-resolution features, to reduce the detail blur and improve image\nfidelity. Specifically, we train two multi-level VAEs to project compressed\ndark images and normal-light images into their latent spaces respectively.\nSecondly, we leverage a latent mapping network to transform features from\ncompressed dark space to normal-light space. Specifically, since the\ndegradation models of darkness and compression are different from each other,\nthe latent mapping process is divided mapping into enlightening branch and\ndeblocking branch. Comprehensive experiments demonstrate that the proposed\nmethod achieves state-of-the-art performance in compressed dark image\nenhancement.\n', '  Low-Light Image Enhancement (LLIE) task tends to restore the details and\nvisual information from corrupted low-light images. Most existing methods learn\nthe mapping function between low/normal-light images by Deep Neural Networks\n(DNNs) on sRGB and HSV color space. Nevertheless, enhancement involves\namplifying image signals, and applying these color spaces to low-light images\nwith a low signal-to-noise ratio can introduce sensitivity and instability into\nthe enhancement process. Consequently, this results in the presence of color\nartifacts and brightness artifacts in the enhanced images. To alleviate this\nproblem, we propose a novel trainable color space, named\nHorizontal/Vertical-Intensity (HVI). It not only decouples brightness and color\nfrom RGB channels to mitigate the instability during enhancement but also\nadapts to low-light images in different illumination ranges due to the\ntrainable parameters. Further, we design a novel Color and Intensity Decoupling\nNetwork (CIDNet) with two branches dedicated to processing the decoupled image\nbrightness and color in the HVI space. Within CIDNet, we introduce the\nLightweight Cross-Attention (LCA) module to facilitate interaction between\nimage structure and content information in both branches, while also\nsuppressing noise in low-light images. Finally, we conducted 22 quantitative\nand qualitative experiments to show that the proposed CIDNet outperforms the\nstate-of-the-art methods on 11 datasets. The code is available at\nhttps://github.com/Fediory/HVI-CIDNet.\n'] , ['  Blind single image super-resolution (SISR) is a challenging task in image\nprocessing due to the ill-posed nature of the inverse problem. Complex\ndegradations present in real life images make it difficult to solve this\nproblem using na\\""ive deep learning approaches, where models are often trained\non synthetically generated image pairs. Most of the effort so far has been\nfocused on solving the inverse problem under some constraints, such as for a\nlimited space of blur kernels and/or assuming noise-free input images. Yet,\nthere is a gap in the literature to provide a well-generalized deep\nlearning-based solution that performs well on images with unknown and highly\ncomplex degradations. In this paper, we propose IKR-Net (Iterative Kernel\nReconstruction Network) for blind SISR. In the proposed approach, kernel and\nnoise estimation and high-resolution image reconstruction are carried out\niteratively using dedicated deep models. The iterative refinement provides\nsignificant improvement in both the reconstructed image and the estimated blur\nkernel even for noisy inputs. IKR-Net provides a generalized solution that can\nhandle any type of blur and level of noise in the input low-resolution image.\nIKR-Net achieves state-of-the-art results in blind SISR, especially for noisy\nimages with motion blur.\n', ""  Hazy images degrade visual quality, and dehazing is a crucial prerequisite\nfor subsequent processing tasks. Most current dehazing methods rely on neural\nnetworks and face challenges such as high computational parameter pressure and\nweak generalization capabilities. This paper introduces PriorNet--a novel,\nlightweight, and highly applicable dehazing network designed to significantly\nimprove the clarity and visual quality of hazy images while avoiding excessive\ndetail extraction issues. The core of PriorNet is the original\nMulti-Dimensional Interactive Attention (MIA) mechanism, which effectively\ncaptures a wide range of haze characteristics, substantially reducing the\ncomputational load and generalization difficulties associated with complex\nsystems. By utilizing a uniform convolutional kernel size and incorporating\nskip connections, we have streamlined the feature extraction process.\nSimplifying the number of layers and architecture not only enhances dehazing\nefficiency but also facilitates easier deployment on edge devices. Extensive\ntesting across multiple datasets has demonstrated PriorNet's exceptional\nperformance in dehazing and clarity restoration, maintaining image detail and\ncolor fidelity in single-image dehazing tasks. Notably, with a model size of\njust 18Kb, PriorNet showcases superior dehazing generalization capabilities\ncompared to other methods. Our research makes a significant contribution to\nadvancing image dehazing technology, providing new perspectives and tools for\nthe field and related domains, particularly emphasizing the importance of\nimproving universality and deployability.\n"", '  In recent years, as computer vision tasks have increasingly relied on\nhigh-quality image inputs, the task of image dehazing has received significant\nattention. Previously, many methods based on priors and deep learning have been\nproposed to address the task of image dehazing. Ignoring the domain gap between\ndifferent data, former de-hazing methods usually adopt multiple datasets for\nexplicit training, which often makes the methods themselves be violated. To\naddress this problem, we propose a novel method of internal and external data\naugmentation to improve the existing dehazing methodology. By using cross-data\nexternal augmentor. The dataset inherits samples from different domains that\nare firmly aligned, making the model learn more robust and generalizable\nfeatures. By using the internal data augmentation method, the model can fully\nexploit local information within the images, thereby obtaining more image\ndetails. To demonstrate the effectiveness of our proposed method, we conduct\ntraining on both the Natural Image Dataset (NID) and the Remote Sensing Image\nDataset (RSID). Experimental results show that our method clearly resolves the\ndomain gap in different dehazing datasets and presents a new pipeline for joint\ntraining in the dehazing task. Our approach significantly outperforms other\nadvanced methods in dehazing and produces dehazed images that are closest to\nreal haze-free images. The code will be available at:\nhttps://github.com/wengzp1/ScaleUpDehazing\n'] , [""  Interior design is all about creating spaces that look and feel good.\nHowever, the subjective nature of aesthetic preferences presents a significant\nchallenge in defining and quantifying what makes an interior design visually\nappealing. The current paper addresses this gap by introducing a novel\nmethodology for quantifying and predicting aesthetic preferences in interior\ndesign. Our study combines fuzzy logic with image processing techniques. We\ncollected a dataset of interior design images from social media platforms,\nfocusing on essential visual attributes such as color harmony, lightness, and\ncomplexity. We integrate these features using weighted average to compute a\ngeneral aesthetic score. Our approach considers individual color preferences in\ncalculating the overall aesthetic preference. We initially gather user ratings\nfor primary colors like red, brown, and others to understand their preferences.\nThen, we use the pixel count of the top five dominant colors in the image to\nget the color scheme preference. The color scheme preference and the aesthetic\nscore are then passed as inputs to the fuzzy inference system to calculate an\noverall preference score. This score represents a comprehensive measure of the\nuser's preference for a particular interior design, considering their color\nchoices and general aesthetic appeal. We used the 2AFC (Two-Alternative Forced\nChoice) method to validate our methodology, achieving a notable hit rate of\n0.7. This study can help designers and professionals better understand and meet\npeople's interior design preferences, especially in a world that relies heavily\non digital media.\n"", ""  As people's aesthetic preferences for images are far from understood, image\naesthetic assessment is a challenging artificial intelligence task. The range\nof factors underlying this task is almost unlimited, but we know that some\naesthetic attributes affect those preferences. In this study, we present a\nmulti-task convolutional neural network that takes into account these\nattributes. The proposed neural network jointly learns the attributes along\nwith the overall aesthetic scores of images. This multi-task learning framework\nallows for effective generalization through the utilization of shared\nrepresentations. Our experiments demonstrate that the proposed method\noutperforms the state-of-the-art approaches in predicting overall aesthetic\nscores for images in one benchmark of image aesthetics. We achieve near-human\nperformance in terms of overall aesthetic scores when considering the\nSpearman's rank correlations. Moreover, our model pioneers the application of\nmulti-tasking in another benchmark, serving as a new baseline for future\nresearch. Notably, our approach achieves this performance while using fewer\nparameters compared to existing multi-task neural networks in the literature,\nand consequently makes our method more efficient in terms of computational\ncomplexity.\n"", ""  Modern vision models are trained on very large noisy datasets. While these\nmodels acquire strong capabilities, they may not follow the user's intent to\noutput the desired results in certain aspects, e.g., visual aesthetic,\npreferred style, and responsibility. In this paper, we target the realm of\nvisual aesthetics and aim to align vision models with human aesthetic standards\nin a retrieval system. Advanced retrieval systems usually adopt a cascade of\naesthetic models as re-rankers or filters, which are limited to low-level\nfeatures like saturation and perform poorly when stylistic, cultural or\nknowledge contexts are involved. We find that utilizing the reasoning ability\nof large language models (LLMs) to rephrase the search query and extend the\naesthetic expectations can make up for this shortcoming. Based on the above\nfindings, we propose a preference-based reinforcement learning method that\nfine-tunes the vision models to distill the knowledge from both LLMs reasoning\nand the aesthetic models to better align the vision models with human\naesthetics. Meanwhile, with rare benchmarks designed for evaluating retrieval\nsystems, we leverage large multi-modality model (LMM) to evaluate the aesthetic\nperformance with their strong abilities. As aesthetic assessment is one of the\nmost subjective tasks, to validate the robustness of LMM, we further propose a\nnovel dataset named HPIR to benchmark the alignment with human aesthetics.\nExperiments demonstrate that our method significantly enhances the aesthetic\nbehaviors of the vision models, under several metrics. We believe the proposed\nalgorithm can be a general practice for aligning vision models with human\nvalues.\n""]",Image Processing and Enhancement,Image Enhancement and Camera Processing
110,"""Quantization Techniques for Deep Neural Networks"" , ""Quantization Techniques for Diffusion Models"" , ""Model Compression and Quantization Techniques"" , Vector Quantization Autoencoders","['quantization', 'quantizing', 'quantized', 'quantize', 'quanvolutional', 'imagenet', 'cnns', 'bits', 'fpgas', 'compression'] , ['quantization', 'quantized', 'diffusion', 'imagenet', 'denoising', 'pixel', 'deblurring', 'noising', 'generative', 'images'] , ['quantization', 'compression', 'softmax', 'compressed', 'quantize', 'compressing', 'quantized', 'memory', 'pruning', 'efficient'] , ['autoencoders', 'autoencoder', 'encoder', 'quantization', 'decoder', 'softmax', 'coding', 'quantized', 'codebook', 'representations']","['  Quantization has become a mainstream compression technique for reducing model\nsize, computational requirements, and energy consumption for modern deep neural\nnetworks (DNNs). With improved numerical support in recent hardware, including\nmultiple variants of integer and floating point, mixed-precision quantization\nhas become necessary to achieve high-quality results with low model cost. Prior\nmixed-precision methods have performed either a post-training quantization\nsearch, which compromises on accuracy, or a differentiable quantization search,\nwhich leads to high memory usage from branching. Therefore, we propose the\nfirst one-shot mixed-precision quantization search that eliminates the need for\nretraining in both integer and low-precision floating point models. We evaluate\nour search (FLIQS) on multiple convolutional and vision transformer networks to\ndiscover Pareto-optimal models. Our approach improves upon uniform precision,\nmanual mixed-precision, and recent integer quantization search methods. With\ninteger models, we increase the accuracy of ResNet-18 on ImageNet by 1.31% and\nResNet-50 by 0.90% with equivalent model cost over previous methods.\nAdditionally, for the first time, we explore a novel mixed-precision\nfloating-point search and improve MobileNetV2 by up to 0.98% compared to prior\nstate-of-the-art FP8 models. Finally, we extend FLIQS to simultaneously search\na joint quantization and neural architecture space and improve the ImageNet\naccuracy by 2.69% with similar model cost on a MobileNetV2 search space.\n', '  Quantization of the weights and activations is one of the main methods to\nreduce the computational footprint of Deep Neural Networks (DNNs) training.\nCurrent methods enable 4-bit quantization of the forward phase. However, this\nconstitutes only a third of the training process. Reducing the computational\nfootprint of the entire training process requires the quantization of the\nneural gradients, i.e., the loss gradients with respect to the outputs of\nintermediate neural layers.\n  Previous works separately showed that accurate 4-bit quantization of the\nneural gradients needs to (1) be unbiased and (2) have a log scale. However, no\nprevious work aimed to combine both ideas, as we do in this work. Specifically,\nwe examine the importance of having unbiased quantization in quantized neural\nnetwork training, where to maintain it, and how to combine it with logarithmic\nquantization. Based on this, we suggest a $\\textit{logarithmic unbiased\nquantization}$ (LUQ) method to quantize both the forward and backward phases to\n4-bit, achieving state-of-the-art results in 4-bit training without the\noverhead. For example, in ResNet50 on ImageNet, we achieved a degradation of\n1.1%. We further improve this to a degradation of only 0.32% after three epochs\nof high precision fine-tuning, combined with a variance reduction method --\nwhere both these methods add overhead comparable to previously suggested\nmethods.\n', '  Low-bit quantization emerges as one of the most promising compression\napproaches for deploying deep neural networks on edge devices. Mixed-precision\nquantization leverages a mixture of bit-widths to unleash the accuracy and\nefficiency potential of quantized models. However, existing mixed-precision\nquantization methods rely on simulations in high-performance devices to achieve\naccuracy and efficiency trade-offs in immense search spaces. This leads to a\nnon-negligible gap between the estimated efficiency metrics and the actual\nhardware that makes quantized models far away from the optimal accuracy and\nefficiency, and also causes the quantization process to rely on additional\nhigh-performance devices. In this paper, we propose an On-Chip Hardware-Aware\nQuantization (OHQ) framework, performing hardware-aware mixed-precision\nquantization on deployed edge devices to achieve accurate and efficient\ncomputing. Specifically, for efficiency metrics, we built an On-Chip\nQuantization Aware pipeline, which allows the quantization process to perceive\nthe actual hardware efficiency of the quantization operator and avoid\noptimization errors caused by inaccurate simulation. For accuracy metrics, we\npropose Mask-Guided Quantization Estimation technology to effectively estimate\nthe accuracy impact of operators in the on-chip scenario, getting rid of the\ndependence of the quantization process on high computing power. By synthesizing\ninsights from quantized models and hardware through linear optimization, we can\nobtain optimized bit-width configurations to achieve outstanding performance on\naccuracy and efficiency. We evaluate inference accuracy and acceleration with\nquantization for various architectures and compression ratios on hardware. OHQ\nachieves 70% and 73% accuracy for ResNet-18 and MobileNetV3, respectively, and\ncan reduce latency by 15~30% compared to INT8 on real deployment.\n'] , ['  Diffusion models have revolutionized image synthesis, setting new benchmarks\nin quality and creativity. However, their widespread adoption is hindered by\nthe intensive computation required during the iterative denoising process.\nPost-training quantization (PTQ) presents a solution to accelerate sampling,\naibeit at the expense of sample quality, extremely in low-bit settings.\nAddressing this, our study introduces a unified Quantization Noise Correction\nScheme (QNCD), aimed at minishing quantization noise throughout the sampling\nprocess. We identify two primary quantization challenges: intra and inter\nquantization noise. Intra quantization noise, mainly exacerbated by embeddings\nin the resblock module, extends activation quantization ranges, increasing\ndisturbances in each single denosing step. Besides, inter quantization noise\nstems from cumulative quantization deviations across the entire denoising\nprocess, altering data distributions step-by-step. QNCD combats these through\nembedding-derived feature smoothing for eliminating intra quantization noise\nand an effective runtime noise estimatiation module for dynamicly filtering\ninter quantization noise. Extensive experiments demonstrate that our method\noutperforms previous quantization methods for diffusion models, achieving\nlossless results in W4A8 and W8A8 quantization settings on ImageNet (LDM-4).\nCode is available at: https://github.com/huanpengchu/QNCD\n', '  Diffusion models have achieved great success in image generation tasks\nthrough iterative noise estimation. However, the heavy denoising process and\ncomplex neural networks hinder their low-latency applications in real-world\nscenarios. Quantization can effectively reduce model complexity, and\npost-training quantization (PTQ), which does not require fine-tuning, is highly\npromising in accelerating the denoising process. Unfortunately, we find that\ndue to the highly dynamic distribution of activations in different denoising\nsteps, existing PTQ methods for diffusion models suffer from distribution\nmismatch issues at both calibration sample level and reconstruction output\nlevel, which makes the performance far from satisfactory, especially in low-bit\ncases. In this paper, we propose Enhanced Distribution Alignment for\nPost-Training Quantization of Diffusion Models (EDA-DM) to address the above\nissues. Specifically, at the calibration sample level, we select calibration\nsamples based on the density and diversity in the latent space, thus\nfacilitating the alignment of their distribution with the overall samples; and\nat the reconstruction output level, we propose Fine-grained Block\nReconstruction, which can align the outputs of the quantized model and the\nfull-precision model at different network granularity. Extensive experiments\ndemonstrate that EDA-DM outperforms the existing post-training quantization\nframeworks in both unconditional and conditional generation scenarios. At\nlow-bit precision, the quantized models with our method even outperform the\nfull-precision models on most datasets.\n', '  Recent advancements in diffusion models, particularly the trend of\narchitectural transformation from UNet-based Diffusion to Diffusion Transformer\n(DiT), have significantly improved the quality and scalability of image\nsynthesis. Despite the incredible generative quality, the large computational\nrequirements of these large-scale models significantly hinder the deployments\nin real-world scenarios. Post-training Quantization (PTQ) offers a promising\nsolution by compressing model sizes and speeding up inference for the\npretrained models while eliminating model retraining. However, we have observed\nthe existing PTQ frameworks exclusively designed for both ViT and conventional\nDiffusion models fall into biased quantization and result in remarkable\nperformance degradation. In this paper, we find that the DiTs typically exhibit\nconsiderable variance in terms of both weight and activation, which easily runs\nout of the limited numerical representations. To address this issue, we devise\nQ-DiT, which seamlessly integrates three techniques: fine-grained quantization\nto manage substantial variance across input channels of weights and\nactivations, an automatic search strategy to optimize the quantization\ngranularity and mitigate redundancies, and dynamic activation quantization to\ncapture the activation changes across timesteps. Extensive experiments on the\nImageNet dataset demonstrate the effectiveness of the proposed Q-DiT.\nSpecifically, when quantizing DiT-XL/2 to W8A8 on ImageNet 256x256, Q-DiT\nachieves a remarkable reduction in FID by 1.26 compared to the baseline. Under\na W4A8 setting, it maintains high fidelity in image generation, showcasing only\na marginal increase in FID and setting a new benchmark for efficient,\nhigh-quality quantization in diffusion transformers. Code is available at\n\\href{https://github.com/Juanerx/Q-DiT}{https://github.com/Juanerx/Q-DiT}.\n'] , ['  The increasing size of deep neural networks necessitates effective model\ncompression to improve computational efficiency and reduce their memory\nfootprint. Sparsity and quantization are two prominent compression methods that\nhave individually demonstrated significant reduction in computational and\nmemory footprints while preserving model accuracy. While effective, the\ninterplay between these two methods remains an open question. In this paper, we\ninvestigate the interaction between these two methods and assess whether their\ncombination impacts final model accuracy. We mathematically prove that applying\nsparsity before quantization is the optimal sequence for these operations,\nminimizing error in computation. Our empirical studies across a wide range of\nmodels, including OPT and Llama model families (125M-8B) and ViT corroborate\nthese theoretical findings. In addition, through rigorous analysis, we\ndemonstrate that sparsity and quantization are not orthogonal; their\ninteraction can significantly harm model accuracy, with quantization error\nplaying a dominant role in this degradation. Our findings extend to the\nefficient deployment of large models in resource-limited compute platforms and\nreduce serving cost, offering insights into best practices for applying these\ncompression methods to maximize efficacy without compromising accuracy.\n', ""  Motivated by the huge success of Transformers in the field of natural\nlanguage processing (NLP), Vision Transformers (ViTs) have been rapidly\ndeveloped and achieved remarkable performance in various computer vision tasks.\nHowever, their huge model sizes and intensive computations hinder ViTs'\ndeployment on embedded devices, calling for effective model compression\nmethods, such as quantization. Unfortunately, due to the existence of\nhardware-unfriendly and quantization-sensitive non-linear operations,\nparticularly {Softmax}, it is non-trivial to completely quantize all operations\nin ViTs, yielding either significant accuracy drops or non-negligible hardware\ncosts. In response to challenges associated with \\textit{standard ViTs}, we\nfocus our attention towards the quantization and acceleration for\n\\textit{efficient ViTs}, which not only eliminate the troublesome Softmax but\nalso integrate linear attention with low computational complexity, and propose\n\\emph{Trio-ViT} accordingly. Specifically, at the algorithm level, we develop a\n{tailored post-training quantization engine} taking the unique activation\ndistributions of Softmax-free efficient ViTs into full consideration, aiming to\nboost quantization accuracy. Furthermore, at the hardware level, we build an\naccelerator dedicated to the specific Convolution-Transformer hybrid\narchitecture of efficient ViTs, thereby enhancing hardware efficiency.\nExtensive experimental results consistently prove the effectiveness of our\nTrio-ViT framework. {Particularly, we can gain up to\n$\\uparrow$$\\mathbf{7.2}\\times$ and $\\uparrow$$\\mathbf{14.6}\\times$ FPS under\ncomparable accuracy over state-of-the-art ViT accelerators, as well as\n$\\uparrow$$\\mathbf{5.9}\\times$ and $\\uparrow$$\\mathbf{2.0}\\times$ DSP\nefficiency.} Codes will be released publicly upon acceptance.\n"", '  Model quantization and compression is widely used techniques to reduce usage\nof computing resource at inference time. While state-of-the-art works have been\nachieved reasonable accuracy with higher bit such as 4bit or 8bit, but still it\nis challenging to quantize/compress a model further, e.g., 1bit or 2bit. To\novercome the challenge, we focus on outliers in weights of a pre-trained model\nwhich disrupt effective lower bit quantization and compression. In this work,\nwe propose Range Restriction Loss (R2-Loss) for building lower bit quantization\nand compression friendly models by removing outliers from weights during\npre-training. By effectively restricting range of weights, we mold the overall\ndistribution into a tight shape to ensure high quantization bit resolution,\ntherefore allowing model compression and quantization techniques can to utilize\ntheir limited numeric representation powers better. We introduce three\ndifferent, L-inf R2-Loss, its extension Margin R2-Loss and a new\nSoft-Min-MaxR2-Loss to be used as an auxiliary loss during full-precision model\ntraining. These R2-Loss can be used in different cases such as L-inf and Margin\nR2-Loss would be effective for symmetric quantization, while Soft-Min-Max\nR2-Loss shows better performance for model compression. In our experiment,\nR2-Loss improves lower bit quantization accuracy with state-of-the-art\npost-training quantization (PTQ), quantization-aware training (QAT), and model\ncompression techniques. With R2-Loss, MobileNet-V2 2bit weight and 8bit\nactivation PTQ, MobileNet-V1 2bit weight and activation QAT, ResNet18 1bit\nweight compression are improved to 59.49% from 50.66%, 59.05% from 55.96%, and\n52.58% from 45.54%, respectively.\n'] , ['  Vector quantization (VQ) is a technique to deterministically learn features\nwith discrete codebook representations. It is commonly performed with a\nvariational autoencoding model, VQ-VAE, which can be further extended to\nhierarchical structures for making high-fidelity reconstructions. However, such\nhierarchical extensions of VQ-VAE often suffer from the codebook/layer collapse\nissue, where the codebook is not efficiently used to express the data, and\nhence degrades reconstruction accuracy. To mitigate this problem, we propose a\nnovel unified framework to stochastically learn hierarchical discrete\nrepresentation on the basis of the variational Bayes framework, called\nhierarchically quantized variational autoencoder (HQ-VAE). HQ-VAE naturally\ngeneralizes the hierarchical variants of VQ-VAE, such as VQ-VAE-2 and\nresidual-quantized VAE (RQ-VAE), and provides them with a Bayesian training\nscheme. Our comprehensive experiments on image datasets show that HQ-VAE\nenhances codebook usage and improves reconstruction performance. We also\nvalidated HQ-VAE in terms of its applicability to a different modality with an\naudio dataset.\n', '  Vector Quantized Variational AutoEncoder (VQ-VAE) is an established technique\nin machine learning for learning discrete representations across various\nmodalities. However, its scalability and applicability are limited by the need\nto retrain the model to adjust the codebook for different data or model scales.\nWe introduce the Rate-Adaptive VQ-VAE (RAQ-VAE) framework, which addresses this\nchallenge with two novel codebook representation methods: a model-based\napproach using a clustering-based technique on an existing well-trained VQ-VAE\nmodel, and a data-driven approach utilizing a sequence-to-sequence (Seq2Seq)\nmodel for variable-rate codebook generation. Our experiments demonstrate that\nRAQ-VAE achieves effective reconstruction performance across multiple rates,\noften outperforming conventional fixed-rate VQ-VAE models. This work enhances\nthe adaptability and performance of VQ-VAEs, with broad applications in data\nreconstruction, generation, and computer vision tasks.\n', '  The dimensionality of the embedding and the number of available embeddings (\nalso called codebook size) are critical factors influencing the performance of\nVector Quantization(VQ), a discretization process used in many models such as\nthe Vector Quantized Variational Autoencoder (VQ-VAE) architecture. This study\nexamines the balance between the codebook sizes and dimensions of embeddings in\nVQ, while maintaining their product constant. Traditionally, these hyper\nparameters are static during training; however, our findings indicate that\naugmenting the codebook size while simultaneously reducing the embedding\ndimension can significantly boost the effectiveness of the VQ-VAE. As a result,\nthe strategic selection of codebook size and embedding dimensions, while\npreserving the capacity of the discrete codebook space, is critically\nimportant. To address this, we propose a novel adaptive dynamic quantization\napproach, underpinned by the Gumbel-Softmax mechanism, which allows the model\nto autonomously determine the optimal codebook configuration for each data\ninstance. This dynamic discretizer gives the VQ-VAE remarkable flexibility.\nThorough empirical evaluations across multiple benchmark datasets validate the\nnotable performance enhancements achieved by our approach, highlighting the\nsignificant potential of adaptive dynamic quantization to improve model\nperformance.\n']",Quantization and Compression Techniques for Deep Learning Models,"""Quantization Techniques for Deep Neural Networks"""
111,"Vision Transformers and Efficient Compression Methods , Image and Data Compression Techniques , Video Compression and Bitrate Optimization , Information-Theoretic Compression and Generalization","['convnets', 'cnn', 'imagenet', 'cnns', 'convnet', 'convolutions', 'attention', 'convolutional', 'vision', 'visual'] , ['compression', 'encoder', 'encoding', 'compressing', 'compressed', 'decoding', 'decoder', 'bitrate', 'coding', 'jpeg'] , ['bitrate', 'streaming', 'bitstream', 'videos', 'encoder', 'compression', 'encoding', 'vvc', 'decoding', 'decoder'] , ['compression', 'decoding', 'compressed', 'encoder', 'information', 'bottleneck', 'decoder', 'entropy', 'generalization', 'encoding']","['  Hierarchical vision transformers (ViTs) have two advantages over conventional\nViTs. First, hierarchical ViTs achieve linear computational complexity with\nrespect to image size by local self-attention. Second, hierarchical ViTs create\nhierarchical feature maps by merging image patches in deeper layers for dense\nprediction. However, existing pruning methods ignore the unique properties of\nhierarchical ViTs and use the magnitude value as the weight importance. This\napproach leads to two main drawbacks. First, the ""local"" attention weights are\ncompared at a ""global"" level, which may cause some ""locally"" important weights\nto be pruned due to their relatively small magnitude ""globally"". The second\nissue with magnitude pruning is that it fails to consider the distinct weight\ndistributions of the network, which are essential for extracting coarse to\nfine-grained features at various hierarchical levels.\n  To solve the aforementioned issues, we have developed a Data-independent\nModule-Aware Pruning method (DIMAP) to compress hierarchical ViTs. To ensure\nthat ""local"" attention weights at different hierarchical levels are compared\nfairly in terms of their contribution, we treat them as a module and examine\ntheir contribution by analyzing their information distortion. Furthermore, we\nintroduce a novel weight metric that is solely based on weights and does not\nrequire input images, thereby eliminating the dependence on the patch merging\nprocess. Our method validates its usefulness and strengths on Swin Transformers\nof different sizes on ImageNet-1k classification. Notably, the top-5 accuracy\ndrop is only 0.07% when we remove 52.5% FLOPs and 52.7% parameters of Swin-B.\nWhen we reduce 33.2% FLOPs and 33.2% parameters of Swin-S, we can even achieve\na 0.8% higher relative top-5 accuracy than the original model. Code is\navailable at: https://github.com/he-y/Data-independent-Module-Aware-Pruning\n', '  Vision Transformer (ViT) has emerged as a prominent architecture for various\ncomputer vision tasks. In ViT, we divide the input image into patch tokens and\nprocess them through a stack of self attention blocks. However, unlike\nConvolutional Neural Networks (CNN), ViTs simple architecture has no\ninformative inductive bias (e.g., locality,etc. ). Due to this, ViT requires a\nlarge amount of data for pre-training. Various data efficient approaches (DeiT)\nhave been proposed to train ViT on balanced datasets effectively. However,\nlimited literature discusses the use of ViT for datasets with long-tailed\nimbalances. In this work, we introduce DeiT-LT to tackle the problem of\ntraining ViTs from scratch on long-tailed datasets. In DeiT-LT, we introduce an\nefficient and effective way of distillation from CNN via distillation DIST\ntoken by using out-of-distribution images and re-weighting the distillation\nloss to enhance focus on tail classes. This leads to the learning of local\nCNN-like features in early ViT blocks, improving generalization for tail\nclasses. Further, to mitigate overfitting, we propose distilling from a flat\nCNN teacher, which leads to learning low-rank generalizable features for DIST\ntokens across all ViT blocks. With the proposed DeiT-LT scheme, the\ndistillation DIST token becomes an expert on the tail classes, and the\nclassifier CLS token becomes an expert on the head classes. The experts help to\neffectively learn features corresponding to both the majority and minority\nclasses using a distinct set of tokens within the same ViT architecture. We\nshow the effectiveness of DeiT-LT for training ViT from scratch on datasets\nranging from small-scale CIFAR-10 LT to large-scale iNaturalist-2018.\n', '  Attention-based vision models, such as Vision Transformer (ViT) and its\nvariants, have shown promising performance in various computer vision tasks.\nHowever, these emerging architectures suffer from large model sizes and high\ncomputational costs, calling for efficient model compression solutions. To\ndate, pruning ViTs has been well studied, while other compression strategies\nthat have been widely applied in CNN compression, e.g., model factorization, is\nlittle explored in the context of ViT compression. This paper explores an\nefficient method for compressing vision transformers to enrich the toolset for\nobtaining compact attention-based vision models. Based on the new insight on\nthe multi-head attention layer, we develop a highly efficient ViT compression\nsolution, which outperforms the state-of-the-art pruning methods. For\ncompressing DeiT-small and DeiT-base models on ImageNet, our proposed approach\ncan achieve 0.45% and 0.76% higher top-1 accuracy even with fewer parameters.\nOur finding can also be applied to improve the customization efficiency of\ntext-to-image diffusion models, with much faster training (up to $2.6\\times$\nspeedup) and lower extra storage cost (up to $1927.5\\times$ reduction) than the\nexisting works.\n'] , ['  In the field of neural data compression, the prevailing focus has been on\noptimizing algorithms for either classical distortion metrics, such as PSNR or\nSSIM, or human perceptual quality. With increasing amounts of data consumed by\nmachines rather than humans, a new paradigm of machine-oriented\ncompression$\\unicode{x2013}$which prioritizes the retention of features salient\nfor machine perception over traditional human-centric\ncriteria$\\unicode{x2013}$has emerged, creating several new challenges to the\ndevelopment, evaluation, and deployment of systems utilizing lossy compression.\nIn particular, it is unclear how different approaches to lossy compression will\naffect the performance of downstream machine perception tasks. To address this\nunder-explored area, we evaluate various perception\nmodels$\\unicode{x2013}$including image classification, image segmentation,\nspeech recognition, and music source separation$\\unicode{x2013}$under severe\nlossy compression. We utilize several popular codecs spanning conventional,\nneural, and generative compression architectures. Our results indicate three\nkey findings: (1) using generative compression, it is feasible to leverage\nhighly compressed data while incurring a negligible impact on machine\nperceptual quality; (2) machine perceptual quality correlates strongly with\ndeep similarity metrics, indicating a crucial role of these metrics in the\ndevelopment of machine-oriented codecs; and (3) using lossy compressed\ndatasets, (e.g. ImageNet) for pre-training can lead to counter-intuitive\nscenarios where lossy compression increases machine perceptual quality rather\nthan degrading it. To encourage engagement on this growing area of research,\nour code and experiments are available at:\nhttps://github.com/danjacobellis/MPQ.\n', ""  In the field of medical image compression, Implicit Neural Representation\n(INR) networks have shown remarkable versatility due to their flexible\ncompression ratios, yet they are constrained by a one-to-one fitting approach\nthat results in lengthy encoding times. Our novel method,\n``\\textbf{UniCompress}'', innovatively extends the compression capabilities of\nINR by being the first to compress multiple medical data blocks using a single\nINR network. By employing wavelet transforms and quantization, we introduce a\ncodebook containing frequency domain information as a prior input to the INR\nnetwork. This enhances the representational power of INR and provides\ndistinctive conditioning for different image blocks. Furthermore, our research\nintroduces a new technique for the knowledge distillation of implicit\nrepresentations, simplifying complex model knowledge into more manageable\nformats to improve compression ratios. Extensive testing on CT and electron\nmicroscopy (EM) datasets has demonstrated that UniCompress outperforms\ntraditional INR methods and commercial compression solutions like HEVC,\nespecially in complex and high compression scenarios. Notably, compared to\nexisting INR techniques, UniCompress achieves a 4$\\sim$5 times increase in\ncompression speed, marking a significant advancement in the field of medical\nimage compression. Codes will be publicly available.\n"", '  In this age of information, images are a critical medium for storing and\ntransmitting information. With the rapid growth of image data amount, visual\ncompression and visual data perception are two important research topics\nattracting a lot attention. However, those two topics are rarely discussed\ntogether and follow separate research path. Due to the compact compressed\ndomain representation offered by learning-based image compression methods,\nthere exists possibility to have one stream targeting both efficient data\nstorage and compression, and machine perception tasks. In this paper, we\npropose a layered generative image compression model achieving high human\nvision-oriented image reconstructed quality, even at extreme compression\nratios. To obtain analysis efficiency and flexibility, a task-agnostic\nlearning-based compression model is proposed, which effectively supports\nvarious compressed domain-based analytical tasks while reserves outstanding\nreconstructed perceptual quality, compared with traditional and learning-based\ncodecs. In addition, joint optimization schedule is adopted to acquire best\nbalance point among compression ratio, reconstructed image quality, and\ndownstream perception performance. Experimental results verify that our\nproposed compressed domain-based multi-task analysis method can achieve\ncomparable analysis results against the RGB image-based methods with up to\n99.6% bit rate saving (i.e., compared with taking original RGB image as the\nanalysis model input). The practical ability of our model is further justified\nfrom model size and information fidelity aspects.\n'] , ['  Volumetric video based on Neural Radiance Field (NeRF) holds vast potential\nfor various 3D applications, but its substantial data volume poses significant\nchallenges for compression and transmission. Current NeRF compression lacks the\nflexibility to adjust video quality and bitrate within a single model for\nvarious network and device capacities. To address these issues, we propose HPC,\na novel hierarchical progressive volumetric video coding framework achieving\nvariable bitrate using a single model. Specifically, HPC introduces a\nhierarchical representation with a multi-resolution residual radiance field to\nreduce temporal redundancy in long-duration sequences while simultaneously\ngenerating various levels of detail. Then, we propose an end-to-end progressive\nlearning approach with a multi-rate-distortion loss function to jointly\noptimize both hierarchical representation and compression. Our HPC trained only\nonce can realize multiple compression levels, while the current methods need to\ntrain multiple fixed-bitrate models for different rate-distortion (RD)\ntradeoffs. Extensive experiments demonstrate that HPC achieves flexible quality\nlevels with variable bitrate by a single model and exhibits competitive RD\nperformance, even outperforming fixed-bitrate models across various datasets.\n', '  Providing high-quality video with efficient bitrate is a main challenge in\nvideo industry. The traditional one-size-fits-all scheme for bitrate ladders is\ninefficient and reaching the best content-aware decision computationally\nimpractical due to extensive encodings required. To mitigate this, we propose a\nbitrate and complexity efficient bitrate ladder prediction method using\ntransfer learning and spatio-temporal features. We propose: (1) using feature\nmaps from well-known pre-trained DNNs to predict rate-quality behavior with\nlimited training data; and (2) improving highest quality rung efficiency by\npredicting minimum bitrate for top quality and using it for the top rung. The\nmethod tested on 102 video scenes demonstrates 94.1% reduction in complexity\nversus brute-force at 1.71% BD-Rate expense. Additionally, transfer learning\nwas thoroughly studied through four networks and ablation studies.\n', '  Adaptive video streaming requires efficient bitrate ladder construction to\nmeet heterogeneous network conditions and end-user demands. Per-title optimized\nencoding typically traverses numerous encoding parameters to search the\nPareto-optimal operating points for each video. Recently, researchers have\nattempted to predict the content-optimized bitrate ladder for pre-encoding\noverhead reduction. However, existing methods commonly estimate the encoding\nparameters on the Pareto front and still require subsequent pre-encodings. In\nthis paper, we propose to directly predict the optimal transcoding resolution\nat each preset bitrate for efficient bitrate ladder construction. We adopt a\nTemporal Attentive Gated Recurrent Network to capture spatial-temporal features\nand predict transcoding resolutions as a multi-task classification problem. We\ndemonstrate that content-optimized bitrate ladders can thus be efficiently\ndetermined without any pre-encoding. Our method well approximates the\nground-truth bitrate-resolution pairs with a slight Bj{\\o}ntegaard Delta rate\nloss of 1.21% and significantly outperforms the state-of-the-art fixed ladder.\n'] , ['  The distortion-rate function of output-constrained lossy source coding with\nlimited common randomness is analyzed for the special case of squared error\ndistortion measure. An explicit expression is obtained when both source and\nreconstruction distributions are Gaussian. This further leads to a partial\ncharacterization of the information-theoretic limit of quadratic Gaussian\nrate-distortion-perception coding with the perception measure given by\nKullback-Leibler divergence or squared quadratic Wasserstein distance.\n', ""  In a feedforward network, Transfer Entropy (TE) can be used to measure the\ninfluence that one layer has on another by quantifying the information transfer\nbetween them during training. According to the Information Bottleneck\nprinciple, a neural model's internal representation should compress the input\ndata as much as possible while still retaining sufficient information about the\noutput. Information Plane analysis is a visualization technique used to\nunderstand the trade-off between compression and information preservation in\nthe context of the Information Bottleneck method by plotting the amount of\ninformation in the input data against the compressed representation. The claim\nthat there is a causal link between information-theoretic compression and\ngeneralization, measured by mutual information, is plausible, but results from\ndifferent studies are conflicting. In contrast to mutual information, TE can\ncapture temporal relationships between variables. To explore such links, in our\nnovel approach we use TE to quantify information transfer between neural layers\nand perform Information Plane analysis. We obtained encouraging experimental\nresults, opening the possibility for further investigations.\n"", '  The ability of machine learning (ML) algorithms to generalize well to unseen\ndata has been studied through the lens of information theory, by bounding the\ngeneralization error with the input-output mutual information (MI), i.e., the\nMI between the training data and the learned hypothesis. Yet, these bounds have\nlimited practicality for modern ML applications (e.g., deep learning), due to\nthe difficulty of evaluating MI in high dimensions. Motivated by recent\nfindings on the compressibility of neural networks, we consider algorithms that\noperate by slicing the parameter space, i.e., trained on random\nlower-dimensional subspaces. We introduce new, tighter information-theoretic\ngeneralization bounds tailored for such algorithms, demonstrating that slicing\nimproves generalization. Our bounds offer significant computational and\nstatistical advantages over standard MI bounds, as they rely on scalable\nalternative measures of dependence, i.e., disintegrated mutual information and\n$k$-sliced mutual information. Then, we extend our analysis to algorithms whose\nparameters do not need to exactly lie on random subspaces, by leveraging\nrate-distortion theory. This strategy yields generalization bounds that\nincorporate a distortion term measuring model compressibility under slicing,\nthereby tightening existing bounds without compromising performance or\nrequiring model compression. Building on this, we propose a regularization\nscheme enabling practitioners to control generalization through\ncompressibility. Finally, we empirically validate our results and achieve the\ncomputation of non-vacuous information-theoretic generalization bounds for\nneural networks, a task that was previously out of reach.\n']",Image and Video Compression Techniques,Image and Data Compression Techniques
112,"Variational Inference for Bayesian Models , Variational Autoencoders and Latent Variable Models , ""Variational Inference and Entropy in Generative Models""","['variational', 'unnormalized', 'inference', 'posteriors', 'gradient', 'probabilistic', 'divergence', 'likelihood', 'reparameterization', 'posterior'] , ['autoencoders', 'autoencoder', 'autoenocoder', 'generative', 'encoder', 'variational', 'decoder', 'regularization', 'priors', 'latent'] , ['entropy', 'variational', 'generative', 'gaussian', 'divergence', 'densities', 'estimation', 'approximations', 'likelihood', 'mixtures']","['  Variational families with full-rank covariance approximations are known not\nto work well in black-box variational inference (BBVI), both empirically and\ntheoretically. In fact, recent computational complexity results for BBVI have\nestablished that full-rank variational families scale poorly with the\ndimensionality of the problem compared to e.g. mean-field families. This is\nparticularly critical to hierarchical Bayesian models with local variables;\ntheir dimensionality increases with the size of the datasets. Consequently, one\ngets an iteration complexity with an explicit (\\mathcal{O}(N^2)) dependence on\nthe dataset size (N). In this paper, we explore a theoretical middle ground\nbetween mean-field variational families and full-rank families: structured\nvariational families. We rigorously prove that certain scale matrix structures\ncan achieve a better iteration complexity of (\\mathcal{O}\\left(N\\right)),\nimplying better scaling with respect to (N). We empirically verify our\ntheoretical results on large-scale hierarchical models.\n', '  Estimating a distribution given access to its unnormalized density is pivotal\nin Bayesian inference, where the posterior is generally known only up to an\nunknown normalizing constant. Variational inference and Markov chain Monte\nCarlo methods are the predominant tools for this task; however, both methods\nare often challenging to apply reliably, particularly when the posterior has\ncomplex geometry. Here, we introduce Soft Contrastive Variational Inference\n(SoftCVI), which allows a family of variational objectives to be derived\nthrough a contrastive estimation framework. These objectives have zero variance\ngradient when the variational approximation is exact, without the need for\nspecialized gradient estimators. The approach involves parameterizing a\nclassifier in terms of the variational distribution, which allows the inference\ntask to be reframed as a contrastive estimation problem, aiming to identify a\nsingle true posterior sample among a set of samples. Despite this framing, we\ndo not require positive or negative samples, but rather learn by sampling the\nvariational distribution and computing ground truth soft classification labels\nfrom the unnormalized posterior itself. We empirically investigate the\nperformance on a variety of Bayesian inference tasks, using both using both\nsimple (e.g. normal) and expressive (normalizing flow) variational\ndistributions. We find that SoftCVI objectives often outperform other commonly\nused variational objectives.\n', '  We provide the first convergence guarantee for full black-box variational\ninference (BBVI), also known as Monte Carlo variational inference. While\npreliminary investigations worked on simplified versions of BBVI (e.g., bounded\ndomain, bounded support, only optimizing for the scale, and such), our setup\ndoes not need any such algorithmic modifications. Our results hold for\nlog-smooth posterior densities with and without strong log-concavity and the\nlocation-scale variational family. Also, our analysis reveals that certain\nalgorithm design choices commonly employed in practice, particularly, nonlinear\nparameterizations of the scale of the variational approximation, can result in\nsuboptimal convergence rates. Fortunately, running BBVI with proximal\nstochastic gradient descent fixes these limitations, and thus achieves the\nstrongest known convergence rate guarantees. We evaluate this theoretical\ninsight by comparing proximal SGD against other standard implementations of\nBBVI on large-scale Bayesian inference problems.\n'] , [""  We consider the task of estimating variational autoencoders (VAEs) when the\ntraining data is incomplete. We show that missing data increases the complexity\nof the model's posterior distribution over the latent variables compared to the\nfully-observed case. The increased complexity may adversely affect the fit of\nthe model due to a mismatch between the variational and model posterior\ndistributions. We introduce two strategies based on (i) finite\nvariational-mixture and (ii) imputation-based variational-mixture distributions\nto address the increased posterior complexity. Through a comprehensive\nevaluation of the proposed approaches, we show that variational mixtures are\neffective at improving the accuracy of VAE estimation from incomplete data.\n"", ""  Traditional Variational Autoencoders (VAEs) are constrained by the\nlimitations of the Evidence Lower Bound (ELBO) formulation, particularly when\nutilizing simplistic, non-analytic, or unknown prior distributions. These\nlimitations inhibit the VAE's ability to generate high-quality samples and\nprovide clear, interpretable latent representations. This work introduces the\nEntropy Decomposed Variational Autoencoder (ED-VAE), a novel re-formulation of\nthe ELBO that explicitly includes entropy and cross-entropy components. This\nreformulation significantly enhances model flexibility, allowing for the\nintegration of complex and non-standard priors. By providing more detailed\ncontrol over the encoding and regularization of latent spaces, ED-VAE not only\nimproves interpretability but also effectively captures the complex\ninteractions between latent variables and observed data, thus leading to better\ngenerative performance.\n"", '  The posterior collapse phenomenon in variational autoencoder (VAE), where the\nvariational posterior distribution closely matches the prior distribution, can\nhinder the quality of the learned latent variables. As a consequence of\nposterior collapse, the latent variables extracted by the encoder in VAE\npreserve less information from the input data and thus fail to produce\nmeaningful representations as input to the reconstruction process in the\ndecoder. While this phenomenon has been an actively addressed topic related to\nVAE performance, the theory for posterior collapse remains underdeveloped,\nespecially beyond the standard VAE. In this work, we advance the theoretical\nunderstanding of posterior collapse to two important and prevalent yet less\nstudied classes of VAE: conditional VAE and hierarchical VAE. Specifically, via\na non-trivial theoretical analysis of linear conditional VAE and hierarchical\nVAE with two levels of latent, we prove that the cause of posterior collapses\nin these models includes the correlation between the input and output of the\nconditional VAE and the effect of learnable encoder variance in the\nhierarchical VAE. We empirically validate our theoretical findings for linear\nconditional and hierarchical VAE and demonstrate that these results are also\npredictive for non-linear cases with extensive experiments.\n'] , ['  Standard probabilistic sparse coding assumes a Laplace prior, a linear\nmapping from latents to observables, and Gaussian observable distributions. We\nhere derive a solely entropy-based learning objective for the parameters of\nstandard sparse coding. The novel variational objective has the following\nfeatures: (A) unlike MAP approximations, it uses non-trivial posterior\napproximations for probabilistic inference; (B) unlike for previous non-trivial\napproximations, the novel objective is fully analytical; and (C) the objective\nallows for a novel principled form of annealing. The objective is derived by\nfirst showing that the standard ELBO objective converges to a sum of entropies,\nwhich matches similar recent results for generative models with Gaussian\npriors. The conditions under which the ELBO becomes equal to entropies are then\nshown to have analytical solutions, which leads to the fully analytical\nobjective. Numerical experiments are used to demonstrate the feasibility of\nlearning with such entropy-based ELBOs. We investigate different posterior\napproximations including Gaussians with correlated latents and deep amortized\napproximations. Furthermore, we numerically investigate entropy-based annealing\nwhich results in improved learning. Our main contributions are theoretical,\nhowever, and they are twofold: (1) for non-trivial posterior approximations, we\nprovide the (to the knowledge of the authors) first analytical ELBO objective\nfor standard probabilistic sparse coding; and (2) we provide the first\ndemonstration on how a recently shown convergence of the ELBO to entropy sums\ncan be used for learning.\n', '  The central objective function of a variational autoencoder (VAE) is its\nvariational lower bound (the ELBO). Here we show that for standard (i.e.,\nGaussian) VAEs the ELBO converges to a value given by the sum of three\nentropies: the (negative) entropy of the prior distribution, the expected\n(negative) entropy of the observable distribution, and the average entropy of\nthe variational distributions (the latter is already part of the ELBO). Our\nderived analytical results are exact and apply for small as well as for\nintricate deep networks for encoder and decoder. Furthermore, they apply for\nfinitely and infinitely many data points and at any stationary point (including\nlocal maxima and saddle points). The result implies that the ELBO can for\nstandard VAEs often be computed in closed-form at stationary points while the\noriginal ELBO requires numerical approximations of integrals. As a main\ncontribution, we provide the proof that the ELBO for VAEs is at stationary\npoints equal to entropy sums. Numerical experiments then show that the obtained\nanalytical results are sufficiently precise also in those vicinities of\nstationary points that are reached in practice. Furthermore, we discuss how the\nnovel entropy form of the ELBO can be used to analyze and understand learning\nbehavior. More generally, we believe that our contributions can be useful for\nfuture theoretical and practical studies on VAE learning as they provide novel\ninformation on those points in parameters space that optimization of VAEs\nconverges to.\n', ""  The variational lower bound (a.k.a. ELBO or free energy) is the central\nobjective for many established as well as many novel algorithms for\nunsupervised learning. During learning such algorithms change model parameters\nto increase the variational lower bound. Learning usually proceeds until\nparameters have converged to values close to a stationary point of the learning\ndynamics. In this purely theoretical contribution, we show that (for a very\nlarge class of generative models) the variational lower bound is at all\nstationary points of learning equal to a sum of entropies. For standard machine\nlearning models with one set of latents and one set of observed variables, the\nsum consists of three entropies: (A) the (average) entropy of the variational\ndistributions, (B) the negative entropy of the model's prior distribution, and\n(C) the (expected) negative entropy of the observable distribution. The\nobtained result applies under realistic conditions including: finite numbers of\ndata points, at any stationary point (including saddle points) and for any\nfamily of (well behaved) variational distributions. The class of generative\nmodels for which we show the equality to entropy sums contains many well-known\ngenerative models. As concrete examples we discuss Sigmoid Belief Networks,\nprobabilistic PCA and (Gaussian and non-Gaussian) mixture models. The result\nalso applies for standard (Gaussian) variational autoencoders, a special case\nthat has been shown previously (Damm et al., 2023). The prerequisites we use to\nshow equality to entropy sums are relatively mild. Concretely, the\ndistributions of a given generative model have to be of the exponential family,\nand the model has to satisfy a parameterization criterion (which is usually\nfulfilled). Proving the equality of the ELBO to entropy sums at stationary\npoints (under the stated conditions) is the main contribution of this work.\n""]",Variational Methods for Bayesian Modeling and Generative Learning,"""Variational Inference and Entropy in Generative Models"""
113,"Flow-based Generative Modeling Techniques , Generative Flow Networks (GFlowNets) , Generative Architectural Design , Riemannian Manifolds in Generative Models and Optimization , Energy-Based Generative Models , Offline Optimization with Generative Models , Generative Models for Population Synthesis","['flow', 'flows', 'generative', 'modeling', 'perflow', 'diffusion', 'sde', 'denoising', 'stochastic', 'unnormalized'] , ['gflownet', 'gflownets', 'generative', 'flow', 'eflownets', 'flows', 'networks', 'learned', 'reward', 'rewards'] , ['architectural', 'designs', 'architecture', 'architects', 'design', 'facades', 'generative', 'designers', 'designer', 'creativity'] , ['riemannian', 'manifold', 'manifolds', 'geodesics', 'geodesic', 'generative', 'gps', 'divergence', 'spatiotemporal', 'gradient'] , ['generative', 'autoencoder', 'adversarial', 'models', 'variational', 'energy', 'flow', 'modeling', 'rnn', 'generator'] , ['optimizing', 'optimize', 'generative', 'optimization', 'offline', 'bandit', 'surrogate', 'search', 'gradients', 'algorithms'] , ['generative', 'populations', 'census', 'generating', 'population', 'autoencoders', 'data', 'generation', 'samples', 'sample']","['  Continuous normalizing flows (CNFs) learn an ordinary differential equation\nto transform prior samples into data. Flow matching (FM) has recently emerged\nas a simulation-free approach for training CNFs by regressing a velocity model\ntowards the conditional velocity field. However, on constrained domains, the\nlearned velocity model may lead to undesirable flows that result in highly\nunnatural samples, e.g., oversaturated images, due to both flow matching error\nand simulation error. To address this, we add a boundary constraint term to\nCNFs, which leads to reflected CNFs that keep trajectories within the\nconstrained domains. We propose reflected flow matching (RFM) to train the\nvelocity model in reflected CNFs by matching the conditional velocity fields in\na simulation-free manner, similar to the vanilla FM. Moreover, the analytical\nform of conditional velocity fields in RFM avoids potentially biased\napproximations, making it superior to existing score-based generative models on\nconstrained domains. We demonstrate that RFM achieves comparable or better\nresults on standard image benchmarks and produces high-quality\nclass-conditioned samples under high guidance weight.\n', '  Score-based generative models are a popular class of generative modelling\ntechniques relying on stochastic differential equations (SDE). From their\ninception, it was realized that it was also possible to perform generation\nusing ordinary differential equations (ODE) rather than SDE. This led to the\nintroduction of the probability flow ODE approach and denoising diffusion\nimplicit models. Flow matching methods have recently further extended these\nODE-based approaches and approximate a flow between two arbitrary probability\ndistributions. Previous work derived bounds on the approximation error of\ndiffusion models under the stochastic sampling regime, given assumptions on the\n$L^2$ loss. We present error bounds for the flow matching procedure using fully\ndeterministic sampling, assuming an $L^2$ bound on the approximation error and\na certain regularity condition on the data distributions.\n', '  Gaussian denoising has emerged as a powerful method for constructing\nsimulation-free continuous normalizing flows for generative modeling. Despite\ntheir empirical successes, theoretical properties of these flows and the\nregularizing effect of Gaussian denoising have remained largely unexplored. In\nthis work, we aim to address this gap by investigating the well-posedness of\nsimulation-free continuous normalizing flows built on Gaussian denoising.\nThrough a unified framework termed Gaussian interpolation flow, we establish\nthe Lipschitz regularity of the flow velocity field, the existence and\nuniqueness of the flow, and the Lipschitz continuity of the flow map and the\ntime-reversed flow map for several rich classes of target distributions. This\nanalysis also sheds light on the auto-encoding and cycle consistency properties\nof Gaussian interpolation flows. Additionally, we study the stability of these\nflows in source distributions and perturbations of the velocity field, using\nthe quadratic Wasserstein distance as a metric. Our findings offer valuable\ninsights into the learning techniques employed in Gaussian interpolation flows\nfor generative modeling, providing a solid theoretical foundation for\nend-to-end error analyses of learning Gaussian interpolation flows with\nempirical observations.\n'] , ['  Generative Flow Networks (GFlowNets) treat sampling from distributions over\ncompositional discrete spaces as a sequential decision-making problem, training\na stochastic policy to construct objects step by step. Recent studies have\nrevealed strong connections between GFlowNets and entropy-regularized\nreinforcement learning. Building on these insights, we propose to enhance\nplanning capabilities of GFlowNets by applying Monte Carlo Tree Search (MCTS).\nSpecifically, we show how the MENTS algorithm (Xiao et al., 2019) can be\nadapted for GFlowNets and used during both training and inference. Our\nexperiments demonstrate that this approach improves the sample efficiency of\nGFlowNet training and the generation fidelity of pre-trained GFlowNet models.\n', '  Generative Flow Networks (GFlowNets, GFNs) are a generative framework for\nlearning unnormalized probability mass functions over discrete spaces. Since\ntheir inception, GFlowNets have proven to be useful for learning generative\nmodels in applications where the majority of the discrete space is unvisited\nduring training. This has inspired some to hypothesize that GFlowNets, when\npaired with deep neural networks (DNNs), have favourable generalization\nproperties. In this work, we empirically verify some of the hypothesized\nmechanisms of generalization of GFlowNets. In particular, we find that the\nfunctions that GFlowNets learn to approximate have an implicit underlying\nstructure which facilitate generalization. We also find that GFlowNets are\nsensitive to being trained offline and off-policy; however, the reward\nimplicitly learned by GFlowNets is robust to changes in the training\ndistribution.\n', '  The Generative Flow Network (GFlowNet) is a probabilistic framework in which\nan agent learns a stochastic policy and flow functions to sample objects with\nprobability proportional to an unnormalized reward function. GFlowNets share a\nstrong resemblance to reinforcement learning (RL), that typically aims to\nmaximize reward, due to their sequential decision-making processes. Recent\nworks have studied connections between GFlowNets and maximum entropy (MaxEnt)\nRL, which modifies the standard objective of RL agents by learning an\nentropy-regularized objective. However, a critical theoretical gap persists:\ndespite the apparent similarities in their sequential decision-making nature, a\ndirect link between GFlowNets and standard RL has yet to be discovered, while\nbridging this gap could further unlock the potential of both fields. In this\npaper, we establish a new connection between GFlowNets and policy evaluation\nfor a uniform policy. Surprisingly, we find that the resulting value function\nfor the uniform policy has a close relationship to the flows in GFlowNets.\nLeveraging these insights, we further propose a novel rectified policy\nevaluation (RPE) algorithm, which achieves the same reward-matching effect as\nGFlowNets, offering a new perspective. We compare RPE, MaxEnt RL, and GFlowNets\nin a number of benchmarks, and show that RPE achieves competitive results\ncompared to previous approaches. This work sheds light on the previously\nunexplored connection between (non-MaxEnt) RL and GFlowNets, potentially\nopening new avenues for future research in both fields.\n'] , ['  3D shape generation techniques leveraging deep learning have garnered\nsignificant interest from both the computer vision and architectural design\ncommunities, promising to enrich the content in the virtual environment.\nHowever, research on virtual architectural design remains limited, particularly\nregarding designer-AI collaboration and deep learning-assisted design. In our\nsurvey, we reviewed 149 related articles (81.2% of articles published between\n2019 and 2023) covering architectural design, 3D shape techniques, and virtual\nenvironments. Through scrutinizing the literature, we first identify the\nprinciples of virtual architecture and illuminate its current production\nchallenges, including datasets, multimodality, design intuition, and generative\nframeworks. We then introduce the latest approaches to designing and generating\nvirtual buildings leveraging 3D shape generation and summarize four\ncharacteristics of various approaches to virtual architecture. Based on our\nanalysis, we expound on four research agendas, including agency, communication,\nuser consideration, and integrating tools. Additionally, we highlight four\nimportant enablers of ubiquitous interaction with immersive systems in deep\nlearning-assisted architectural generation. Our work contributes to fostering\nunderstanding between designers and deep learning techniques, broadening access\nto designer-AI collaboration. We advocate for interdisciplinary efforts to\naddress this timely research topic, facilitating content designing and\ngeneration in the virtual environment.\n', '  Generative Artificial Intelligence (AI) has pioneered new methodological\nparadigms in architectural design, significantly expanding the innovative\npotential and efficiency of the design process. This paper explores the\nextensive applications of generative AI technologies in architectural design, a\ntrend that has benefited from the rapid development of deep generative models.\nThis article provides a comprehensive review of the basic principles of\ngenerative AI and large-scale models and highlights the applications in the\ngeneration of 2D images, videos, and 3D models. In addition, by reviewing the\nlatest literature from 2020, this paper scrutinizes the impact of generative AI\ntechnologies at different stages of architectural design, from generating\ninitial architectural 3D forms to producing final architectural imagery. The\nmarked trend of research growth indicates an increasing inclination within the\narchitectural design community towards embracing generative AI, thereby\ncatalyzing a shared enthusiasm for research. These research cases and\nmethodologies have not only proven to enhance efficiency and innovation\nsignificantly but have also posed challenges to the conventional boundaries of\narchitectural creativity. Finally, we point out new directions for design\ninnovation and articulate fresh trajectories for applying generative AI in the\narchitectural domain. This article provides the first comprehensive literature\nreview about generative AI for architectural design, and we believe this work\ncan facilitate more research work on this significant topic in architecture.\n', '  The determination of space layout is one of the primary activities in the\nschematic design stage of an architectural project. The initial layout planning\ndefines the shape, dimension, and circulation pattern of internal spaces; which\ncan also affect performance and cost of the construction. When carried out\nmanually, space layout planning can be complicated, repetitive and time\nconsuming. In this work, a generative design framework for the automatic\ngeneration of spatial architectural layout has been developed. The proposed\napproach integrates a novel physics-inspired parametric model for space layout\nplanning and an evolutionary optimisation metaheuristic. Results revealed that\nsuch a generative design framework can generate a wide variety of design\nsuggestions at the schematic design stage, applicable to complex design\nproblems.\n'] , ['  Learning the distribution of data on Riemannian manifolds is crucial for\nmodeling data from non-Euclidean space, which is required by many applications\nin diverse scientific fields. Yet, existing generative models on manifolds\nsuffer from expensive divergence computation or rely on approximations of heat\nkernel. These limitations restrict their applicability to simple geometries and\nhinder scalability to high dimensions. In this work, we introduce the\nRiemannian Diffusion Mixture, a principled framework for building a generative\ndiffusion process on manifolds. Instead of following the denoising approach of\nprevious diffusion models, we construct a diffusion process using a mixture of\nbridge processes derived on general manifolds without requiring heat kernel\nestimations. We develop a geometric understanding of the mixture process,\nderiving the drift as a weighted mean of tangent directions to the data points\nthat guides the process toward the data distribution. We further propose a\nscalable training objective for learning the mixture process that readily\napplies to general manifolds. Our method achieves superior performance on\ndiverse manifolds with dramatically reduced number of in-training simulation\nsteps for general manifolds.\n', '  The density ratio of two probability distributions is one of the fundamental\ntools in mathematical and computational statistics and machine learning, and it\nhas a variety of known applications. Therefore, density ratio estimation from\nfinite samples is a very important task, but it is known to be unstable when\nthe distributions are distant from each other. One approach to address this\nproblem is density ratio estimation using incremental mixtures of the two\ndistributions. We geometrically reinterpret existing methods for density ratio\nestimation based on incremental mixtures. We show that these methods can be\nregarded as iterating on the Riemannian manifold along a particular curve\nbetween the two probability distributions. Making use of the geometry of the\nmanifold, we propose to consider incremental density ratio estimation along\ngeneralized geodesics on this manifold. To achieve such a method requires Monte\nCarlo sampling along geodesics via transformations of the two distributions. We\nshow how to implement an iterative algorithm to sample along these geodesics\nand show how changing the distances along the geodesic affect the variance and\naccuracy of the estimation of the density ratio. Our experiments demonstrate\nthat the proposed approach outperforms the existing approaches using\nincremental mixtures that do not take the geometry of the\n', ""  We consider the fundamental task of optimising a real-valued function defined\nin a potentially high-dimensional Euclidean space, such as the loss function in\nmany machine-learning tasks or the logarithm of the probability distribution in\nstatistical inference. We use Riemannian geometry notions to redefine the\noptimisation problem of a function on the Euclidean space to a Riemannian\nmanifold with a warped metric, and then find the function's optimum along this\nmanifold. The warped metric chosen for the search domain induces a\ncomputational friendly metric-tensor for which optimal search directions\nassociated with geodesic curves on the manifold becomes easier to compute.\nPerforming optimization along geodesics is known to be generally infeasible,\nyet we show that in this specific manifold we can analytically derive Taylor\napproximations up to third-order. In general these approximations to the\ngeodesic curve will not lie on the manifold, however we construct suitable\nretraction maps to pull them back onto the manifold. Therefore, we can\nefficiently optimize along the approximate geodesic curves. We cover the\nrelated theory, describe a practical optimization algorithm and empirically\nevaluate it on a collection of challenging optimisation benchmarks. Our\nproposed algorithm, using 3rd-order approximation of geodesics, tends to\noutperform standard Euclidean gradient-based counterparts in term of number of\niterations until convergence.\n""] , ['  Energy-Based Models (EBMs) have emerged as a powerful framework in the realm\nof generative modeling, offering a unique perspective that aligns closely with\nprinciples of statistical mechanics. This review aims to provide physicists\nwith a comprehensive understanding of EBMs, delineating their connection to\nother generative models such as Generative Adversarial Networks (GANs),\nVariational Autoencoders (VAEs), and Normalizing Flows. We explore the sampling\ntechniques crucial for EBMs, including Markov Chain Monte Carlo (MCMC) methods,\nand draw parallels between EBM concepts and statistical mechanics, highlighting\nthe significance of energy functions and partition functions. Furthermore, we\ndelve into state-of-the-art training methodologies for EBMs, covering recent\nadvancements and their implications for enhanced model performance and\nefficiency. This review is designed to clarify the often complex\ninterconnections between these models, which can be challenging due to the\ndiverse communities working on the topic.\n', '  Energy-Based Models (EBMs) are an important class of probabilistic models,\nalso known as random fields and undirected graphical models. EBMs are\nun-normalized and thus radically different from other popular self-normalized\nprobabilistic models such as hidden Markov models (HMMs), autoregressive\nmodels, generative adversarial nets (GANs) and variational auto-encoders\n(VAEs). Over the past years, EBMs have attracted increasing interest not only\nfrom the core machine learning community, but also from application domains\nsuch as speech, vision, natural language processing (NLP) and so on, due to\nsignificant theoretical and algorithmic progress. The sequential nature of\nspeech and language also presents special challenges and needs a different\ntreatment from processing fix-dimensional data (e.g., images). Therefore, the\npurpose of this monograph is to present a systematic introduction to\nenergy-based models, including both algorithmic progress and applications in\nspeech and language processing. First, the basics of EBMs are introduced,\nincluding classic models, recent models parameterized by neural networks,\nsampling methods, and various learning methods from the classic learning\nalgorithms to the most advanced ones. Then, the application of EBMs in three\ndifferent scenarios is presented, i.e., for modeling marginal, conditional and\njoint distributions, respectively. 1) EBMs for sequential data with\napplications in language modeling, where the main focus is on the marginal\ndistribution of a sequence itself; 2) EBMs for modeling conditional\ndistributions of target sequences given observation sequences, with\napplications in speech recognition, sequence labeling and text generation; 3)\nEBMs for modeling joint distributions of both sequences of observations and\ntargets, and their applications in semi-supervised learning and calibrated\nnatural language understanding.\n', ""  Generative models have shown strong generation ability while efficient\nlikelihood estimation is less explored. Energy-based models~(EBMs) define a\nflexible energy function to parameterize unnormalized densities efficiently but\nare notorious for being difficult to train. Adversarial EBMs introduce a\ngenerator to form a minimax training game to avoid expensive MCMC sampling used\nin traditional EBMs, but a noticeable gap between adversarial EBMs and other\nstrong generative models still exists. Inspired by diffusion-based models, we\nembedded EBMs into each denoising step to split a long-generated process into\nseveral smaller steps. Besides, we employ a symmetric Jeffrey divergence and\nintroduce a variational posterior distribution for the generator's training to\naddress the main challenges that exist in adversarial EBMs. Our experiments\nshow significant improvement in generation compared to existing adversarial\nEBMs, while also providing a useful energy function for efficient density\nestimation.\n""] , ['  This paper considers the problem of offline optimization, where the objective\nfunction is unknown except for a collection of ``offline"" data examples. While\nrecent years have seen a flurry of work on applying various machine learning\ntechniques to the offline optimization problem, the majority of these work\nfocused on learning a surrogate of the unknown objective function and then\napplying existing optimization algorithms. While the idea of modeling the\nunknown objective function is intuitive and appealing, from the learning point\nof view it also makes it very difficult to tune the objective of the learner\naccording to the objective of optimization. Instead of learning and then\noptimizing the unknown objective function, in this paper we take on a less\nintuitive but more direct view that optimization can be thought of as a process\nof sampling from a generative model. To learn an effective generative model\nfrom the offline data examples, we consider the standard technique of\n``re-weighting"", and our main technical contribution is a probably\napproximately correct (PAC) lower bound on the natural optimization objective,\nwhich allows us to jointly learn a weight function and a score-based generative\nmodel. The robustly competitive performance of the proposed approach is\ndemonstrated via empirical studies using the standard offline optimization\nbenchmarks.\n', '  Offline model-based optimization (MBO) aims to maximize a black-box objective\nfunction using only an offline dataset of designs and scores. A prevalent\napproach involves training a conditional generative model on existing designs\nand their associated scores, followed by the generation of new designs\nconditioned on higher target scores. However, these newly generated designs\noften underperform due to the lack of high-scoring training data. To address\nthis challenge, we introduce a novel method, Design Editing for Offline\nModel-based Optimization (DEMO), which consists of two phases. In the first\nphase, termed pseudo-target distribution generation, we apply gradient ascent\non the offline dataset using a trained surrogate model, producing a synthetic\ndataset where the predicted scores serve as new labels. A conditional diffusion\nmodel is subsequently trained on this synthetic dataset to capture a\npseudo-target distribution, which enhances the accuracy of the conditional\ndiffusion model in generating higher-scoring designs. Nevertheless, the\npseudo-target distribution is susceptible to noise stemming from inaccuracies\nin the surrogate model, consequently predisposing the conditional diffusion\nmodel to generate suboptimal designs. We hence propose the second phase,\nexisting design editing, to directly incorporate the high-scoring features from\nthe offline dataset into design generation. In this phase, top designs from the\noffline dataset are edited by introducing noise, which are subsequently refined\nusing the conditional diffusion model to produce high-scoring designs. Overall,\nhigh-scoring designs begin with inheriting high-scoring features from the\nsecond phase and are further refined with a more accurate conditional diffusion\nmodel in the first phase. Empirical evaluations on 7 offline MBO tasks show\nthat DEMO outperforms various baseline methods.\n', '  Offline optimization is an emerging problem in many experimental engineering\ndomains including protein, drug or aircraft design, where online\nexperimentation to collect evaluation data is too expensive or dangerous. To\navoid that, one has to optimize an unknown function given only its offline\nevaluation at a fixed set of inputs. A naive solution to this problem is to\nlearn a surrogate model of the unknown function and optimize this surrogate\ninstead. However, such a naive optimizer is prone to erroneous overestimation\nof the surrogate (possibly due to over-fitting on a biased sample of function\nevaluation) on inputs outside the offline dataset. Prior approaches addressing\nthis challenge have primarily focused on learning robust surrogate models.\nHowever, their search strategies are derived from the surrogate model rather\nthan the actual offline data. To fill this important gap, we introduce a new\nlearning-to-search perspective for offline optimization by reformulating it as\nan offline reinforcement learning problem. Our proposed policy-guided gradient\nsearch approach explicitly learns the best policy for a given surrogate model\ncreated from the offline data. Our empirical results on multiple benchmarks\ndemonstrate that the learned optimization policy can be combined with existing\noffline surrogates to significantly improve the optimization performance.\n'] , ['  A common objective in the analysis of tabular data is estimating the\nconditional distribution (in contrast to only producing predictions) of a set\nof ""outcome"" variables given a set of ""covariates"", which is sometimes referred\nto as the ""density regression"" problem. Beyond estimation on the conditional\ndistribution, the generative ability of drawing synthetic samples from the\nlearned conditional distribution is also desired as it further widens the range\nof applications. We propose a flow-based generative model tailored for the\ndensity regression task on tabular data. Our flow applies a sequence of\ntree-based piecewise-linear transforms on initial uniform noise to eventually\ngenerate samples from complex conditional densities of (univariate or\nmultivariate) outcomes given the covariates and allows efficient analytical\nevaluation of the fitted conditional density on any point in the sample space.\nWe introduce a training algorithm for fitting the tree-based transforms using a\ndivide-and-conquer strategy that transforms maximum likelihood training of the\ntree-flow into training a collection of binary classifiers--one at each tree\nsplit--under cross-entropy loss. We assess the performance of our method under\nout-of-sample likelihood evaluation and compare it with a variety of\nstate-of-the-art conditional density learners on a range of simulated and real\nbenchmark tabular datasets. Our method consistently achieves comparable or\nsuperior performance at a fraction of the training and sampling budget.\nFinally, we demonstrate the utility of our method\'s generative ability through\nan application to generating synthetic longitudinal microbiome compositional\ndata based on training our flow on a publicly available microbiome study.\n', '  Household and individual-level sociodemographic data are essential for\nunderstanding human-infrastructure interaction and policymaking. However, the\nPublic Use Microdata Sample (PUMS) offers only a sample at the state level,\nwhile census tract data only provides the marginal distributions of variables\nwithout correlations. Therefore, we need an accurate synthetic population\ndataset that maintains consistent variable correlations observed in microdata,\npreserves household-individual and individual-individual relationships, adheres\nto state-level statistics, and accurately represents the geographic\ndistribution of the population. We propose a deep generative framework\nleveraging the variational autoencoder (VAE) to generate a synthetic population\nwith the aforementioned features. The methodological contributions include (1)\na new data structure for capturing household-individual and\nindividual-individual relationships, (2) a transfer learning process with\npre-training and fine-tuning steps to generate households and individuals whose\naggregated distributions align with the census tract marginal distribution, and\n(3) decoupled binary cross-entropy (D-BCE) loss function enabling distribution\nshift and out-of-sample records generation. Model results for an application in\nDelaware, USA demonstrate the ability to ensure the realism of generated\nhousehold-individual records and accurately describe population statistics at\nthe census tract level compared to existing methods. Furthermore, testing in\nNorth Carolina, USA yielded promising results, supporting the transferability\nof our method.\n', ""  Population synthesis involves generating synthetic yet realistic\nrepresentations of a target population of micro-agents for behavioral modeling\nand simulation. Traditional methods, often reliant on target population\nsamples, such as census data or travel surveys, face limitations due to high\ncosts and small sample sizes, particularly at smaller geographical scales. We\npropose a novel framework based on copulas to generate synthetic data for\ntarget populations where only empirical marginal distributions are known. This\nmethod utilizes samples from different populations with similar marginal\ndependencies, introduces a spatial component into population synthesis, and\nconsiders various information sources for more realistic generators.\nConcretely, the process involves normalizing the data and treat it as\nrealizations of a given copula, and then training a generative model before\nincorporating the information on the marginals of the target population.\nUtilizing American Community Survey data, we assess our framework's performance\nthrough standardized root mean squared error (SRMSE) and so-called sampled\nzeros. We focus on its capacity to transfer a model learned from one population\nto another. Our experiments include transfer tests between regions at the same\ngeographical level as well as to lower geographical levels, hence evaluating\nthe framework's adaptability in varied spatial contexts. We compare Bayesian\nNetworks, Variational Autoencoders, and Generative Adversarial Networks, both\nindividually and combined with our copula framework. Results show that the\ncopula enhances machine learning methods in matching the marginals of the\nreference data. Furthermore, it consistently surpasses Iterative Proportional\nFitting in terms of SRMSE in the transferability experiments, while introducing\nunique observations not found in the original training sample.\n""]",Generative Modeling Techniques,Flow-based Generative Modeling Techniques
114,"""Generative AI in Education and Assessment"" , Generative AI and Open Data , Generative AI Integration and Applications , ""Generative AI in IoT and Smart Systems"" , Generative Bridge Design with AI","['assessment', 'assessments', 'educational', 'education', 'academic', 'ai', 'academia', 'pedagogical', 'students', 'educators'] , ['ai', 'generative', 'openai', 'aied', 'creativity', 'intelligence', 'artificial', 'creators', 'software', 'openness'] , ['ai', 'automation', 'generative', 'prototyping', 'tools', 'programming', 'intelligence', 'tooling', 'genai', 'technology'] , ['iot', 'ioe', 'generative', 'ai', 'cloud', 'devices', 'smart', 'intelligent', 'sensing', 'gai'] , ['bridge', 'autoencoder', 'generative', 'keras', 'tensorflow', 'pixelcnn', 'span', 'dataset', 'variational', 'arch']","[""  The rise of Artificial Intelligence (AI) and Generative Artificial\nIntelligence (GenAI) in higher education necessitates assessment reform. This\nstudy addresses a critical gap by exploring student and academic staff\nexperiences with AI and GenAI tools, focusing on their familiarity and comfort\nwith current and potential future applications in learning and assessment. An\nonline survey collected data from 35 academic staff and 282 students across two\nuniversities in Vietnam and one in Singapore, examining GenAI familiarity,\nperceptions of its use in assessment marking and feedback, knowledge checking\nand participation, and experiences of GenAI text detection.\n  Descriptive statistics and reflexive thematic analysis revealed a generally\nlow familiarity with GenAI among both groups. GenAI feedback was viewed\nnegatively; however, it was viewed more positively when combined with\ninstructor feedback. Academic staff were more accepting of GenAI text detection\ntools and grade adjustments based on detection results compared to students.\nQualitative analysis identified three themes: unclear understanding of text\ndetection tools, variability in experiences with GenAI detectors, and mixed\nfeelings about GenAI's future impact on educational assessment. These findings\nhave major implications regarding the development of policies and practices for\nGenAI-enabled assessment and feedback in higher education.\n"", ""  The advancements in Generative Artificial Intelligence (GenAI) provide\nopportunities to enrich educational experiences, but also raise concerns about\nacademic integrity. Many educators have expressed anxiety and hesitation in\nintegrating GenAI in their teaching practices, and are in needs of\nrecommendations and guidance from their institutions that can support them to\nincorporate GenAI in their classrooms effectively. In order to respond to\nhigher educators' needs, this study aims to explore how universities and\neducators respond and adapt to the development of GenAI in their academic\ncontexts by analyzing academic policies and guidelines established by\ntop-ranked U.S. universities regarding the use of GenAI, especially ChatGPT.\nData sources include academic policies, statements, guidelines, and relevant\nresources provided by the top 100 universities in the U.S. Results show that\nthe majority of these universities adopt an open but cautious approach towards\nGenAI. Primary concerns lie in ethical usage, accuracy, and data privacy. Most\nuniversities actively respond and provide diverse types of resources, such as\nsyllabus templates, workshops, shared articles, and one-on-one consultations\nfocusing on a range of topics: general technical introduction, ethical\nconcerns, pedagogical applications, preventive strategies, data privacy,\nlimitations, and detective tools. The findings provide four practical\npedagogical implications for educators in teaching practices: accept its\npresence, align its use with learning objectives, evolve curriculum to prevent\nmisuse, and adopt multifaceted evaluation strategies rather than relying on AI\ndetectors. Two recommendations are suggested for educators in policy making:\nestablish discipline-specific policies and guidelines, and manage sensitive\ninformation carefully.\n"", '  Recent developments in Generative Artificial Intelligence (GenAI) have\ncreated a paradigm shift in multiple areas of society, and the use of these\ntechnologies is likely to become a defining feature of education in coming\ndecades. GenAI offers transformative pedagogical opportunities, while\nsimultaneously posing ethical and academic challenges. Against this backdrop,\nwe outline a practical, simple, and sufficiently comprehensive tool to allow\nfor the integration of GenAI tools into educational assessment: the AI\nAssessment Scale (AIAS).\n  The AIAS empowers educators to select the appropriate level of GenAI usage in\nassessments based on the learning outcomes they seek to address. The AIAS\noffers greater clarity and transparency for students and educators, provides a\nfair and equitable policy tool for institutions to work with, and offers a\nnuanced approach which embraces the opportunities of GenAI while recognising\nthat there are instances where such tools may not be pedagogically appropriate\nor necessary.\n  By adopting a practical, flexible approach that can be implemented quickly,\nthe AIAS can form a much-needed starting point to address the current\nuncertainty and anxiety regarding GenAI in education. As a secondary objective,\nwe engage with the current literature and advocate for a refocused discourse on\nGenAI tools in education, one which foregrounds how technologies can help\nsupport and enhance teaching and learning, which contrasts with the current\nfocus on GenAI as a facilitator of academic misconduct.\n'] , ['  Since late 2022, generative AI has taken the world by storm, with widespread\nuse of tools including ChatGPT, Gemini, and Claude. Generative AI and large\nlanguage model (LLM) applications are transforming how individuals find and\naccess data and knowledge. However, the intricate relationship between open\ndata and generative AI, and the vast potential it holds for driving innovation\nin this field remain underexplored areas. This white paper seeks to unpack the\nrelationship between open data and generative AI and explore possible\ncomponents of a new Fourth Wave of Open Data: Is open data becoming AI ready?\nIs open data moving towards a data commons approach? Is generative AI making\nopen data more conversational? Will generative AI improve open data quality and\nprovenance? Towards this end, we provide a new Spectrum of Scenarios framework.\nThis framework outlines a range of scenarios in which open data and generative\nAI could intersect and what is required from a data quality and provenance\nperspective to make open data ready for those specific scenarios. These\nscenarios include: pertaining, adaptation, inference and insight generation,\ndata augmentation, and open-ended exploration. Through this process, we found\nthat in order for data holders to embrace generative AI to improve open data\naccess and develop greater insights from open data, they first must make\nprogress around five key areas: enhance transparency and documentation, uphold\nquality and integrity, promote interoperability and standards, improve\naccessibility and useability, and address ethical considerations.\n', '  In the next few years, applications of Generative AI are expected to\nrevolutionize a number of different areas, ranging from science & medicine to\neducation. The potential for these seismic changes has triggered a lively\ndebate about potential risks and resulted in calls for tighter regulation, in\nparticular from some of the major tech companies who are leading in AI\ndevelopment. This regulation is likely to put at risk the budding field of\nopen-source Generative AI. We argue for the responsible open sourcing of\ngenerative AI models in the near and medium term. To set the stage, we first\nintroduce an AI openness taxonomy system and apply it to 40 current large\nlanguage models. We then outline differential benefits and risks of open versus\nclosed source AI and present potential risk mitigation, ranging from best\npractices to calls for technical and scientific contributions. We hope that\nthis report will add a much needed missing voice to the current public\ndiscourse on near to mid-term AI safety and other societal impact.\n', '  Generative AI applications present unique design challenges. As generative AI\ntechnologies are increasingly being incorporated into mainstream applications,\nthere is an urgent need for guidance on how to design user experiences that\nfoster effective and safe use. We present six principles for the design of\ngenerative AI applications that address unique characteristics of generative AI\nUX and offer new interpretations and extensions of known issues in the design\nof AI applications. Each principle is coupled with a set of design strategies\nfor implementing that principle via UX capabilities or through the design\nprocess. The principles and strategies were developed through an iterative\nprocess involving literature review, feedback from design practitioners,\nvalidation against real-world generative AI applications, and incorporation\ninto the design process of two generative AI applications. We anticipate the\nprinciples to usefully inform the design of generative AI applications by\ndriving actionable design recommendations.\n'] , [""  This workshop paper presents a critical examination of the integration of\nGenerative AI (Gen AI) into the academic writing process, focusing on the use\nof AI as a collaborative tool. It contrasts the performance and interaction of\ntwo AI models, Gemini and ChatGPT, through a collaborative inquiry approach\nwhere researchers engage in facilitated sessions to design prompts that elicit\nspecific AI responses for crafting research outlines. This case study\nhighlights the importance of prompt design, output analysis, and recognizing\nthe AI's limitations to ensure responsible and effective AI integration in\nscholarly work. Preliminary findings suggest that prompt variation\nsignificantly affects output quality and reveals distinct capabilities and\nconstraints of each model. The paper contributes to the field of Human-Computer\nInteraction by exploring effective prompt strategies and providing a\ncomparative analysis of Gen AI models, ultimately aiming to enhance AI-assisted\nacademic writing and prompt a deeper dialogue within the HCI community.\n"", ""  In the last decade, despite rapid advancements in artificial intelligence\n(AI) transforming many industry practices, construction largely lags in\nadoption. Recently, the emergence and rapid adoption of advanced large language\nmodels (LLM) like OpenAI's GPT, Google's PaLM, and Meta's Llama have shown\ngreat potential and sparked considerable global interest. However, the current\nsurge lacks a study investigating the opportunities and challenges of\nimplementing Generative AI (GenAI) in the construction sector, creating a\ncritical knowledge gap for researchers and practitioners. This underlines the\nnecessity to explore the prospects and complexities of GenAI integration.\nBridging this gap is fundamental to optimizing GenAI's early-stage adoption\nwithin the construction sector. Given GenAI's unprecedented capabilities to\ngenerate human-like content based on learning from existing content, we reflect\non two guiding questions: What will the future bring for GenAI in the\nconstruction industry? What are the potential opportunities and challenges in\nimplementing GenAI in the construction industry? This study delves into\nreflected perception in literature, analyzes the industry perception using\nprogramming-based word cloud and frequency analysis, and integrates authors'\nopinions to answer these questions. This paper recommends a conceptual GenAI\nimplementation framework, provides practical recommendations, summarizes future\nresearch questions, and builds foundational literature to foster subsequent\nresearch expansion in GenAI within the construction and its allied architecture\n& engineering domains.\n"", '  The emergence of Generative Artificial Intelligence (AI) and Large Language\nModels (LLMs) has marked a new era of Natural Language Processing (NLP),\nintroducing unprecedented capabilities that are revolutionizing various\ndomains. This paper explores the current state of these cutting-edge\ntechnologies, demonstrating their remarkable advancements and wide-ranging\napplications. Our paper contributes to providing a holistic perspective on the\ntechnical foundations, practical applications, and emerging challenges within\nthe evolving landscape of Generative AI and LLMs. We believe that understanding\nthe generative capabilities of AI systems and the specific context of LLMs is\ncrucial for researchers, practitioners, and policymakers to collaboratively\nshape the responsible and ethical integration of these technologies into\nvarious domains. Furthermore, we identify and address main research gaps,\nproviding valuable insights to guide future research endeavors within the AI\nresearch community.\n'] , ['  The success of Artificial Intelligence (AI) in multiple disciplines and\nvertical domains in recent years has promoted the evolution of mobile\nnetworking and the future Internet toward an AI-integrated Internet-of-Things\n(IoT) era. Nevertheless, most AI techniques rely on data generated by physical\ndevices (e.g., mobile devices and network nodes) or specific applications\n(e.g., fitness trackers and mobile gaming). To bypass this circumvent,\nGenerative AI (GAI), a.k.a. AI-generated content (AIGC), has emerged as a\npowerful AI paradigm; thanks to its ability to efficiently learn complex data\ndistributions and generate synthetic data to represent the original data in\nvarious forms. This impressive feature is projected to transform the management\nof mobile networking and diversify the current services and applications\nprovided. On this basis, this work presents a concise tutorial on the role of\nGAIs in mobile and wireless networking. In particular, this survey first\nprovides the fundamentals of GAI and representative GAI models, serving as an\nessential preliminary to the understanding of the applications of GAI in mobile\nand wireless networking. Then, this work provides a comprehensive review of\nstate-of-the-art studies and GAI applications in network management, wireless\nsecurity, semantic communication, and lessons learned from the open literature.\nFinally, this work summarizes the current research on GAI for mobile and\nwireless networking by outlining important challenges that need to be resolved\nto facilitate the development and applicability of GAI in this edge-cutting\narea.\n', '  This study confronts the growing challenges of energy consumption and the\ndepletion of energy resources, particularly in the context of smart buildings.\nAs the demand for energy increases alongside the necessity for efficient\nbuilding maintenance, it becomes imperative to explore innovative energy\nmanagement solutions. We present a comprehensive review of Internet of Things\n(IoT)-based frameworks aimed at smart city energy management, highlighting the\npivotal role of IoT devices in addressing these issues due to their\ncompactness, sensing, measurement, and computing capabilities. Our review\nmethodology encompasses a thorough analysis of existing literature on IoT\narchitectures and frameworks for intelligent energy management applications. We\nfocus on systems that not only collect and store data but also support\nintelligent analysis for monitoring, controlling, and enhancing system\nefficiency. Additionally, we examine the potential for these frameworks to\nserve as platforms for the development of third-party applications, thereby\nextending their utility and adaptability. The findings from our review indicate\nthat IoT-based frameworks offer significant potential to reduce energy\nconsumption and environmental impact in smart buildings. Through the adoption\nof intelligent mechanisms and solutions, these frameworks facilitate effective\nenergy management, leading to improved system efficiency and sustainability.\nConsidering these findings, we recommend further exploration and adoption of\nIoT-based wireless sensing systems in smart buildings as a strategic approach\nto energy management. Our review underscores the importance of incorporating\nintelligent analysis and enabling the development of third-party applications\nwithin the IoT framework to efficiently meet the evolving energy demands and\nmaintenance challenges\n', '  The Internet of things (IoT) can significantly enhance the quality of human\nlife, specifically in healthcare, attracting extensive attentions to\nIoT-healthcare services. Meanwhile, the human digital twin (HDT) is proposed as\nan innovative paradigm that can comprehensively characterize the replication of\nthe individual human body in the digital world and reflect its physical status\nin real time. Naturally, HDT is envisioned to empower IoT-healthcare beyond the\napplication of healthcare monitoring by acting as a versatile and vivid human\ndigital testbed, simulating the outcomes and guiding the practical treatments.\nHowever, successfully establishing HDT requires high-fidelity virtual modeling\nand strong information interactions but possibly with scarce, biased and noisy\ndata. Fortunately, a recent popular technology called generative artificial\nintelligence (GAI) may be a promising solution because it can leverage advanced\nAI algorithms to automatically create, manipulate, and modify valuable while\ndiverse data. This survey particularly focuses on the implementation of\nGAI-driven HDT in IoT-healthcare. We start by introducing the background of\nIoT-healthcare and the potential of GAI-driven HDT. Then, we delve into the\nfundamental techniques and present the overall framework of GAI-driven HDT.\nAfter that, we explore the realization of GAI-driven HDT in detail, including\nGAI-enabled data acquisition, communication, data management, digital modeling,\nand data analysis. Besides, we discuss typical IoT-healthcare applications that\ncan be revolutionized by GAI-driven HDT, namely personalized health monitoring\nand diagnosis, personalized prescription, and personalized rehabilitation.\nFinally, we conclude this survey by highlighting some future research\ndirections.\n'] , ['  Try to generate new bridge types using generative artificial intelligence\ntechnology. The grayscale images of the bridge facade with the change of\ncomponent width was rendered by 3dsMax animation software, and then the OpenCV\nmodule performed an appropriate amount of geometric transformation (rotation,\nhorizontal scale, vertical scale) to obtain the image dataset of three-span\nbeam bridge, arch bridge, cable-stayed bridge and suspension bridge. Based on\nPython programming language, TensorFlow and Keras deep learning platform\nframework, variational autoencoder was constructed and trained, and\nlow-dimensional bridge-type latent space that is convenient for vector\noperations was obtained. Variational autoencoder can combine two bridge types\non the basis of the original of human into one that is a new bridge type.\nGenerative artificial intelligence technology can assist bridge designers in\nbridge-type innovation, and can be used as copilot.\n', '  Try to generate new bridge types using generative artificial intelligence\ntechnology. Symmetric structured image dataset of three-span beam bridge, arch\nbridge, cable-stayed bridge and suspension bridge are used . Based on Python\nprogramming language, TensorFlow and Keras deep learning platform framework ,\nas well as Wasserstein loss function and Lipschitz constraints, generative\nadversarial network is constructed and trained. From the obtained low\ndimensional bridge-type latent space sampling, new bridge types with asymmetric\nstructures can be generated. Generative adversarial network can create new\nbridge types by organically combining different structural components on the\nbasis of human original bridge types. It has a certain degree of human original\nability. Generative artificial intelligence technology can open up imagination\nspace and inspire humanity.\n', '  Try to generate new bridge types using generative artificial intelligence\ntechnology. Using symmetric structured image dataset of three-span beam bridge,\narch bridge, cable-stayed bridge and suspension bridge , based on Python\nprogramming language, TensorFlow and Keras deep learning platform framework ,\nPixelCNN is constructed and trained. The model can capture the statistical\nstructure of the images and calculate the probability distribution of the next\npixel when the previous pixels are given. From the obtained latent space\nsampling, new bridge types different from the training dataset can be\ngenerated. PixelCNN can organically combine different structural components on\nthe basis of human original bridge types, creating new bridge types that have a\ncertain degree of human original ability. Autoregressive models cannot\nunderstand the meaning of the sequence, while multimodal models combine\nregression and autoregressive models to understand the sequence. Multimodal\nmodels should be the way to achieve artificial general intelligence in the\nfuture.\n']",Generative AI Applications and Implications,Generative AI Integration and Applications
115,"Generative Adversarial Networks (GANs) Theory and Applications , ""Medical Imaging with Generative Adversarial Networks"" , ""Generative Adversarial Networks for Imaging and Microscopy"" , Synthetic Tabular Data Generation with GANs","['gans', 'gan', 'generative', 'adversarial', 'flowgan', 'codegan', 'cgan', 'cyclegan', 'stylegan', 'inception'] , ['gans', 'gan', 'generative', 'cyclegan', 'imaging', 'mri', 'echogan', 'segmentation', 'tomography', 'adversarial'] , ['gans', 'gan', 'microscopy', 'dcgan', 'generative', 'cgan', 'autoencoders', 'imaging', 'adversarial', 'microscope'] , ['gans', 'gan', 'generative', 'adversarial', 'autoencoders', 'cgan', 'castgan', 'ctgan', 'datasets', 'gansemble']","['  The empirical success of Generative Adversarial Networks (GANs) caused an\nincreasing interest in theoretical research. The statistical literature is\nmainly focused on Wasserstein GANs and generalizations thereof, which\nespecially allow for good dimension reduction properties. Statistical results\nfor Vanilla GANs, the original optimization problem, are still rather limited\nand require assumptions such as smooth activation functions and equal\ndimensions of the latent space and the ambient space. To bridge this gap, we\ndraw a connection from Vanilla GANs to the Wasserstein distance. By doing so,\nexisting results for Wasserstein GANs can be extended to Vanilla GANs. In\nparticular, we obtain an oracle inequality for Vanilla GANs in Wasserstein\ndistance. The assumptions of this oracle inequality are designed to be\nsatisfied by network architectures commonly used in practice, such as\nfeedforward ReLU networks. By providing a quantitative result for the\napproximation of a Lipschitz function by a feedforward ReLU network with\nbounded H\\""older norm, we conclude a rate of convergence for Vanilla GANs as\nwell as Wasserstein GANs as estimators of the unknown probability distribution.\n', ""  Modern GANs achieve remarkable performance in terms of generating realistic\nand diverse samples. This has led many to believe that ``GANs capture the\ntraining data manifold''. In this work we show that this interpretation is\nwrong. We empirically show that the manifold learned by modern GANs does not\nfit the training distribution: specifically the manifold does not pass through\nthe training examples and passes closer to out-of-distribution images than to\nin-distribution images. We also investigate the distribution over images\nimplied by the prior over the latent codes and study whether modern GANs learn\na density that approximates the training distribution. Surprisingly, we find\nthat the learned density is very far from the data distribution and that GANs\ntend to assign higher density to out-of-distribution images. Finally, we\ndemonstrate that the set of images used to train modern GANs are often not part\nof the typical set described by the GANs' distribution.\n"", '  We investigate the impact of the input dimension on the generalization error\nin generative adversarial networks (GANs). In particular, we first provide both\ntheoretical and practical evidence to validate the existence of an optimal\ninput dimension (OID) that minimizes the generalization error. Then, to\nidentify the OID, we introduce a novel framework called generalized GANs\n(G-GANs), which includes existing GANs as a special case. By incorporating the\ngroup penalty and the architecture penalty developed in the paper, G-GANs have\nseveral intriguing features. First, our framework offers adaptive\ndimensionality reduction from the initial dimension to a dimension necessary\nfor generating the target distribution. Second, this reduction in\ndimensionality also shrinks the required size of the generator network\narchitecture, which is automatically identified by the proposed architecture\npenalty. Both reductions in dimensionality and the generator network\nsignificantly improve the stability and the accuracy of the estimation and\nprediction. Theoretical support for the consistent selection of the input\ndimension and the generator network is provided. Third, the proposed algorithm\ninvolves an end-to-end training process, and the algorithm allows for dynamic\nadjustments between the input dimension and the generator network during\ntraining, further enhancing the overall performance of G-GANs. Extensive\nexperiments conducted with simulated and benchmark data demonstrate the\nsuperior performance of G-GANs. In particular, compared to that of\noff-the-shelf methods, G-GANs achieves an average improvement of 45.68% in the\nCT slice dataset, 43.22% in the MNIST dataset and 46.94% in the FashionMNIST\ndataset in terms of the maximum mean discrepancy or Frechet inception distance.\nMoreover, the features generated based on the input dimensions identified by\nG-GANs align with visually significant features.\n'] , ['  Large annotated datasets are required for training deep learning models, but\nin medical imaging data sharing is often complicated due to ethics,\nanonymization and data protection legislation. Generative AI models, such as\ngenerative adversarial networks (GANs) and diffusion models, can today produce\nvery realistic synthetic images, and can potentially facilitate data sharing.\nHowever, in order to share synthetic medical images it must first be\ndemonstrated that they can be used for training different networks with\nacceptable performance. Here, we therefore comprehensively evaluate four GANs\n(progressive GAN, StyleGAN 1-3) and a diffusion model for the task of brain\ntumor segmentation (using two segmentation networks, U-Net and a Swin\ntransformer). Our results show that segmentation networks trained on synthetic\nimages reach Dice scores that are 80% - 90% of Dice scores when training with\nreal images, but that memorization of the training images can be a problem for\ndiffusion models if the original dataset is too small. Our conclusion is that\nsharing synthetic medical images is a viable option to sharing real images, but\nthat further work is required. The trained generative models and the generated\nsynthetic images are shared on AIDA data hub\n', '  Efficient and accurate brain ventricle segmentation from clinical CT scans is\ncritical for emergency surgeries like ventriculostomy. With the challenges in\npoor soft tissue contrast and a scarcity of well-annotated databases for\nclinical brain CTs, we introduce a novel uncertainty-aware ventricle\nsegmentation technique without the need of CT segmentation ground truths by\nleveraging diffusion-model-based domain adaptation. Specifically, our method\nemploys the diffusion Schr\\""odinger Bridge and an attention recurrent residual\nU-Net to capitalize on unpaired CT and MRI scans to derive automatic CT\nsegmentation from those of the MRIs, which are more accessible. Importantly, we\npropose an end-to-end, joint training framework of image translation and\nsegmentation tasks, and demonstrate its benefit over training individual tasks\nseparately. By comparing the proposed method against similar setups using two\ndifferent GAN models for domain adaptation (CycleGAN and CUT), we also reveal\nthe advantage of diffusion models towards improved segmentation and image\ntranslation quality. With a Dice score of 0.78$\\pm$0.27, our proposed method\noutperformed the compared methods, including SynSeg-Net, while providing\nintuitive uncertainty measures to further facilitate quality control of the\nautomatic segmentation outcomes. The implementation of our proposed method is\navailable at: https://github.com/HealthX-Lab/DiffusionSynCTSeg.\n', ""  In many clinical settings, the use of both Computed Tomography (CT) and\nMagnetic Resonance (MRI) is necessary to pursue a thorough understanding of the\npatient's anatomy and to plan a suitable therapeutical strategy; this is often\nthe case in MRI-based radiotherapy, where CT is always necessary to prepare the\ndose delivery, as it provides the essential information about the radiation\nabsorption properties of the tissues. Sometimes, MRI is preferred to contour\nthe target volumes. However, this approach is often not the most efficient, as\nit is more expensive, time-consuming and, most importantly, stressful for the\npatients. To overcome this issue, in this work, we analyse the capabilities of\ndifferent configurations of Deep Learning models to generate synthetic CT scans\nfrom MRI, leveraging the power of Generative Adversarial Networks (GANs) and,\nin particular, the CycleGAN architecture, capable of working in an unsupervised\nmanner and without paired images, which were not available. Several CycleGAN\nmodels were trained unsupervised to generate CT scans from different MRI\nmodalities with and without contrast agents. To overcome the problem of not\nhaving a ground truth, distribution-based metrics were used to assess the\nmodel's performance quantitatively, together with a qualitative evaluation\nwhere physicians were asked to differentiate between real and synthetic images\nto understand how realistic the generated images were. The results show how,\ndepending on the input modalities, the models can have very different\nperformances; however, models with the best quantitative results, according to\nthe distribution-based metrics used, can generate very difficult images to\ndistinguish from the real ones, even for physicians, demonstrating the\napproach's potential.\n""] , [""  Modelling the impact of a material's mesostructure on device level\nperformance typically requires access to 3D image data containing all the\nrelevant information to define the geometry of the simulation domain. This\nimage data must include sufficient contrast between phases to distinguish each\nmaterial, be of high enough resolution to capture the key details, but also\nhave a large enough field-of-view to be representative of the material in\ngeneral. It is rarely possible to obtain data with all of these properties from\na single imaging technique. In this paper, we present a method for combining\ninformation from pairs of distinct but complementary imaging techniques in\norder to accurately reconstruct the desired multi-phase, high resolution,\nrepresentative, 3D images. Specifically, we use deep convolutional generative\nadversarial networks to implement super-resolution, style transfer and\ndimensionality expansion. To demonstrate the widespread applicability of this\ntool, two pairs of datasets are used to validate the quality of the volumes\ngenerated by fusing the information from paired imaging techniques. Three key\nmesostructural metrics are calculated in each case to show the accuracy of this\nmethod. Having confidence in the accuracy of our method, we then demonstrate\nits power by applying to a real data pair from a lithium ion battery electrode,\nwhere the required 3D high resolution image data is not available anywhere in\nthe literature. We believe this approach is superior to previously reported\nstatistical material reconstruction methods both in terms of its fidelity and\nease of use. Furthermore, much of the data required to train this algorithm\nalready exists in the literature, waiting to be combined. As such, our\nopen-access code could precipitate a step change by generating the hard to\nobtain high quality image volumes necessary to simulate behaviour at the\nmesoscale.\n"", '  In this paper, an image recognition algorithm based on the combination of\ndeep learning and generative adversarial network (GAN) is studied, and compared\nwith traditional image recognition methods. The purpose of this study is to\nevaluate the advantages and application prospects of deep learning technology,\nespecially GAN, in the field of image recognition. Firstly, this paper reviews\nthe basic principles and techniques of traditional image recognition methods,\nincluding the classical algorithms based on feature extraction such as SIFT,\nHOG and their combination with support vector machine (SVM), random forest, and\nother classifiers. Then, the working principle, network structure, and unique\nadvantages of GAN in image generation and recognition are introduced. In order\nto verify the effectiveness of GAN in image recognition, a series of\nexperiments are designed and carried out using multiple public image data sets\nfor training and testing. The experimental results show that compared with\ntraditional methods, GAN has excellent performance in processing complex\nimages, recognition accuracy, and anti-noise ability. Specifically, Gans are\nbetter able to capture high-dimensional features and details of images,\nsignificantly improving recognition performance. In addition, Gans shows unique\nadvantages in dealing with image noise, partial missing information, and\ngenerating high-quality images.\n', '  Confocal fluorescence microscopy is one of the most accessible and widely\nused imaging techniques for the study of biological processes. Scanning\nconfocal microscopy allows the capture of high-quality images from 3D samples,\nyet suffers from well-known limitations such as photobleaching and\nphototoxicity of specimens caused by intense light exposure, which limits its\nuse in some applications, especially for living cells. Cellular damage can be\nalleviated by changing imaging parameters to reduce light exposure, often at\nthe expense of image quality. Machine/deep learning methods for single-image\nsuper-resolution (SISR) can be applied to restore image quality by upscaling\nlower-resolution (LR) images to produce high-resolution images (HR). These SISR\nmethods have been successfully applied to photo-realistic images due partly to\nthe abundance of publicly available data. In contrast, the lack of publicly\navailable data partly limits their application and success in scanning confocal\nmicroscopy. In this paper, we introduce a large scanning confocal microscopy\ndataset named SR-CACO-2 that is comprised of low- and high-resolution image\npairs marked for three different fluorescent markers. It allows the evaluation\nof performance of SISR methods on three different upscaling levels (X2, X4,\nX8). SR-CACO-2 contains the human epithelial cell line Caco-2 (ATCC HTB-37),\nand it is composed of 22 tiles that have been translated in the form of 9,937\nimage patches for experiments with SISR methods. Given the new SR-CACO-2\ndataset, we also provide benchmarking results for 15 state-of-the-art methods\nthat are representative of the main SISR families. Results show that these\nmethods have limited success in producing high-resolution textures, indicating\nthat SR-CACO-2 represents a challenging problem. Our dataset, code and\npretrained weights are available: https://github.com/sbelharbi/sr-caco-2.\n'] , [""  Synthetic tabular data generation becomes crucial when real data is limited,\nexpensive to collect, or simply cannot be used due to privacy concerns.\nHowever, producing good quality synthetic data is challenging. Several\nprobabilistic, statistical, generative adversarial networks (GANs), and\nvariational auto-encoder (VAEs) based approaches have been presented for\nsynthetic tabular data generation. Once generated, evaluating the quality of\nthe synthetic data is quite challenging. Some of the traditional metrics have\nbeen used in the literature but there is lack of a common, robust, and single\nmetric. This makes it difficult to properly compare the effectiveness of\ndifferent synthetic tabular data generation methods. In this paper we propose a\nnew universal metric, TabSynDex, for robust evaluation of synthetic data. The\nproposed metric assesses the similarity of synthetic data with real data\nthrough different component scores which evaluate the characteristics that are\ndesirable for ``high quality'' synthetic data. Being a single score metric and\nhaving an implicit bound, TabSynDex can also be used to observe and evaluate\nthe training of neural network based approaches. This would help in obtaining\ninsights that was not possible earlier. We present several baseline models for\ncomparative analysis of the proposed evaluation metric with existing generative\nmodels. We also give a comparative analysis between TabSynDex and existing\nsynthetic tabular data evaluation metrics. This shows the effectiveness and\nuniversality of our metric over the existing metrics. Source Code:\n\\url{https://github.com/vikram2000b/tabsyndex}\n"", '  Advancements in science rely on data sharing. In medicine, where personal\ndata are often involved, synthetic tabular data generated by generative\nadversarial networks (GANs) offer a promising avenue. However, existing GANs\nstruggle to capture the complexities of real-world tabular data, which often\ncontain a mix of continuous and categorical variables with potential imbalances\nand dependencies. We propose a novel correlation- and mean-aware loss function\ndesigned to address these challenges as a regularizer for GANs. To ensure a\nrigorous evaluation, we establish a comprehensive benchmarking framework using\nten real-world datasets and eight established tabular GAN baselines. The\nproposed loss function demonstrates statistically significant improvements over\nexisting methods in capturing the true data distribution, significantly\nenhancing the quality of synthetic data generated with GANs. The benchmarking\nframework shows that the enhanced synthetic data quality leads to improved\nperformance in downstream machine learning (ML) tasks, ultimately paving the\nway for easier data sharing.\n', '  Generative adversarial networks (GANs) have drawn considerable attention in\nrecent years for their proven capability in generating synthetic data which can\nbe utilised for multiple purposes. While GANs have demonstrated tremendous\nsuccesses in producing synthetic data samples that replicate the dynamics of\nthe original datasets, the validity of the synthetic data and the underlying\nprivacy concerns represent major challenges which are not sufficiently\naddressed. In this work, we design a cascaded tabular GAN framework (CasTGAN)\nfor generating realistic tabular data with a specific focus on the validity of\nthe output. In this context, validity refers to the the dependency between\nfeatures that can be found in the real data, but is typically misrepresented by\ntraditional generative models. Our key idea entails that employing a cascaded\narchitecture in which a dedicated generator samples each feature, the synthetic\noutput becomes more representative of the real data. Our experimental results\ndemonstrate that our model is capable of generating synthetic tabular data that\ncan be used for fitting machine learning models. In addition, our model\ncaptures well the constraints and the correlations between the features of the\nreal data, especially the high dimensional datasets. Furthermore, we evaluate\nthe risk of white-box privacy attacks on our model and subsequently show that\napplying some perturbations to the auxiliary learners in CasTGAN increases the\noverall robustness of our model against targeted attacks.\n']",Generative Adversarial Networks (GANs) and Their Applications,"""Generative Adversarial Networks for Imaging and Microscopy"""
116,"""Artistic Typography and Calligraphy"" , Poetry Generation and Analysis , Artistic Style Transfer and Generation","['artistic', 'artworks', 'artwork', 'creativity', 'aesthetics', 'aesthetic', 'typography', 'wordart', 'calligraphy', 'aesthetically'] , ['poetry2image', 'poetry', 'poems', 'poets', 'rhyme', 'rhyming', 'poem', 'syllables', 'literary', 'poetic'] , ['artworks', 'artists', 'artistic', 'stylistic', 'styles', 'artness', 'artwork', 'generative', 'stylization', 'artist']","['  As a communication channel, body movements have been widely explored in\nbehavioral studies and kinesics. Performing and visual arts share the same\ninterests but focus on documenting and representing human body movements, such\nas for dance notation and visual work creation. This paper investigates body\nmovements in oriental calligraphy and how to apply calligraphy principles to\nstimulate and archive body movements. Through an artwork (Wushu), the authors\nexperiment with an interactive and generative approach to engage the audience\'s\nbodily participation and archive the body movements as a compendium of\ngenerated calligraphy. The audience assumes the role of both writers and\nreaders; creating (""writing"") and appreciating (""reading"") the generated\ncalligraphy becomes a cyclical process within this infinite ""Book,"" which can\nmotivate further attention and discussions concerning Chinese characters and\ncalligraphy.\n', '  This paper introduces the WordArt Designer API, a novel framework for\nuser-driven artistic typography synthesis utilizing Large Language Models\n(LLMs) on ModelScope. We address the challenge of simplifying artistic\ntypography for non-professionals by offering a dynamic, adaptive, and\ncomputationally efficient alternative to traditional rigid templates. Our\napproach leverages the power of LLMs to understand and interpret user input,\nfacilitating a more intuitive design process. We demonstrate through various\ncase studies how users can articulate their aesthetic preferences and\nfunctional requirements, which the system then translates into unique and\ncreative typographic designs. Our evaluations indicate significant improvements\nin user satisfaction, design flexibility, and creative expression over existing\nsystems. The WordArt Designer API not only democratizes the art of typography\nbut also opens up new possibilities for personalized digital communication and\ndesign.\n', ""  MetaDesigner revolutionizes artistic typography synthesis by leveraging the\nstrengths of Large Language Models (LLMs) to drive a design paradigm centered\naround user engagement. At the core of this framework lies a multi-agent system\ncomprising the Pipeline, Glyph, and Texture agents, which collectively enable\nthe creation of customized WordArt, ranging from semantic enhancements to the\nimposition of complex textures. MetaDesigner incorporates a comprehensive\nfeedback mechanism that harnesses insights from multimodal models and user\nevaluations to refine and enhance the design process iteratively. Through this\nfeedback loop, the system adeptly tunes hyperparameters to align with\nuser-defined stylistic and thematic preferences, generating WordArt that not\nonly meets but exceeds user expectations of visual appeal and contextual\nrelevance. Empirical validations highlight MetaDesigner's capability to\neffectively serve diverse WordArt applications, consistently producing\naesthetically appealing and context-sensitive results.\n""] , [""  Large language models (LLMs) can now generate and recognize text in a wide\nrange of styles and genres, including highly specialized, creative genres like\npoetry. But what do LLMs really know about poetry? What can they know about\npoetry? We develop a task to evaluate how well LLMs recognize a specific aspect\nof poetry, poetic form, for more than 20 forms and formal elements in the\nEnglish language. Poetic form captures many different poetic features,\nincluding rhyme scheme, meter, and word or line repetition. We use this task to\nreflect on LLMs' current poetic capabilities, as well as the challenges and\npitfalls of creating NLP benchmarks for poetry and for other creative tasks. In\nparticular, we use this task to audit and reflect on the poems included in\npopular pretraining datasets. Our findings have implications for NLP\nresearchers interested in model evaluation, digital humanities and cultural\nanalytics scholars, and cultural heritage professionals.\n"", '  Text-to-image generation models often struggle with key element loss or\nsemantic confusion in tasks involving Chinese classical poetry.Addressing this\nissue through fine-tuning models needs considerable training costs.\nAdditionally, manual prompts for re-diffusion adjustments need professional\nknowledge. To solve this problem, we propose Poetry2Image, an iterative\ncorrection framework for images generated from Chinese classical poetry.\nUtilizing an external poetry dataset, Poetry2Image establishes an automated\nfeedback and correction loop, which enhances the alignment between poetry and\nimage through image generation models and subsequent re-diffusion modifications\nsuggested by large language models (LLM). Using a test set of 200 sentences of\nChinese classical poetry, the proposed method--when integrated with five\npopular image generation models--achieves an average element completeness of\n70.63%, representing an improvement of 25.56% over direct image generation. In\ntests of semantic correctness, our method attains an average semantic\nconsistency of 80.09%. The study not only promotes the dissemination of ancient\npoetry culture but also offers a reference for similar non-fine-tuning methods\nto enhance LLM generation.\n', '  Natural Language Generation (NLG), and more generally generative AI, are\namong the currently most impactful research fields. Creative NLG, such as\nautomatic poetry generation, is a fascinating niche in this area. While most\nprevious research has focused on forms of the Turing test when evaluating\nautomatic poetry generation - can humans distinguish between automatic and\nhuman generated poetry - we evaluate the diversity of automatically generated\npoetry, by comparing distributions of generated poetry to distributions of\nhuman poetry along structural, lexical, semantic and stylistic dimensions,\nassessing different model types (word vs. character-level, general purpose LLMs\nvs. poetry-specific models), including the very recent LLaMA3, and types of\nfine-tuning (conditioned vs. unconditioned). We find that current automatic\npoetry systems are considerably underdiverse along multiple dimensions - they\noften do not rhyme sufficiently, are semantically too uniform and even do not\nmatch the length distribution of human poetry. Our experiments reveal, however,\nthat style-conditioning and character-level modeling clearly increases\ndiversity across virtually all dimensions we explore. Our identified\nlimitations may serve as the basis for more genuinely diverse future poetry\ngeneration models.\n'] , [""  Large-scale Text-to-Image (T2I) models have rapidly gained prominence across\ncreative fields, generating visually compelling outputs from textual prompts.\nHowever, controlling these models to ensure consistent style remains\nchallenging, with existing methods necessitating fine-tuning and manual\nintervention to disentangle content and style. In this paper, we introduce\nStyleAligned, a novel technique designed to establish style alignment among a\nseries of generated images. By employing minimal `attention sharing' during the\ndiffusion process, our method maintains style consistency across images within\nT2I models. This approach allows for the creation of style-consistent images\nusing a reference style through a straightforward inversion operation. Our\nmethod's evaluation across diverse styles and text prompts demonstrates\nhigh-quality synthesis and fidelity, underscoring its efficacy in achieving\nconsistent style across various inputs.\n"", '  Recent text-to-image generative models such as Stable Diffusion are extremely\nadept at mimicking and generating copyrighted content, raising concerns amongst\nartists that their unique styles may be improperly copied. Understanding how\ngenerative models copy ""artistic style"" is more complex than duplicating a\nsingle image, as style is comprised by a set of elements (or signature) that\nfrequently co-occurs across a body of work, where each individual work may vary\nsignificantly. In our paper, we first reformulate the problem of ""artistic\ncopyright infringement"" to a classification problem over image sets, instead of\nprobing image-wise similarities. We then introduce ArtSavant, a practical\n(i.e., efficient and easy to understand) tool to (i) determine the unique style\nof an artist by comparing it to a reference dataset of works from 372 artists\ncurated from WikiArt, and (ii) recognize if the identified style reappears in\ngenerated images. We leverage two complementary methods to perform artistic\nstyle classification over image sets, includingTagMatch, which is a novel\ninherently interpretable and attributable method, making it more suitable for\nbroader use by non-technical stake holders (artists, lawyers, judges, etc).\nLeveraging ArtSavant, we then perform a large-scale empirical study to provide\nquantitative insight on the prevalence of artistic style copying across 3\npopular text-to-image generative models. Namely, amongst a dataset of prolific\nartists (including many famous ones), only 20% of them appear to have their\nstyles be at a risk of copying via simple prompting of today\'s popular\ntext-to-image generative models.\n', ""  In this work we introduce a novel medical image style transfer method,\nStyleMapper, that can transfer medical scans to an unseen style with access to\nlimited training data. This is made possible by training our model on unlimited\npossibilities of simulated random medical imaging styles on the training set,\nmaking our work more computationally efficient when compared with other style\ntransfer methods. Moreover, our method enables arbitrary style transfer:\ntransferring images to styles unseen in training. This is useful for medical\nimaging, where images are acquired using different protocols and different\nscanner models, resulting in a variety of styles that data may need to be\ntransferred between. Methods: Our model disentangles image content from style\nand can modify an image's style by simply replacing the style encoding with one\nextracted from a single image of the target style, with no additional\noptimization required. This also allows the model to distinguish between\ndifferent styles of images, including among those that were unseen in training.\nWe propose a formal description of the proposed model. Results: Experimental\nresults on breast magnetic resonance images indicate the effectiveness of our\nmethod for style transfer. Conclusion: Our style transfer method allows for the\nalignment of medical images taken with different scanners into a single unified\nstyle dataset, allowing for the training of other downstream tasks on such a\ndataset for tasks such as classification, object detection and others.\n""]",Artistic and Creative AI Applications,Artistic Style Transfer and Generation
117,"Narrative Analysis and Storytelling , ""Procedural Narrative Generation for Games""","['narratives', 'storytelling', 'narrative', 'writing', 'linguistic', 'conversations', 'stories', 'summaries', 'texts', 'discourse'] , ['storytelling', 'narratives', 'narrative', 'gameplay', 'procedural', 'storyline', 'rpgs', 'drama', 'characters', 'dialogue']","['  Empathy serves as a cornerstone in enabling prosocial behaviors, and can be\nevoked through sharing of personal experiences in stories. While empathy is\ninfluenced by narrative content, intuitively, people respond to the way a story\nis told as well, through narrative style. Yet the relationship between empathy\nand narrative style is not fully understood. In this work, we empirically\nexamine and quantify this relationship between style and empathy using LLMs and\nlarge-scale crowdsourcing studies. We introduce a novel, theory-based taxonomy,\nHEART (Human Empathy and Narrative Taxonomy) that delineates elements of\nnarrative style that can lead to empathy with the narrator of a story. We\nestablish the performance of LLMs in extracting narrative elements from HEART,\nshowing that prompting with our taxonomy leads to reasonable, human-level\nannotations beyond what prior lexicon-based methods can do. To show empirical\nuse of our taxonomy, we collect a dataset of empathy judgments of stories via a\nlarge-scale crowdsourcing study with N=2,624 participants. We show that\nnarrative elements extracted via LLMs, in particular, vividness of emotions and\nplot volume, can elucidate the pathways by which narrative style cultivates\nempathy towards personal stories. Our work suggests that such models can be\nused for narrative analyses that lead to human-centered social and behavioral\ninsights.\n', ""  This research explores the nuanced differences in texts produced by AI and\nthose written by humans, aiming to elucidate how language is expressed\ndifferently by AI and humans. Through comprehensive statistical data analysis,\nthe study investigates various linguistic traits, patterns of creativity, and\npotential biases inherent in human-written and AI- generated texts. The\nsignificance of this research lies in its contribution to understanding AI's\ncreative capabilities and its impact on literature, communication, and societal\nframeworks. By examining a meticulously curated dataset comprising 500K essays\nspanning diverse topics and genres, generated by LLMs, or written by humans,\nthe study uncovers the deeper layers of linguistic expression and provides\ninsights into the cognitive processes underlying both AI and human-driven\ntextual compositions. The analysis revealed that human-authored essays tend to\nhave a higher total word count on average than AI-generated essays but have a\nshorter average word length compared to AI- generated essays, and while both\ngroups exhibit high levels of fluency, the vocabulary diversity of Human\nauthored content is higher than AI generated content. However, AI- generated\nessays show a slightly higher level of novelty, suggesting the potential for\ngenerating more original content through AI systems. The paper addresses\nchallenges in assessing the language generation capabilities of AI models and\nemphasizes the importance of datasets that reflect the complexities of human-AI\ncollaborative writing. Through systematic preprocessing and rigorous\nstatistical analysis, this study offers valuable insights into the evolving\nlandscape of AI-generated content and informs future developments in natural\nlanguage processing (NLP).\n"", ""  Evaluations of creative stories generated by large language models (LLMs)\noften focus on objective properties of the text, such as its style, coherence,\nand toxicity. While these metrics are indispensable, they do not speak to a\nstory's subjective, psychological impact from a reader's perspective. We\nintroduce the Psychological Depth Scale (PDS), a novel framework rooted in\nliterary theory that measures an LLM's ability to produce authentic and\nnarratively complex stories that provoke emotion, empathy, and engagement. We\nempirically validate our framework by showing that humans can consistently\nevaluate stories based on PDS (0.72 Krippendorff's alpha). We also explore\ntechniques for automating the PDS to easily scale future analyses. GPT-4o,\ncombined with a novel Mixture-of-Personas (MoP) prompting strategy, achieves an\naverage Spearman correlation of $0.51$ with human judgment while Llama-3-70B\nscores as high as 0.68 for empathy. Finally, we compared the depth of stories\nauthored by both humans and LLMs. Surprisingly, GPT-4 stories either surpassed\nor were statistically indistinguishable from highly-rated human-written stories\nsourced from Reddit. By shifting the focus from text to reader, the\nPsychological Depth Scale is a validated, automated, and systematic means of\nmeasuring the capacity of LLMs to connect with humans through the stories they\ntell.\n""] , [""  World-building, the process of developing both the narrative and physical\nworld of a game, plays a vital role in the game's experience.\nCritically-acclaimed independent and AAA video games are praised for strong\nworld-building, with game maps that masterfully intertwine with and elevate the\nnarrative, captivating players and leaving a lasting impression. However,\ndesigning game maps that support a desired narrative is challenging, as it\nrequires satisfying complex constraints from various considerations. Most\nexisting map generation methods focus on considerations about gameplay\nmechanics or map topography, while the need to support the story is typically\nneglected. As a result, extensive manual adjustment is still required to design\na game world that facilitates particular stories. In this work, we approach\nthis problem by introducing an extra layer of plot facility layout design that\nis independent of the underlying map generation method in a world-building\npipeline.\n  Concretely, we define (plot) facility layout tasks as the tasks of assigning\nconcrete locations on a game map to abstract locations mentioned in a given\nstory (plot facilities), following spatial constraints derived from the story.\nWe present two methods for solving these tasks automatically: an evolutionary\ncomputation based approach through Covariance Matrix Adaptation Evolution\nStrategy (CMA-ES), and a Reinforcement Learning (RL) based approach. We develop\na method of generating datasets of facility layout tasks, create a gym-like\nenvironment for experimenting with and evaluating different methods, and\nfurther analyze the two methods with comprehensive experiments, aiming to\nprovide insights for solving facility layout tasks. We will release the code\nand a dataset containing 10, 000 tasks of different scales.\n"", ""  This research introduces Procedural Artificial Narrative using Generative AI\n(PANGeA), a structured approach for leveraging large language models (LLMs),\nguided by a game designer's high-level criteria, to generate narrative content\nfor turn-based role-playing video games (RPGs). Distinct from prior\napplications of LLMs used for video game design, PANGeA innovates by not only\ngenerating game level data (which includes, but is not limited to, setting, key\nitems, and non-playable characters (NPCs)), but by also fostering dynamic,\nfree-form interactions between the player and the environment that align with\nthe procedural game narrative. The NPCs generated by PANGeA are\npersonality-biased and express traits from the Big 5 Personality Model in their\ngenerated responses. PANGeA addresses challenges behind ingesting free-form\ntext input, which can prompt LLM responses beyond the scope of the game\nnarrative. A novel validation system that uses the LLM's intelligence evaluates\ntext input and aligns generated responses with the unfolding narrative. Making\nthese interactions possible, PANGeA is supported by a server that hosts a\ncustom memory system that supplies context for augmenting generated responses\nthus aligning them with the procedural narrative. For its broad application,\nthe server has a REST interface enabling any game engine to integrate directly\nwith PANGeA, as well as an LLM interface adaptable with local or private LLMs.\nPANGeA's ability to foster dynamic narrative generation by aligning responses\nwith the procedural narrative is demonstrated through an empirical study and\nablation test of two versions of a demo game. These are, a custom,\nbrowser-based GPT and a Unity demo. As the results show, PANGeA holds potential\nto assist game designers in using LLMs to generate narrative-consistent content\neven when provided varied and unpredictable, free-form text input.\n"", '  Automated plot generation for games enhances the player\'s experience by\nproviding rich and immersive narrative experience that adapts to the player\'s\nactions. Traditional approaches adopt a symbolic narrative planning method\nwhich limits the scale and complexity of the generated plot by requiring\nextensive knowledge engineering work. Recent advancements use Large Language\nModels (LLMs) to drive the behavior of virtual characters, allowing plots to\nemerge from interactions between characters and their environments. However,\nthe emergent nature of such decentralized plot generation makes it difficult\nfor authors to direct plot progression. We propose a novel plot creation\nworkflow that mediates between a writer\'s authorial intent and the emergent\nbehaviors from LLM-driven character simulation, through a novel authorial\nstructure called ""abstract acts"". The writers define high-level plot outlines\nthat are later transformed into concrete character action sequences via an\nLLM-based narrative planning process, based on the game world state. The\nprocess creates ""living stories"" that dynamically adapt to various game world\nstates, resulting in narratives co-created by the author, character simulation,\nand player. We present StoryVerse as a proof-of-concept system to demonstrate\nthis plot creation workflow. We showcase the versatility of our approach with\nexamples in different stories and game environments.\n']",Narrative Generation and Analysis in Games and Interactive Media,Narrative Analysis and Storytelling
118,Food Pairing and Recipe Generation,"['recipes', 'cuisine', 'foods', 'recipe', 'recipemc', 'chef', 'food', 'culinary', 'meals', 'meal']","['  In the early 2000s, renowned chef Heston Blumenthal formulated his ""food\npairing"" hypothesis, positing that if foods share many flavor compounds, then\nthey tend to taste good when eaten together. In 2011, Ahn et al. conducted a\nstudy using a dataset of recipes, ingredients, and flavor compounds, finding\nthat, in Western cuisine, ingredients in recipes often share more flavor\ncompounds than expected by chance, indicating a natural tendency towards food\npairing. Building upon Ahn\'s research, our work applies state-of-the-art\ncollaborative filtering techniques to the dataset, providing a tool that can\nrecommend new foods to add in recipes, retrieve missing ingredients and advise\nagainst certain combinations. We create our recommender in two ways, by taking\ninto account ingredients appearances in recipes or shared flavor compounds\nbetween foods. While our analysis confirms the existence of food pairing, the\nrecipe-based recommender performs significantly better than the flavor-based\none, leading to the conclusion that food pairing is just one of the principles\nto take into account when creating recipes. Furthermore, and more\ninterestingly, we find that food pairing in data is mostly due to trivial\ncouplings of very similar ingredients, leading to a reconsideration of its\ncurrent role in recipes, from being an already existing feature to a key to\nopen up new scenarios in gastronomy. Our flavor-based recommender can thus\nleverage this novel concept and provide a new tool to lead culinary innovation.\n', ""  Large Multi-modal Models (LMMs) have significantly advanced a variety of\nvision-language tasks. The scalability and availability of high-quality\ntraining data play a pivotal role in the success of LMMs. In the realm of food,\nwhile comprehensive food datasets such as Recipe1M offer an abundance of\ningredient and recipe information, they often fall short of providing ample\ndata for nutritional analysis. The Recipe1M+ dataset, despite offering a subset\nfor nutritional evaluation, is limited in the scale and accuracy of nutrition\ninformation. To bridge this gap, we introduce Uni-Food, a unified food dataset\nthat comprises over 100,000 images with various food labels, including\ncategories, ingredients, recipes, and ingredient-level nutritional information.\nUni-Food is designed to provide a more holistic approach to food data analysis,\nthereby enhancing the performance and capabilities of LMMs in this domain. To\nmitigate the conflicts arising from multi-task supervision during fine-tuning\nof LMMs, we introduce a novel Linear Rectification Mixture of Diverse Experts\n(RoDE) approach. RoDE utilizes a diverse array of experts to address tasks of\nvarying complexity, thereby facilitating the coordination of trainable\nparameters, i.e., it allocates more parameters for more complex tasks and,\nconversely, fewer parameters for simpler tasks. RoDE implements linear\nrectification union to refine the router's functionality, thereby enhancing the\nefficiency of sparse task allocation. These design choices endow RoDE with\nfeatures that ensure GPU memory efficiency and ease of optimization. Our\nexperimental results validate the effectiveness of our proposed approach in\naddressing the inherent challenges of food-related multitasking.\n"", '  Food computing has emerged as a prominent multidisciplinary field of research\nin recent years. An ambitious goal of food computing is to develop end-to-end\nintelligent systems capable of autonomously producing recipe information for a\nfood image. Current image-to-recipe methods are retrieval-based and their\nsuccess depends heavily on the dataset size and diversity, as well as the\nquality of learned embeddings. Meanwhile, the emergence of powerful\nattention-based vision and language models presents a promising avenue for\naccurate and generalizable recipe generation, which has yet to be extensively\nexplored. This paper proposes FIRE, a novel multimodal methodology tailored to\nrecipe generation in the food computing domain, which generates the food title,\ningredients, and cooking instructions based on input food images. FIRE\nleverages the BLIP model to generate titles, utilizes a Vision Transformer with\na decoder for ingredient extraction, and employs the T5 model to generate\nrecipes incorporating titles and ingredients as inputs. We showcase two\npractical applications that can benefit from integrating FIRE with large\nlanguage model prompting: recipe customization to fit recipes to user\npreferences and recipe-to-code transformation to enable automated cooking\nprocesses. Our experimental findings validate the efficacy of our proposed\napproach, underscoring its potential for future advancements and widespread\nadoption in food computing.\n']",Food Informatics and Recipe Generation,Food Pairing and Recipe Generation
119,"3D Human Pose Estimation from 2D Data , ""6D Pose Estimation and Scene Reconstruction""","['pose', 'poses', '3d', 'camera', '3dhp', 'cameras', 'posture', '3dsp', 'human3', 'dor3d'] , ['pose', 'poses', 'camera', 'cameras', 'scenes', '3d', 'articulated', 'vision', 'scene', 'slam']","['  We present a generative approach to forecast long-term future human behavior\nin 3D, requiring only weak supervision from readily available 2D human action\ndata. This is a fundamental task enabling many downstream applications. The\nrequired ground-truth data is hard to capture in 3D (mocap suits, expensive\nsetups) but easy to acquire in 2D (simple RGB cameras). Thus, we design our\nmethod to only require 2D RGB data at inference time while being able to\ngenerate 3D human motion sequences. We use a differentiable 2D projection\nscheme in an autoregressive manner for weak supervision, and an adversarial\nloss for 3D regularization. Our method predicts long and complex human behavior\nsequences (e.g., cooking, assembly) consisting of multiple sub-actions. We\ntackle this in a semantically hierarchical manner, jointly predicting\nhigh-level coarse action labels together with their low-level fine-grained\nrealizations as characteristic 3D human poses. We observe that these two action\nrepresentations are coupled in nature, and joint prediction benefits both\naction and pose forecasting. Our experiments demonstrate the complementary\nnature of joint action and 3D pose prediction: our joint approach outperforms\neach task treated individually, enables robust longer-term sequence prediction,\nand improves over alternative approaches to forecast actions and characteristic\n3D poses.\n', '  Monocular Human Pose Estimation (HPE) aims at determining the 3D positions of\nhuman joints from a single 2D image captured by a camera. However, a single 2D\npoint in the image may correspond to multiple points in 3D space. Typically,\nthe uniqueness of the 2D-3D relationship is approximated using an orthographic\nor weak-perspective camera model. In this study, instead of relying on\napproximations, we advocate for utilizing the full perspective camera model.\nThis involves estimating camera parameters and establishing a precise,\nunambiguous 2D-3D relationship. To do so, we introduce the EPOCH framework,\ncomprising two main components: the pose lifter network (LiftNet) and the pose\nregressor network (RegNet). LiftNet utilizes the full perspective camera model\nto precisely estimate the 3D pose in an unsupervised manner. It takes a 2D pose\nand camera parameters as inputs and produces the corresponding 3D pose\nestimation. These inputs are obtained from RegNet, which starts from a single\nimage and provides estimates for the 2D pose and camera parameters. RegNet\nutilizes only 2D pose data as weak supervision. Internally, RegNet predicts a\n3D pose, which is then projected to 2D using the estimated camera parameters.\nThis process enables RegNet to establish the unambiguous 2D-3D relationship.\nOur experiments show that modeling the lifting as an unsupervised task with a\ncamera in-the-loop results in better generalization to unseen data. We obtain\nstate-of-the-art results for the 3D HPE on the Human3.6M and MPI-INF-3DHP\ndatasets. Our code is available at: [Github link upon acceptance, see\nsupplementary materials].\n', '  Its numerous applications make multi-human 3D pose estimation a remarkably\nimpactful area of research. Nevertheless, assuming a multiple-view system\ncomposed of several regular RGB cameras, 3D multi-pose estimation presents\nseveral challenges. First of all, each person must be uniquely identified in\nthe different views to separate the 2D information provided by the cameras.\nSecondly, the 3D pose estimation process from the multi-view 2D information of\neach person must be robust against noise and potential occlusions in the\nscenario. In this work, we address these two challenges with the help of deep\nlearning. Specifically, we present a model based on Graph Neural Networks\ncapable of predicting the cross-view correspondence of the people in the\nscenario along with a Multilayer Perceptron that takes the 2D points to yield\nthe 3D poses of each person. These two models are trained in a self-supervised\nmanner, thus avoiding the need for large datasets with 3D annotations.\n'] , [""  In this work, we introduce a novel method for calculating the 6DoF pose of an\nobject using a single RGB-D image. Unlike existing methods that either directly\npredict objects' poses or rely on sparse keypoints for pose recovery, our\napproach addresses this challenging task using dense correspondence, i.e., we\nregress the object coordinates for each visible pixel. Our method leverages\nexisting object detection methods. We incorporate a re-projection mechanism to\nadjust the camera's intrinsic matrix to accommodate cropping in RGB-D images.\nMoreover, we transform the 3D object coordinates into a residual\nrepresentation, which can effectively reduce the output space and yield\nsuperior performance. We conducted extensive experiments to validate the\nefficacy of our approach for 6D pose estimation. Our approach outperforms most\nprevious methods, especially in occlusion scenarios, and demonstrates notable\nimprovements over the state-of-the-art methods. Our code is available on\nhttps://github.com/AI-Application-and-Integration-Lab/RDPN6D.\n"", '  Pose regression networks predict the camera pose of a query image relative to\na known environment. Within this family of methods, absolute pose regression\n(APR) has recently shown promising accuracy in the range of a few centimeters\nin position error. APR networks encode the scene geometry implicitly in their\nweights. To achieve high accuracy, they require vast amounts of training data\nthat, realistically, can only be created using novel view synthesis in a\ndays-long process. This process has to be repeated for each new scene again and\nagain. We present a new approach to pose regression, map-relative pose\nregression (marepo), that satisfies the data hunger of the pose regression\nnetwork in a scene-agnostic fashion. We condition the pose regressor on a\nscene-specific map representation such that its pose predictions are relative\nto the scene map. This allows us to train the pose regressor across hundreds of\nscenes to learn the generic relation between a scene-specific map\nrepresentation and the camera pose. Our map-relative pose regressor can be\napplied to new map representations immediately or after mere minutes of\nfine-tuning for the highest accuracy. Our approach outperforms previous pose\nregression methods by far on two public datasets, indoor and outdoor. Code is\navailable: https://nianticlabs.github.io/marepo\n', '  Reconstructing 4D scenes from video inputs is a crucial yet challenging task.\nConventional methods usually rely on the assumptions of multi-view video\ninputs, known camera parameters, or static scenes, all of which are typically\nabsent under in-the-wild scenarios. In this paper, we relax all these\nconstraints and tackle a highly ambitious but practical task, which we termed\nas AnyV4D: we assume only one monocular video is available without any camera\nparameters as input, and we aim to recover the dynamic 4D world alongside the\ncamera poses. To this end, we introduce GFlow, a new framework that utilizes\nonly 2D priors (depth and optical flow) to lift a video (3D) to a 4D explicit\nrepresentation, entailing a flow of Gaussian splatting through space and time.\nGFlow first clusters the scene into still and moving parts, then applies a\nsequential optimization process that optimizes camera poses and the dynamics of\n3D Gaussian points based on 2D priors and scene clustering, ensuring fidelity\namong neighboring points and smooth movement across frames. Since dynamic\nscenes always introduce new content, we also propose a new pixel-wise\ndensification strategy for Gaussian points to integrate new visual content.\nMoreover, GFlow transcends the boundaries of mere 4D reconstruction; it also\nenables tracking of any points across frames without the need for prior\ntraining and segments moving objects from the scene in an unsupervised way.\nAdditionally, the camera poses of each frame can be derived from GFlow,\nallowing for rendering novel views of a video scene through changing camera\npose. By employing the explicit representation, we may readily conduct\nscene-level or object-level editing as desired, underscoring its versatility\nand power. Visit our project website at: https://littlepure2333.github.io/GFlow\n']",3D Pose Estimation and Scene Understanding,3D Human Pose Estimation from 2D Data
120,"""3D Scene Generation and Editing"" , Scene Graph Generation with Semantic Diversity , ""3D Avatar Generation and Animation""","['3d', 'scenes', 'scene', 'depth', 'viewpoints', 'generative', 'rendering', 'view', '3dgs', 'viewpoint'] , ['attention', 'scene', 'visual', 'memory', 'semantic', 'mpnn', 'features', 'videos', 'objects', 'segmentation'] , ['articulated', 'pose', 'poses', 'portrait', '3d', 'animation', 'avatars', 'vr', 'avatar', 'camera']","['  Editing a local region or a specific object in a 3D scene represented by a\nNeRF or consistently blending a new realistic object into the scene is\nchallenging, mainly due to the implicit nature of the scene representation. We\npresent Blended-NeRF, a robust and flexible framework for editing a specific\nregion of interest in an existing NeRF scene, based on text prompts, along with\na 3D ROI box. Our method leverages a pretrained language-image model to steer\nthe synthesis towards a user-provided text prompt, along with a 3D MLP model\ninitialized on an existing NeRF scene to generate the object and blend it into\na specified region in the original scene. We allow local editing by localizing\na 3D ROI box in the input scene, and blend the content synthesized inside the\nROI with the existing scene using a novel volumetric blending technique. To\nobtain natural looking and view-consistent results, we leverage existing and\nnew geometric priors and 3D augmentations for improving the visual fidelity of\nthe final result. We test our framework both qualitatively and quantitatively\non a variety of real 3D scenes and text prompts, demonstrating realistic\nmulti-view consistent results with much flexibility and diversity compared to\nthe baselines. Finally, we show the applicability of our framework for several\n3D editing applications, including adding new objects to a scene,\nremoving/replacing/altering existing objects, and texture conversion.\n', '  Text-driven 3D scene generation techniques have made rapid progress in recent\nyears. Their success is mainly attributed to using existing generative models\nto iteratively perform image warping and inpainting to generate 3D scenes.\nHowever, these methods heavily rely on the outputs of existing models, leading\nto error accumulation in geometry and appearance that prevent the models from\nbeing used in various scenarios (e.g., outdoor and unreal scenarios). To\naddress this limitation, we generatively refine the newly generated local views\nby querying and aggregating global 3D information, and then progressively\ngenerate the 3D scene. Specifically, we employ a tri-plane features-based NeRF\nas a unified representation of the 3D scene to constrain global 3D consistency,\nand propose a generative refinement network to synthesize new contents with\nhigher quality by exploiting the natural image prior from 2D diffusion model as\nwell as the global 3D information of the current scene. Our extensive\nexperiments demonstrate that, in comparison to previous methods, our approach\nsupports wide variety of scene generation and arbitrary camera trajectories\nwith improved visual quality and 3D consistency.\n', '  Generating 3D scenes is a challenging open problem, which requires\nsynthesizing plausible content that is fully consistent in 3D space. While\nrecent methods such as neural radiance fields excel at view synthesis and 3D\nreconstruction, they cannot synthesize plausible details in unobserved regions\nsince they lack a generative capability. Conversely, existing generative\nmethods are typically not capable of reconstructing detailed, large-scale\nscenes in the wild, as they use limited-capacity 3D scene representations,\nrequire aligned camera poses, or rely on additional regularizers. In this work,\nwe introduce the first diffusion model able to perform fast, detailed\nreconstruction and generation of real-world 3D scenes. To achieve this, we make\nthree contributions. First, we introduce a new neural scene representation,\nIB-planes, that can efficiently and accurately represent large 3D scenes,\ndynamically allocating more capacity as needed to capture details visible in\neach image. Second, we propose a denoising-diffusion framework to learn a prior\nover this novel 3D scene representation, using only 2D images without the need\nfor any additional supervision signal such as masks or depths. This supports 3D\nreconstruction and generation in a unified architecture. Third, we develop a\nprincipled approach to avoid trivial 3D solutions when integrating the\nimage-based rendering with the diffusion model, by dropping out representations\nof some images. We evaluate the model on several challenging datasets of real\nand synthetic images, and demonstrate superior results on generation, novel\nview synthesis and 3D reconstruction.\n'] , ['  The scene graph generation (SGG) task involves detecting objects within an\nimage and predicting predicates that represent the relationships between the\nobjects. However, in SGG benchmark datasets, each subject-object pair is\nannotated with a single predicate even though a single predicate may exhibit\ndiverse semantics (i.e., semantic diversity), existing SGG models are trained\nto predict the one and only predicate for each pair. This in turn results in\nthe SGG models to overlook the semantic diversity that may exist in a\npredicate, thus leading to biased predictions. In this paper, we propose a\nnovel model-agnostic Semantic Diversity-aware Prototype-based Learning (DPL)\nframework that enables unbiased predictions based on the understanding of the\nsemantic diversity of predicates. Specifically, DPL learns the regions in the\nsemantic space covered by each predicate to distinguish among the various\ndifferent semantics that a single predicate can represent. Extensive\nexperiments demonstrate that our proposed model-agnostic DPL framework brings\nsignificant performance improvement on existing SGG models, and also\neffectively understands the semantic diversity of predicates.\n', ""  Scene Graph Generation (SGG) endeavors to predict the relationships between\nsubjects and objects in a given image. Nevertheless, the long-tail distribution\nof relations often leads to biased prediction on coarse labels, presenting a\nsubstantial hurdle in SGG. To address this issue, researchers focus on unbiased\nSGG and introduce data transfer methods to transfer coarse-grained predicates\ninto fine-grained ones across the entire dataset. However, these methods\nencounter two primary challenges: 1) They overlook the inherent context\nconstraints imposed by subject-object pairs, leading to erroneous relations\ntransfer. 2) Additional retraining process are required after the data\ntransfer, which incurs substantial computational costs. To overcome these\nlimitations, we introduce the first plug-and-play one-stage data transfer\npipeline in SGG, termed Adaptive Label Finetuning (ALF), which eliminates the\nneed for extra retraining sessions and meanwhile significantly enhance models'\nrelation recognition capability across various SGG benchmark approaches.\nSpecifically, ALF consists of two components: Adaptive Label Construction (ALC)\nand Adaptive Iterative Learning (AIL). By imposing Predicate-Context\nConstraints within relation space, ALC adaptively re-ranks and selects\ncandidate relations in reference to model's predictive logits utilizing the\nRestriction-Based Judgment techniques, achieving robust relation transfer.\nSupervised with labels transferred by ALC, AIL iteratively finetunes the SGG\nmodels in an auto-regressive manner, which mitigates the substantial\ncomputational costs arising from the retraining process. Extensive experiments\ndemonstrate that ALF achieves a 16% improvement in mR@100 compared to the\ntypical SGG method Motif, with only a 6% increase in calculation costs compared\nto the state-of-the-art method IETrans.\n"", '  Scene graph generation (SGG) models have suffered from inherent problems\nregarding the benchmark datasets such as the long-tailed predicate distribution\nand missing annotation problems. In this work, we aim to alleviate the\nlong-tailed problem of SGG by utilizing unannotated triplets. To this end, we\nintroduce a Self-Training framework for SGG (ST-SGG) that assigns pseudo-labels\nfor unannotated triplets based on which the SGG models are trained. While there\nhas been significant progress in self-training for image recognition, designing\na self-training framework for the SGG task is more challenging due to its\ninherent nature such as the semantic ambiguity and the long-tailed distribution\nof predicate classes. Hence, we propose a novel pseudo-labeling technique for\nSGG, called Class-specific Adaptive Thresholding with Momentum (CATM), which is\na model-agnostic framework that can be applied to any existing SGG models.\nFurthermore, we devise a graph structure learner (GSL) that is beneficial when\nadopting our proposed self-training framework to the state-of-the-art\nmessage-passing neural network (MPNN)-based SGG models. Our extensive\nexperiments verify the effectiveness of ST-SGG on various SGG models,\nparticularly in enhancing the performance on fine-grained predicate classes.\n'] , ['  Hand avatars play a pivotal role in a wide array of digital interfaces,\nenhancing user immersion and facilitating natural interaction within virtual\nenvironments. While previous studies have focused on photo-realistic hand\nrendering, little attention has been paid to reconstruct the hand geometry with\nfine details, which is essential to rendering quality. In the realms of\nextended reality and gaming, on-the-fly rendering becomes imperative. To this\nend, we introduce an expressive hand avatar, named XHand, that is designed to\ncomprehensively generate hand shape, appearance, and deformations in real-time.\nTo obtain fine-grained hand meshes, we make use of three feature embedding\nmodules to predict hand deformation displacements, albedo, and linear blending\nskinning weights, respectively. To achieve photo-realistic hand rendering on\nfine-grained meshes, our method employs a mesh-based neural renderer by\nleveraging mesh topological consistency and latent codes from embedding\nmodules. During training, a part-aware Laplace smoothing strategy is proposed\nby incorporating the distinct levels of regularization to effectively maintain\nthe necessary details and eliminate the undesired artifacts. The experimental\nevaluations on InterHand2.6M and DeepHandMesh datasets demonstrate the efficacy\nof XHand, which is able to recover high-fidelity geometry and texture for hand\nanimations across diverse poses in real-time. To reproduce our results, we will\nmake the full implementation publicly available at\nhttps://github.com/agnJason/XHand.\n', ""  We introduce VOODOO XP: a 3D-aware one-shot head reenactment method that can\ngenerate highly expressive facial expressions from any input driver video and a\nsingle 2D portrait. Our solution is real-time, view-consistent, and can be\ninstantly used without calibration or fine-tuning. We demonstrate our solution\non a monocular video setting and an end-to-end VR telepresence system for\ntwo-way communication. Compared to 2D head reenactment methods, 3D-aware\napproaches aim to preserve the identity of the subject and ensure\nview-consistent facial geometry for novel camera poses, which makes them\nsuitable for immersive applications. While various facial disentanglement\ntechniques have been introduced, cutting-edge 3D-aware neural reenactment\ntechniques still lack expressiveness and fail to reproduce complex and\nfine-scale facial expressions. We present a novel cross-reenactment\narchitecture that directly transfers the driver's facial expressions to\ntransformer blocks of the input source's 3D lifting module. We show that highly\neffective disentanglement is possible using an innovative multi-stage\nself-supervision approach, which is based on a coarse-to-fine strategy,\ncombined with an explicit face neutralization and 3D lifted frontalization\nduring its initial training stage. We further integrate our novel head\nreenactment solution into an accessible high-fidelity VR telepresence system,\nwhere any person can instantly build a personalized neural head avatar from any\nphoto and bring it to life using the headset. We demonstrate state-of-the-art\nperformance in terms of expressiveness and likeness preservation on a large set\nof diverse subjects and capture conditions.\n"", '  Recent advances in generative diffusion models have enabled the previously\nunfeasible capability of generating 3D assets from a single input image or a\ntext prompt. In this work, we aim to enhance the quality and functionality of\nthese models for the task of creating controllable, photorealistic human\navatars. We achieve this by integrating a 3D morphable model into the\nstate-of-the-art multi-view-consistent diffusion approach. We demonstrate that\naccurate conditioning of a generative pipeline on the articulated 3D model\nenhances the baseline model performance on the task of novel view synthesis\nfrom a single image. More importantly, this integration facilitates a seamless\nand accurate incorporation of facial expression and body pose control into the\ngeneration process. To the best of our knowledge, our proposed framework is the\nfirst diffusion model to enable the creation of fully 3D-consistent,\nanimatable, and photorealistic human avatars from a single image of an unseen\nsubject; extensive quantitative and qualitative evaluations demonstrate the\nadvantages of our approach over existing state-of-the-art avatar creation\nmodels on both novel view and novel expression synthesis tasks. The code for\nour project is publicly available.\n']",Computer-Generated 3D Content Creation,"""3D Scene Generation and Editing"""
121,"Text-Guided Image Editing and Generation , Video Generation and Editing Models , Controlled Text Generation , Text-Driven Video Editing and Generation","['attention', 'generative', 'editing', 'images', 'scenes', 'text', 'guided', 'image', 'visual', 'scene'] , ['videos', 'videopoet', 'videoscore', 'videodrafter', 'videofeedback', 'scenes', 'videodirectorgpt', 'magicvideo', 'generative', 'video'] , ['nlg', 'text', 'generate', 'texts', 'sentences', 'generation', 'generated', 'constraints', 'language', 'lexical'] , ['videos', 'videoshop', 'gans', 'clips', 'scenes', 'video', 'frames', 'compression', 'editing', 'generative']","['  With the advancement of image-to-image diffusion models guided by text,\nsignificant progress has been made in image editing. However, a persistent\nchallenge remains in seamlessly incorporating objects into images based on\ntextual instructions, without relying on extra user-provided guidance. Text and\nimages are inherently distinct modalities, bringing out difficulties in fully\ncapturing the semantic intent conveyed through language and accurately\ntranslating that into the desired visual modifications. Therefore, text-guided\nimage editing models often produce generations with residual object attributes\nthat do not fully align with human expectations. To address this challenge, the\nmodels should comprehend the image content effectively away from a disconnect\nbetween the provided textual editing prompts and the actual modifications made\nto the image. In our paper, we propose a novel method called Locate and Forget\n(LaF), which effectively locates potential target concepts in the image for\nmodification by comparing the syntactic trees of the target prompt and scene\ndescriptions in the input image, intending to forget their existence clues in\nthe generated image. Compared to the baselines, our method demonstrates its\nsuperiority in text-guided image editing tasks both qualitatively and\nquantitatively.\n', '  The generation of high-quality human images through text-to-image (T2I)\nmethods is a significant yet challenging task. Distinct from general image\ngeneration, human image synthesis must satisfy stringent criteria related to\nhuman pose, anatomy, and alignment with textual prompts, making it particularly\ndifficult to achieve realistic results. Recent advancements in T2I generation\nbased on diffusion models have shown promise, yet challenges remain in meeting\nhuman-specific preferences. In this paper, we introduce a novel approach\ntailored specifically for human image generation utilizing Direct Preference\nOptimization (DPO). Specifically, we introduce an efficient method for\nconstructing a specialized DPO dataset for training human image generation\nmodels without the need for costly human feedback. We also propose a modified\nloss function that enhances the DPO training process by minimizing artifacts\nand improving image fidelity. Our method demonstrates its versatility and\neffectiveness in generating human images, including personalized text-to-image\ngeneration. Through comprehensive evaluations, we show that our approach\nsignificantly advances the state of human image generation, achieving superior\nresults in terms of natural anatomies, poses, and text-image alignment.\n', '  Personalized text-to-image (P-T2I) generation aims to create new, text-guided\nimages featuring the personalized subject with a few reference images. However,\nbalancing the trade-off relationship between prompt fidelity and identity\npreservation remains a critical challenge. To address the issue, we propose a\nnovel P-T2I method called Layout-and-Retouch, consisting of two stages: 1)\nlayout generation and 2) retouch. In the first stage, our step-blended\ninference utilizes the inherent sample diversity of vanilla T2I models to\nproduce diversified layout images, while also enhancing prompt fidelity. In the\nsecond stage, multi-source attention swapping integrates the context image from\nthe first stage with the reference image, leveraging the structure from the\ncontext image and extracting visual features from the reference image. This\nachieves high prompt fidelity while preserving identity characteristics.\nThrough our extensive experiments, we demonstrate that our method generates a\nwide variety of images with diverse layouts while maintaining the unique\nidentity features of the personalized objects, even with challenging text\nprompts. This versatility highlights the potential of our framework to handle\ncomplex conditions, significantly enhancing the diversity and applicability of\npersonalized image synthesis.\n'] , ['  Recent video generative models primarily rely on carefully written text\nprompts for specific tasks, like inpainting or style editing. They require\nlabor-intensive textual descriptions for input videos, hindering their\nflexibility to adapt personal/raw videos to user specifications. This paper\nproposes RACCooN, a versatile and user-friendly video-to-paragraph-to-video\ngenerative framework that supports multiple video editing capabilities such as\nremoval, addition, and modification, through a unified pipeline. RACCooN\nconsists of two principal stages: Video-to-Paragraph (V2P) and\nParagraph-to-Video (P2V). In the V2P stage, we automatically describe video\nscenes in well-structured natural language, capturing both the holistic context\nand focused object details. Subsequently, in the P2V stage, users can\noptionally refine these descriptions to guide the video diffusion model,\nenabling various modifications to the input video, such as removing, changing\nsubjects, and/or adding new objects. The proposed approach stands out from\nother methods through several significant contributions: (1) RACCooN suggests a\nmulti-granular spatiotemporal pooling strategy to generate well-structured\nvideo descriptions, capturing both the broad context and object details without\nrequiring complex human annotations, simplifying precise video content editing\nbased on text for users. (2) Our video generative model incorporates\nauto-generated narratives or instructions to enhance the quality and accuracy\nof the generated content. It supports the addition of video objects,\ninpainting, and attribute modification within a unified framework, surpassing\nexisting video editing and inpainting benchmarks. The proposed framework\ndemonstrates impressive versatile capabilities in video-to-paragraph\ngeneration, video content editing, and can be incorporated into other SoTA\nvideo generative models for further enhancement.\n', '  Image generative models have made significant progress in generating\nrealistic and diverse images, supported by comprehensive guidance from various\nevaluation metrics. However, current video generative models struggle to\ngenerate even short video clips, with limited tools that provide insights for\nimprovements. Current video evaluation metrics are simple adaptations of image\nmetrics by switching the embeddings with video embedding networks, which may\nunderestimate the unique characteristics of video. Our analysis reveals that\nthe widely used Frechet Video Distance (FVD) has a stronger emphasis on the\nspatial aspect than the temporal naturalness of video and is inherently\nconstrained by the input size of the embedding networks used, limiting it to 16\nframes. Additionally, it demonstrates considerable instability and diverges\nfrom human evaluations. To address the limitations, we propose STREAM, a new\nvideo evaluation metric uniquely designed to independently evaluate spatial and\ntemporal aspects. This feature allows comprehensive analysis and evaluation of\nvideo generative models from various perspectives, unconstrained by video\nlength. We provide analytical and experimental evidence demonstrating that\nSTREAM provides an effective evaluation tool for both visual and temporal\nquality of videos, offering insights into area of improvement for video\ngenerative models. To the best of our knowledge, STREAM is the first evaluation\nmetric that can separately assess the temporal and spatial aspects of videos.\nOur code is available at https://github.com/pro2nit/STREAM.\n', ""  Recent text-to-video (T2V) generation methods have seen significant\nadvancements. However, the majority of these works focus on producing short\nvideo clips of a single event (i.e., single-scene videos). Meanwhile, recent\nlarge language models (LLMs) have demonstrated their capability in generating\nlayouts and programs to control downstream visual modules. This prompts an\nimportant question: can we leverage the knowledge embedded in these LLMs for\ntemporally consistent long video generation? In this paper, we propose\nVideoDirectorGPT, a novel framework for consistent multi-scene video generation\nthat uses the knowledge of LLMs for video content planning and grounded video\ngeneration. Specifically, given a single text prompt, we first ask our video\nplanner LLM (GPT-4) to expand it into a 'video plan', which includes the scene\ndescriptions, the entities with their respective layouts, the background for\neach scene, and consistency groupings of the entities. Next, guided by this\nvideo plan, our video generator, named Layout2Vid, has explicit control over\nspatial layouts and can maintain temporal consistency of entities across\nmultiple scenes, while being trained only with image-level annotations. Our\nexperiments demonstrate that our proposed VideoDirectorGPT framework\nsubstantially improves layout and movement control in both single- and\nmulti-scene video generation and can generate multi-scene videos with\nconsistency, while achieving competitive performance with SOTAs in open-domain\nsingle-scene T2V generation. Detailed ablation studies, including dynamic\nadjustment of layout control strength with an LLM and video generation with\nuser-provided images, confirm the effectiveness of each component of our\nframework and its future potential.\n""] , [""  Recent approaches to controlled text generation (CTG) often involve\nmanipulating the weights or logits of base language models (LMs) at decoding\ntime. However, these methods are inapplicable to latest black-box LMs and\nineffective at preserving the core semantics of the base LM's original\ngenerations. In this work, we propose Locate&Edit(L&E), an efficient and\nflexible energy-based approach to CTG, which edits text outputs from a base LM\nusing off-the-shelf energy models. Given text outputs from the base LM, L&E\nfirst locates spans that are most relevant to constraints (e.g., toxicity)\nutilizing energy models, and then edits these spans by replacing them with more\nsuitable alternatives. Importantly, our method is compatible with black-box\nLMs, as it requires only the text outputs. Also, since L&E doesn't mandate\nspecific architecture for its component models, it can work with a diverse\ncombination of available off-the-shelf models. Moreover, L&E preserves the base\nLM's original generations, by selectively modifying constraint-related aspects\nof the texts and leaving others unchanged. These targeted edits also ensure\nthat L&E operates efficiently. Our experiments confirm that L&E achieves\nsuperior semantic preservation of the base LM generations and speed, while\nsimultaneously obtaining competitive or improved constraint satisfaction.\nFurthermore, we analyze how the granularity of energy distribution impacts CTG\nperformance and find that fine-grained, regression-based energy models improve\nconstraint satisfaction, compared to conventional binary classifier energy\nmodels.\n"", '  Controlled Text Generation (CTG) aims to produce texts that exhibit specific\ndesired attributes. In this study, we introduce a pluggable CTG framework for\nLarge Language Models (LLMs) named Dynamic Attribute Graphs-based controlled\ntext generation (DATG). This framework utilizes an attribute scorer to evaluate\nthe attributes of sentences generated by LLMs and constructs dynamic attribute\ngraphs. DATG modulates the occurrence of key attribute words and key\nanti-attribute words, achieving effective attribute control without\ncompromising the original capabilities of the model. We conduct experiments\nacross four datasets in two tasks: toxicity mitigation and sentiment\ntransformation, employing five LLMs as foundational models. Our findings\nhighlight a remarkable enhancement in control accuracy, achieving a peak\nimprovement of 19.29% over baseline methods in the most favorable task across\nfour datasets. Additionally, we observe a significant decrease in perplexity,\nmarkedly improving text fluency.\n', ""  Controllable text generation is a growing field within natural language\ngeneration (NLG) that focuses on producing text that meets specific constraints\nin real-world applications. Previous approaches, such as plug-and-play\ncontrollers (PPCs), aimed to steer the properties of generated text in a\nflexible manner. However, these methods often compromised the integrity of the\nlanguage model's decoding process, resulting in less smooth text generation.\nAlternatively, other techniques utilized multiple attribute prompts to align\nthe generated text with desired attributes, but this approach required prompt\ndesign for each attribute and was dependent on the size of the language model.\nThis paper introduces a novel method for flexible attribute control in text\ngeneration using pre-trained language models (PLMs). The proposed approach aims\nto enhance the fluency of generated text by guiding the generation process with\nPPCs. The key idea is to dynamically adjust the distribution of generated text\nby modifying prompts, effectively constraining the output space of the language\nmodel and influencing the desired attribute. To enable smooth cooperation\nbetween the PLM and the PPC, our work innovatively proposes a new model\nfine-tuning method: Reinforcement Learning with Dynamic Adjust Feedback\n(RLDAF).This fine-tuning process adapts a small subset of the language model's\nparameters based on the generating actions taken during the PPC control\nprocess. The resulting harmonious collaboration between the PLM and PPC leads\nto improved smoothness in text generation during inference. Extensive\nexperiments were conducted on the SST2 dataset, and the proposed method\noutperformed previous approaches in various evaluation metrics, including text\nfluency and attribute consistency.\n""] , ['  Text-driven diffusion-based video editing presents a unique challenge not\nencountered in image editing literature: establishing real-world motion. Unlike\nexisting video editing approaches, here we focus on score distillation sampling\nto circumvent the standard reverse diffusion process and initiate optimization\nfrom videos that already exhibit natural motion. Our analysis reveals that\nwhile video score distillation can effectively introduce new content indicated\nby target text, it can also cause significant structure and motion deviation.\nTo counteract this, we propose to match space-time self-similarities of the\noriginal video and the edited video during the score distillation. Thanks to\nthe use of score distillation, our approach is model-agnostic, which can be\napplied for both cascaded and non-cascaded video diffusion frameworks. Through\nextensive comparisons with leading methods, our approach demonstrates its\nsuperiority in altering appearances while accurately preserving the original\nstructure and motion.\n', '  Recent advances in text-to-image (T2I) diffusion models have enabled\nimpressive image generation capabilities guided by text prompts. However,\nextending these techniques to video generation remains challenging, with\nexisting text-to-video (T2V) methods often struggling to produce high-quality\nand motion-consistent videos. In this work, we introduce Control-A-Video, a\ncontrollable T2V diffusion model that can generate videos conditioned on text\nprompts and reference control maps like edge and depth maps. To tackle video\nquality and motion consistency issues, we propose novel strategies to\nincorporate content prior and motion prior into the diffusion-based generation\nprocess. Specifically, we employ a first-frame condition scheme to transfer\nvideo generation from the image domain. Additionally, we introduce\nresidual-based and optical flow-based noise initialization to infuse motion\npriors from reference videos, promoting relevance among frame latents for\nreduced flickering. Furthermore, we present a Spatio-Temporal Reward Feedback\nLearning (ST-ReFL) algorithm that optimizes the video diffusion model using\nmultiple reward models for video quality and motion consistency, leading to\nsuperior outputs. Comprehensive experiments demonstrate that our framework\ngenerates higher-quality, more consistent videos compared to existing\nstate-of-the-art methods in controllable text-to-video generation\n', '  Diffusion models have made tremendous progress in text-driven image and video\ngeneration. Now text-to-image foundation models are widely applied to various\ndownstream image synthesis tasks, such as controllable image generation and\nimage editing, while downstream video synthesis tasks are less explored for\nseveral reasons. First, it requires huge memory and computation overhead to\ntrain a video generation foundation model. Even with video foundation models,\nadditional costly training is still required for downstream video synthesis\ntasks. Second, although some works extend image diffusion models into videos in\na training-free manner, temporal consistency cannot be well preserved. Finally,\nthese adaption methods are specifically designed for one task and fail to\ngeneralize to different tasks. To mitigate these issues, we propose a\ntraining-free general-purpose video synthesis framework, coined as {\\bf\nBIVDiff}, via bridging specific image diffusion models and general\ntext-to-video foundation diffusion models. Specifically, we first use a\nspecific image diffusion model (e.g., ControlNet and Instruct Pix2Pix) for\nframe-wise video generation, then perform Mixed Inversion on the generated\nvideo, and finally input the inverted latents into the video diffusion models\n(e.g., VidRD and ZeroScope) for temporal smoothing. This decoupled framework\nenables flexible image model selection for different purposes with strong task\ngeneralization and high efficiency. To validate the effectiveness and general\nuse of BIVDiff, we perform a wide range of video synthesis tasks, including\ncontrollable video generation, video editing, video inpainting, and\noutpainting.\n']",Text-Guided Visual Content Generation and Editing,Text-Driven Video Editing and Generation
122,"Controllable Video Animation , Video Summarization Techniques , Image and Video Matting Techniques , ""Video Generation and Animation of Dance and Motion""","['videos', 'filmmaking', 'animations', 'animation', 'motions', 'motion', '3d', 'camera', 'generative', 'mofa_video'] , ['summarizer', 'summarizing', 'summarization', 'summaries', 'videos', 'smmarization', 'videoxum', 'caption', 'youtube', 'audiovisual'] , ['matting', 'mattes', 'matte', 'pixels', 'segmentation', 'pixel', 'videos', 'ximagenet', 'images', 'objectpi'] , ['choreography', 'dance', 'rhythm', 'motions', 'animations', 'animation', 'motion', 'pose', 'poses', 'videos']","['  We present MOFA-Video, an advanced controllable image animation method that\ngenerates video from the given image using various additional controllable\nsignals (such as human landmarks reference, manual trajectories, and another\neven provided video) or their combinations. This is different from previous\nmethods which only can work on a specific motion domain or show weak control\nabilities with diffusion prior. To achieve our goal, we design several\ndomain-aware motion field adapters (\\ie, MOFA-Adapters) to control the\ngenerated motions in the video generation pipeline. For MOFA-Adapters, we\nconsider the temporal motion consistency of the video and generate the dense\nmotion flow from the given sparse control conditions first, and then, the\nmulti-scale features of the given image are wrapped as a guided feature for\nstable video diffusion generation. We naively train two motion adapters for the\nmanual trajectories and the human landmarks individually since they both\ncontain sparse information about the control. After training, the MOFA-Adapters\nin different domains can also work together for more controllable video\ngeneration. Project Page: https://myniuuu.github.io/MOFA_Video/\n', ""  Filmmaking and animation production often require sophisticated techniques\nfor coordinating camera transitions and object movements, typically involving\nlabor-intensive real-world capturing. Despite advancements in generative AI for\nvideo creation, achieving precise control over motion for interactive video\nasset generation remains challenging. To this end, we propose Image Conductor,\na method for precise control of camera transitions and object movements to\ngenerate video assets from a single image. An well-cultivated training strategy\nis proposed to separate distinct camera and object motion by camera LoRA\nweights and object LoRA weights. To further address cinematographic variations\nfrom ill-posed trajectories, we introduce a camera-free guidance technique\nduring inference, enhancing object movements while eliminating camera\ntransitions. Additionally, we develop a trajectory-oriented video motion data\ncuration pipeline for training. Quantitative and qualitative experiments\ndemonstrate our method's precision and fine-grained control in generating\nmotion-controllable videos from images, advancing the practical application of\ninteractive video synthesis. Project webpage available at\nhttps://liyaowei-stu.github.io/project/ImageConductor/\n"", '  Human image animation involves generating videos from a character photo,\nallowing user control and unlocking potential for video and movie production.\nWhile recent approaches yield impressive results using high-quality training\ndata, the inaccessibility of these datasets hampers fair and transparent\nbenchmarking. Moreover, these approaches prioritize 2D human motion and\noverlook the significance of camera motions in videos, leading to limited\ncontrol and unstable video generation. To demystify the training data, we\npresent HumanVid, the first large-scale high-quality dataset tailored for human\nimage animation, which combines crafted real-world and synthetic data. For the\nreal-world data, we compile a vast collection of copyright-free real-world\nvideos from the internet. Through a carefully designed rule-based filtering\nstrategy, we ensure the inclusion of high-quality videos, resulting in a\ncollection of 20K human-centric videos in 1080P resolution. Human and camera\nmotion annotation is accomplished using a 2D pose estimator and a SLAM-based\nmethod. For the synthetic data, we gather 2,300 copyright-free 3D avatar assets\nto augment existing available 3D assets. Notably, we introduce a rule-based\ncamera trajectory generation method, enabling the synthetic pipeline to\nincorporate diverse and precise camera motion annotation, which can rarely be\nfound in real-world data. To verify the effectiveness of HumanVid, we establish\na baseline model named CamAnimate, short for Camera-controllable Human\nAnimation, that considers both human and camera motions as conditions. Through\nextensive experimentation, we demonstrate that such simple baseline training on\nour HumanVid achieves state-of-the-art performance in controlling both human\npose and camera motions, setting a new benchmark. Code and data will be\npublicly available at https://github.com/zhenzhiwang/HumanVid/.\n'] , ['  With the surge in the amount of video data, video summarization techniques,\nincluding visual-modal(VM) and textual-modal(TM) summarization, are attracting\nmore and more attention. However, unimodal summarization inevitably loses the\nrich semantics of the video. In this paper, we focus on a more comprehensive\nvideo summarization task named Bimodal Semantic Summarization of Videos\n(BiSSV). Specifically, we first construct a large-scale dataset, BIDS, in\n(video, VM-Summary, TM-Summary) triplet format. Unlike traditional processing\nmethods, our construction procedure contains a VM-Summary extraction algorithm\naiming to preserve the most salient content within long videos. Based on BIDS,\nwe propose a Unified framework UBiSS for the BiSSV task, which models the\nsaliency information in the video and generates a TM-summary and VM-summary\nsimultaneously. We further optimize our model with a list-wise ranking-based\nobjective to improve its capacity to capture highlights. Lastly, we propose a\nmetric, $NDCG_{MS}$, to provide a joint evaluation of the bimodal summary.\nExperiments show that our unified framework achieves better performance than\nmulti-stage summarization pipelines. Code and data are available at\nhttps://github.com/MeiYutingg/UBiSS.\n', '  Video summarization aims to distill the most important information from a\nsource video to produce either an abridged clip or a textual narrative.\nTraditionally, different methods have been proposed depending on whether the\noutput is a video or text, thus ignoring the correlation between the two\nsemantically related tasks of visual summarization and textual summarization.\nWe propose a new joint video and text summarization task. The goal is to\ngenerate both a shortened video clip along with the corresponding textual\nsummary from a long video, collectively referred to as a cross-modal summary.\nThe generated shortened video clip and text narratives should be semantically\nwell aligned. To this end, we first build a large-scale human-annotated dataset\n-- VideoXum (X refers to different modalities). The dataset is reannotated\nbased on ActivityNet. After we filter out the videos that do not meet the\nlength requirements, 14,001 long videos remain in our new dataset. Each video\nin our reannotated dataset has human-annotated video summaries and the\ncorresponding narrative summaries. We then design a novel end-to-end model --\nVTSUM-BILP to address the challenges of our proposed task. Moreover, we propose\na new metric called VT-CLIPScore to help evaluate the semantic consistency of\ncross-modality summary. The proposed model achieves promising performance on\nthis new task and establishes a benchmark for future research.\n', ""  Video summarization aims to create short, accurate, and cohesive summaries of\nlonger videos. Despite the existence of various video summarization datasets, a\nnotable limitation is their limited amount of source videos, which hampers the\neffective fine-tuning of advanced large vision-language models (VLMs).\nAdditionally, most existing datasets are created for video-to-video\nsummarization, overlooking the contemporary need for multimodal video content\nsummarization. Recent efforts have been made to expand from unimodal to\nmultimodal video summarization, categorizing the task into three sub-tasks\nbased on the summary's modality: video-to-video (V2V), video-to-text (V2T), and\na combination of video and text summarization (V2VT). However, the textual\nsummaries in previous multimodal datasets are inadequate. To address these\nissues, we introduce Instruct-V2Xum, a cross-modal video summarization dataset\nfeaturing 30,000 diverse videos sourced from YouTube, with lengths ranging from\n40 to 940 seconds and an average summarization ratio of 16.39\\%. Each video\nsummary in Instruct-V2Xum is paired with a textual summary that references\nspecific frame indexes, facilitating the generation of aligned video and\ntextual summaries. In addition, we propose a new video summarization framework\nnamed V2Xum-LLM. V2Xum-LLM, specifically V2Xum-LLaMA in this study, is the\nfirst framework that unifies different video summarization tasks into one large\nlanguage model's (LLM) text decoder and achieves task-controllable video\nsummarization with temporal prompts and task instructions. Experiments show\nthat V2Xum-LLaMA outperforms strong baseline models on multiple video\nsummarization tasks. Furthermore, we propose an enhanced evaluation metric for\nV2V and V2VT summarization tasks.\n""] , ['  Matting with a static background, often referred to as ``Background Matting""\n(BGM), has garnered significant attention within the computer vision community\ndue to its pivotal role in various practical applications like webcasting and\nphoto editing. Nevertheless, achieving highly accurate background matting\nremains a formidable challenge, primarily owing to the limitations inherent in\nconventional RGB images. These limitations manifest in the form of\nsusceptibility to varying lighting conditions and unforeseen shadows.\n  In this paper, we leverage the rich depth information provided by the\nRGB-Depth (RGB-D) cameras to enhance background matting performance in\nreal-time, dubbed DART. Firstly, we adapt the original RGB-based BGM algorithm\nto incorporate depth information. The resulting model\'s output undergoes\nrefinement through Bayesian inference, incorporating a background depth prior.\nThe posterior prediction is then translated into a ""trimap,"" which is\nsubsequently fed into a state-of-the-art matting algorithm to generate more\nprecise alpha mattes. To ensure real-time matting capabilities, a critical\nrequirement for many real-world applications, we distill the backbone of our\nmodel from a larger and more versatile BGM network. Our experiments demonstrate\nthe superior performance of the proposed method. Moreover, thanks to the\ndistillation operation, our method achieves a remarkable processing speed of 33\nframes per second (fps) on a mid-range edge-computing device. This high\nefficiency underscores DART\'s immense potential for deployment in mobile\napplications}\n', '  Human matting is a foundation task in image and video processing, where human\nforeground pixels are extracted from the input. Prior works either improve the\naccuracy by additional guidance or improve the temporal consistency of a single\ninstance across frames. We propose a new framework MaGGIe, Masked Guided\nGradual Human Instance Matting, which predicts alpha mattes progressively for\neach human instances while maintaining the computational cost, precision, and\nconsistency. Our method leverages modern architectures, including transformer\nattention and sparse convolution, to output all instance mattes simultaneously\nwithout exploding memory and latency. Although keeping constant inference costs\nin the multiple-instance scenario, our framework achieves robust and versatile\nperformance on our proposed synthesized benchmarks. With the higher quality\nimage and video matting benchmarks, the novel multi-instance synthesis approach\nfrom publicly available sources is introduced to increase the generalization of\nmodels in real-world scenarios.\n', '  Human instance matting aims to estimate an alpha matte for each human\ninstance in an image, which is extremely challenging and has rarely been\nstudied so far. Despite some efforts to use instance segmentation to generate a\ntrimap for each instance and apply trimap-based matting methods, the resulting\nalpha mattes are often inaccurate due to inaccurate segmentation. In addition,\nthis approach is computationally inefficient due to multiple executions of the\nmatting method. To address these problems, this paper proposes a novel\nEnd-to-End Human Instance Matting (E2E-HIM) framework for simultaneous multiple\ninstance matting in a more efficient manner. Specifically, a general perception\nnetwork first extracts image features and decodes instance contexts into latent\ncodes. Then, a united guidance network exploits spatial attention and semantics\nembedding to generate united semantics guidance, which encodes the locations\nand semantic correspondences of all instances. Finally, an instance matting\nnetwork decodes the image features and united semantics guidance to predict all\ninstance-level alpha mattes. In addition, we construct a large-scale human\ninstance matting dataset (HIM-100K) comprising over 100,000 human images with\ninstance alpha matte labels. Experiments on HIM-100K demonstrate the proposed\nE2E-HIM outperforms the existing methods on human instance matting with 50%\nlower errors and 5X faster speed (6 instances in a 640X640 image). Experiments\non the PPM-100, RWP-636, and P3M datasets demonstrate that E2E-HIM also\nachieves competitive performance on traditional human matting.\n'] , ['  Recent years have seen a tremendous improvement in the quality of video\ngeneration and editing approaches. While several techniques focus on editing\nappearance, few address motion. Current approaches using text, trajectories, or\nbounding boxes are limited to simple motions, so we specify motions with a\nsingle motion reference video instead. We further propose to use a pre-trained\nimage-to-video model rather than a text-to-video model. This approach allows us\nto preserve the exact appearance and position of a target object or scene and\nhelps disentangle appearance from motion. Our method, called motion-textual\ninversion, leverages our observation that image-to-video models extract\nappearance mainly from the (latent) image input, while the text/image embedding\ninjected via cross-attention predominantly controls motion. We thus represent\nmotion using text/image embedding tokens. By operating on an inflated\nmotion-text embedding containing multiple text/image embedding tokens per\nframe, we achieve a high temporal motion granularity. Once optimized on the\nmotion reference video, this embedding can be applied to various target images\nto generate videos with semantically similar motions. Our approach does not\nrequire spatial alignment between the motion reference video and target image,\ngeneralizes across various domains, and can be applied to various tasks such as\nfull-body and face reenactment, as well as controlling the motion of inanimate\nobjects and the camera. We empirically demonstrate the effectiveness of our\nmethod in the semantic video motion transfer task, significantly outperforming\nexisting methods in this context.\n', '  Synthesizing human motion with a global structure, such as a choreography, is\na challenging task. Existing methods tend to concentrate on local smooth pose\ntransitions and neglect the global context or the theme of the motion. In this\nwork, we present a music-driven motion synthesis framework that generates\nlong-term sequences of human motions which are synchronized with the input\nbeats, and jointly form a global structure that respects a specific dance\ngenre. In addition, our framework enables generation of diverse motions that\nare controlled by the content of the music, and not only by the beat. Our\nmusic-driven dance synthesis framework is a hierarchical system that consists\nof three levels: pose, motif, and choreography. The pose level consists of an\nLSTM component that generates temporally coherent sequences of poses. The motif\nlevel guides sets of consecutive poses to form a movement that belongs to a\nspecific distribution using a novel motion perceptual-loss. And the\nchoreography level selects the order of the performed movements and drives the\nsystem to follow the global structure of a dance genre. Our results demonstrate\nthe effectiveness of our music-driven framework to generate natural and\nconsistent movements on various dance types, having control over the content of\nthe synthesized motions, and respecting the overall structure of the dance.\n', ""  Automated choreography advances by generating dance from music. Current\nmethods create skeleton keypoint sequences, not full dance videos, and cannot\nmake specific individuals dance, limiting their real-world use. These methods\nalso need precise keypoint annotations, making data collection difficult and\nrestricting the use of self-made video datasets. To overcome these challenges,\nwe introduce a novel task: generating dance videos directly from images of\nindividuals guided by music. This task enables the dance generation of specific\nindividuals without requiring keypoint annotations, making it more versatile\nand applicable to various situations. Our solution, the Dance Any Beat\nDiffusion model (DabFusion), utilizes a reference image and a music piece to\ngenerate dance videos featuring various dance types and choreographies. The\nmusic is analyzed by our specially designed music encoder, which identifies\nessential features including dance style, movement, and rhythm. DabFusion\nexcels in generating dance videos not only for individuals in the training\ndataset but also for any previously unseen person. This versatility stems from\nits approach of generating latent optical flow, which contains all necessary\nmotion information to animate any person in the image. We evaluate DabFusion's\nperformance using the AIST++ dataset, focusing on video quality, audio-video\nsynchronization, and motion-music alignment. We propose a 2D Motion-Music\nAlignment Score (2D-MM Align), which builds on the Beat Alignment Score to more\neffectively evaluate motion-music alignment for this new task. Experiments show\nthat our DabFusion establishes a solid baseline for this innovative task. Video\nresults can be found on our project page: https://DabFusion.github.io.\n""]",Video and Image Processing Techniques,Controllable Video Animation
123,"Remote Sensing Change Detection , Change Point Detection Methods","['ctm_remote_sensing_change_detection', 'cnn', 'changes', 'changeanywhere', 'change', 'detection', 'features', 'sensing', 'imagery', 'mapchange'] , ['changepoints', 'changepoint', 'observations', 'detection', 'detecting', 'cusum', 'detect', 'outliers', 'changes', 'cumulative']","[""  Remote sensing change detection is crucial for understanding the dynamics of\nour planet's surface, facilitating the monitoring of environmental changes,\nevaluating human impact, predicting future trends, and supporting\ndecision-making. In this work, we introduce a novel approach for change\ndetection that can leverage off-the-shelf, unlabeled remote sensing images in\nthe training process by pre-training a Denoising Diffusion Probabilistic Model\n(DDPM) - a class of generative models used in image synthesis. DDPMs learn the\ntraining data distribution by gradually converting training images into a\nGaussian distribution using a Markov chain. During inference (i.e., sampling),\nthey can generate a diverse set of samples closer to the training distribution,\nstarting from Gaussian noise, achieving state-of-the-art image synthesis\nresults. However, in this work, our focus is not on image synthesis but on\nutilizing it as a pre-trained feature extractor for the downstream application\nof change detection. Specifically, we fine-tune a lightweight change classifier\nutilizing the feature representations produced by the pre-trained DDPM\nalongside change labels. Experiments conducted on the LEVIR-CD, WHU-CD,\nDSIFN-CD, and CDD datasets demonstrate that the proposed DDPM-CD method\nsignificantly outperforms the existing state-of-the-art change detection\nmethods in terms of F1 score, IoU, and overall accuracy, highlighting the\npivotal role of pre-trained DDPM as a feature extractor for downstream\napplications. We have made both the code and pre-trained models available at\nhttps://github.com/wgcban/ddpm-cd\n"", '  Remote sensing change detection (CD) is a pivotal technique that pinpoints\nchanges on a global scale based on multi-temporal images. With the recent\nexpansion of deep learning, supervised deep learning-based CD models have shown\nsatisfactory performance. However, CD sample labeling is very time-consuming as\nit is densely labeled and requires expert knowledge. To alleviate this problem,\nwe introduce ChangeAnywhere, a novel CD sample generation method using the\nsemantic latent diffusion model and single-temporal images. Specifically,\nChangeAnywhere leverages the relative ease of acquiring large single-temporal\nsemantic datasets to generate large-scale, diverse, and semantically annotated\nbi-temporal CD datasets. ChangeAnywhere captures the two essentials of CD\nsamples, i.e., change implies semantically different, and non-change implies\nreasonable change under the same semantic constraints. We generated\nChangeAnywhere-100K, the largest synthesis CD dataset with 100,000 pairs of CD\nsamples based on the proposed method. The ChangeAnywhere-100K significantly\nimproved both zero-shot and few-shot performance on two CD benchmark datasets\nfor various deep learning-based CD models, as demonstrated by transfer\nexperiments. This paper delineates the enormous potential of ChangeAnywhere for\nCD sample generation and demonstrates the subsequent enhancement of model\nperformance. Therefore, ChangeAnywhere offers a potent tool for remote sensing\nCD. All codes and pre-trained models will be available at\nhttps://github.com/tangkai-RS/ChangeAnywhere.\n', '  The existing change detection(CD) methods can be summarized as the\nvisual-first change detection (ViFi-CD) paradigm, which first extracts change\nfeatures from visual differences and then assigns them specific semantic\ninformation. However, CD is essentially dependent on change regions of interest\n(CRoIs), meaning that the CD results are directly determined by the semantics\nchanges of interest, making its primary image factor semantic of interest\nrather than visual. The ViFi-CD paradigm can only assign specific semantics of\ninterest to specific change features extracted from visual differences, leading\nto the inevitable omission of potential CRoIs and the inability to adapt to\ndifferent CRoI CD tasks. In other words, changes in other CRoIs cannot be\ndetected by the ViFi-CD method without retraining the model or significantly\nmodifying the method. This paper introduces a new CD paradigm, the\nsemantic-first CD (SeFi-CD) paradigm. The core idea of SeFi-CD is to first\nperceive the dynamic semantics of interest and then visually search for change\nfeatures related to the semantics. Based on the SeFi-CD paradigm, we designed\nAnything You Want Change Detection (AUWCD). Experiments on public datasets\ndemonstrate that the AUWCD outperforms the current state-of-the-art CD methods,\nachieving an average F1 score 5.01\\% higher than that of these advanced\nsupervised baselines on the SECOND dataset, with a maximum increase of 13.17\\%.\nThe proposed SeFi-CD offers a novel CD perspective and approach.\n'] , [""  The objective of change point detection is to identify abrupt changes at\npotentially multiple points within a data sequence. This task is particularly\nchallenging in the online setting where various types of changes can occur,\nincluding shifts in both the marginal and joint distributions of the data. This\npaper tackles these challenges by sequentially tracking correlation matrices on\nthe Riemannian geometry, where the geodesic distances accurately capture the\ndevelopment of correlations. We propose Rio-CPD, a non-parametric\ncorrelation-aware online change point detection framework that combines the\nRiemannian geometry of the manifold of symmetric positive definite matrices and\nthe cumulative sum statistic (CUSUM) for detecting change points. Rio-CPD\nenhances CUSUM by computing the geodesic distance from present observations to\nthe Fr\\'echet mean of previous observations. With careful choice of metrics\nequipped to the Riemannian geometry, Rio-CPD is simple and computationally\nefficient. Experimental results on both synthetic and real-world datasets\ndemonstrate that Rio-CPD outperforms existing methods in detection accuracy and\nefficiency.\n"", '  In the problem of quickest change detection (QCD), a change occurs at some\nunknown time in the distribution of a sequence of independent observations.\nThis work studies a QCD problem where the change is either a bad change, which\nwe aim to detect, or a confusing change, which is not of our interest. Our\nobjective is to detect a bad change as quickly as possible while avoiding\nraising a false alarm for pre-change or a confusing change. We identify a\nspecific set of pre-change, bad change, and confusing change distributions that\npose challenges beyond the capabilities of standard Cumulative Sum (CuSum)\nprocedures. Proposing novel CuSum-based detection procedures, S-CuSum and\nJ-CuSum, leveraging two CuSum statistics, we offer solutions applicable across\nall kinds of pre-change, bad change, and confusing change distributions. For\nboth S-CuSum and J-CuSum, we provide analytical performance guarantees and\nvalidate them by numerical results. Furthermore, both procedures are\ncomputationally efficient as they only require simple recursive updates.\n', '  Change-point detection, detecting an abrupt change in the data distribution\nfrom sequential data, is a fundamental problem in statistics and machine\nlearning. CUSUM is a popular statistical method for online change-point\ndetection due to its efficiency from recursive computation and constant memory\nrequirement, and it enjoys statistical optimality. CUSUM requires knowing the\nprecise pre- and post-change distribution. However, post-change distribution is\nusually unknown a priori since it represents anomaly and novelty. Classic CUSUM\ncan perform poorly when there is a model mismatch with actual data. While\nlikelihood ratio-based methods encounter challenges facing high dimensional\ndata, neural networks have become an emerging tool for change-point detection\nwith computational efficiency and scalability. In this paper, we introduce a\nneural network CUSUM (NN-CUSUM) for online change-point detection. We also\npresent a general theoretical condition when the trained neural networks can\nperform change-point detection and what losses can achieve our goal. We further\nextend our analysis by combining it with the Neural Tangent Kernel theory to\nestablish learning guarantees for the standard performance metrics, including\nthe average run length (ARL) and expected detection delay (EDD). The strong\nperformance of NN-CUSUM is demonstrated in detecting change-point in\nhigh-dimensional data using both synthetic and real-world data.\n']",Change Detection and Analysis in Remote Sensing and Time Series Data,Remote Sensing Change Detection
124,Pavement Crack Detection and Segmentation,"['cnns', 'cracknet', 'convolutional', 'pavement', 'cracks', 'segmentation', 'efficientnetv2', 'pavements', 'yolov5', 'images']","[""  Anomalous crack region detection is a typical binary semantic segmentation\ntask, which aims to detect pixels representing cracks on pavement surface\nimages automatically by algorithms. Although existing deep learning-based\nmethods have achieved outcoming results on specific public pavement datasets,\nthe performance would deteriorate dramatically on imbalanced datasets. The\ninput datasets used in such tasks suffer from severely between-class imbalanced\nproblems, hence, it is a core challenge to obtain a robust performance on\ndiverse pavement datasets with generic deep learning models. To address this\nproblem, in this work, we propose a deep learning framework based on\nconditional Generative Adversarial Networks (cGANs) for the anomalous crack\nregion detection tasks at the pixel level. In particular, the proposed\nframework containing a cGANs and a novel auxiliary network is developed to\nenhance and stabilize the generator's performance under two alternative\ntraining stages, when estimating a multiscale probability feature map from\nheterogeneous and imbalanced inputs iteratively. Moreover, several attention\nmechanisms and entropy strategies are incorporated into the cGANs architecture\nand the auxiliary network separately to mitigate further the performance\ndeterioration of model training on severely imbalanced datasets. We implement\nextensive experiments on six accessible pavement datasets. The experimental\nresults from both visual and quantitative evaluation show that the proposed\nframework can achieve state-of-the-art results on these datasets efficiently\nand robustly without acceleration of computation complexity.\n"", '  Due to the varying intensity of pavement cracks, the complexity of\ntopological structure, and the noise of texture background, image\nclassification for asphalt pavement cracking has proven to be a challenging\nproblem. Fatigue cracking, also known as alligator cracking, is one of the\ncommon distresses of asphalt pavement. It is thus important to detect and\nmonitor the condition of alligator cracking on roadway pavements. Most research\nin this area has typically focused on pixel-level detection of cracking using\nlimited datasets. A novel deep convolutional neural network that can achieve\ntwo objectives is proposed. The first objective of the proposed neural network\nis to classify presence of fatigue cracking based on pavement surface images.\nThe second objective is to classify the fatigue cracking severity level based\non the Distress Identification Manual (DIM) standard. In this paper, a databank\nof 4484 high-resolution pavement surface images is established in which images\nare taken locally in the Town of Blacksburg, Virginia, USA. In the data\npre-preparation, over 4000 images are labeled into 4 categories manually\naccording to DIM standards. A four-layer convolutional neural network model is\nthen built to achieve the goal of classification of images by pavement crack\nseverity category. The trained model reached the highest accuracy among all\nexisting methods. After only 30 epochs of training, the model achieved a crack\nexistence classification accuracy of 96.23% and a severity level classification\naccuracy of 96.74%. After 20 epochs of training, the model achieved a pavement\nmarking presence classification accuracy of 97.64%.\n', ""  Cracks pose safety risks to infrastructure and cannot be overlooked. The\nprevailing structures in existing crack segmentation networks predominantly\nconsist of CNNs or Transformers. However, CNNs exhibit a deficiency in global\nmodeling capability, hindering the representation to entire crack features.\nTransformers can capture long-range dependencies but suffer from high and\nquadratic complexity. Recently, Mamba has garnered extensive attention due to\nits linear spatial and computational complexity and its powerful global\nperception. This study explores the representation capabilities of Mamba to\ncrack features. Specifically, this paper uncovers the connection between Mamba\nand the attention mechanism, providing a profound insight, an attention\nperspective, into interpreting Mamba and devising a novel Mamba module\nfollowing the principles of attention blocks, namely CrackMamba. We compare\nCrackMamba with the most prominent visual Mamba modules, Vim and Vmamba, on two\ndatasets comprising asphalt pavement and concrete pavement cracks, and steel\ncracks, respectively. The quantitative results show that CrackMamba stands out\nas the sole Mamba block consistently enhancing the baseline model's performance\nacross all evaluation measures, while reducing its parameters and computational\ncosts. Moreover, this paper substantiates that Mamba can achieve global\nreceptive fields through both theoretical analysis and visual interpretability.\nThe discoveries of this study offer a dual contribution. First, as a\nplug-and-play and simple yet effective Mamba module, CrackMamba exhibits\nimmense potential for integration into various crack segmentation models.\nSecond, the proposed innovative Mamba design concept, integrating Mamba with\nthe attention mechanism, holds significant reference value for all Mamba-based\ncomputer vision models, not limited to crack segmentation networks, as\ninvestigated in this study.\n""]",Pavement Crack Detection and Analysis using Deep Learning Techniques,Pavement Crack Detection and Segmentation
125,"Fault Detection and Diagnosis in Industrial Systems , ""Predictive Maintenance for Industrial Equipment""","['faults', 'fault', 'detect', 'feature', 'detection', 'features', 'monitoring', 'machinery', 'wavelet', 'machines'] , ['turbine', 'prediction', 'lstm', 'forecasting', 'turbines', 'neural', 'forecast', 'scada', 'forecasts', 'monitoring']","['  Domain generalization achieves fault diagnosis on unseen modes. In process\nindustrial systems, fault samples are limited, and only single-mode fault data\ncan be obtained. Extracting domain-invariant fault features from single-mode\ndata for unseen mode fault diagnosis poses challenges. Existing methods utilize\na generator module to simulate samples of unseen modes. However, multi-mode\nsamples contain complex spatiotemporal information, which brings significant\ndifficulties to accurate sample generation. Therefore, double gradient reversal\nnetwork (DGRN) is proposed. First, the model is pre-trained to acquire fault\nknowledge from the single seen mode. Then, pseudo-fault feature generation\nstrategy is designed by Adaptive instance normalization, to simulate fault\nfeatures of unseen mode. The dual adversarial training strategy is created to\nenhance the diversity of pseudo-fault features, which models unseen modes with\nsignificant distribution differences. Subsequently, domain-invariant feature\nextraction strategy is constructed by contrastive learning and adversarial\nlearning. This strategy extracts common features of faults and helps multi-mode\nfault diagnosis. Finally, the experiments were conducted on Tennessee Eastman\nprocess and continuous stirred-tank reactor. The experiments demonstrate that\nDGRN achieves high classification accuracy on unseen modes while maintaining a\nsmall model size.\n', '  Bearings are one of the vital components of rotating machines that are prone\nto unexpected faults. Therefore, bearing fault diagnosis and condition\nmonitoring is essential for reducing operational costs and downtime in numerous\nindustries. In various production conditions, bearings can be operated under a\nrange of loads and speeds, which causes different vibration patterns associated\nwith each fault type. Normal data is ample as systems usually work in desired\nconditions. On the other hand, fault data is rare, and in many conditions,\nthere is no data recorded for the fault classes. Accessing fault data is\ncrucial for developing data-driven fault diagnosis tools that can improve both\nthe performance and safety of operations. To this end, a novel algorithm based\non Conditional Generative Adversarial Networks (CGANs) is introduced. Trained\non the normal and fault data on any actual fault conditions, this algorithm\ngenerates fault data from normal data of target conditions. The proposed method\nis validated on a real-world bearing dataset, and fault data are generated for\ndifferent conditions. Several state-of-the-art classifiers and visualization\nmodels are implemented to evaluate the quality of the synthesized data. The\nresults demonstrate the efficacy of the proposed algorithm.\n', '  One important characteristic of modern fault classification systems is the\nability to flag the system when faced with previously unseen fault types. This\nwork considers the unknown fault detection capabilities of deep neural\nnetwork-based fault classifiers. Specifically, we propose a methodology on how,\nwhen available, labels regarding the fault taxonomy can be used to increase\nunknown fault detection performance without sacrificing model performance. To\nachieve this, we propose to utilize soft label techniques to improve the\nstate-of-the-art deep novel fault detection techniques during the training\nprocess and novel hierarchically consistent detection statistics for online\nnovel fault detection. Finally, we demonstrated increased detection performance\non novel fault detection in inspection images from the hot steel rolling\nprocess, with results well replicated across multiple scenarios and baseline\ndetection methods.\n'] , ['  In the process industry, optimizing production lines for long-term efficiency\nrequires real-time monitoring and analysis of operation states to fine-tune\nproduction line parameters. However, the complexity in operational logic and\nthe intricate coupling of production process parameters make it difficult to\ndevelop an accurate mathematical model for the entire process, thus hindering\nthe deployment of efficient optimization mechanisms. In view of these\ndifficulties, we propose to deploy a digital twin of the production line by\ndigitally abstracting its physical layout and operational logic. By iteratively\nmapping the real-world data reflecting equipment operation status and product\nquality inspection in the digital twin, we adopt a quality prediction model for\nproduction process based on self-attention-enabled temporal convolutional\nneural networks. This model enables the data-driven state evolution of the\ndigital twin. The digital twin takes a role of aggregating the information of\nactual operating conditions and the results of quality-sensitive analysis,\nwhich facilitates the optimization of process production quality with\nvirtual-reality evolution under multi-dimensional constraints. Leveraging the\ndigital twin model as an information-flow carrier, we extract temporal features\nfrom key process indicators and establish a production process quality\nprediction model based on the proposed composite neural network. Our operation\nexperiments on a specific tobacco shredding line demonstrate that the proposed\ndigital twin-based production process optimization method fosters seamless\nintegration between virtual and real production lines. This integration\nachieves an average operating status prediction accuracy of over 98\\% and\nnear-optimal production process control.\n', '  In this study, we leverage SCADA data from diverse wind turbines to predict\npower output, employing advanced time series methods, specifically Functional\nNeural Networks (FNN) and Long Short-Term Memory (LSTM) networks. A key\ninnovation lies in the ensemble of FNN and LSTM models, capitalizing on their\ncollective learning. This ensemble approach outperforms individual models,\nensuring stable and accurate power output predictions. Additionally, machine\nlearning techniques are applied to detect wind turbine performance\ndeterioration, enabling proactive maintenance strategies and health assessment.\nCrucially, our analysis reveals the uniqueness of each wind turbine,\nnecessitating tailored models for optimal predictions. These insight\nunderscores the importance of providing automatized customization for different\nturbines to keep human modeling effort low. Importantly, the methodologies\ndeveloped in this analysis are not limited to wind turbines; they can be\nextended to predict and optimize performance in various machinery, highlighting\nthe versatility and applicability of our research across diverse industrial\ncontexts.\n', '  We provide a condition monitoring system for wind farms, based on normal\nbehaviour modelling using a probabilistic multi-layer perceptron with transfer\nlearning via fine-tuning. The model predicts the output power of the wind\nturbine under normal behaviour based on features retrieved from supervisory\ncontrol and data acquisition (SCADA) systems. Its advantages are that (i) it\ncan be trained with SCADA data of at least a few years, (ii) it can incorporate\nall SCADA data of all wind turbines in a wind farm as features, (iii) it\nassumes that the output power follows a normal density with heteroscedastic\nvariance and (iv) it can predict the output of one wind turbine by borrowing\nstrength from the data of all other wind turbines in a farm. Probabilistic\nguidelines for condition monitoring are given via a CUSUM control chart. We\nillustrate the performance of our model in a real SCADA data example which\nprovides evidence that it outperforms other probabilistic prediction models.\n']",Predictive Maintenance and Fault Detection in Industrial Systems,"""Predictive Maintenance for Industrial Equipment"""
126,"Process Mining and Analytics , Concept Drift Detection in Data Streams","['processes', 'process', 'bpm', 'mining', 'analytics', 'discovering', 'activity', 'discovery', 'activities', 'manufacturing'] , ['drift', 'drifts', 'driftlens', 'drifting', 'classifiers', 'drifted', 'streams', 'adaptive', 'stream', 'detection']","['  Process mining, as a high-level field in data mining, plays a crucial role in\nenhancing operational efficiency and decision-making across organizations. In\nthis survey paper, we delve into the growing significance and ongoing trends in\nthe field of process mining, advocating a specific viewpoint on its contents,\napplication, and development in modern businesses and process management,\nparticularly in cross-organizational settings. We first summarize the framework\nof process mining, common industrial applications, and the latest advances\ncombined with artificial intelligence, such as workflow optimization,\ncompliance checking, and performance analysis. Then, we propose a holistic\nframework for intelligent process analysis and outline initial methodologies in\ncross-organizational settings, highlighting both challenges and opportunities.\nThis particular perspective aims to revolutionize process mining by leveraging\nartificial intelligence to offer sophisticated solutions for complex,\nmulti-organizational data analysis. By integrating advanced machine learning\ntechniques, we can enhance predictive capabilities, streamline processes, and\nfacilitate real-time decision-making. Furthermore, we pinpoint avenues for\nfuture investigations within the research community, encouraging the\nexploration of innovative algorithms, data integration strategies, and\nprivacy-preserving methods to fully harness the potential of process mining in\ndiverse, interconnected business environments.\n', '  In the rapidly evolving field of business process management, there is a\ngrowing need for analytical tools that can transform complex data into\nactionable insights. This research introduces a novel approach by integrating\nLarge Language Models (LLMs), such as ChatGPT, into process mining tools,\nmaking process analytics more accessible to a wider audience. The study aims to\ninvestigate how ChatGPT enhances analytical capabilities, improves user\nexperience, increases accessibility, and optimizes the architectural frameworks\nof process mining tools. The key innovation of this research lies in developing\na tailored prompt engineering strategy for each process mining submodule,\nensuring that the AI-generated outputs are accurate and relevant to the\ncontext. The integration architecture follows an Extract, Transform, Load (ETL)\nprocess, which includes various process mining engine modules and utilizes\nzero-shot and optimized prompt engineering techniques. ChatGPT is connected via\nAPIs and receives structured outputs from the process mining modules, enabling\nconversational interactions. To validate the effectiveness of this approach,\nthe researchers used data from 17 companies that employ BehfaLab\'s Process\nMining Tool. The results showed significant improvements in user experience,\nwith an expert panel rating 72% of the results as ""Good"". This research\ncontributes to the advancement of business process analysis methodologies by\ncombining process mining with artificial intelligence. Future research\ndirections include further optimization of prompt engineering, exploration of\nintegration with other AI technologies, and assessment of scalability across\nvarious business environments. This study paves the way for continuous\ninnovation at the intersection of process mining and artificial intelligence,\npromising to revolutionize the way businesses analyze and optimize their\nprocesses.\n', '  The process mining community has recently recognized the potential of large\nlanguage models (LLMs) for tackling various process mining tasks. Initial\nstudies report the capability of LLMs to support process analysis and even, to\nsome extent, that they are able to reason about how processes work. This latter\nproperty suggests that LLMs could also be used to tackle process mining tasks\nthat benefit from an understanding of process behavior. Examples of such tasks\ninclude (semantic) anomaly detection and next activity prediction, which both\ninvolve considerations of the meaning of activities and their inter-relations.\nIn this paper, we investigate the capabilities of LLMs to tackle such\nsemantics-aware process mining tasks. Furthermore, whereas most works on the\nintersection of LLMs and process mining only focus on testing these models out\nof the box, we provide a more principled investigation of the utility of LLMs\nfor process mining, including their ability to obtain process mining knowledge\npost-hoc by means of in-context learning and supervised fine-tuning.\nConcretely, we define three process mining tasks that benefit from an\nunderstanding of process semantics and provide extensive benchmarking datasets\nfor each of them. Our evaluation experiments reveal that (1) LLMs fail to solve\nchallenging process mining tasks out of the box and when provided only a\nhandful of in-context examples, (2) but they yield strong performance when\nfine-tuned for these tasks, consistently surpassing smaller, encoder-based\nlanguage models.\n'] , ['  Continuous learning from an immense volume of data streams becomes\nexceptionally critical in the internet era. However, data streams often do not\nconform to the same distribution over time, leading to a phenomenon called\nconcept drift. Since a fixed static model is unreliable for inferring\nconcept-drifted data streams, establishing an adaptive mechanism for detecting\nconcept drift is crucial. Current methods for concept drift detection primarily\nassume that the labels or error rates of downstream models are given and/or\nunderlying statistical properties exist in data streams. These approaches,\nhowever, struggle to address high-dimensional data streams with intricate\nirregular distribution shifts, which are more prevalent in real-world\nscenarios. In this paper, we propose MCD-DD, a novel concept drift detection\nmethod based on maximum concept discrepancy, inspired by the maximum mean\ndiscrepancy. Our method can adaptively identify varying forms of concept drift\nby contrastive learning of concept embeddings without relying on labels or\nstatistical properties. With thorough experiments under synthetic and\nreal-world scenarios, we demonstrate that the proposed method outperforms\nexisting baselines in identifying concept drifts and enables qualitative\nanalysis with high explainability.\n', ""  Concept drift detection is crucial for many AI systems to ensure the system's\nreliability. These systems often have to deal with large amounts of data or\nreact in real-time. Thus, drift detectors must meet computational requirements\nor constraints with a comprehensive performance evaluation. However, so far,\nthe focus of developing drift detectors is on inference quality, e.g. accuracy,\nbut not on computational performance, such as runtime. Many of the previous\nworks consider computational performance only as a secondary objective and do\nnot have a benchmark for such evaluation. Hence, we propose and explain\nperformance engineering for unsupervised concept drift detection that reflects\non computational complexities, benchmarking, and performance analysis. We\nprovide the computational complexities of existing unsupervised drift detectors\nand discuss why further computational performance investigations are required.\nHence, we state and substantiate the aspects of a benchmark for unsupervised\ndrift detection reflecting on inference quality and computational performance.\nFurthermore, we demonstrate performance analysis practices that have proven\ntheir effectiveness in High-Performance Computing, by tracing two drift\ndetectors and displaying their performance data.\n"", '  Uncertain changes in data streams present challenges for machine learning\nmodels to dynamically adapt and uphold performance in real-time. Particularly,\nclassification boundary change, also known as real concept drift, is the major\ncause of classification performance deterioration. However, accurately\ndetecting real concept drift remains challenging because the theoretical\nfoundations of existing drift detection methods - two-sample distribution tests\nand monitoring classification error rate, both suffer from inherent limitations\nsuch as the inability to distinguish virtual drift (changes not affecting the\nclassification boundary, will introduce unnecessary model maintenance), limited\nstatistical power, or high computational cost. Furthermore, no existing\ndetection method can provide information on the trend of the drift, which could\nbe invaluable for model maintenance. This work presents a novel real concept\ndrift detection method based on Neighbor-Searching Discrepancy, a new statistic\nthat measures the classification boundary difference between two samples. The\nproposed method is able to detect real concept drift with high accuracy while\nignoring virtual drift. It can also indicate the direction of the\nclassification boundary change by identifying the invasion or retreat of a\ncertain class, which is also an indicator of separability change between\nclasses. A comprehensive evaluation of 11 experiments is conducted, including\nempirical verification of the proposed theory using artificial datasets, and\nexperimental comparisons with commonly used drift handling methods on\nreal-world datasets. The results show that the proposed theory is robust\nagainst a range of distributions and dimensions, and the drift detection method\noutperforms state-of-the-art alternative methods.\n']",Process Analysis and Adaptive Learning in Dynamic Data Environments,Concept Drift Detection in Data Streams
127,"Class Imbalance in Classification Tasks , Credit Card Fraud Detection","['classifiers', 'classifier', 'classification', 'classifications', 'imbalance', 'imbalanced', 'svm', 'roc', 'accuracy', 'classes'] , ['fraud', 'banking', 'classification', 'bank', 'banks', 'credit', 'fraudulent', 'lending', 'card', 'lenders']","['  Class imbalance remains a significant challenge in machine learning,\nparticularly for tabular data classification tasks. While Gradient Boosting\nDecision Trees (GBDT) models have proven highly effective for such tasks, their\nperformance can be compromised when dealing with imbalanced datasets. This\npaper presents the first comprehensive study on adapting class-balanced loss\nfunctions to three GBDT algorithms across various tabular classification tasks,\nincluding binary, multi-class, and multi-label classification. We conduct\nextensive experiments on multiple datasets to evaluate the impact of\nclass-balanced losses on different GBDT models, establishing a valuable\nbenchmark. Our results demonstrate the potential of class-balanced loss\nfunctions to enhance GBDT performance on imbalanced datasets, offering a robust\napproach for practitioners facing class imbalance challenges in real-world\napplications. Additionally, we introduce a Python package that facilitates the\nintegration of class-balanced loss functions into GBDT workflows, making these\nadvanced techniques accessible to a wider audience.\n', '  Although binary classification is a well-studied problem in computer vision,\ntraining reliable classifiers under severe class imbalance remains a\nchallenging problem. Recent work has proposed techniques that mitigate the\neffects of training under imbalance by modifying the loss functions or\noptimization methods. While this work has led to significant improvements in\nthe overall accuracy in the multi-class case, we observe that slight changes in\nhyperparameter values of these methods can result in highly variable\nperformance in terms of Receiver Operating Characteristic (ROC) curves on\nbinary problems with severe imbalance. To reduce the sensitivity to\nhyperparameter choices and train more general models, we propose training over\na family of loss functions, instead of a single loss function. We develop a\nmethod for applying Loss Conditional Training (LCT) to an imbalanced\nclassification problem. Extensive experiment results, on both CIFAR and Kaggle\ncompetition datasets, show that our method improves model performance and is\nmore robust to hyperparameter choices. Code is available at\nhttps://github.com/klieberman/roc_lct.\n', ""  We show that established performance metrics in binary classification, such\nas the F-score, the Jaccard similarity coefficient or Matthews' correlation\ncoefficient (MCC), are not robust to class imbalance in the sense that if the\nproportion of the minority class tends to $0$, the true positive rate (TPR) of\nthe Bayes classifier under these metrics tends to $0$ as well. Thus, in\nimbalanced classification problems, these metrics favour classifiers which\nignore the minority class. To alleviate this issue we introduce robust\nmodifications of the F-score and the MCC for which, even in strongly imbalanced\nsettings, the TPR is bounded away from $0$. We numerically illustrate the\nbehaviour of the various performance metrics in simulations as well as on a\ncredit default data set. We also discuss connections to the ROC and\nprecision-recall curves and give recommendations on how to combine their usage\nwith performance metrics.\n""] , ['  Financial institutions and businesses face an ongoing challenge from\nfraudulent transactions, prompting the need for effective detection methods.\nDetecting credit card fraud is crucial for identifying and preventing\nunauthorized transactions.Timely detection of fraud enables investigators to\ntake swift actions to mitigate further losses. However, the investigation\nprocess is often time-consuming, limiting the number of alerts that can be\nthoroughly examined each day. Therefore, the primary objective of a fraud\ndetection model is to provide accurate alerts while minimizing false alarms and\nmissed fraud cases. In this paper, we introduce a state-of-the-art hybrid\nensemble (ENS) dependable Machine learning (ML) model that intelligently\ncombines multiple algorithms with proper weighted optimization using Grid\nsearch, including Decision Tree (DT), Random Forest (RF), K-Nearest Neighbor\n(KNN), and Multilayer Perceptron (MLP), to enhance fraud identification. To\naddress the data imbalance issue, we employ the Instant Hardness Threshold\n(IHT) technique in conjunction with Logistic Regression (LR), surpassing\nconventional approaches. Our experiments are conducted on a publicly available\ncredit card dataset comprising 284,807 transactions. The proposed model\nachieves impressive accuracy rates of 99.66%, 99.73%, 98.56%, and 99.79%, and a\nperfect 100% for the DT, RF, KNN, MLP and ENS models, respectively. The hybrid\nensemble model outperforms existing works, establishing a new benchmark for\ndetecting fraudulent transactions in high-frequency scenarios. The results\nhighlight the effectiveness and reliability of our approach, demonstrating\nsuperior performance metrics and showcasing its exceptional potential for\nreal-world fraud detection applications.\n', '  Credit card fraud is a major cause of national concern in the Nigerian\nfinancial sector, affecting hundreds of transactions per second and impacting\ninternational ecommerce negatively. Despite the rapid spread and adoption of\nonline marketing, millions of Nigerians are prevented from transacting in\nseveral countries with local credit cards due to bans and policies directed at\nrestricting credit card fraud. Presently, a myriad of technologies exist to\ndetect fraudulent transactions, a few of which are adopted by Nigerian\nfinancial institutions to proactively manage the situation. Fraud detection\nallows institutions to restrict offenders from networks and with a centralized\nbanking identity management system, such as the Bank Verification Number used\nby the Central Bank of Nigeria, offenders who may have stolen other identities\ncan be backtraced and their bank accounts frozen. This paper aims to compare\nthe effectiveness of two fraud detection technologies that are projected to\nwork fully independent of human intervention to possibly predict and detect\nfraudulent credit card transactions. Autoencoders as an unsupervised tensorflow\nbased anomaly detection technique generally offers greater performance in\ndimensionality reduction than the Principal Component Analysis, and this theory\nwas tested out on Nigerian credit card transaction data. Results demonstrate\nthat autoencoders are better suited to analyzing complex and extensive datasets\nand offer more reliable results with minimal mislabeling than the PCA\nalgorithm.\n', '  Credit card fraud detection is a critical challenge in the financial sector,\ndemanding sophisticated approaches to accurately identify fraudulent\ntransactions. This research proposes an innovative methodology combining Neural\nNetworks (NN) and Synthet ic Minority Over-sampling Technique (SMOTE) to\nenhance the detection performance. The study addresses the inherent imbalance\nin credit card transaction data, focusing on technical advancements for robust\nand precise fraud detection. Results demonstrat e that the integration of NN\nand SMOTE exhibits superior precision, recall, and F1-score compared to\ntraditional models, highlighting its potential as an advanced solution for\nhandling imbalanced datasets in credit card fraud detection scenarios. This\nrese arch contributes to the ongoing efforts to develop effective and efficient\nmechanisms for safeguarding financial transactions from fraudulent activities.\n']",Machine Learning for Imbalanced Classification and Fraud Detection,Credit Card Fraud Detection
128,"Anomaly Detection Methods and Techniques , Outlier Detection Methods , Anomaly Detection Methods","['anomaly', 'anomalies', 'anomalous', 'detections', 'detection', 'detecting', 'detect', 'autoencoders', 'supervised', 'unusual'] , ['outlier', 'outliers', 'outlierness', 'inlier', 'inliers', 'anomaly', 'supervised', 'robust', 'unsupervised', 'detection'] , ['anomaly', 'anomalies', 'detecting', 'detection', 'supervised', 'datasets', 'boosting', 'hyperspectral', 'monitoring', 'unsupervised']","['  With the rapid development of the Internet, various types of anomaly traffic\nare threatening network security. We consider the problem of anomaly network\ntraffic detection and propose a three-stage anomaly detection framework using\nonly normal traffic. Our framework can generate pseudo anomaly samples without\nprior knowledge of anomalies to achieve the detection of anomaly data. Firstly,\nwe employ a reconstruction method to learn the deep representation of normal\nsamples. Secondly, these representations are normalized to a standard normal\ndistribution using a bidirectional flow module. To simulate anomaly samples, we\nadd noises to the normalized representations which are then passed through the\ngeneration direction of the bidirectional flow module. Finally, a simple\nclassifier is trained to differentiate the normal samples and pseudo anomaly\nsamples in the latent space. During inference, our framework requires only two\nmodules to detect anomalous samples, leading to a considerable reduction in\nmodel size. According to the experiments, our method achieves the state\nof-the-art results on the common benchmarking datasets of anomaly network\ntraffic detection. The code is given in the\nhttps://github.com/ZxuanDang/ATD-via-Flows.git\n', '  Unsupervised Anomaly Detection (UAD) plays a crucial role in identifying\nabnormal patterns within data without labeled examples, holding significant\npractical implications across various domains. Although the individual\ncontributions of representation learning and clustering to anomaly detection\nare well-established, their interdependencies remain under-explored due to the\nabsence of a unified theoretical framework. Consequently, their collective\npotential to enhance anomaly detection performance remains largely untapped. To\nbridge this gap, in this paper, we propose a novel probabilistic mixture model\nfor anomaly detection to establish a theoretical connection among\nrepresentation learning, clustering, and anomaly detection. By maximizing a\nnovel anomaly-aware data likelihood, representation learning and clustering can\neffectively reduce the adverse impact of anomalous data and collaboratively\nbenefit anomaly detection. Meanwhile, a theoretically substantiated anomaly\nscore is naturally derived from this framework. Lastly, drawing inspiration\nfrom gravitational analysis in physics, we have devised an improved anomaly\nscore that more effectively harnesses the combined power of representation\nlearning and clustering. Extensive experiments, involving 17 baseline methods\nacross 30 diverse datasets, validate the effectiveness and generalization\ncapability of the proposed method, surpassing state-of-the-art methods.\n', '  Transformer, as one of the most advanced neural network models in Natural\nLanguage Processing (NLP), exhibits diverse applications in the field of\nanomaly detection. To inspire research on Transformer-based anomaly detection,\nthis review offers a fresh perspective on the concept of anomaly detection. We\nexplore the current challenges of anomaly detection and provide detailed\ninsights into the operating principles of Transformer and its variants in\nanomaly detection tasks. Additionally, we delineate various application\nscenarios for Transformer-based anomaly detection models and discuss the\ndatasets and evaluation metrics employed. Furthermore, this review highlights\nthe key challenges in Transformer-based anomaly detection research and conducts\na comprehensive analysis of future research trends in this domain. The review\nincludes an extensive compilation of over 100 core references related to\nTransformer-based anomaly detection. To the best of our knowledge, this is the\nfirst comprehensive review that focuses on the research related to Transformer\nin the context of anomaly detection. We hope that this paper can provide\ndetailed technical information to researchers interested in Transformer-based\nanomaly detection tasks.\n'] , ['  Graph outlier detection is a prominent task of research and application in\nthe realm of graph neural networks. It identifies the outlier nodes that\nexhibit deviation from the majority in the graph. One of the fundamental\nchallenges confronting supervised graph outlier detection algorithms is the\nprevalent issue of class imbalance, where the scarcity of outlier instances\ncompared to normal instances often results in suboptimal performance.\nConventional methods mitigate the imbalance by reweighting instances in the\nestimation of the loss function, assigning higher weights to outliers and lower\nweights to inliers. Nonetheless, these strategies are prone to overfitting and\nunderfitting, respectively. Recently, generative models, especially diffusion\nmodels, have demonstrated their efficacy in synthesizing high-fidelity images.\nDespite their extraordinary generation quality, their potential in data\naugmentation for supervised graph outlier detection remains largely\nunderexplored.\n  To bridge this gap, we introduce GODM, a novel data augmentation for\nmitigating class imbalance in supervised Graph Outlier detection with latent\nDiffusion Models. Specifically, our proposed method consists of three key\ncomponents: (1) Variantioanl Encoder maps the heterogeneous information\ninherent within the graph data into a unified latent space. (2) Graph Generator\nsynthesizes graph data that are statistically similar to real outliers from\nlatent space, and (3) Latent Diffusion Model learns the latent space\ndistribution of real organic data by iterative denoising. Extensive experiments\nconducted on multiple datasets substantiate the effectiveness and efficiency of\nGODM. The case study further demonstrated the generation quality of our\nsynthetic data. To foster accessibility and reproducibility, we encapsulate\nGODM into a plug-and-play package and release it at the Python Package Index\n(PyPI).\n', '  In recent years, multi-view outlier detection (MVOD) methods have advanced\nsignificantly, aiming to identify outliers within multi-view datasets. A key\npoint is to better detect class outliers and class-attribute outliers, which\nonly exist in multi-view data. However, existing methods either is not able to\nreduce the impact of outliers when learning view-consistent information, or\nstruggle in cases with varying neighborhood structures. Moreover, most of them\ndo not apply to partial multi-view data in real-world scenarios. To overcome\nthese drawbacks, we propose a novel method named Regularized Contrastive\nPartial Multi-view Outlier Detection (RCPMOD). In this framework, we utilize\ncontrastive learning to learn view-consistent information and distinguish\noutliers by the degree of consistency. Specifically, we propose (1) An\noutlier-aware contrastive loss with a potential outlier memory bank to\neliminate their bias motivated by a theoretical analysis. (2) A neighbor\nalignment contrastive loss to capture the view-shared local structural\ncorrelation. (3) A spreading regularization loss to prevent the model from\noverfitting over outliers. With the Cross-view Relation Transfer technique, we\ncould easily impute the missing view samples based on the features of\nneighbors. Experimental results on four benchmark datasets demonstrate that our\nproposed approach could outperform state-of-the-art competitors under different\nsettings.\n', '  Discriminative learning effectively predicts true object class for image\nclassification. However, it often results in false positives for outliers,\nposing critical concerns in applications like autonomous driving and video\nsurveillance systems. Previous attempts to address this challenge involved\ntraining image classifiers through contrastive learning using actual outlier\ndata or synthesizing outliers for self-supervised learning. Furthermore,\nunsupervised generative modeling of inliers in pixel space has shown limited\nsuccess for outlier detection. In this work, we introduce a quantile-based\nmaximum likelihood objective for learning the inlier distribution to improve\nthe outlier separation during inference. Our approach fits a normalizing flow\nto pre-trained discriminative features and detects the outliers according to\nthe evaluated log-likelihood. The experimental evaluation demonstrates the\neffectiveness of our method as it surpasses the performance of the\nstate-of-the-art unsupervised methods for outlier detection. The results are\nalso competitive compared with a recent self-supervised approach for outlier\ndetection. Our work allows to reduce dependency on well-sampled negative\ntraining data, which is especially important for domains like medical\ndiagnostics or remote sensing.\n'] , ['  Fault detection is crucial in industrial systems to prevent failures and\noptimize performance by distinguishing abnormal from normal operating\nconditions. Data-driven methods have been gaining popularity for fault\ndetection tasks as the amount of condition monitoring data from complex\nindustrial systems increases. Despite these advances, early fault detection\nremains a challenge under real-world scenarios. The high variability of\noperating conditions and environments makes it difficult to collect\ncomprehensive training datasets that can represent all possible operating\nconditions, especially in the early stages of system operation. Furthermore,\nthese variations often evolve over time, potentially leading to entirely new\ndata distributions in the future that were previously unseen. These challenges\nprevent direct knowledge transfer across different units and over time, leading\nto the distribution gap between training and testing data and inducing\nperformance degradation of those methods in real-world scenarios. To overcome\nthis, our work introduces a novel approach for continuous test-time domain\nadaptation. This enables early-stage robust anomaly detection by addressing\ndomain shifts and limited data representativeness issues. We propose a\nTest-time domain Adaptation Anomaly Detection (TAAD) framework that separates\ninput variables into system parameters and measurements, employing two domain\nadaptation modules to independently adapt to each input category. This method\nallows for effective adaptation to evolving operating conditions and is\nparticularly beneficial in systems with scarce data. Our approach, tested on a\nreal-world pump monitoring dataset, shows significant improvements over\nexisting domain adaptation methods in fault detection, demonstrating enhanced\naccuracy and reliability.\n', '  Automating real-time anomaly detection is essential for identifying rare\ntransients in the era of large-scale astronomical surveys. Modern survey\ntelescopes are generating tens of thousands of alerts per night, and future\ntelescopes, such as the Vera C. Rubin Observatory, are projected to increase\nthis number dramatically. Currently, most anomaly detection algorithms for\nastronomical transients rely either on hand-crafted features extracted from\nlight curves or on features generated through unsupervised representation\nlearning, which are then coupled with standard machine learning anomaly\ndetection algorithms. In this work, we introduce an alternative approach to\ndetecting anomalies: using the penultimate layer of a neural network classifier\nas the latent space for anomaly detection. We then propose a novel method,\nnamed Multi-Class Isolation Forests (MCIF), which trains separate isolation\nforests for each class to derive an anomaly score for a light curve from the\nlatent space representation given by the classifier. This approach\nsignificantly outperforms a standard isolation forest. We also use a simpler\ninput method for real-time transient classifiers which circumvents the need for\ninterpolation in light curves and helps the neural network model inter-passband\nrelationships and handle irregular sampling. Our anomaly detection pipeline\nidentifies rare classes including kilonovae, pair-instability supernovae, and\nintermediate luminosity transients shortly after trigger on simulated Zwicky\nTransient Facility light curves. Using a sample of our simulations that matched\nthe population of anomalies expected in nature (54 anomalies and 12,040 common\ntransients), our method was able to discover $41\\pm3$ anomalies (~75% recall)\nafter following up the top 2000 (~15%) ranked transients. Our novel method\nshows that classifiers can be effectively repurposed for real-time anomaly\ndetection.\n', '  The majority of existing hyperspectral anomaly detection (HAD) methods use\nthe low-rank representation (LRR) model to separate the background and anomaly\ncomponents, where the anomaly component is optimized by handcrafted sparse\npriors (e.g., $\\ell_{2,1}$-norm). However, this may not be ideal since they\noverlook the spatial structure present in anomalies and make the detection\nresult largely dependent on manually set sparsity. To tackle these problems, we\nredefine the optimization criterion for the anomaly component in the LRR model\nwith a self-supervised network called self-supervised anomaly prior (SAP). This\nprior is obtained by the pretext task of self-supervised learning, which is\ncustomized to learn the characteristics of hyperspectral anomalies.\nSpecifically, this pretext task is a classification task to distinguish the\noriginal hyperspectral image (HSI) and the pseudo-anomaly HSI, where the\npseudo-anomaly is generated from the original HSI and designed as a prism with\narbitrary polygon bases and arbitrary spectral bands. In addition, a\ndual-purified strategy is proposed to provide a more refined background\nrepresentation with an enriched background dictionary, facilitating the\nseparation of anomalies from complex backgrounds. Extensive experiments on\nvarious hyperspectral datasets demonstrate that the proposed SAP offers a more\naccurate and interpretable solution than other advanced HAD methods.\n']",Anomaly and Outlier Detection Methods,Anomaly Detection Methods
129,"Out-of-Distribution Detection and Robust Deep Learning , Deep Clustering Methods and Distribution Learning , Out-of-Distribution Detection Methods","['softmax', 'imagenet', 'classification', 'detection', 'accuracy', 'datasets', 'deep', 'ood', 'robust', 'neural'] , ['clustering_softmax_predictions', 'cluster', 'online_hard_clustering', 'clustering', 'clusters', 'supervised', 'embedding', 'deep', 'unsupervised', 'autoencoder'] , ['detection', 'detecting', 'outliers', 'outlier', 'classifier', 'inlier', 'ood', 'classified', 'novelty', 'od']","['  Out-of-distribution (OOD) learning often relies heavily on statistical\napproaches or predefined assumptions about OOD data distributions, hindering\ntheir efficacy in addressing multifaceted challenges of OOD generalization and\nOOD detection in real-world deployment environments. This paper presents a\nnovel framework for OOD learning with human feedback, which can provide\ninvaluable insights into the nature of OOD shifts and guide effective model\nadaptation. Our framework capitalizes on the freely available unlabeled data in\nthe wild that captures the environmental test-time OOD distributions under both\ncovariate and semantic shifts. To harness such data, our key idea is to\nselectively provide human feedback and label a small number of informative\nsamples from the wild data distribution, which are then used to train a\nmulti-class classifier and an OOD detector. By exploiting human feedback, we\nenhance the robustness and reliability of machine learning models, equipping\nthem with the capability to handle OOD scenarios with greater precision. We\nprovide theoretical insights on the generalization error bounds to justify our\nalgorithm. Extensive experiments show the superiority of our method,\noutperforming the current state-of-the-art by a significant margin.\n', '  Deep learning models excel when the data distribution during training aligns\nwith testing data. Yet, their performance diminishes when faced with\nout-of-distribution (OOD) samples, leading to great interest in the field of\nOOD detection. Current approaches typically assume that OOD samples originate\nfrom an unconcentrated distribution complementary to the training distribution.\nWhile this assumption is appropriate in the traditional unsupervised OOD\n(U-OOD) setting, it proves inadequate when considering the place of deployment\nof the underlying deep learning model. To better reflect this real-world\nscenario, we introduce the novel setting of continual U-OOD detection. To\ntackle this new setting, we propose a method that starts from a U-OOD detector,\nwhich is agnostic to the OOD distribution, and slowly updates during deployment\nto account for the actual OOD distribution. Our method uses a new U-OOD scoring\nfunction that combines the Mahalanobis distance with a nearest-neighbor\napproach. Furthermore, we design a confidence-scaled few-shot OOD detector that\noutperforms previous methods. We show our method greatly improves upon strong\nbaselines from related fields.\n', '  With the rapid advancement in the performance of deep neural networks (DNNs),\nthere has been significant interest in deploying and incorporating artificial\nintelligence (AI) systems into real-world scenarios. However, many DNNs lack\nthe ability to represent uncertainty, often exhibiting excessive confidence\neven when making incorrect predictions. To ensure the reliability of AI\nsystems, particularly in safety-critical cases, DNNs should transparently\nreflect the uncertainty in their predictions. In this paper, we investigate\nrobust post-hoc uncertainty calibration methods for DNNs within the context of\nmulti-class classification tasks. While previous studies have made notable\nprogress, they still face challenges in achieving robust calibration,\nparticularly in scenarios involving out-of-distribution (OOD). We identify that\nprevious methods lack adaptability to individual input data and struggle to\naccurately estimate uncertainty when processing inputs drawn from the wild\ndataset. To address this issue, we introduce a novel instance-wise calibration\nmethod based on an energy model. Our method incorporates energy scores instead\nof softmax confidence scores, allowing for adaptive consideration of DNN\nuncertainty for each prediction within a logit space. In experiments, we show\nthat the proposed method consistently maintains robust performance across the\nspectrum, spanning from in-distribution to OOD scenarios, when compared to\nother state-of-the-art methods.\n'] , ['  Distribution learning finds probability density functions from a set of data\nsamples, whereas clustering aims to group similar data points to form clusters.\nAlthough there are deep clustering methods that employ distribution learning\nmethods, past work still lacks theoretical analysis regarding the relationship\nbetween clustering and distribution learning. Thus, in this work, we provide a\ntheoretical analysis to guide the optimization of clustering via distribution\nlearning. To achieve better results, we embed deep clustering guided by a\ntheoretical analysis. Furthermore, the distribution learning method cannot\nalways be directly applied to data. To overcome this issue, we introduce a\nclustering-oriented distribution learning method called Monte-Carlo\nMarginalization for Clustering. We integrate Monte-Carlo Marginalization for\nClustering into Deep Clustering, resulting in Deep Clustering via Distribution\nLearning (DCDL). Eventually, the proposed DCDL achieves promising results\ncompared to state-of-the-art methods on popular datasets. Considering a\nclustering task, the new distribution learning method outperforms previous\nmethods as well.\n', '  Deep clustering methods improve the performance of clustering tasks by\njointly optimizing deep representation learning and clustering. While numerous\ndeep clustering algorithms have been proposed, most of them rely on\nartificially constructed pseudo targets for performing clustering. This\nconstruction process requires some prior knowledge, and it is challenging to\ndetermine a suitable pseudo target for clustering. To address this issue, we\npropose a deep embedding clustering algorithm driven by sample stability\n(DECS), which eliminates the requirement of pseudo targets. Specifically, we\nstart by constructing the initial feature space with an autoencoder and then\nlearn the cluster-oriented embedding feature constrained by sample stability.\nThe sample stability aims to explore the deterministic relationship between\nsamples and all cluster centroids, pulling samples to their respective clusters\nand keeping them away from other clusters with high determinacy. We analyzed\nthe convergence of the loss using Lipschitz continuity in theory, which\nverifies the validity of the model. The experimental results on five datasets\nillustrate that the proposed method achieves superior performance compared to\nstate-of-the-art clustering approaches.\n', '  In the face of complex natural images, existing deep clustering algorithms\nfall significantly short in terms of clustering accuracy when compared to\nsupervised classification methods, making them less practical. This paper\nintroduces an image clustering algorithm based on self-supervised pretrained\nmodels and latent feature distribution optimization, substantially enhancing\nclustering performance. It is found that: (1) For complex natural images, we\neffectively enhance the discriminative power of latent features by leveraging\nself-supervised pretrained models and their fine-tuning, resulting in improved\nclustering performance. (2) In the latent feature space, by searching for\nk-nearest neighbor images for each training sample and shortening the distance\nbetween the training sample and its nearest neighbor, the discriminative power\nof latent features can be further enhanced, and clustering performance can be\nimproved. (3) In the latent feature space, reducing the distance between sample\nfeatures and the nearest predefined cluster centroids can optimize the\ndistribution of latent features, therefore further improving clustering\nperformance. Through experiments on multiple datasets, our approach outperforms\nthe latest clustering algorithms and achieves state-of-the-art clustering\nresults. When the number of categories in the datasets is small, such as\nCIFAR-10 and STL-10, and there are significant differences between categories,\nour clustering algorithm has similar accuracy to supervised methods without\nusing pretrained models, slightly lower than supervised methods using\npre-trained models. The code linked algorithm is\nhttps://github.com/LihengHu/semi.\n'] , ['  Out-of-distribution (OOD) detection is crucial for deploying robust machine\nlearning models, especially in areas where security is critical. However,\ntraditional OOD detection methods often fail to capture complex data\ndistributions from large scale date. In this paper, we present a novel approach\nfor OOD detection that leverages the generative ability of diffusion models and\nthe powerful feature extraction capabilities of CLIP. By using these features\nas conditional inputs to a diffusion model, we can reconstruct the images after\nencoding them with CLIP. The difference between the original and reconstructed\nimages is used as a signal for OOD identification. The practicality and\nscalability of our method is increased by the fact that it does not require\nclass-specific labeled ID data, as is the case with many other methods.\nExtensive experiments on several benchmark datasets demonstrates the robustness\nand effectiveness of our method, which have significantly improved the\ndetection accuracy.\n', ""  Few-shot OOD detection focuses on recognizing out-of-distribution (OOD)\nimages that belong to classes unseen during training, with the use of only a\nsmall number of labeled in-distribution (ID) images. Up to now, a mainstream\nstrategy is based on large-scale vision-language models, such as CLIP. However,\nthese methods overlook a crucial issue: the lack of reliable OOD supervision\ninformation, which can lead to biased boundaries between in-distribution (ID)\nand OOD. To tackle this problem, we propose CLIP-driven Outliers\nSynthesis~(CLIP-OS). Firstly, CLIP-OS enhances patch-level features' perception\nby newly proposed patch uniform convolution, and adaptively obtains the\nproportion of ID-relevant information by employing CLIP-surgery-discrepancy,\nthus achieving separation between ID-relevant and ID-irrelevant. Next, CLIP-OS\nsynthesizes reliable OOD data by mixing up ID-relevant features from different\nclasses to provide OOD supervision information. Afterward, CLIP-OS leverages\nsynthetic OOD samples by unknown-aware prompt learning to enhance the\nseparability of ID and OOD. Extensive experiments across multiple benchmarks\ndemonstrate that CLIP-OS achieves superior few-shot OOD detection capability.\n"", '  Detecting out-of-distribution (OOD) samples is crucial for ensuring the\nsafety of machine learning systems and has shaped the field of OOD detection.\nMeanwhile, several other problems are closely related to OOD detection,\nincluding anomaly detection (AD), novelty detection (ND), open set recognition\n(OSR), and outlier detection (OD). To unify these problems, a generalized OOD\ndetection framework was proposed, taxonomically categorizing these five\nproblems. However, Vision Language Models (VLMs) such as CLIP have\nsignificantly changed the paradigm and blurred the boundaries between these\nfields, again confusing researchers. In this survey, we first present a\ngeneralized OOD detection v2, encapsulating the evolution of AD, ND, OSR, OOD\ndetection, and OD in the VLM era. Our framework reveals that, with some field\ninactivity and integration, the demanding challenges have become OOD detection\nand AD. In addition, we also highlight the significant shift in the definition,\nproblem settings, and benchmarks; we thus feature a comprehensive review of the\nmethodology for OOD detection, including the discussion over other related\ntasks to clarify their relationship to OOD detection. Finally, we explore the\nadvancements in the emerging Large Vision Language Model (LVLM) era, such as\nGPT-4V. We conclude this survey with open challenges and future directions.\n']",Deep Learning for Out-of-Distribution Detection and Robustness,Out-of-Distribution Detection and Robust Deep Learning
130,"Sign Language Processing and Translation , Hand Gesture Recognition Systems , ""Gesture Synthesis and Analysis in Multimodal Communication""","['signwriting', 'gestures', 'gesture', 'signbank', 'sign', 'signllm', 'signcl', 'signclip', 'signers', 'signing'] , ['gestures', 'gesture', 'gestureprint', 'touch', 'hands', 'hand', 'multimodal', 'touchpad', 'recognition', 'classifiers'] , ['gestures', 'gesture', 'gesturegpt', 'gesturing', 'multimodal', 'nonverbal', 'speech', 'embodied', 'audio', 'cues']","['  Even for better-studied sign languages like American Sign Language (ASL),\ndata is the bottleneck for machine learning research. The situation is worse\nyet for the many other sign languages used by Deaf/Hard of Hearing communities\naround the world. In this paper, we present YouTube-SL-25, a large-scale,\nopen-domain multilingual corpus of sign language videos with seemingly\nwell-aligned captions drawn from YouTube. With >3000 hours of videos across >25\nsign languages, YouTube-SL-25 is a) >3x the size of YouTube-ASL, b) the largest\nparallel sign language dataset to date, and c) the first or largest parallel\ndataset for many of its component languages. We provide baselines for\nsign-to-text tasks using a unified multilingual multitask model based on T5 and\nreport scores on benchmarks across 4 sign languages. The results demonstrate\nthat multilingual transfer benefits both higher- and lower-resource sign\nlanguages within YouTube-SL-25.\n', '  Sign language understanding has made significant strides; however, there is\nstill no viable solution for generating sign sequences directly from entire\nspoken content, e.g., text or speech. In this paper, we propose a unified\nframework for continuous sign language production, easing communication between\nsign and non-sign language users. In particular, a sequence diffusion model,\nutilizing embeddings extracted from text or speech, is crafted to generate sign\npredictions step by step. Moreover, by creating a joint embedding space for\ntext, audio, and sign, we bind these modalities and leverage the semantic\nconsistency among them to provide informative feedback for the model training.\nThis embedding-consistency learning strategy minimizes the reliance on sign\ntriplets and ensures continuous model refinement, even with a missing audio\nmodality. Experiments on How2Sign and PHOENIX14T datasets demonstrate that our\nmodel achieves competitive performance in sign language production.\n', '  Sign Language Translation (SLT) is a challenging task that aims to translate\nsign videos into spoken language. Inspired by the strong translation\ncapabilities of large language models (LLMs) that are trained on extensive\nmultilingual text corpora, we aim to harness off-the-shelf LLMs to handle SLT.\nIn this paper, we regularize the sign videos to embody linguistic\ncharacteristics of spoken language, and propose a novel SignLLM framework to\ntransform sign videos into a language-like representation for improved\nreadability by off-the-shelf LLMs. SignLLM comprises two key modules: (1) The\nVector-Quantized Visual Sign module converts sign videos into a sequence of\ndiscrete character-level sign tokens, and (2) the Codebook Reconstruction and\nAlignment module converts these character-level tokens into word-level sign\nrepresentations using an optimal transport formulation. A sign-text alignment\nloss further bridges the gap between sign and text tokens, enhancing semantic\ncompatibility. We achieve state-of-the-art gloss-free results on two\nwidely-used SLT benchmarks.\n'] , [""  Hand gestures can provide a natural means of human-computer interaction and\nenable people who cannot speak to communicate efficiently. Existing hand\ngesture recognition methods heavily depend on pre-defined gestures, however,\nmotor-impaired individuals require new gestures tailored to each individual's\ngesture motion and style. Gesture samples collected from different persons have\ndistribution shifts due to their health conditions, the severity of the\ndisability, motion patterns of the arms, etc. In this paper, we introduce the\nLatent Embedding Exploitation (LEE) mechanism in our replay-based Few-Shot\nContinual Learning (FSCL) framework that significantly improves the performance\nof fine-tuning a model for out-of-distribution data. Our method produces a\ndiversified latent feature space by leveraging a preserved latent embedding\nknown as gesture prior knowledge, along with intra-gesture divergence derived\nfrom two additional embeddings. Thus, the model can capture latent statistical\nstructure in highly variable gestures with limited samples. We conduct an\nexperimental evaluation using the SmartWatch Gesture and the Motion Gesture\ndatasets. The proposed method results in an average test accuracy of 57.0%,\n64.6%, and 69.3% by using one, three, and five samples for six different\ngestures. Our method helps motor-impaired persons leverage wearable devices,\nand their unique styles of movement can be learned and applied in\nhuman-computer interaction and social communication. Code is available at:\nhttps://github.com/riyadRafiq/wearable-latent-embedding-exploitation\n"", '  As robots are expected to get more involved in people\'s everyday lives,\nframeworks that enable intuitive user interfaces are in demand. Hand gesture\nrecognition systems provide a natural way of communication and, thus, are an\nintegral part of seamless Human-Robot Interaction (HRI). Recent years have\nwitnessed an immense evolution of computational models powered by deep\nlearning. However, state-of-the-art models fall short in expanding across\ndifferent gesture domains, such as emblems and co-speech. In this paper, we\npropose a novel hybrid hand gesture recognition system. Our architecture\nenables learning both static and dynamic gestures: by capturing a so-called\n""snapshot"" of the gesture performance at its peak, we integrate the hand pose\nalong with the dynamic movement. Moreover, we present a method for analyzing\nthe motion profile of a gesture to uncover its dynamic characteristics and\nwhich allows regulating a static channel based on the amount of motion. Our\nevaluation demonstrates the superiority of our approach on two gesture\nbenchmarks compared to a CNNLSTM baseline. We also provide an analysis on a\ngesture class basis that unveils the potential of our Snapture architecture for\nperformance improvements. Thanks to its modular implementation, our framework\nallows the integration of other multimodal data like facial expressions and\nhead tracking, which are important cues in HRI scenarios, into one\narchitecture. Thus, our work contributes both to gesture recognition research\nand machine learning applications for non-verbal communication with robots.\n', '  Artificial intelligence (AI) has made significant advances in recent years\nand opened up new possibilities in exploring applications in various fields\nsuch as biomedical, robotics, education, industry, etc. Among these fields,\nhuman hand gesture recognition is a subject of study that has recently emerged\nas a research interest in robotic hand control using electromyography (EMG).\nSurface electromyography (sEMG) is a primary technique used in EMG, which is\npopular due to its non-invasive nature and is used to capture gesture movements\nusing signal acquisition devices placed on the surface of the forearm.\nMoreover, these signals are pre-processed to extract significant handcrafted\nfeatures through time and frequency domain analysis. These are helpful and act\nas input to machine learning (ML) models to identify hand gestures. However,\nhandling multiple classes and biases are major limitations that can affect the\nperformance of an ML model. Therefore, to address this issue, a new mixture of\nexperts extra tree (MEET) model is proposed to identify more accurate and\neffective hand gesture movements. This model combines individual ML models\nreferred to as experts, each focusing on a minimal class of two. Moreover, a\nfully trained model known as the gate is employed to weigh the output of\nindividual expert models. This amalgamation of the expert models with the gate\nmodel is known as a mixture of experts extra tree (MEET) model. In this study,\nfour subjects with six hand gesture movements have been considered and their\nidentification is evaluated among eleven models, including the MEET classifier.\nResults elucidate that the MEET classifier performed best among other\nalgorithms and identified hand gesture movement accurately.\n'] , [""  This paper focuses on enhancing human-agent communication by integrating\nspatial context into virtual agents' non-verbal behaviors, specifically\ngestures. Recent advances in co-speech gesture generation have primarily\nutilized data-driven methods, which create natural motion but limit the scope\nof gestures to those performed in a void. Our work aims to extend these methods\nby enabling generative models to incorporate scene information into\nspeech-driven gesture synthesis. We introduce a novel synthetic gesture dataset\ntailored for this purpose. This development represents a critical step toward\ncreating embodied conversational agents that interact more naturally with their\nenvironment and users.\n"", '  Gesture synthesis has gained significant attention as a critical research\nfield, aiming to produce contextually appropriate and natural gestures\ncorresponding to speech or textual input. Although deep learning-based\napproaches have achieved remarkable progress, they often overlook the rich\nsemantic information present in the text, leading to less expressive and\nmeaningful gestures. In this letter, we propose GesGPT, a novel approach to\ngesture generation that leverages the semantic analysis capabilities of large\nlanguage models , such as ChatGPT. By capitalizing on the strengths of LLMs for\ntext analysis, we adopt a controlled approach to generate and integrate\nprofessional gestures and base gestures through a text parsing script,\nresulting in diverse and meaningful gestures. Firstly, our approach involves\nthe development of prompt principles that transform gesture generation into an\nintention classification problem using ChatGPT. We also conduct further\nanalysis on emphasis words and semantic words to aid in gesture generation.\nSubsequently, we construct a specialized gesture lexicon with multiple semantic\nannotations, decoupling the synthesis of gestures into professional gestures\nand base gestures. Finally, we merge the professional gestures with base\ngestures. Experimental results demonstrate that GesGPT effectively generates\ncontextually appropriate and expressive gestures.\n', ""  Gestures are inherent to human interaction and often complement speech in\nface-to-face communication, forming a multimodal communication system. An\nimportant task in gesture analysis is detecting a gesture's beginning and end.\nResearch on automatic gesture detection has primarily focused on visual and\nkinematic information to detect a limited set of isolated or silent gestures\nwith low variability, neglecting the integration of speech and vision signals\nto detect gestures that co-occur with speech. This work addresses this gap by\nfocusing on co-speech gesture detection, emphasising the synchrony between\nspeech and co-speech hand gestures. We address three main challenges: the\nvariability of gesture forms, the temporal misalignment between gesture and\nspeech onsets, and differences in sampling rate between modalities. We\ninvestigate extended speech time windows and employ separate backbone models\nfor each modality to address the temporal misalignment and sampling rate\ndifferences. We utilize Transformer encoders in cross-modal and early fusion\ntechniques to effectively align and integrate speech and skeletal sequences.\nThe study results show that combining visual and speech information\nsignificantly enhances gesture detection performance. Our findings indicate\nthat expanding the speech buffer beyond visual time segments improves\nperformance and that multimodal integration using cross-modal and early fusion\ntechniques outperforms baseline methods using unimodal and late fusion methods.\nAdditionally, we find a correlation between the models' gesture prediction\nconfidence and low-level speech frequency features potentially associated with\ngestures. Overall, the study provides a better understanding and detection\nmethods for co-speech gestures, facilitating the analysis of multimodal\ncommunication.\n""]",Multimodal Human-Computer Interaction and Communication,Sign Language Processing and Translation
131,"Human Activity Recognition with Wearable Sensors , Skeleton-based Action Recognition , Stance Detection and Analysis","['activity', 'activities', 'wearable', 'sensing', 'tracking', 'fitness', 'pose', 'recognizing', 'recognition', 'datasets'] , ['actions', 'action', 'multimodal', 'activities', 'videos', 'activity', 'recognition', 'features', 'cnn', 'motion'] , ['stances', 'stance', 'annotators', 'annotation', 'sentiment', 'semantics', 'annotated', 'contextual', 'tweets', 'bias']","['  Sensor-based human activity recognition (HAR) has been an active research\narea, owing to its applications in smart environments, assisted living,\nfitness, healthcare, etc. Recently, deep learning based end-to-end training has\nresulted in state-of-the-art performance in domains such as computer vision and\nnatural language, where large amounts of annotated data are available. However,\nlarge quantities of annotated data are not available for sensor-based HAR.\nMoreover, the real-world settings on which the HAR is performed differ in terms\nof sensor modalities, classification tasks, and target users. To address this\nproblem, transfer learning has been employed extensively. In this survey, we\nfocus on these transfer learning methods in the application domains of smart\nhome and wearables-based HAR. In particular, we provide a problem-solution\nperspective by categorizing and presenting the works in terms of their\ncontributions and the challenges they address. We also present an updated view\nof the state-of-the-art for both application domains. Based on our analysis of\n205 papers, we highlight the gaps in the literature and provide a roadmap for\naddressing them. This survey provides a reference to the HAR community, by\nsummarizing the existing works and providing a promising research agenda.\n', '  The proliferation of deep learning has significantly advanced various fields,\nyet Human Activity Recognition (HAR) has not fully capitalized on these\ndevelopments, primarily due to the scarcity of labeled datasets. Despite the\nintegration of advanced Inertial Measurement Units (IMUs) in ubiquitous\nwearable devices like smartwatches and fitness trackers, which offer\nself-labeled activity data from users, the volume of labeled data remains\ninsufficient compared to domains where deep learning has achieved remarkable\nsuccess. Addressing this gap, in this paper, we propose a novel approach to\nimprove wearable sensor-based HAR by introducing a pose-to-sensor network model\nthat generates sensor data directly from 3D skeleton pose sequences. our method\nsimultaneously trains the pose-to-sensor network and a human activity\nclassifier, optimizing both data reconstruction and activity recognition. Our\ncontributions include the integration of simultaneous training, direct\npose-to-sensor generation, and a comprehensive evaluation on the MM-Fit\ndataset. Experimental results demonstrate the superiority of our framework with\nsignificant performance improvements over baseline methods.\n', '  Human activity recognition (HAR) from on-body sensors is a core functionality\nin many AI applications: from personal health, through sports and wellness to\nIndustry 4.0. A key problem holding up progress in wearable sensor-based HAR,\ncompared to other ML areas, such as computer vision, is the unavailability of\ndiverse and labeled training data. Particularly, while there are innumerable\nannotated images available in online repositories, freely available sensor data\nis sparse and mostly unlabeled. We propose an unsupervised statistical\nfeature-guided diffusion model specifically optimized for wearable sensor-based\nhuman activity recognition with devices such as inertial measurement unit (IMU)\nsensors. The method generates synthetic labeled time-series sensor data without\nrelying on annotated training data. Thereby, it addresses the scarcity and\nannotation difficulties associated with real-world sensor data. By conditioning\nthe diffusion model on statistical information such as mean, standard\ndeviation, Z-score, and skewness, we generate diverse and representative\nsynthetic sensor data. We conducted experiments on public human activity\nrecognition datasets and compared the method to conventional oversampling and\nstate-of-the-art generative adversarial network methods. Experimental results\ndemonstrate that this can improve the performance of human activity recognition\nand outperform existing techniques.\n'] , ['  The fine-grained action analysis of the existing action datasets is\nchallenged by insufficient action categories, low fine granularities, limited\nmodalities, and tasks. In this paper, we propose a Multi-modality and\nMulti-task dataset of Figure Skating (MMFS) which was collected from the World\nFigure Skating Championships. MMFS, which possesses action recognition and\naction quality assessment, captures RGB, skeleton, and is collected the score\nof actions from 11671 clips with 256 categories including spatial and temporal\nlabels. The key contributions of our dataset fall into three aspects as\nfollows. (1) Independently spatial and temporal categories are first proposed\nto further explore fine-grained action recognition and quality assessment. (2)\nMMFS first introduces the skeleton modality for complex fine-grained action\nquality assessment. (3) Our multi-modality and multi-task dataset encourage\nmore action analysis models. To benchmark our dataset, we adopt RGB-based and\nskeleton-based baseline methods for action recognition and action quality\nassessment.\n', '  Supervised and self-supervised learning are two main training paradigms for\nskeleton-based human action recognition. However, the former one-hot\nclassification requires labor-intensive predefined action categories\nannotations, while the latter involves skeleton transformations (e.g.,\ncropping) in the pretext tasks that may impair the skeleton structure. To\naddress these challenges, we introduce a novel skeleton-based training\nframework (C$^2$VL) based on Cross-modal Contrastive learning that uses the\nprogressive distillation to learn task-agnostic human skeleton action\nrepresentation from the Vision-Language knowledge prompts. Specifically, we\nestablish the vision-language action concept space through vision-language\nknowledge prompts generated by pre-trained large multimodal models (LMMs),\nwhich enrich the fine-grained details that the skeleton action space lacks.\nMoreover, we propose the intra-modal self-similarity and inter-modal\ncross-consistency softened targets in the cross-modal contrastive process to\nprogressively control and guide the degree of pulling vision-language knowledge\nprompts and corresponding skeletons closer. These soft instance discrimination\nand self-knowledge distillation strategies contribute to the learning of better\nskeleton-based action representations from the noisy skeleton-vision-language\npairs. During the inference phase, our method requires only the skeleton data\nas the input for action recognition and no longer for vision-language prompts.\nExtensive experiments show that our method achieves state-of-the-art results on\nNTU RGB+D 60, NTU RGB+D 120, and PKU-MMD datasets. The code will be available\nin the future.\n', '  One-shot skeleton action recognition, which aims to learn a skeleton action\nrecognition model with a single training sample, has attracted increasing\ninterest due to the challenge of collecting and annotating large-scale skeleton\naction data. However, most existing studies match skeleton sequences by\ncomparing their feature vectors directly which neglects spatial structures and\ntemporal orders of skeleton data. This paper presents a novel one-shot skeleton\naction recognition technique that handles skeleton action recognition via\nmulti-scale spatial-temporal feature matching. We represent skeleton data at\nmultiple spatial and temporal scales and achieve optimal feature matching from\ntwo perspectives. The first is multi-scale matching which captures the\nscale-wise semantic relevance of skeleton data at multiple spatial and temporal\nscales simultaneously. The second is cross-scale matching which handles\ndifferent motion magnitudes and speeds by capturing sample-wise relevance\nacross multiple scales. Extensive experiments over three large-scale datasets\n(NTU RGB+D, NTU RGB+D 120, and PKU-MMD) show that our method achieves superior\none-shot skeleton action recognition, and it outperforms the state-of-the-art\nconsistently by large margins.\n'] , [""  Social media platforms are rich sources of opinionated content. Stance\ndetection allows the automatic extraction of users' opinions on various topics\nfrom such content. We focus on zero-shot stance detection, where the model's\nsuccess relies on (a) having knowledge about the target topic; and (b) learning\ngeneral reasoning strategies that can be employed for new topics. We present\nStance Reasoner, an approach to zero-shot stance detection on social media that\nleverages explicit reasoning over background knowledge to guide the model's\ninference about the document's stance on a target. Specifically, our method\nuses a pre-trained language model as a source of world knowledge, with the\nchain-of-thought in-context learning approach to generate intermediate\nreasoning steps. Stance Reasoner outperforms the current state-of-the-art\nmodels on 3 Twitter datasets, including fully supervised models. It can better\ngeneralize across targets, while at the same time providing explicit and\ninterpretable explanations for its predictions.\n"", ""  Stance detection classifies stance relations (namely, Favor, Against, or\nNeither) between comments and targets. Pretrained language models (PLMs) are\nwidely used to mine the stance relation to improve the performance of stance\ndetection through pretrained knowledge. However, PLMs also embed ``bad''\npretrained knowledge concerning stance into the extracted stance relation\nsemantics, resulting in pretrained stance bias. It is not trivial to measure\npretrained stance bias due to its weak quantifiability. In this paper, we\npropose Relative Counterfactual Contrastive Learning (RCCL), in which\npretrained stance bias is mitigated as relative stance bias instead of absolute\nstance bias to overtake the difficulty of measuring bias. Firstly, we present a\nnew structural causal model for characterizing complicated relationships among\ncontext, PLMs and stance relations to locate pretrained stance bias. Then,\nbased on masked language model prediction, we present a target-aware relative\nstance sample generation method for obtaining relative bias. Finally, we use\ncontrastive learning based on counterfactual theory to mitigate pretrained\nstance bias and preserve context stance relation. Experiments show that the\nproposed method is superior to stance detection and debiasing baselines.\n"", '  Stance detection is the view towards a specific target by a given context\n(\\textit{e.g.} tweets, commercial reviews). Target-related knowledge is often\nneeded to assist stance detection models in understanding the target well and\nmaking detection correctly. However, prevailing works for knowledge-infused\nstance detection predominantly incorporate target knowledge from a singular\nsource that lacks knowledge verification in limited domain knowledge. The\nlow-resource training data further increases the challenge for the data-driven\nlarge models in this task. To address those challenges, we propose a\ncollaborative knowledge infusion approach for low-resource stance detection\ntasks, employing a combination of aligned knowledge enhancement and efficient\nparameter learning techniques. Specifically, our stance detection approach\nleverages target background knowledge collaboratively from different knowledge\nsources with the help of knowledge alignment. Additionally, we also introduce\nthe parameter-efficient collaborative adaptor with a staged optimization\nalgorithm, which collaboratively addresses the challenges associated with\nlow-resource stance detection tasks from both network structure and learning\nperspectives. To assess the effectiveness of our method, we conduct extensive\nexperiments on three public stance detection datasets, including low-resource\nand cross-target settings. The results demonstrate significant performance\nimprovements compared to the existing stance detection approaches.\n']",Human Activity and Stance Analysis with Multimodal Sensing and AI,Skeleton-based Action Recognition
132,"Underwater Acoustic Signal Processing and Recognition , Object Detection in Underwater Environments","['acoustic', 'bioacoustics', 'recordings', 'bioacoustic', 'birdnet', 'audio', 'supervised', 'recognizing', 'recognition', 'underwater'] , ['underwater', 'submerged', 'cnn', 'sonar', 'recognition', 'camera', 'lidar', 'detection', 'detecting', 'detector']","['  This paper comprehensively reviews recent advances in underwater acoustic\nsignal denoising, an area critical for improving the reliability and clarity of\nunderwater communication and monitoring systems. Despite significant progress\nin the field, the complex nature of underwater environments poses unique\nchallenges that complicate the denoising process. We begin by outlining the\nfundamental challenges associated with underwater acoustic signal processing,\nincluding signal attenuation, noise variability, and the impact of\nenvironmental factors. The review then systematically categorizes and discusses\nvarious denoising algorithms, such as conventional, decomposition-based, and\nlearning-based techniques, highlighting their applications, advantages, and\nlimitations. Evaluation metrics and experimental datasets are also reviewed.\nThe paper concludes with a list of open questions and recommendations for\nfuture research directions, emphasizing the need for developing more robust\ndenoising techniques that can adapt to the dynamic underwater acoustic\nenvironment.\n', '  This paper presents a novel deep learning approach for analyzing massive\nunderwater acoustic data by leveraging a model trained on a broad spectrum of\nnon-underwater (aerial) sounds. Recognizing the challenge in labeling vast\namounts of underwater data, we propose a two-fold methodology to accelerate\nthis labor-intensive procedure.\n  The first part of our approach involves PCA and UMAP visualization of the\nunderwater data using the feature vectors of an aerial sound recognition model.\nThis enables us to cluster the data in a two dimensional space and listen to\npoints within these clusters to understand their defining characteristics. This\ninnovative method simplifies the process of selecting candidate labels for\nfurther training.\n  In the second part, we train a neural network model using both the selected\nunderwater data and the non-underwater dataset. We conducted a quantitative\nanalysis to measure the precision, recall, and F1 score of our model for\nrecognizing airgun sounds, a common type of underwater sound. The F1 score\nachieved by our model exceeded 84.3%, demonstrating the effectiveness of our\napproach in analyzing underwater acoustic data.\n  The methodology presented in this paper holds significant potential to reduce\nthe amount of labor required in underwater data analysis and opens up new\npossibilities for further research in the field of cross-domain data analysis.\n', '  With the rapid advancement of technology, the recognition of underwater\nacoustic signals in complex environments has become increasingly crucial.\nCurrently, mainstream underwater acoustic signal recognition relies primarily\non time-frequency analysis to extract spectral features, finding widespread\napplications in the field. However, existing recognition methods heavily depend\non expert systems, facing limitations such as restricted knowledge bases and\nchallenges in handling complex relationships. These limitations stem from the\ncomplexity and maintenance difficulties associated with rules or inference\nengines. Recognizing the potential advantages of deep learning in handling\nintricate relationships, this paper proposes a method utilizing neural networks\nfor underwater acoustic signal recognition. The proposed approach involves\ncontinual learning of features extracted from spectra for the classification of\nunderwater acoustic signals. Deep learning models can automatically learn\nabstract features from data and continually adjust weights during training to\nenhance classification performance.\n'] , ['  Synthetic Aperture Radar (SAR) object detection has gained significant\nattention recently due to its irreplaceable all-weather imaging capabilities.\nHowever, this research field suffers from both limited public datasets (mostly\ncomprising <2K images with only mono-category objects) and inaccessible source\ncode. To tackle these challenges, we establish a new benchmark dataset and an\nopen-source method for large-scale SAR object detection. Our dataset,\nSARDet-100K, is a result of intense surveying, collecting, and standardizing 10\nexisting SAR detection datasets, providing a large-scale and diverse dataset\nfor research purposes. To the best of our knowledge, SARDet-100K is the first\nCOCO-level large-scale multi-class SAR object detection dataset ever created.\nWith this high-quality dataset, we conducted comprehensive experiments and\nuncovered a crucial challenge in SAR object detection: the substantial\ndisparities between the pretraining on RGB datasets and finetuning on SAR\ndatasets in terms of both data domain and model structure. To bridge these\ngaps, we propose a novel Multi-Stage with Filter Augmentation (MSFA)\npretraining framework that tackles the problems from the perspective of data\ninput, domain transition, and model migration. The proposed MSFA method\nsignificantly enhances the performance of SAR object detection models while\ndemonstrating exceptional generalizability and flexibility across diverse\nmodels. This work aims to pave the way for further advancements in SAR object\ndetection. The dataset and code is available at\nhttps://github.com/zcablii/SARDet_100K.\n', '  Underwater monocular depth estimation serves as the foundation for tasks such\nas 3D reconstruction of underwater scenes. However, due to the influence of\nlight and medium, the underwater environment undergoes a distinctive imaging\nprocess, which presents challenges in accurately estimating depth from a single\nimage. The existing methods fail to consider the unique characteristics of\nunderwater environments, leading to inadequate estimation results and limited\ngeneralization performance. Furthermore, underwater depth estimation requires\nextracting and fusing both local and global features, which is not fully\nexplored in existing methods. In this paper, an end-to-end learning framework\nfor underwater monocular depth estimation called UMono is presented, which\nincorporates underwater image formation model characteristics into network\narchitecture, and effectively utilize both local and global features of\nunderwater image. Experimental results demonstrate that the proposed method is\neffective for underwater monocular depth estimation and outperforms the\nexisting methods in both quantitative and qualitative analyses.\n', '  Degraded underwater images decrease the accuracy of underwater object\ndetection. However, existing methods for underwater image enhancement mainly\nfocus on improving the indicators in visual aspects, which may not benefit the\ntasks of underwater image detection, and may lead to serious degradation in\nperformance. To alleviate this problem, we proposed a bidirectional-guided\nmethod for underwater object detection, referred to as BG-YOLO. In the proposed\nmethod, network is organized by constructing an enhancement branch and a\ndetection branch in a parallel way. The enhancement branch consists of a\ncascade of an image enhancement subnet and an object detection subnet. And the\ndetection branch only consists of a detection subnet. A feature guided module\nconnects the shallow convolution layer of the two branches. When training the\nenhancement branch, the object detection subnet in the enhancement branch\nguides the image enhancement subnet to be optimized towards the direction that\nis most conducive to the detection task. The shallow feature map of the trained\nenhancement branch will be output to the feature guided module, constraining\nthe optimization of detection branch through consistency loss and prompting\ndetection branch to learn more detailed information of the objects. And hence\nthe detection performance will be refined. During the detection tasks, only\ndetection branch will be reserved so that no additional cost of computation\nwill be introduced. Extensive experiments demonstrate that the proposed method\nshows significant improvement in performance of the detector in severely\ndegraded underwater scenes while maintaining a remarkable detection speed.\n']",Object Detection and Signal Processing in Underwater Environments,Underwater Acoustic Signal Processing and Recognition
133,"Respiratory Sound Analysis for Disease Detection , Sound Event Localization and Detection (SELD)","['audiological', 'audio', 'recordings', 'respiratory', 'recording', 'voice', 'hearing', 'lung', 'acoustic', 'als'] , ['audio', 'interaural', 'recordings', 'binaural', 'acoustical', 'soundscape', 'auditory', 'microphone', 'acoustic', 'microphones']","['  Compared with invasive examinations that require tissue sampling, respiratory\nsound testing is a non-invasive examination method that is safer and easier for\npatients to accept. In this study, we introduce Rene, a pioneering large-scale\nmodel tailored for respiratory sound recognition. Rene has been rigorously\nfine-tuned with an extensive dataset featuring a broad array of respiratory\naudio samples, targeting disease detection, sound pattern classification, and\nevent identification. Our innovative approach applies a pre-trained speech\nrecognition model to process respiratory sounds, augmented with patient medical\nrecords. The resulting multi-modal deep-learning framework addresses\ninterpretability and real-time diagnostic challenges that have hindered\nprevious respiratory-focused models. Benchmark comparisons reveal that Rene\nsignificantly outperforms existing models, achieving improvements of 10.27%,\n16.15%, 15.29%, and 18.90% in respiratory event detection and audio\nclassification on the SPRSound database. Disease prediction accuracy on the\nICBHI database improved by 23% over the baseline in both mean average and\nharmonic scores. Moreover, we have developed a real-time respiratory sound\ndiscrimination system utilizing the Rene architecture. Employing\nstate-of-the-art Edge AI technology, this system enables rapid and accurate\nresponses for respiratory sound\nauscultation(https://github.com/zpforlove/Rene).\n', '  Respiratory audio, such as coughing and breathing sounds, has predictive\npower for a wide range of healthcare applications, yet is currently\nunder-explored. The main problem for those applications arises from the\ndifficulty in collecting large labeled task-specific data for model\ndevelopment. Generalizable respiratory acoustic foundation models pretrained\nwith unlabeled data would offer appealing advantages and possibly unlock this\nimpasse. However, given the safety-critical nature of healthcare applications,\nit is pivotal to also ensure openness and replicability for any proposed\nfoundation model solution. To this end, we introduce OPERA, an OPEn Respiratory\nAcoustic foundation model pretraining and benchmarking system, as the first\napproach answering this need. We curate large-scale respiratory audio datasets\n(~136K samples, 440 hours), pretrain three pioneering foundation models, and\nbuild a benchmark consisting of 19 downstream respiratory health tasks for\nevaluation. Our pretrained models demonstrate superior performance (against\nexisting acoustic models pretrained with general audio on 16 out of 19 tasks)\nand generalizability (to unseen datasets and new respiratory audio modalities).\nThis highlights the great promise of respiratory acoustic foundation models and\nencourages more studies using OPERA as an open resource to accelerate research\non respiratory audio for health. The system is accessible from\nhttps://github.com/evelyn0414/OPERA.\n', '  This study aims to develop an auxiliary diagnostic system for classifying\nabnormal lung respiratory sounds, enhancing the accuracy of automatic abnormal\nbreath sound classification through an innovative multi-label learning approach\nand multi-head attention mechanism. Addressing the issue of class imbalance and\nlack of diversity in existing respiratory sound datasets, our study employs a\nlightweight and highly accurate model, using a two-dimensional label set to\nrepresent multiple respiratory sound characteristics. Our method achieved a\n59.2% ICBHI score in the four-category task on the ICBHI2017 dataset,\ndemonstrating its advantages in terms of lightweight and high accuracy. This\nstudy not only improves the accuracy of automatic diagnosis of lung respiratory\nsound abnormalities but also opens new possibilities for clinical applications.\n'] , [""  In the Sound Event Localization and Detection (SELD) task, Transformer-based\nmodels have demonstrated impressive capabilities. However, the quadratic\ncomplexity of the Transformer's self-attention mechanism results in\ncomputational inefficiencies. In this paper, we propose a network architecture\nfor SELD called SELD-Mamba, which utilizes Mamba, a selective state-space\nmodel. We adopt the Event-Independent Network V2 (EINV2) as the foundational\nframework and replace its Conformer blocks with bidirectional Mamba blocks to\ncapture a broader range of contextual information while maintaining\ncomputational efficiency. Additionally, we implement a two-stage training\nmethod, with the first stage focusing on Sound Event Detection (SED) and\nDirection of Arrival (DoA) estimation losses, and the second stage\nreintroducing the Source Distance Estimation (SDE) loss. Our experimental\nresults on the 2024 DCASE Challenge Task3 dataset demonstrate the effectiveness\nof the selective state-space model in SELD and highlight the benefits of the\ntwo-stage training approach in enhancing SELD performance.\n"", '  Sound event localization and detection (SELD) is an important task in machine\nlistening. Major advancements rely on simulated data with sound events in\nspecific rooms and strong spatio-temporal labels. SELD data is simulated by\nconvolving spatialy-localized room impulse responses (RIRs) with sound\nwaveforms to place sound events in a soundscape. However, RIRs require manual\ncollection in specific rooms. We present SpatialScaper, a library for SELD data\nsimulation and augmentation. Compared to existing tools, SpatialScaper emulates\nvirtual rooms via parameters such as size and wall absorption. This allows for\nparameterized placement (including movement) of foreground and background sound\nsources. SpatialScaper also includes data augmentation pipelines that can be\napplied to existing SELD data. As a case study, we use SpatialScaper to add\nrooms to the DCASE SELD data. Training a model with our data led to progressive\nperformance improves as a direct function of acoustic diversity. These results\nshow that SpatialScaper is valuable to train robust SELD models.\n', '  Sound Event Detection and Localization (SELD) is a combined task of\nidentifying sound events and their corresponding direction-of-arrival (DOA).\nWhile this task has numerous applications and has been extensively researched\nin recent years, it fails to provide full information about the sound source\nposition. In this paper, we overcome this problem by extending the task to\nSound Event Detection, Localization with Distance Estimation (3D SELD). We\nstudy two ways of integrating distance estimation within the SELD core - a\nmulti-task approach, in which the problem is tackled by a separate model\noutput, and a single-task approach obtained by extending the multi-ACCDOA\nmethod to include distance information. We investigate both methods for the\nAmbisonic and binaural versions of STARSS23: Sony-TAU Realistic Spatial\nSoundscapes 2023. Moreover, our study involves experiments on the loss function\nrelated to the distance estimation part. Our results show that it is possible\nto perform 3D SELD without any degradation of performance in sound event\ndetection and DOA estimation.\n']",Audio Analysis for Health and Acoustic Applications,Sound Event Localization and Detection (SELD)
134,"EEG Signal Analysis and Decoding Techniques , ECG Signal Processing and Analysis , ""Earthquake Monitoring and Seismic Signal Processing""","['electroencephalogram', 'eeg', 'electroencephalography', 'bci', 'neural', 'eeg_glt', 'brain', 'bcis', 'fmri', 'electrode'] , ['ecg', 'electrocardiogram', 'electrocardiograms', 'ecgs', 'arrhythmia', 'heartbeat', 'arrhythmias', 'electrocardiography', 'cardiac', 'recordings'] , ['earthquakes', 'earthquake', 'seismic', 'seismicity', 'geotechnical', 'disasters', 'landslide', 'tsunami', 'ground', 'seismograms']","['  Electroencephalogram (EEG) is a non-invasive technique to record\nbioelectrical signals. Integrating supervised deep learning techniques with EEG\nsignals has recently facilitated automatic analysis across diverse EEG-based\ntasks. However, the label issues of EEG signals have constrained the\ndevelopment of EEG-based deep models. Obtaining EEG annotations is difficult\nthat requires domain experts to guide collection and labeling, and the\nvariability of EEG signals among different subjects causes significant label\nshifts. To solve the above challenges, self-supervised learning (SSL) has been\nproposed to extract representations from unlabeled samples through\nwell-designed pretext tasks. This paper concentrates on integrating SSL\nframeworks with temporal EEG signals to achieve efficient representation and\nproposes a systematic review of the SSL for EEG signals. In this paper, 1) we\nintroduce the concept and theory of self-supervised learning and typical SSL\nframeworks. 2) We provide a comprehensive review of SSL for EEG analysis,\nincluding taxonomy, methodology, and technique details of the existing\nEEG-based SSL frameworks, and discuss the difference between these methods. 3)\nWe investigate the adaptation of the SSL approach to various downstream tasks,\nincluding the task description and related benchmark datasets. 4) Finally, we\ndiscuss the potential directions for future SSL-EEG research.\n', ""  Decoding linguistic information from non-invasive brain signals using EEG has\ngained increasing research attention due to its vast applicational potential.\nRecently, a number of works have adopted a generative-based framework to decode\nelectroencephalogram (EEG) signals into sentences by utilizing the power\ngenerative capacity of pretrained large language models (LLMs). However, this\napproach has several drawbacks that hinder the further development of\nlinguistic applications for brain-computer interfaces (BCIs). Specifically, the\nability of the EEG encoder to learn semantic information from EEG data remains\nquestionable, and the LLM decoder's tendency to generate sentences based on its\ntraining memory can be hard to avoid. These issues necessitate a novel approach\nfor converting EEG signals into sentences. In this paper, we propose a novel\ntwo-step pipeline that addresses these limitations and enhances the validity of\nlinguistic EEG decoding research. We first confirm that word-level semantic\ninformation can be learned from EEG data recorded during natural reading by\ntraining a Conformer encoder via a masked contrastive objective for word-level\nclassification. To achieve sentence decoding results, we employ a training-free\nretrieval method to retrieve sentences based on the predictions from the EEG\nencoder. Extensive experiments and ablation studies were conducted in this\npaper for a comprehensive evaluation of the proposed approach. Visualization of\nthe top prediction candidates reveals that our model effectively groups EEG\nsegments into semantic categories with similar meanings, thereby validating its\nability to learn patterns from unspoken EEG recordings. Despite the exploratory\nnature of this work, these results suggest that our method holds promise for\nproviding more reliable solutions for converting EEG signals into text.\n"", '  Electroencephalography (EEG) signals, known for convenient non-invasive\nacquisition but low signal-to-noise ratio, have recently gained substantial\nattention due to the potential to decode natural images. This paper presents a\nself-supervised framework to demonstrate the feasibility of learning image\nrepresentations from EEG signals, particularly for object recognition. The\nframework utilizes image and EEG encoders to extract features from paired image\nstimuli and EEG responses. Contrastive learning aligns these two modalities by\nconstraining their similarity. With the framework, we attain significantly\nabove-chance results on a comprehensive EEG-image dataset, achieving a top-1\naccuracy of 15.6% and a top-5 accuracy of 42.8% in challenging 200-way\nzero-shot tasks. Moreover, we perform extensive experiments to explore the\nbiological plausibility by resolving the temporal, spatial, spectral, and\nsemantic aspects of EEG signals. Besides, we introduce attention modules to\ncapture spatial correlations, providing implicit evidence of the brain activity\nperceived from EEG data. These findings yield valuable insights for neural\ndecoding and brain-computer interfaces in real-world scenarios. The code will\nbe released on https://github.com/eeyhsong/NICE-EEG.\n'] , ['  Electrocardiogram (ECG) signals play a pivotal role in cardiovascular\ndiagnostics, providing essential information on the electrical activity of the\nheart. However, the inherent noise and limited resolution in ECG recordings can\nhinder accurate interpretation and diagnosis. In this paper, we propose a novel\nmodel for ECG super resolution (SR) that uses a DNAE to enhance temporal and\nfrequency information inside ECG signals. Our approach addresses the\nlimitations of traditional ECG signal processing techniques. Our model takes in\ninput 5-second length ECG windows sampled at 50 Hz (very low resolution) and it\nis able to reconstruct a denoised super-resolution signal with an x10\nupsampling rate (sampled at 500 Hz). We trained the proposed DCAE-SR on public\navailable myocardial infraction ECG signals. Our method demonstrates superior\nperformance in reconstructing high-resolution ECG signals from very\nlow-resolution signals with a sampling rate of 50 Hz. We compared our results\nwith the current deep-learning literature approaches for ECG super-resolution\nand some non-deep learning reproducible methods that can perform both\nsuper-resolution and denoising. We obtained current state-of-the-art\nperformances in super-resolution of very low resolution ECG signals frequently\ncorrupted by ECG artifacts. We were able to obtain a signal-to-noise ratio of\n12.20 dB (outperforms previous 4.68 dB), mean squared error of 0.0044\n(outperforms previous 0.0154) and root mean squared error of 4.86% (outperforms\nprevious 12.40%). In conclusion, our DCAE-SR model offers a robust (to artefact\npresence), versatile and explainable solution to enhance the quality of ECG\nsignals. This advancement holds promise in advancing the field of\ncardiovascular diagnostics, paving the way for improved patient care and\nhigh-quality clinical decisions\n', '  Within cardiovascular disease detection using deep learning applied to ECG\nsignals, the complexities of handling physiological signals have sparked\ngrowing interest in leveraging deep generative models for effective data\naugmentation. In this paper, we introduce a novel versatile approach based on\ndenoising diffusion probabilistic models for ECG synthesis, addressing three\nscenarios: (i) heartbeat generation, (ii) partial signal imputation, and (iii)\nfull heartbeat forecasting. Our approach presents the first generalized\nconditional approach for ECG synthesis, and our experimental results\ndemonstrate its effectiveness for various ECG-related tasks. Moreover, we show\nthat our approach outperforms other state-of-the-art ECG generative models and\ncan enhance the performance of state-of-the-art classifiers.\n', '  Cardiovascular disease is a major life-threatening condition that is commonly\nmonitored using electrocardiogram (ECG) signals. However, these signals are\noften contaminated by various types of noise at different intensities,\nsignificantly interfering with downstream tasks. Therefore, denoising ECG\nsignals and increasing the signal-to-noise ratio is crucial for cardiovascular\nmonitoring. In this paper, we propose a deep learning method that combines a\none-dimensional convolutional layer with transformer architecture for denoising\nECG signals. The convolutional layer processes the ECG signal by various\nkernel/patch sizes and generates an embedding called multi-scale patch\nembedding. The embedding then is used as the input of a transformer network and\nenhances the capability of the transformer for denoising the ECG signal.\n'] , ['  Seismograms, the fundamental seismic records, have revolutionized earthquake\nresearch and monitoring. Recent advancements in deep learning have further\nenhanced seismic signal processing, leading to even more precise and effective\nearthquake monitoring capabilities. This paper introduces a foundational deep\nlearning model, the Seismogram Transformer (SeisT), designed for a variety of\nearthquake monitoring tasks. SeisT combines multiple modules tailored to\ndifferent tasks and exhibits impressive out-of-distribution generalization\nperformance, outperforming or matching state-of-the-art models in tasks like\nearthquake detection, seismic phase picking, first-motion polarity\nclassification, magnitude estimation, back-azimuth estimation, and epicentral\ndistance estimation. The performance scores on the tasks are 0.96, 0.96, 0.68,\n0.95, 0.86, 0.55, and 0.81, respectively. The most significant improvements, in\ncomparison to existing models, are observed in phase-P picking, phase-S\npicking, and magnitude estimation, with gains of 1.7%, 9.5%, and 8.0%,\nrespectively. Our study, through rigorous experiments and evaluations, suggests\nthat SeisT has the potential to contribute to the advancement of seismic signal\nprocessing and earthquake research.\n', '  Predicting high-fidelity ground motions for future earthquakes is crucial for\nseismic hazard assessment and infrastructure resilience. Conventional empirical\nsimulations suffer from sparse sensor distribution and geographically localized\nearthquake locations, while physics-based methods are computationally intensive\nand require accurate representations of Earth structures and earthquake\nsources. We propose a novel artificial intelligence (AI) simulator, Conditional\nGenerative Modeling for Ground Motion (CGM-GM), to synthesize high-frequency\nand spatially continuous earthquake ground motion waveforms. CGM-GM leverages\nearthquake magnitudes and geographic coordinates of earthquakes and sensors as\ninputs, learning complex wave physics and Earth heterogeneities, without\nexplicit physics constraints. This is achieved through a probabilistic\nautoencoder that captures latent distributions in the time-frequency domain and\nvariational sequential models for prior and posterior distributions. We\nevaluate the performance of CGM-GM using small-magnitude earthquake records\nfrom the San Francisco Bay Area, a region with high seismic risks. CGM-GM\ndemonstrates a strong potential for outperforming a state-of-the-art\nnon-ergodic empirical ground motion model and shows great promise in seismology\nand beyond.\n', '  This article surveys the growing interest in utilizing Deep Learning (DL) as\na powerful tool to address challenging problems in earthquake engineering.\nDespite decades of advancement in domain knowledge, issues such as uncertainty\nin earthquake occurrence, unpredictable seismic loads, nonlinear structural\nresponses, and community engagement remain difficult to tackle using\ndomain-specific methods. DL offers promising solutions by leveraging its\ndata-driven capacity for nonlinear mapping, sequential data modeling, automatic\nfeature extraction, dimensionality reduction, optimal decision-making, etc.\nHowever, the literature lacks a comprehensive review that systematically covers\na consistent scope intersecting DL and earthquake engineering. To bridge the\ngap, the article first discusses methodological advances to elucidate various\napplicable DL techniques, such as multi-layer perceptron (MLP), convolutional\nneural network (CNN), recurrent neural network (RNN), generative adversarial\nnetwork (GAN), autoencoder (AE), transfer learning (TL), reinforcement learning\n(RL), and graph neural network (GNN). A thorough research landscape is then\ndisclosed by exploring various DL applications across different research\ntopics, including vision-based seismic damage assessment and structural\ncharacterization, seismic demand and damage state prediction, seismic response\nhistory prediction, regional seismic risk assessment and community resilience,\nground motion (GM) for engineering use, seismic response control, and the\ninverse problem of system/damage identification. Suitable DL techniques for\neach research topic are identified, emphasizing the preeminence of CNN for\nvision-based tasks, RNN for sequential data, RL for community resilience, and\nunsupervised learning for GM analysis. The article also discusses opportunities\nand challenges for leveraging DL in earthquake engineering research and\npractice.\n']",Signal Processing and Analysis in Biomedical and Geophysical Applications,ECG Signal Processing and Analysis
135,"Analog Circuit Design and Automation , Experiment Design and Analysis","['circuits', 'circuitsynth', 'analogcoder', 'circuitvae', 'hardware', 'circuit', 'analog', 'chip', 'transistor', 'engineers'] , ['experimentation', 'experimenters', 'statistical', 'experiments', 'metrics', 'empirical', 'randomized', 'testing', 'estimators', 'metric']","['  Analog circuit design is a significant task in modern chip technology,\nfocusing on the selection of component types, connectivity, and parameters to\nensure proper circuit functionality. Despite advances made by Large Language\nModels (LLMs) in digital circuit design, the complexity and scarcity of data in\nanalog circuitry pose significant challenges. To mitigate these issues, we\nintroduce AnalogCoder, the first training-free LLM agent for designing analog\ncircuits through Python code generation. Firstly, AnalogCoder incorporates a\nfeedback-enhanced flow with tailored domain-specific prompts, enabling the\nautomated and self-correcting design of analog circuits with a high success\nrate. Secondly, it proposes a circuit tool library to archive successful\ndesigns as reusable modular sub-circuits, simplifying composite circuit\ncreation. Thirdly, extensive experiments on a benchmark designed to cover a\nwide range of analog circuit tasks show that AnalogCoder outperforms other\nLLM-based methods. It has successfully designed 20 circuits, 5 more than\nstandard GPT-4o. We believe AnalogCoder can significantly improve the\nlabor-intensive chip design process, enabling non-experts to design analog\ncircuits efficiently.\n', ""  The electronic design automation of analog circuits has been a longstanding\nchallenge in the integrated circuit field due to the huge design space and\ncomplex design trade-offs among circuit specifications. In the past decades,\nintensive research efforts have mostly been paid to automate the transistor\nsizing with a given circuit topology. By recognizing the graph nature of\ncircuits, this paper presents a Circuit Graph Neural Network (CktGNN) that\nsimultaneously automates the circuit topology generation and device sizing\nbased on the encoder-dependent optimization subroutines. Particularly, CktGNN\nencodes circuit graphs using a two-level GNN framework (of nested GNN) where\ncircuits are represented as combinations of subgraphs in a known subgraph\nbasis. In this way, it significantly improves design efficiency by reducing the\nnumber of subgraphs to perform message passing. Nonetheless, another critical\nroadblock to advancing learning-assisted circuit design automation is a lack of\npublic benchmarks to perform canonical assessment and reproducible research. To\ntackle the challenge, we introduce Open Circuit Benchmark (OCB), an\nopen-sourced dataset that contains $10$K distinct operational amplifiers with\ncarefully-extracted circuit specifications. OCB is also equipped with\ncommunicative circuit generation and evaluation capabilities such that it can\nhelp to generalize CktGNN to design various analog circuits by producing\ncorresponding datasets. Experiments on OCB show the extraordinary advantages of\nCktGNN through representation-based optimization frameworks over other recent\npowerful GNN baselines and human experts' manual designs. Our work paves the\nway toward a learning-based open-sourced design automation for analog circuits.\nOur source code is available at \\url{https://github.com/zehao-dong/CktGNN}.\n"", '  Analog and radio-frequency circuit design requires extensive exploration of\nboth circuit topology and parameters to meet specific design criteria like\npower consumption and bandwidth. Designers must review state-of-the-art\ntopology configurations in the literature and sweep various circuit parameters\nwithin each configuration. This design process is highly specialized and\ntime-intensive, particularly as the number of circuit parameters increases and\nthe circuit becomes more complex. Prior research has explored the potential of\nmachine learning to enhance circuit design procedures. However, these studies\nprimarily focus on simple circuits, overlooking the more practical and complex\nanalog and radio-frequency systems. A major obstacle for bearing the power of\nmachine learning in circuit design is the availability of a generic and diverse\ndataset, along with robust metrics, which are essential for thoroughly\nevaluating and improving machine learning algorithms in the analog and\nradio-frequency circuit domain. We present AICircuit, a comprehensive\nmulti-level dataset and benchmark for developing and evaluating ML algorithms\nin analog and radio-frequency circuit design. AICircuit comprises seven\ncommonly used basic circuits and two complex wireless transceiver systems\ncomposed of multiple circuit blocks, encompassing a wide array of design\nscenarios encountered in real-world applications. We extensively evaluate\nvarious ML algorithms on the dataset, revealing the potential of ML algorithms\nin learning the mapping from the design specifications to the desired circuit\nparameters.\n'] , ['  In designing an online A/B experiment, it is crucial to select a sample size\nand duration that ensure the resulting confidence interval (CI) for the\ntreatment effect is the right width to detect an effect of meaningful magnitude\nwith sufficient statistical power without wasting resources. While the\nrelationship between sample size and CI width is well understood, the effect of\nexperiment duration on CI width remains less clear. This paper provides an\nanalytical formula for the width of a CI based on a ratio treatment effect\nestimator as a function of both sample size (N) and duration (T). The formula\nis derived from a mixed effects model with two variance components. One\ncomponent, referred to as the temporal variance, persists over time for\nexperiments where the same users are kept in the same experiment arm across\ndifferent days. The remaining error variance component, by contrast, decays to\nzero as T gets large. The formula we derive introduces a key parameter that we\ncall the user-specific temporal correlation (UTC), which quantifies the\nrelative sizes of the two variance components and can be estimated from\nhistorical experiments. Higher UTC indicates a slower decay in CI width over\ntime. On the other hand, when the UTC is 0 -- as for experiments where users\nshuffle in and out of the experiment across days -- the CI width decays at the\nstandard parametric 1/T rate. We also study how access to pre-period data for\nthe users in the experiment affects the CI width decay. We show our formula\nclosely explains CI widths on real A/B experiments at YouTube.\n', '  In many randomized experiments, the treatment effect of the long-term metric\n(i.e. the primary outcome of interest) is often difficult or infeasible to\nmeasure. Such long-term metrics are often slow to react to changes and\nsufficiently noisy they are challenging to faithfully estimate in short-horizon\nexperiments. A common alternative is to measure several short-term proxy\nmetrics in the hope they closely track the long-term metric -- so they can be\nused to effectively guide decision-making in the near-term. We introduce a new\nstatistical framework to both define and construct an optimal proxy metric for\nuse in a homogeneous population of randomized experiments. Our procedure first\nreduces the construction of an optimal proxy metric in a given experiment to a\nportfolio optimization problem which depends on the true latent treatment\neffects and noise level of experiment under consideration. We then denoise the\nobserved treatment effects of the long-term metric and a set of proxies in a\nhistorical corpus of randomized experiments to extract estimates of the latent\ntreatment effects for use in the optimization problem. One key insight derived\nfrom our approach is that the optimal proxy metric for a given experiment is\nnot apriori fixed; rather it should depend on the sample size (or effective\nnoise level) of the randomized experiment for which it is deployed. To\ninstantiate and evaluate our framework, we employ our methodology in a large\ncorpus of randomized experiments from an industrial recommendation system and\nconstruct proxy metrics that perform favorably relative to several baselines.\n', ""  Online controlled experiments, colloquially known as A/B-tests, are the bread\nand butter of real-world recommender system evaluation. Typically, end-users\nare randomly assigned some system variant, and a plethora of metrics are then\ntracked, collected, and aggregated throughout the experiment. A North Star\nmetric (e.g. long-term growth or revenue) is used to assess which system\nvariant should be deemed superior. As a result, most collected metrics are\nsupporting in nature, and serve to either (i) provide an understanding of how\nthe experiment impacts user experience, or (ii) allow for confident\ndecision-making when the North Star metric moves insignificantly (i.e. a false\nnegative or type-II error). The latter is not straightforward: suppose a\ntreatment variant leads to fewer but longer sessions, with more views but fewer\nengagements; should this be considered a positive or negative outcome?\n  The question then becomes: how do we assess a supporting metric's utility\nwhen it comes to decision-making using A/B-testing? Online platforms typically\nrun dozens of experiments at any given time. This provides a wealth of\ninformation about interventions and treatment effects that can be used to\nevaluate metrics' utility for online evaluation. We propose to collect this\ninformation and leverage it to quantify type-I, type-II, and type-III errors\nfor the metrics of interest, alongside a distribution of measurements of their\nstatistical power (e.g. $z$-scores and $p$-values). We present results and\ninsights from building this pipeline at scale for two large-scale short-video\nplatforms: ShareChat and Moj; leveraging hundreds of past experiments to find\nonline metrics with high statistical power.\n""]",Design Automation and Experimentation in Engineering,Analog Circuit Design and Automation
136,"Particle Physics Detector Simulation , Particle Beam Dynamics in Accelerators","['particles', 'particle', 'collider', 'lhc', 'qcd', 'detectors', 'quarks', 'quark', 'detector', 'lhcb'] , ['autoencoder', 'accelerator', 'neuralized', 'autoencoded', 'accelerators', 'lstm', 'particle', 'beam', 'reactor', 'particles']","[""  The Large Hadron Collider's high luminosity era presents major computational\nchallenges in the analysis of collision events. Large amounts of Monte Carlo\n(MC) simulation will be required to constrain the statistical uncertainties of\nthe simulated datasets below these of the experimental data. Modelling of\nhigh-energy particles propagating through the calorimeter section of the\ndetector is the most computationally intensive MC simulation task. We introduce\na technique combining recent advancements in generative models and quantum\nannealing for fast and efficient simulation of high-energy particle-calorimeter\ninteractions.\n"", '  In modern collider experiments, the quest to explore fundamental interactions\nbetween elementary particles has reached unparalleled levels of precision.\nSignatures from particle physics detectors are low-level objects (such as\nenergy depositions or tracks) encoding the physics of collisions (the final\nstate particles of hard scattering interactions). The complete simulation of\nthem in a detector is a computational and storage-intensive task. To address\nthis computational bottleneck in particle physics, alternative approaches have\nbeen developed, introducing additional assumptions and trade off accuracy for\nspeed.The field has seen a surge in interest in surrogate modeling the detector\nsimulation, fueled by the advancements in deep generative models. These models\naim to generate responses that are statistically identical to the observed\ndata. In this paper, we conduct a comprehensive and exhaustive taxonomic review\nof the existing literature on the simulation of detector signatures from both\nmethodological and application-wise perspectives. Initially, we formulate the\nproblem of detector signature simulation and discuss its different variations\nthat can be unified. Next, we classify the state-of-the-art methods into five\ndistinct categories based on their underlying model architectures, summarizing\ntheir respective generation strategies. Finally, we shed light on the\nchallenges and opportunities that lie ahead in detector signature simulation,\nsetting the stage for future research and development.\n', '  The detection of out-of-distribution data points is a common task in particle\nphysics. It is used for monitoring complex particle detectors or for\nidentifying rare and unexpected events that may be indicative of new phenomena\nor physics beyond the Standard Model. Recent advances in Machine Learning for\nanomaly detection have encouraged the utilization of such techniques on\nparticle physics problems. This review article provides an overview of the\nstate-of-the-art techniques for anomaly detection in particle physics using\nmachine learning. We discuss the challenges associated with anomaly detection\nin large and complex data sets, such as those produced by high-energy particle\ncolliders, and highlight some of the successful applications of anomaly\ndetection in particle physics experiments.\n'] , ['  Particle accelerators are complex systems that focus, guide, and accelerate\nintense charged particle beams to high energy. Beam diagnostics present a\nchallenging problem due to limited non-destructive measurements,\ncomputationally demanding simulations, and inherent uncertainties in the\nsystem. We propose a two-step unsupervised deep learning framework named as\nConditional Latent Autoregressive Recurrent Model (CLARM) for learning the\nspatiotemporal dynamics of charged particles in accelerators. CLARM consists of\na Conditional Variational Autoencoder (CVAE) transforming six-dimensional phase\nspace into a lower-dimensional latent distribution and a Long Short-Term Memory\n(LSTM) network capturing temporal dynamics in an autoregressive manner. The\nCLARM can generate projections at various accelerator modules by sampling and\ndecoding the latent space representation. The model also forecasts future\nstates (downstream locations) of charged particles from past states (upstream\nlocations). The results demonstrate that the generative and forecasting ability\nof the proposed approach is promising when tested against a variety of\nevaluation metrics.\n', '  Charged particle dynamics under the influence of electromagnetic fields is a\nchallenging spatiotemporal problem. Many high performance physics-based\nsimulators for predicting behavior in a charged particle beam are\ncomputationally expensive, limiting their utility for solving inverse problems\nonline. The problem of estimating upstream six-dimensional phase space given\ndownstream measurements of charged particles in an accelerator is an inverse\nproblem of growing importance. This paper introduces a reverse Latent Evolution\nModel (rLEM) designed for temporal inversion of forward beam dynamics. In this\ntwo-step self-supervised deep learning framework, we utilize a Conditional\nVariational Autoencoder (CVAE) to project 6D phase space projections of a\ncharged particle beam into a lower-dimensional latent distribution.\nSubsequently, we autoregressively learn the inverse temporal dynamics in the\nlatent space using a Long Short-Term Memory (LSTM) network. The coupled\nCVAE-LSTM framework can predict 6D phase space projections across all upstream\naccelerating sections based on single or multiple downstream phase space\nmeasurements as inputs. The proposed model also captures the aleatoric\nuncertainty of the high-dimensional input data within the latent space. This\nuncertainty, which reflects potential uncertain measurements at a given module,\nis propagated through the LSTM to estimate uncertainty bounds for all upstream\npredictions, demonstrating the robustness of the LSTM against in-distribution\nvariations in the input data.\n', '  Addressing the charged particle beam diagnostics in accelerators poses a\nformidable challenge, demanding high-fidelity simulations in limited\ncomputational time. Machine learning (ML) based surrogate models have emerged\nas a promising tool for non-invasive charged particle beam diagnostics. Trained\nML models can make predictions much faster than computationally expensive\nphysics simulations. In this work, we have proposed a temporally structured\nvariational autoencoder model to autoregressively forecast the spatiotemporal\ndynamics of the 15 unique 2D projections of 6D phase space of charged particle\nbeam as it travels through the LANSCE linear accelerator. In the model, VAE\nembeds the phase space projections into a lower dimensional latent space. A\nlong-short-term memory network then learns the temporal correlations in the\nlatent space. The trained network can evolve the phase space projections across\nfurther modules provided the first few modules as inputs. The model predicts\nall the projections across different modules with low mean squared error and\nhigh structural similarity index.\n']",Particle Physics and Accelerator Technologies,Particle Physics Detector Simulation
137,"Turbulent Flow Simulation and Modeling , Plasma Simulation and Modeling in Tokamaks","['turbulent', 'flows', 'turbulence', 'flow', 'aerodynamic', 'reynolds', 'vortex', 'stokes', 'vorticity', 'fluids'] , ['tokamaks', 'tokamak', 'plasma', 'plasmas', 'fusion', 'simulations', 'modeling', 'magnetohydrodynamics', 'modelling', 'dynamics']","['  Fluid data completion is a research problem with high potential benefit for\nboth experimental and computational fluid dynamics. An effective fluid data\ncompletion method reduces the required number of sensors in a fluid dynamics\nexperiment, and allows a coarser and more adaptive mesh for a Computational\nFluid Dynamics (CFD) simulation. However, the ill-posed nature of the fluid\ndata completion problem makes it prohibitively difficult to obtain a\ntheoretical solution and presents high numerical uncertainty and instability\nfor a data-driven approach (e.g., a neural network model). To address these\nchallenges, we leverage recent advancements in computer vision, employing the\nvector quantization technique to map both complete and incomplete fluid data\nspaces onto discrete-valued lower-dimensional representations via a two-stage\nlearning procedure. We demonstrated the effectiveness of our approach on\nKolmogorov flow data (Reynolds number: 1000) occluded by masks of different\nsize and arrangement. Experimental results show that our proposed model\nconsistently outperforms benchmark models under different occlusion settings in\nterms of point-wise reconstruction accuracy as well as turbulent energy\nspectrum and vorticity distribution.\n', ""  Computational Fluid Dynamics (CFD) serves as a powerful tool for simulating\nfluid flow across diverse industries. High-resolution CFD simulations offer\nvaluable insights into fluid behavior and flow patterns, aiding in optimizing\ndesign features or enhancing system performance. However, as resolution\nincreases, computational data requirements and time increase proportionately.\nThis presents a persistent challenge in CFD. Recently, efforts have been\ndirected towards accurately predicting fine-mesh simulations using coarse-mesh\nsimulations, with geometry and boundary conditions as input. Drawing\ninspiration from models designed for super-resolution, deep learning techniques\nlike UNets have been applied to address this challenge. However, these existing\nmethods are limited to structured data and fail if the mesh is unstructured due\nto its inability to convolute. Additionally, incorporating geometry/mesh\ninformation in the training process introduces drawbacks such as increased data\nrequirements, challenges in generalizing to unseen geometries for the same\nphysical phenomena, and issues with robustness to mesh distortions. To address\nthese concerns, we propose a novel framework, PointSAGE a mesh-independent\nnetwork that leverages the unordered, mesh-less nature of Pointcloud to learn\nthe complex fluid flow and directly predict fine simulations, completely\nneglecting mesh information. Utilizing an adaptable framework, the model\naccurately predicts the fine data across diverse point cloud sizes, regardless\nof the training dataset's dimension. We have evaluated the effectiveness of\nPointSAGE on diverse datasets in different scenarios, demonstrating notable\nresults and a significant acceleration in computational time in generating fine\nsimulations compared to standard CFD techniques.\n"", '  Simulations of turbulent flows in 3D are one of the most expensive\nsimulations in computational fluid dynamics (CFD). Many works have been written\non surrogate models to replace numerical solvers for fluid flows with faster,\nlearned, autoregressive models. However, the intricacies of turbulence in three\ndimensions necessitate training these models with very small time steps, while\ngenerating realistic flow states requires either long roll-outs with many steps\nand significant error accumulation or starting from a known, realistic flow\nstate - something we aimed to avoid in the first place. Instead, we propose to\napproach turbulent flow simulation as a generative task directly learning the\nmanifold of all possible turbulent flow states without relying on any initial\nflow state. For our experiments, we introduce a challenging 3D turbulence\ndataset of high-resolution flows and detailed vortex structures caused by\nvarious objects and derive two novel sample evaluation metrics for turbulent\nflows. On this dataset, we show that our generative model captures the\ndistribution of turbulent flows caused by unseen objects and generates\nhigh-quality, realistic samples amenable for downstream applications without\naccess to any initial state.\n'] , ['  We explore the possibility of fully replacing a plasma physics kinetic\nsimulator with a graph neural network-based simulator. We focus on this class\nof surrogate models given the similarity between their message-passing update\nmechanism and the traditional physics solver update, and the possibility of\nenforcing known physical priors into the graph construction and update. We show\nthat our model learns the kinetic plasma dynamics of the one-dimensional plasma\nmodel, a predecessor of contemporary kinetic plasma simulation codes, and\nrecovers a wide range of well-known kinetic plasma processes, including plasma\nthermalization, electrostatic fluctuations about thermal equilibrium, and the\ndrag on a fast sheet and Landau damping. We compare the performance against the\noriginal plasma model in terms of run-time, conservation laws, and temporal\nevolution of key physical quantities. The limitations of the model are\npresented and possible directions for higher-dimensional surrogate models for\nkinetic plasmas are discussed.\n', '  Reduced-order plasma models that can efficiently predict plasma behavior\nacross various settings and configurations are highly sought after yet elusive.\nThe demand for such models has surged in the past decade due to their potential\nto facilitate scientific research and expedite the development of plasma\ntechnologies. In line with the advancements in computational power and\ndata-driven methods, we introduce the ""Phi Method"" in this two-part article.\nPart I presents this novel algorithm, which employs constrained regression on a\ncandidate term library informed by numerical discretization schemes to discover\ndiscretized systems of differential equations. We demonstrate Phi Method\'s\nefficacy in deriving reliable and robust reduced-order models (ROMs) for three\ntest cases: the Lorenz attractor, flow past a cylinder, and a 1D\nHall-thruster-representative plasma. Part II will delve into the method\'s\napplication for parametric dynamics discovery. Our results show that ROMs\nderived from the Phi Method provide remarkably accurate predictions of systems\'\nbehavior, whether derived from steady-state or transient-state data. This\nunderscores the method\'s potential for transforming plasma system modeling.\n', '  Predicting plasma evolution within a Tokamak reactor is crucial to realizing\nthe goal of sustainable fusion. Capabilities in forecasting the spatio-temporal\nevolution of plasma rapidly and accurately allow us to quickly iterate over\ndesign and control strategies on current Tokamak devices and future reactors.\nModelling plasma evolution using numerical solvers is often expensive,\nconsuming many hours on supercomputers, and hence, we need alternative\ninexpensive surrogate models. We demonstrate accurate predictions of plasma\nevolution both in simulation and experimental domains using deep learning-based\nsurrogate modelling tools, viz., Fourier Neural Operators (FNO). We show that\nFNO has a speedup of six orders of magnitude over traditional solvers in\npredicting the plasma dynamics simulated from magnetohydrodynamic models, while\nmaintaining a high accuracy (MSE in the normalised domain $\\approx$ $10^{-5}$).\nOur modified version of the FNO is capable of solving multi-variable Partial\nDifferential Equations (PDE), and can capture the dependence among the\ndifferent variables in a single model. FNOs can also predict plasma evolution\non real-world experimental data observed by the cameras positioned within the\nMAST Tokamak, i.e., cameras looking across the central solenoid and the\ndivertor in the Tokamak. We show that FNOs are able to accurately forecast the\nevolution of plasma and have the potential to be deployed for real-time\nmonitoring. We also illustrate their capability in forecasting the plasma\nshape, the locations of interactions of the plasma with the central solenoid\nand the divertor for the full (available) duration of the plasma shot within\nMAST. The FNO offers a viable alternative for surrogate modelling as it is\nquick to train and infer, and requires fewer data points, while being able to\ndo zero-shot super-resolution and getting high-fidelity solutions.\n']",Fluid Dynamics and Plasma Simulation,Turbulent Flow Simulation and Modeling
138,"Cellular Dynamics and Interactions , Neural Cellular Automata for Morphogenesis and Dynamics","['dynamics', 'cells', 'multicellular', 'networks', 'cell', 'neural', 'interactions', 'dynamical', 'cellular', 'motion'] , ['automata', 'neural', 'evolution', 'neuronal', 'evolved', 'developmental', 'morphogenesis', 'dynamics', 'automaton', 'cells']","['  Learning to represent and simulate the dynamics of physical systems is a\ncrucial yet challenging task. Existing equivariant Graph Neural Network (GNN)\nbased methods have encapsulated the symmetry of physics, \\emph{e.g.},\ntranslations, rotations, etc, leading to better generalization ability.\nNevertheless, their frame-to-frame formulation of the task overlooks the\nnon-Markov property mainly incurred by unobserved dynamics in the environment.\nIn this paper, we reformulate dynamics simulation as a spatio-temporal\nprediction task, by employing the trajectory in the past period to recover the\nNon-Markovian interactions. We propose Equivariant Spatio-Temporal Attentive\nGraph Networks (ESTAG), an equivariant version of spatio-temporal GNNs, to\nfulfill our purpose. At its core, we design a novel Equivariant Discrete\nFourier Transform (EDFT) to extract periodic patterns from the history frames,\nand then construct an Equivariant Spatial Module (ESM) to accomplish spatial\nmessage passing, and an Equivariant Temporal Module (ETM) with the forward\nattention and equivariant pooling mechanisms to aggregate temporal message. We\nevaluate our model on three real datasets corresponding to the molecular-,\nprotein- and macro-level. Experimental results verify the effectiveness of\nESTAG compared to typical spatio-temporal GNNs and equivariant GNNs.\n', '  Regulation of cell proliferation is a crucial aspect of tissue development\nand homeostasis and plays a major role in morphogenesis, wound healing, and\ntumor invasion. A phenomenon of such regulation is contact inhibition, which\ndescribes the dramatic slowing of proliferation, cell migration and individual\ncell growth when multiple cells are in contact with each other. While many\nphysiological, molecular and genetic factors are known, the mechanism of\ncontact inhibition is still not fully understood. In particular, the relevance\nof cellular signaling due to interfacial contact for contact inhibition is\nstill debated. Cellular automata (CA) have been employed in the past as\nnumerically efficient mathematical models to study the dynamics of cell\nensembles, but they are not suitable to explore the origins of contact\ninhibition as such agent-based models assume fixed cell sizes. We develop a\nminimal, data-driven model to simulate the dynamics of planar cell cultures by\nextending a probabilistic CA to incorporate size changes of individual cells\nduring growth and cell division. We successfully apply this model to previous\nin-vitro experiments on contact inhibition in epithelial tissue: After a\nsystematic calibration of the model parameters to measurements of single-cell\ndynamics, our CA model quantitatively reproduces independent measurements of\nemergent, culture-wide features, like colony size, cell density and collective\ncell migration. In particular, the dynamics of the CA model also exhibit the\ntransition from a low-density confluent regime to a stationary postconfluent\nregime with a rapid decrease in cell size and motion. This implies that the\nvolume exclusion principle, a mechanical constraint which is the only\ninter-cellular interaction incorporated in the model, paired with a\nsize-dependent proliferation rate is sufficient to generate the observed\ncontact inhibition.\n', '  We present both a theoretical and a methodological framework that addresses a\ncritical challenge in applying deep learning to physical systems: the\nreconciliation of non-linear expressiveness with SO(3)-equivariance in\npredictions of SO(3)-equivariant quantities. Inspired by covariant theory in\nphysics, we address this problem by exploring the mathematical relationships\nbetween SO(3)-invariant and SO(3)-equivariant quantities and their\nrepresentations. We first construct theoretical SO(3)-invariant quantities\nderived from the SO(3)-equivariant regression targets, and use these invariant\nquantities as supervisory labels to guide the learning of high-quality\nSO(3)-invariant features. Given that SO(3)-invariance is preserved under\nnon-linear operations, the encoding process for invariant features can\nextensively utilize non-linear mappings, thereby fully capturing the non-linear\npatterns inherent in physical systems. Building on this foundation, we propose\na gradient-based mechanism to induce SO(3)-equivariant encodings of various\ndegrees from the learned SO(3)-invariant features. This mechanism can\nincorporate non-linear expressive capabilities into SO(3)-equivariant\nrepresentations, while theoretically preserving their equivariant properties as\nwe prove. We apply our theory and method to the electronic-structure\nHamiltonian prediction tasks, experimental results on eight benchmark databases\ncovering multiple types of elements and challenging scenarios show dramatic\nbreakthroughs on the state-of-the-art prediction accuracy, with improvements of\nup to 40% in predicting Hamiltonians and up to 76% in predicting downstream\nphysical quantities such as occupied orbital energy. Our approach goes beyond\nhandling physical systems and offers a promising general solution to the\ncritical dilemma between equivariance and non-linear expressiveness for the\ndeep learning paradigm.\n'] , [""  Information-theoretic fitness functions are becoming increasingly popular to\nproduce generally useful, task-independent behaviors. One such universal\nfunction, dubbed empowerment, measures the amount of control an agent exerts on\nits environment via its sensorimotor system. Specifically, empowerment attempts\nto maximize the mutual information between an agent's actions and its received\nsensor states at a later point in time. Traditionally, empowerment has been\napplied to a conventional sensorimotor apparatus, such as a robot. Here, we\nexpand the approach to a distributed, multi-agent sensorimotor system embodied\nby a neural cellular automaton (NCA). We show that the addition of empowerment\nas a secondary objective in the evolution of NCA to perform the task of\nmorphogenesis, growing and maintaining a pre-specified shape, results in higher\nfitness compared to evolving for morphogenesis alone. Results suggest there may\nbe a synergistic relationship between morphogenesis and empowerment. That is,\nindirectly selecting for coordination between neighboring cells over the\nduration of development is beneficial to the developmental process itself. Such\na finding may have applications in developmental biology by providing potential\nmechanisms of communication between cells during growth from a single cell to a\nmulticellular, target morphology. Source code for the experiments in this paper\ncan be found at: \\url{https://github.com/caitlingrasso/empowered-nca}.\n"", '  Neural Cellular Automata (NCA) are a powerful combination of machine learning\nand mechanistic modelling. We train NCA to learn complex dynamics from time\nseries of images and PDE trajectories. Our method is designed to identify\nunderlying local rules that govern large scale dynamic emergent behaviours.\nPrevious work on NCA focuses on learning rules that give stationary emergent\nstructures. We extend NCA to capture both transient and stable structures\nwithin the same system, as well as learning rules that capture the dynamics of\nTuring pattern formation in nonlinear Partial Differential Equations (PDEs). We\ndemonstrate that NCA can generalise very well beyond their PDE training data,\nwe show how to constrain NCA to respect given symmetries, and we explore the\neffects of associated hyperparameters on model performance and stability. Being\nable to learn arbitrary dynamics gives NCA great potential as a data driven\nmodelling framework, especially for modelling biological pattern formation.\n', '  Neural Cellular Automata (NCA) is a class of Cellular Automata where the\nupdate rule is parameterized by a neural network that can be trained using\ngradient descent. In this paper, we focus on NCA models used for texture\nsynthesis, where the update rule is inspired by partial differential equations\n(PDEs) describing reaction-diffusion systems. To train the NCA model, the\nspatio-temporal domain is discretized, and Euler integration is used to\nnumerically simulate the PDE. However, whether a trained NCA truly learns the\ncontinuous dynamic described by the corresponding PDE or merely overfits the\ndiscretization used in training remains an open question. We study NCA models\nat the limit where space-time discretization approaches continuity. We find\nthat existing NCA models tend to overfit the training discretization,\nespecially in the proximity of the initial condition, also called ""seed"". To\naddress this, we propose a solution that utilizes uniform noise as the initial\ncondition. We demonstrate the effectiveness of our approach in preserving the\nconsistency of NCA dynamics across a wide range of spatio-temporal\ngranularities. Our improved NCA model enables two new test-time interactions by\nallowing continuous control over the speed of pattern formation and the scale\nof the synthesized patterns. We demonstrate this new NCA feature in our\ninteractive online demo. Our work reveals that NCA models can learn continuous\ndynamics and opens new venues for NCA research from a dynamical system\'s\nperspective.\n']",Cellular Dynamics and Neural Cellular Automata for Morphogenesis and Pattern Formation,Cellular Dynamics and Interactions
139,Synchronization and Oscillations in Complex Systems,"['rhythms', 'synchronization', 'oscillators', 'oscillating', 'synchronize', 'oscillator', 'bifurcation', 'fluctuations', 'dynamics', 'synchronized']","['  Many real-world systems undergo abrupt changes in dynamics as they move\nacross critical points, often with dramatic consequences. Much existing theory\non identifying the time-series signatures of nearby critical points -- such as\nincreased variance and slower timescales -- is derived for the case of fixed,\nlow-amplitude noise. However, real-world systems are often corrupted by unknown\nlevels of noise that can distort these temporal signatures. Here we aimed to\ndevelop noise-robust indicators of the distance to criticality (DTC) for\nsystems affected by dynamical noise in two cases: when the noise amplitude is\nfixed, or is unknown and variable across recordings. To approach this problem,\nwe compare the ability of over 7000 candidate time-series features to track the\nDTC in the vicinity of a supercritical Hopf bifurcation. We recover existing\ntheory in the fixed-noise case, highlighting conventional time-series features\nthat accurately track the DTC. But in the variable-noise setting, where these\nconventional indicators perform poorly, we highlight new types of\nhigh-performing time-series features and show that their success is\naccomplished by capturing the shape of the invariant density (which depends on\nboth the DTC and the noise amplitude) relative to the spread of fast\nfluctuations (which depends on the noise amplitude). We introduce a new\nhigh-performing time-series statistic, the Rescaled Auto-Density (RAD), that\ncombines these two algorithmic components. We then use RAD to provide new\nevidence that brain regions higher in the visual hierarchy are positioned\ncloser to criticality, supporting existing hypotheses about patterns of brain\norganization that are not detected using conventional metrics of the DTC. Our\nresults demonstrate how large-scale algorithmic comparison can yield\ntheoretical insights that can motivate new theory and interpretable algorithms\nfor real-world problems.\n', '  In this study a new method for analyzing synchronization in oscillator\nsystems is proposed using the example of modeling the dynamics of a circuit of\ntwo resistively coupled pulse oscillators. The dynamic characteristic of\nsynchronization is fuzzy entropy (FuzzyEn) calculated a time series composed of\nthe ratios of the number of pulse periods (subharmonic ratio, SHR) during\nphase-locking intervals. Low entropy values indicate strong synchronization,\nwhereas high entropy values suggest weak synchronization between the two\noscillators. This method effectively visualizes synchronized modes of the\ncircuit using entropy maps of synchronization states. Additionally, a\nclassification of synchronization states is proposed based on the dependencies\nof FuzzyEn on the length of embedding vectors of SHR time series. An extension\nof this method for analyzing non-relaxation (non-spike) type signals is\nillustrated using the example of phase-phase coupling rhythms of local field\npotential of rat hippocampus. The entropy-statistical approach using rational\nfractions and pulse signal forms makes this method promising for analyzing\nbiosignal synchronization and implementing the algorithm in mobile digital\nplatforms.\n', ""  The synchronization analysis of limit-cycle oscillators is prevalent in many\nfields, including physics, chemistry, and life sciences. It relies on the phase\ncalculation that utilizes measurements. However, the synchronization of\nspatiotemporal dynamics cannot be analyzed because a standardized method for\ncalculating the phase has not been established. The presence of spatial\nstructure complicates the determination of which measurements should be used\nfor accurate phase calculation. To address this, we explore a method for\ncalculating the phase from the time series of measurements taken at a single\nspatial grid point. The phase is calculated to increase linearly between event\ntimes when the measurement time series intersects the Poincar\\'e section. The\ndifference between the calculated phase and the isochron-based phase, resulting\nfrom the discrepancy between the isochron and the Poincar\\'e section, is\nevaluated using a linear approximation near the limit-cycle solution. We found\nthat the difference is small when measurements are taken from regions that\ndominate the rhythms of the entire spatiotemporal dynamics. Furthermore, we\ninvestigate an alternative method where the Poincar\\'e section is applied to\nthe time series obtained through orthogonal decomposition of the entire\nspatiotemporal dynamics. We present two decomposition schemes that utilize the\nprincipal component analysis. For illustration, the phase is calculated from\nthe measurements of spatiotemporal dynamics exhibiting target waves or\noscillating spots, simulated by weakly coupled FitzHugh-Nagumo\nreaction-diffusion models.\n""]",Complex Systems Dynamics and Synchronization,Synchronization and Oscillations in Complex Systems
140,"State Space Models vs Transformers , ""Improving Transformers for Arithmetic Tasks with Positional Encodings""","['rnns', 'lstms', 'memory', 'softmax', 'attention', 'rnn', 'neural', 'recurrent', 'transformers', 'models'] , ['decoders', 'digits', 'transformers', 'encodings', 'rnns', 'turing', 'digit', 'decoder', 'addition', 'transformer']","[""  Selective state-space models (SSMs) like Mamba overcome some of the\nshortcomings of Transformers, such as quadratic computational complexity with\nsequence length and large inference-time memory requirements from the key-value\ncache. Moreover, recent studies have shown that SSMs can match or exceed the\nlanguage modeling capabilities of Transformers, making them an attractive\nalternative. In a controlled setting (e.g., same data), however, studies so far\nhave only presented small scale experiments comparing SSMs to Transformers. To\nunderstand the strengths and weaknesses of these architectures at larger\nscales, we present a direct comparison between 8B-parameter Mamba, Mamba-2, and\nTransformer models trained on the same datasets of up to 3.5T tokens. We also\ncompare these models to a hybrid architecture consisting of 43% Mamba-2, 7%\nattention, and 50% MLP layers (Mamba-2-Hybrid). Using a diverse set of tasks,\nwe answer the question of whether Mamba models can match Transformers at larger\ntraining budgets. Our results show that while pure SSMs match or exceed\nTransformers on many tasks, they lag behind Transformers on tasks which require\nstrong copying or in-context learning abilities (e.g., 5-shot MMLU, Phonebook)\nor long-context reasoning. In contrast, we find that the 8B Mamba-2-Hybrid\nexceeds the 8B Transformer on all 12 standard tasks we evaluated (+2.65 points\non average) and is predicted to be up to 8x faster when generating tokens at\ninference time. To validate long-context capabilities, we provide additional\nexperiments evaluating variants of the Mamba-2-Hybrid and Transformer extended\nto support 16K, 32K, and 128K sequences. On an additional 23 long-context\ntasks, the hybrid model continues to closely match or exceed the Transformer on\naverage. To enable further study, we release the checkpoints as well as the\ncode used to train our models as part of NVIDIA's Megatron-LM project.\n"", '  Deep neural networks based on state space models (SSMs) are attracting much\nattention in sequence modeling since their computational cost is significantly\nsmaller than that of Transformers. While the capabilities of SSMs have been\nprimarily investigated through experimental comparisons, theoretical\nunderstanding of SSMs is still limited. In particular, there is a lack of\nstatistical and quantitative evaluation of whether SSM can replace\nTransformers. In this paper, we theoretically explore in which tasks SSMs can\nbe alternatives of Transformers from the perspective of estimating\nsequence-to-sequence functions. We consider the setting where the target\nfunction has direction-dependent smoothness and prove that SSMs can estimate\nsuch functions with the same convergence rate as Transformers. Additionally, we\nprove that SSMs can estimate the target function, even if the smoothness\nchanges depending on the input sequence, as well as Transformers. Our results\nshow the possibility that SSMs can replace Transformers when estimating the\nfunctions in certain classes that appear in practice.\n', ""  Foundation models, now powering most of the exciting applications in deep\nlearning, are almost universally based on the Transformer architecture and its\ncore attention module. Many subquadratic-time architectures such as linear\nattention, gated convolution and recurrent models, and structured state space\nmodels (SSMs) have been developed to address Transformers' computational\ninefficiency on long sequences, but they have not performed as well as\nattention on important modalities such as language. We identify that a key\nweakness of such models is their inability to perform content-based reasoning,\nand make several improvements. First, simply letting the SSM parameters be\nfunctions of the input addresses their weakness with discrete modalities,\nallowing the model to selectively propagate or forget information along the\nsequence length dimension depending on the current token. Second, even though\nthis change prevents the use of efficient convolutions, we design a\nhardware-aware parallel algorithm in recurrent mode. We integrate these\nselective SSMs into a simplified end-to-end neural network architecture without\nattention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$\nhigher throughput than Transformers) and linear scaling in sequence length, and\nits performance improves on real data up to million-length sequences. As a\ngeneral sequence model backbone, Mamba achieves state-of-the-art performance\nacross several modalities such as language, audio, and genomics. On language\nmodeling, our Mamba-3B model outperforms Transformers of the same size and\nmatches Transformers twice its size, both in pretraining and downstream\nevaluation.\n""] , ['  The poor performance of transformers on arithmetic tasks seems to stem in\nlarge part from their inability to keep track of the exact position of each\ndigit inside of a large span of digits. We mend this problem by adding an\nembedding to each digit that encodes its position relative to the start of the\nnumber. In addition to the boost these embeddings provide on their own, we show\nthat this fix enables architectural modifications such as input injection and\nrecurrent layers to improve performance even further.\n  With positions resolved, we can study the logical extrapolation ability of\ntransformers. Can they solve arithmetic problems that are larger and more\ncomplex than those in their training data? We find that training on only 20\ndigit numbers with a single GPU for one day, we can reach state-of-the-art\nperformance, achieving up to 99% accuracy on 100 digit addition problems.\nFinally, we show that these gains in numeracy also unlock improvements on other\nmulti-step reasoning tasks including sorting and multiplication.\n', '  Length generalization refers to the ability to extrapolate from short\ntraining sequences to long test sequences and is a challenge for current large\nlanguage models. While prior work has proposed some architecture or data format\nchanges to achieve length generalization, these proposals typically apply to a\nlimited set of tasks. Building on prior scratchpad and Chain-of-Thought (CoT)\ntechniques, we propose Turing Programs, a novel CoT strategy that decomposes an\nalgorithmic task into steps mimicking the computation of a Turing Machine. This\nframework is both universal, as it can accommodate any algorithmic task, and\nsimple, requiring only copying text from the context with small modifications.\nWe show that by using Turing Programs, we obtain robust length generalization\non a range of algorithmic tasks: addition, multiplication and in-context SGD.\nWe then demonstrate that transformers achieve length generalization on random\nTuring Programs, suggesting that length generalization is possible for any\nalgorithmic task. Finally, we theoretically prove that transformers can\nimplement Turing Programs, constructing a simple RASP (Weiss et al.) program\nthat simulates an arbitrary Turing machine.\n', '  Despite the success of Transformers on language understanding, code\ngeneration, and logical reasoning, they still fail to generalize over length on\nbasic arithmetic tasks such as addition and multiplication. A major reason\nbehind this failure is the vast difference in structure between numbers and\ntext; For example, the numbers are typically parsed from right to left, and\nthere is a correspondence between digits at the same position across different\nnumbers. In contrast, for text, such symmetries are quite unnatural. In this\nwork, we propose to encode these semantics explicitly into the model via\nmodified number formatting and custom positional encodings. Empirically, our\nmethod allows a Transformer trained on numbers with at most 5-digits for\naddition and multiplication to generalize up to 50-digit numbers, without using\nadditional data for longer sequences. We further demonstrate that traditional\nabsolute positional encodings (APE) fail to generalize to longer sequences,\neven when trained with augmented data that captures task symmetries. To\nelucidate the importance of explicitly encoding structure, we prove that\nexplicit incorporation of structure via positional encodings is necessary for\nout-of-distribution generalization. Finally, we pinpoint other challenges\ninherent to length generalization beyond capturing symmetries, in particular\ncomplexity of the underlying task, and propose changes in the training\ndistribution to address them.\n']",Advances in Sequence Modeling: Transformers and Alternatives,State Space Models vs Transformers
141,"Temporal Point Process Modeling , Data-driven Modeling of Dynamical Systems","['forecasting', 'rnn', 'temporal', 'predicting', 'prediction', 'neural', 'events', 'models', 'future', 'autoregressive'] , ['modeling', 'dynamics', 'nonlinear', 'odes', 'dynamical', 'autoencoder', 'differential', 'dimensionally', 'dimensional', 'pdes']","['  An extension of the Hawkes process, the Marked Hawkes process distinguishes\nitself by featuring variable jump size across each event, in contrast to the\nconstant jump size observed in a Hawkes process without marks. While extensive\nliterature has been dedicated to the non-parametric estimation of both the\nlinear and non-linear Hawkes process, there remains a significant gap in the\nliterature regarding the marked Hawkes process. In response to this, we propose\na methodology for estimating the conditional intensity of the marked Hawkes\nprocess. We introduce two distinct models: \\textit{Shallow Neural Hawkes with\nmarks}- for Hawkes processes with excitatory kernels and \\textit{Neural Network\nfor Non-Linear Hawkes with Marks}- for non-linear Hawkes processes. Both these\napproaches take the past arrival times and their corresponding marks as the\ninput to obtain the arrival intensity. This approach is entirely\nnon-parametric, preserving the interpretability associated with the marked\nHawkes process. To validate the efficacy of our method, we subject the method\nto synthetic datasets with known ground truth. Additionally, we apply our\nmethod to model cryptocurrency order book data, demonstrating its applicability\nto real-world scenarios.\n', '  Neural Temporal Point Processes (TPPs) have emerged as the primary framework\nfor predicting sequences of events that occur at irregular time intervals, but\ntheir sequential nature can hamper performance for long-horizon forecasts. To\naddress this, we introduce a novel approach that incorporates a diffusion\ngenerative model. The model facilitates sequence-to-sequence prediction,\nallowing multi-step predictions based on historical event sequences. In\ncontrast to previous approaches, our model directly learns the joint\nprobability distribution of types and inter-arrival times for multiple events.\nThis allows us to fully leverage the high dimensional modeling capability of\nmodern generative models. Our model is composed of two diffusion processes, one\nfor the time intervals and one for the event types. These processes interact\nthrough their respective denoising functions, which can take as input\nintermediate representations from both processes, allowing the model to learn\ncomplex interactions. We demonstrate that our proposal outperforms\nstate-of-the-art baselines for long-horizon forecasting of TPP.\n', ""  Temporal Point Processes (TPPs) hold a pivotal role in modeling event\nsequences across diverse domains, including social networking and e-commerce,\nand have significantly contributed to the advancement of recommendation systems\nand information retrieval strategies. Through the analysis of events such as\nuser interactions and transactions, TPPs offer valuable insights into\nbehavioral patterns, facilitating the prediction of future trends. However,\naccurately forecasting future events remains a formidable challenge due to the\nintricate nature of these patterns. The integration of Neural Networks with\nTPPs has ushered in the development of advanced deep TPP models. While these\nmodels excel at processing complex and nonlinear temporal data, they encounter\nlimitations in modeling intensity functions, grapple with computational\ncomplexities in integral computations, and struggle to capture long-range\ntemporal dependencies effectively. In this study, we introduce the CuFun model,\nrepresenting a novel approach to TPPs that revolves around the Cumulative\nDistribution Function (CDF). CuFun stands out by uniquely employing a monotonic\nneural network for CDF representation, utilizing past events as a scaling\nfactor. This innovation significantly bolsters the model's adaptability and\nprecision across a wide range of data scenarios. Our approach addresses several\ncritical issues inherent in traditional TPP modeling: it simplifies\nlog-likelihood calculations, extends applicability beyond predefined density\nfunction forms, and adeptly captures long-range temporal patterns. Our\ncontributions encompass the introduction of a pioneering CDF-based TPP model,\nthe development of a methodology for incorporating past event information into\nfuture event prediction, and empirical validation of CuFun's effectiveness\nthrough extensive experimentation on synthetic and real-world datasets.\n""] , ['  Data-driven modeling of dynamical systems often faces numerous data-related\nchallenges. A fundamental requirement is the existence of a unique set of\nparameters for a chosen model structure, an issue commonly referred to as\nidentifiability. Although this problem is well studied for ordinary\ndifferential equations (ODEs), few studies have focused on the more general\nclass of systems described by differential-algebraic equations (DAEs). Examples\nof DAEs include dynamical systems with algebraic equations representing\nconservation laws or approximating fast dynamics. This work introduces a novel\nidentifiability test for models characterized by nonlinear DAEs. Unlike\nprevious approaches, our test only requires prior knowledge of the system\nequations and does not need nonlinear transformation, index reduction, or\nnumerical integration of the DAEs. We employed our identifiability analysis\nacross a diverse range of DAE models, illustrating how system identifiability\ndepends on the choices of sensors, experimental conditions, and model\nstructures. Given the added challenges involved in identifying DAEs when\ncompared to ODEs, we anticipate that our findings will have broad applicability\nand contribute significantly to the development and validation of data-driven\nmethods for DAEs and other structure-preserving models.\n', '  The simulation of many complex phenomena in engineering and science requires\nsolving expensive, high-dimensional systems of partial differential equations\n(PDEs). To circumvent this, reduced-order models (ROMs) have been developed to\nspeed up computations. However, when governing equations are unknown or\npartially known, typically ROMs lack interpretability and reliability of the\npredicted solutions.\n  In this work we present a data-driven, non-intrusive framework for building\nROMs where the latent variables and dynamics are identified in an interpretable\nmanner and uncertainty is quantified. Starting from a limited amount of\nhigh-dimensional, noisy data the proposed framework constructs an efficient ROM\nby leveraging variational autoencoders for dimensionality reduction along with\na newly introduced, variational version of sparse identification of nonlinear\ndynamics (SINDy), which we refer to as Variational Identification of Nonlinear\nDynamics (VINDy).\n  In detail, the method consists of Variational Encoding of Noisy Inputs (VENI)\nto identify the distribution of reduced coordinates. Simultaneously, we learn\nthe distribution of the coefficients of a pre-determined set of candidate\nfunctions by VINDy. Once trained offline, the identified model can be queried\nfor new parameter instances and new initial conditions to compute the\ncorresponding full-time solutions. The probabilistic setup enables uncertainty\nquantification as the online testing consists of Variational Inference\nnaturally providing Certainty Intervals (VICI). In this work we showcase the\neffectiveness of the newly proposed VINDy method in identifying interpretable\nand accurate dynamical system for the R\\""ossler system with different noise\nintensities and sources. Then the performance of the overall method - named\nVENI, VINDy, VICI - is tested on PDE benchmarks including structural mechanics\nand fluid dynamics.\n', '  Numerical solvers of partial differential equations (PDEs) have been widely\nemployed for simulating physical systems. However, the computational cost\nremains a major bottleneck in various scientific and engineering applications,\nwhich has motivated the development of reduced-order models (ROMs). Recently,\nmachine-learning-based ROMs have gained significant popularity and are\npromising for addressing some limitations of traditional ROM methods,\nespecially for advection dominated systems. In this chapter, we focus on a\nparticular framework known as Latent Space Dynamics Identification (LaSDI),\nwhich transforms the high-fidelity data, governed by a PDE, to simpler and\nlow-dimensional latent-space data, governed by ordinary differential equations\n(ODEs). These ODEs can be learned and subsequently interpolated to make ROM\npredictions. Each building block of LaSDI can be easily modulated depending on\nthe application, which makes the LaSDI framework highly flexible. In\nparticular, we present strategies to enforce the laws of thermodynamics into\nLaSDI models (tLaSDI), enhance robustness in the presence of noise through the\nweak form (WLaSDI), select high-fidelity training data efficiently through\nactive learning (gLaSDI, GPLaSDI), and quantify the ROM prediction uncertainty\nthrough Gaussian processes (GPLaSDI). We demonstrate the performance of\ndifferent LaSDI approaches on Burgers equation, a non-linear heat conduction\nproblem, and a plasma physics problem, showing that LaSDI algorithms can\nachieve relative errors of less than a few percent and up to thousands of times\nspeed-ups.\n']",Advanced Modeling of Temporal and Dynamical Systems,Temporal Point Process Modeling
142,Digital Twin Modeling and Simulation,"['twinning', 'twin', 'twinlab', 'twins', 'modelization', 'modeling', 'neural', 'modelling', 'simulations', 'digital']","[""  A digital twin is a virtual replica of a real-world physical phenomena that\nuses mathematical modeling to characterize and simulate its defining features.\nBy constructing digital twins for disease processes, we can perform in-silico\nsimulations that mimic patients' health conditions and counterfactual outcomes\nunder hypothetical interventions in a virtual setting. This eliminates the need\nfor invasive procedures or uncertain treatment decisions. In this paper, we\npropose a method to identify digital twin model parameters using only\nnoninvasive patient health data. We approach the digital twin modeling as a\ncomposite inverse problem, and observe that its structure resembles pretraining\nand finetuning in self-supervised learning (SSL). Leveraging this, we introduce\na physics-informed SSL algorithm that initially pretrains a neural network on\nthe pretext task of learning a differentiable simulator of a physiological\nprocess. Subsequently, the model is trained to reconstruct physiological\nmeasurements from noninvasive modalities while being constrained by the\nphysical equations learned in pretraining. We apply our method to identify\ndigital twins of cardiac hemodynamics using noninvasive echocardiogram videos,\nand demonstrate its utility in unsupervised disease detection and in-silico\nclinical trials.\n"", '  Digital twins, the cornerstone of Industry 4.0, replicate real-world entities\nthrough computer models, revolutionising fields such as manufacturing\nmanagement and industrial automation. Recent advances in machine learning\nprovide data-driven methods for developing digital twins using discrete-time\ndata and finite-depth models on digital computers. However, this approach fails\nto capture the underlying continuous dynamics and struggles with modelling\ncomplex system behaviour. Additionally, the architecture of digital computers,\nwith separate storage and processing units, necessitates frequent data\ntransfers and Analogue-Digital (A/D) conversion, thereby significantly\nincreasing both time and energy costs. Here, we introduce a memristive neural\nordinary differential equation (ODE) solver for digital twins, which is capable\nof capturing continuous-time dynamics and facilitates the modelling of complex\nsystems using an infinite-depth model. By integrating storage and computation\nwithin analogue memristor arrays, we circumvent the von Neumann bottleneck,\nthus enhancing both speed and energy efficiency. We experimentally validate our\napproach by developing a digital twin of the HP memristor, which accurately\nextrapolates its nonlinear dynamics, achieving a 4.2-fold projected speedup and\na 41.4-fold projected decrease in energy consumption compared to\nstate-of-the-art digital hardware, while maintaining an acceptable error\nmargin. Additionally, we demonstrate scalability through experimentally\ngrounded simulations of Lorenz96 dynamics, exhibiting projected performance\nimprovements of 12.6-fold in speed and 189.7-fold in energy efficiency relative\nto traditional digital approaches. By harnessing the capabilities of fully\nanalogue computing, our breakthrough accelerates the development of digital\ntwins, offering an efficient and rapid solution to meet the demands of Industry\n4.0.\n', ""  A patient's digital twin is a computational model that describes the\nevolution of their health over time. Digital twins have the potential to\nrevolutionize medicine by enabling individual-level computer simulations of\nhuman health, which can be used to conduct more efficient clinical trials or to\nrecommend personalized treatment options. Due to the overwhelming complexity of\nhuman biology, machine learning approaches that leverage large datasets of\nhistorical patients' longitudinal health records to generate patients' digital\ntwins are more tractable than potential mechanistic models. In this manuscript,\nwe describe a neural network architecture that can learn conditional generative\nmodels of clinical trajectories, which we call Digital Twin Generators (DTGs),\nthat can create digital twins of individual patients. We show that the same\nneural network architecture can be trained to generate accurate digital twins\nfor patients across 13 different indications simply by changing the training\nset and tuning hyperparameters. By introducing a general purpose architecture,\nwe aim to unlock the ability to scale machine learning approaches to larger\ndatasets and across more indications so that a digital twin could be created\nfor any patient in the world.\n""]",Digital Twin Technology and Applications,Digital Twin Modeling and Simulation
143,Point Cloud Generation and Registration,"['shapenet', '3d', 'points', 'shapes', 'maps', 'shape', 'mesh', 'surfaces', 'lidar', 'point2ssm']","['  Diffusion models have been popular for point cloud generation tasks. Existing\nworks utilize the forward diffusion process to convert the original point\ndistribution into a noise distribution and then learn the reverse diffusion\nprocess to recover the point distribution from the noise distribution. However,\nthe reverse diffusion process can produce samples with non-smooth points on the\nsurface because of the ignorance of the point cloud geometric properties. We\npropose alleviating the problem by incorporating the local smoothness\nconstraint into the diffusion framework for point cloud generation. Experiments\ndemonstrate the proposed model can generate realistic shapes and smoother point\nclouds, outperforming multiple state-of-the-art methods.\n', '  Point cloud registration aligns 3D point clouds using spatial\ntransformations. It is an important task in computer vision, with applications\nin areas such as augmented reality (AR) and medical imaging. This work explores\nthe intersection of two research trends: the integration of AR into\nimage-guided surgery and the use of deep learning for point cloud registration.\nThe main objective is to evaluate the feasibility of applying deep\nlearning-based point cloud registration methods for image-to-patient\nregistration in augmented reality-guided surgery. We created a dataset of point\nclouds from medical imaging and corresponding point clouds captured with a\npopular AR device, the HoloLens 2. We evaluate three well-established deep\nlearning models in registering these data pairs. While we find that some deep\nlearning methods show promise, we show that a conventional registration\npipeline still outperforms them on our challenging dataset.\n', '  Point cloud registration is a crucial technique in 3D computer vision with a\nwide range of applications. However, this task can be challenging, particularly\nin large fields of view with dynamic objects, environmental noise, or other\nperturbations. To address this challenge, we propose a model called PosDiffNet.\nOur approach performs hierarchical registration based on window-level,\npatch-level, and point-level correspondence. We leverage a graph neural partial\ndifferential equation (PDE) based on Beltrami flow to obtain high-dimensional\nfeatures and position embeddings for point clouds. We incorporate position\nembeddings into a Transformer module based on a neural ordinary differential\nequation (ODE) to efficiently represent patches within points. We employ the\nmulti-level correspondence derived from the high feature similarity scores to\nfacilitate alignment between point clouds. Subsequently, we use registration\nmethods such as SVD-based algorithms to predict the transformation using\ncorresponding point pairs. We evaluate PosDiffNet on several 3D point cloud\ndatasets, verifying that it achieves state-of-the-art (SOTA) performance for\npoint cloud registration in large fields of view with perturbations. The\nimplementation code of experiments is available at\nhttps://github.com/AI-IT-AVs/PosDiffNet.\n']",Point Cloud Processing and Registration,Point Cloud Generation and Registration
144,"Trajectory Analysis and Mobility Patterns , Transportation Systems and Mobility Analysis","['trajectories', 'mobilitygpt', 'gps', 'mobility', 'trips', 'trajcl', 'trajectory', 'travel', 'trip', 'planning'] , ['transportation', 'traffic', 'routes', 'ridesharing', 'travelers', 'travel', 'trips', 'transport', 'mobility', 'passengers']","[""  Recovering intermediate missing GPS points in a sparse trajectory, while\nadhering to the constraints of the road network, could offer deep insights into\nusers' moving behaviors in intelligent transportation systems. Although recent\nstudies have demonstrated the advantages of achieving map-constrained\ntrajectory recovery via an end-to-end manner, they still face two significant\nchallenges. Firstly, existing methods are mostly sequence-based models. It is\nextremely hard for them to comprehensively capture the micro-semantics of\nindividual trajectory, including the information of each GPS point and the\nmovement between two GPS points. Secondly, existing approaches ignore the\nimpact of the macro-semantics, i.e., the road conditions and the people's\nshared travel preferences reflected by a group of trajectories. To address the\nabove challenges, we propose a Micro-Macro Spatial-Temporal Graph-based\nEncoder-Decoder (MM-STGED). Specifically, we model each trajectory as a graph\nto efficiently describe the micro-semantics of trajectory and design a novel\nmessage-passing mechanism to learn trajectory representations. Additionally, we\nextract the macro-semantics of trajectories and further incorporate them into a\nwell-designed graph-based decoder to guide trajectory recovery. Extensive\nexperiments conducted on sparse trajectories with three different sampling\nintervals that are respectively constructed from two real-world trajectory\ndatasets demonstrate the superiority of our proposed model.\n"", '  Trajectory Representation Learning (TRL) is a powerful tool for\nspatial-temporal data analysis and management. TRL aims to convert complicated\nraw trajectories into low-dimensional representation vectors, which can be\napplied to various downstream tasks, such as trajectory classification,\nclustering, and similarity computation. Existing TRL works usually treat\ntrajectories as ordinary sequence data, while some important spatial-temporal\ncharacteristics, such as temporal regularities and travel semantics, are not\nfully exploited. To fill this gap, we propose a novel Self-supervised\ntrajectory representation learning framework with TemporAl Regularities and\nTravel semantics, namely START. The proposed method consists of two stages. The\nfirst stage is a Trajectory Pattern-Enhanced Graph Attention Network (TPE-GAT),\nwhich converts the road network features and travel semantics into\nrepresentation vectors of road segments. The second stage is a Time-Aware\nTrajectory Encoder (TAT-Enc), which encodes representation vectors of road\nsegments in the same trajectory as a trajectory representation vector,\nmeanwhile incorporating temporal regularities with the trajectory\nrepresentation. Moreover, we also design two self-supervised tasks, i.e.,\nspan-masked trajectory recovery and trajectory contrastive learning, to\nintroduce spatial-temporal characteristics of trajectories into the training\nprocess of our START framework. The effectiveness of the proposed method is\nverified by extensive experiments on two large-scale real-world datasets for\nthree downstream tasks. The experiments also demonstrate that our method can be\ntransferred across different cities to adapt heterogeneous trajectory datasets.\n', ""  Understanding human mobility patterns is essential for various applications,\nfrom urban planning to public safety. The individual trajectory such as mobile\nphone location data, while rich in spatio-temporal information, often lacks\nsemantic detail, limiting its utility for in-depth mobility analysis. Existing\nmethods can infer basic routine activity sequences from this data, lacking\ndepth in understanding complex human behaviors and users' characteristics.\nAdditionally, they struggle with the dependency on hard-to-obtain auxiliary\ndatasets like travel surveys. To address these limitations, this paper defines\ntrajectory semantic inference through three key dimensions: user occupation\ncategory, activity sequence, and trajectory description, and proposes the\nTrajectory Semantic Inference with Large Language Models (TSI-LLM) framework to\nleverage LLMs infer trajectory semantics comprehensively and deeply. We adopt\nspatio-temporal attributes enhanced data formatting (STFormat) and design a\ncontext-inclusive prompt, enabling LLMs to more effectively interpret and infer\nthe semantics of trajectory data. Experimental validation on real-world\ntrajectory datasets demonstrates the efficacy of TSI-LLM in deciphering complex\nhuman mobility patterns. This study explores the potential of LLMs in enhancing\nthe semantic analysis of trajectory data, paving the way for more sophisticated\nand accessible human mobility research.\n""] , ['  Mobility service route design requires demand information to operate in a\nservice region. Transit planners and operators can access various data sources\nincluding household travel survey data and mobile device location logs.\nHowever, when implementing a mobility system with emerging technologies,\nestimating demand becomes harder because of limited data resulting in\nuncertainty. This study proposes an artificial intelligence-driven algorithm\nthat combines sequential transit network design with optimal learning to\naddress the operation under limited data. An operator gradually expands its\nroute system to avoid risks from inconsistency between designed routes and\nactual travel demand. At the same time, observed information is archived to\nupdate the knowledge that the operator currently uses. Three learning policies\nare compared within the algorithm: multi-armed bandit, knowledge gradient, and\nknowledge gradient with correlated beliefs. For validation, a new route system\nis designed on an artificial network based on public use microdata areas in New\nYork City. Prior knowledge is reproduced from the regional household travel\nsurvey data. The results suggest that exploration considering correlations can\nachieve better performance compared to greedy choices in general. In future\nwork, the problem may incorporate more complexities such as demand elasticity\nto travel time, no limitations to the number of transfers, and costs for\nexpansion.\n', '  This paper leverages macroscopic models and multi-source spatiotemporal data\ncollected from automatic traffic counters and probe vehicles to accurately\nestimate traffic flow and travel time in links where these measurements are\nunavailable. This problem is critical in transportation planning applications\nwhere the sensor coverage is low and the planned interventions have\nnetwork-wide impacts. The proposed model, named the Macroscopic Traffic\nEstimator (MaTE), can perform network-wide estimations of traffic flow and\ntravel time only using the set of observed measurements of these quantities.\nBecause MaTE is grounded in macroscopic flow theory, all parameters and\nvariables are interpretable. The estimated traffic flow satisfies fundamental\nflow conservation constraints and exhibits an increasing monotonic relationship\nwith the estimated travel time. Using logit-based stochastic traffic assignment\nas the principle for routing flow behavior makes the model fully differentiable\nwith respect to the model parameters. This property facilitates the application\nof computational graphs to learn parameters from vast amounts of spatiotemporal\ndata. We also integrate neural networks and polynomial kernel functions to\ncapture link flow interactions and enrich the mapping of traffic flows into\ntravel times. MaTE also adds a destination choice model and a trip generation\nmodel that uses historical data on the number of trips generated by location.\nExperiments on synthetic data show that the model can accurately estimate\ntravel time and traffic flow in out-of-sample links. Results obtained using\nreal-world multi-source data from a large-scale transportation network suggest\nthat MaTE outperforms data-driven benchmarks, especially in travel time\nestimation. The estimated parameters of MaTE are also informative about the\nhourly change in travel demand and supply characteristics of the transportation\nnetwork.\n', '  Mobility analysis is a crucial element in the research area of transportation\nsystems. Forecasting traffic information offers a viable solution to address\nthe conflict between increasing transportation demands and the limitations of\ntransportation infrastructure. Predicting human travel is significant in aiding\nvarious transportation and urban management tasks, such as taxi dispatch and\nurban planning. Machine learning and deep learning methods are favored for\ntheir flexibility and accuracy. Nowadays, with the advent of large language\nmodels (LLMs), many researchers have combined these models with previous\ntechniques or applied LLMs to directly predict future traffic information and\nhuman travel behaviors. However, there is a lack of comprehensive studies on\nhow LLMs can contribute to this field. This survey explores existing approaches\nusing LLMs for mobility forecasting problems. We provide a literature review\nconcerning the forecasting applications within transportation systems,\nelucidating how researchers utilize LLMs, showcasing recent state-of-the-art\nadvancements, and identifying the challenges that must be overcome to fully\nleverage LLMs in this domain.\n']",Transportation and Mobility Analysis,Transportation Systems and Mobility Analysis
145,"Urban Visual Analysis and Planning , Urban Region Representation and Analysis","['urban', 'urbanvlp', 'urbangenai', 'neighborhoods', 'landscape', 'visual', 'landscapes', 'planning', 'imagery', 'buildings'] , ['urban', 'cities', 'city', 'infrastructure', 'spatial', 'data', 'features', 'geoshapley', 'areas', 'geographical']","[""  The visual appeal of urban environments significantly impacts residents'\nsatisfaction with their living spaces and their overall mood, which in turn,\naffects their health and well-being. Given the resource-intensive nature of\ngathering evaluations on urban visual appeal through surveys or inquiries from\nresidents, there is a constant quest for automated solutions to streamline this\nprocess and support spatial planning. In this study, we applied an\noff-the-shelf AI model to automate the analysis of urban visual appeal, using\nover 1,800 Google Street View images of Helsinki, Finland. By incorporating the\nGPT-4 model with specified criteria, we assessed these images. Simultaneously,\n24 participants were asked to rate the images. Our results demonstrated a\nstrong alignment between GPT-4 and participant ratings, although geographic\ndisparities were noted. Specifically, GPT-4 showed a preference for suburban\nareas with significant greenery, contrasting with participants who found these\nareas less appealing. Conversely, in the city centre and densely populated\nurban regions of Helsinki, GPT-4 assigned lower visual appeal scores than\nparticipant ratings. While there was general agreement between AI and human\nassessments across various locations, GPT-4 struggled to incorporate contextual\nnuances into its ratings, unlike participants, who considered both context and\nfeatures of the urban environment. The study suggests that leveraging AI models\nlike GPT-4 allows spatial planners to gather insights into the visual appeal of\ndifferent areas efficiently, aiding decisions that enhance residents' and\ntravellers' satisfaction and mental health. Although AI models provide valuable\ninsights, human perspectives are essential for a comprehensive understanding of\nurban visual appeal. This will ensure that planning and design decisions\npromote healthy living environments effectively.\n"", '  Urban region profiling aims to learn a low-dimensional representation of a\ngiven urban area while preserving its characteristics, such as demographics,\ninfrastructure, and economic activities, for urban planning and development.\nHowever, prevalent pretrained models, particularly those reliant on satellite\nimagery, face dual challenges. Firstly, concentrating solely on macro-level\npatterns from satellite data may introduce bias, lacking nuanced details at\nmicro levels, such as architectural details at a place.Secondly, the lack of\ninterpretability in pretrained models limits their utility in providing\ntransparent evidence for urban planning. In response to these issues, we devise\na novel framework entitled UrbanVLP based on Vision-Language Pretraining. Our\nUrbanVLP seamlessly integrates multi-granularity information from both macro\n(satellite) and micro (street-view) levels, overcoming the limitations of prior\npretrained models. Moreover, it introduces automatic text generation and\ncalibration, elevating interpretability in downstream applications by producing\nhigh-quality text descriptions of urban imagery. Rigorous experiments conducted\nacross six urban indicator prediction tasks underscore its superior\nperformance.\n', ""  Cities around the world face a critical shortage of affordable and decent\nhousing. Despite its critical importance for policy, our ability to effectively\nmonitor and track progress in urban housing is limited. Deep learning-based\ncomputer vision methods applied to street-level images have been successful in\nthe measurement of socioeconomic and environmental inequalities but did not\nfully utilize temporal images to track urban change as time-varying labels are\noften unavailable. We used self-supervised methods to measure change in London\nusing 15 million street images taken between 2008 and 2021. Our novel\nadaptation of Barlow Twins, Street2Vec, embeds urban structure while being\ninvariant to seasonal and daily changes without manual annotations. It\noutperformed generic embeddings, successfully identified point-level change in\nLondon's housing supply from street-level images, and distinguished between\nmajor and minor change. This capability can provide timely information for\nurban planning and policy decisions toward more liveable, equitable, and\nsustainable cities.\n""] , ['  Representing urban regions accurately and comprehensively is essential for\nvarious urban planning and analysis tasks. Recently, with the expansion of the\ncity, modeling long-range spatial dependencies with multiple data sources plays\nan important role in urban region representation. In this paper, we propose the\nAttentive Graph Enhanced Region Representation Learning (ATGRL) model, which\naims to capture comprehensive dependencies from multiple graphs and learn rich\nsemantic representations of urban regions. Specifically, we propose a\ngraph-enhanced learning module to construct regional graphs by incorporating\nmobility flow patterns, point of interests (POIs) functions, and check-in\nsemantics with noise filtering. Then, we present a multi-graph aggregation\nmodule to capture both local and global spatial dependencies between regions by\nintegrating information from multiple graphs. In addition, we design a\ndual-stage fusion module to facilitate information sharing between different\nviews and efficiently fuse multi-view representations for urban region\nembedding using an improved linear attention mechanism. Finally, extensive\nexperiments on real-world datasets for three downstream tasks demonstrate the\nsuperior performance of our model compared to state-of-the-art methods.\n', '  As cities continue to burgeon, Urban Computing emerges as a pivotal\ndiscipline for sustainable development by harnessing the power of cross-domain\ndata fusion from diverse sources (e.g., geographical, traffic, social media,\nand environmental data) and modalities (e.g., spatio-temporal, visual, and\ntextual modalities). Recently, we are witnessing a rising trend that utilizes\nvarious deep-learning methods to facilitate cross-domain data fusion in smart\ncities. To this end, we propose the first survey that systematically reviews\nthe latest advancements in deep learning-based data fusion methods tailored for\nurban computing. Specifically, we first delve into data perspective to\ncomprehend the role of each modality and data source. Secondly, we classify the\nmethodology into four primary categories: feature-based, alignment-based,\ncontrast-based, and generation-based fusion methods. Thirdly, we further\ncategorize multi-modal urban applications into seven types: urban planning,\ntransportation, economy, public safety, society, environment, and energy.\nCompared with previous surveys, we focus more on the synergy of deep learning\nmethods with urban computing applications. Furthermore, we shed light on the\ninterplay between Large Language Models (LLMs) and urban computing, postulating\nfuture research directions that could revolutionize the field. We firmly\nbelieve that the taxonomy, progress, and prospects delineated in our survey\nstand poised to significantly enrich the research community. The summary of the\ncomprehensive and up-to-date paper list can be found at\nhttps://github.com/yoshall/Awesome-Multimodal-Urban-Computing.\n', '  The digital transformation of modern cities by integrating advanced\ninformation, communication, and computing technologies has marked the epoch of\ndata-driven smart city applications for efficient and sustainable urban\nmanagement. Despite their effectiveness, these applications often rely on\nmassive amounts of high-dimensional and multi-domain data for monitoring and\ncharacterizing different urban sub-systems, presenting challenges in\napplication areas that are limited by data quality and availability, as well as\ncostly efforts for generating urban scenarios and design alternatives. As an\nemerging research area in deep learning, Generative Artificial Intelligence\n(AI) models have demonstrated their unique values in data and code generation.\nThis survey paper aims to explore the innovative integration of generative AI\ntechniques and urban digital twins to address challenges in the realm of smart\ncities in various urban sectors, such as transportation and mobility\nmanagement, energy system operations, building and infrastructure management,\nand urban design. The survey starts with the introduction of popular generative\nAI models with their application areas, followed by a structured review of the\nexisting urban science applications that leverage the autonomous capability of\nthe generative AI techniques to facilitate (a) data augmentation for promoting\nurban monitoring and predictive analytics, (b) synthetic data and scenario\ngeneration, (c) automated 3D city modeling, and (d) generative urban design and\noptimization. Based on the review, this survey discusses potential\nopportunities and technical strategies that integrate generative AI models into\nthe next-generation urban digital twins for more reliable, scalable, and\nautomated management of smart cities.\n']",Urban Planning and Analysis with AI and Data-Driven Approaches,Urban Visual Analysis and Planning
146,"Image Geolocalization Techniques , Geospatial Location Analysis and Geoparsing","['geolocalization', 'geolocators', 'geolocate', 'geolocating', 'geolocation', 'geoclip', 'geo', 'geospatial', 'geoestimation', 'geographic'] , ['geoparsing', 'geospatial', 'geo', 'geolocalization', 'geographic', 'geolocation', 'geographical', 'geolinguistic', 'geollm', 'geolingit']","['  Geolocating images of a ground-level scene entails estimating the location on\nEarth where the picture was taken, in absence of GPS or other location\nmetadata. Typically, methods are evaluated by measuring the Great Circle\nDistance (GCD) between a predicted location and ground truth. However, this\nmeasurement is limited because it only evaluates a single point, not estimates\nof regions or score heatmaps. This is especially important in applications to\nrural, wilderness and under-sampled areas, where finding the exact location may\nnot be possible, and when used in aggregate systems that progressively narrow\ndown locations.\n  In this paper, we introduce a novel metric, Recall vs Area (RvA), which\nmeasures the accuracy of estimated distributions of locations. RvA treats image\ngeolocation results similarly to document retrieval, measuring recall as a\nfunction of area: For a ranked list of (possibly non-contiguous) predicted\nregions, we measure the accumulated area required for the region to contain the\nground truth coordinate. This produces a curve similar to a precision-recall\ncurve, where ""precision"" is replaced by square kilometers area, allowing\nevaluation of performance for different downstream search area budgets.\n  Following directly from this view of the problem, we then examine a simple\nensembling approach to global-scale image geolocation, which incorporates\ninformation from multiple sources to help address domain shift, and can readily\nincorporate multiple models, attribute predictors, and data sources. We study\nits effectiveness by combining the geolocation models GeoEstimation and the\ncurrent SOTA GeoCLIP, with attribute predictors based on ORNL LandScan and\nESA-CCI Land Cover. We find significant improvements in image geolocation for\nareas that are under-represented in the training set, particularly non-urban\nareas, on both Im2GPS3k and Street View images.\n', '  Worldwide geolocalization aims to locate the precise location at the\ncoordinate level of photos taken anywhere on the Earth. It is very challenging\ndue to 1) the difficulty of capturing subtle location-aware visual semantics,\nand 2) the heterogeneous geographical distribution of image data. As a result,\nexisting studies have clear limitations when scaled to a worldwide context.\nThey may easily confuse distant images with similar visual contents, or cannot\nadapt to various locations worldwide with different amounts of relevant data.\nTo resolve these limitations, we propose G3, a novel framework based on\nRetrieval-Augmented Generation (RAG). In particular, G3 consists of three\nsteps, i.e., Geo-alignment, Geo-diversification, and Geo-verification to\noptimize both retrieval and generation phases of worldwide geolocalization.\nDuring Geo-alignment, our solution jointly learns expressive multi-modal\nrepresentations for images, GPS and textual descriptions, which allows us to\ncapture location-aware semantics for retrieving nearby images for a given\nquery. During Geo-diversification, we leverage a prompt ensembling method that\nis robust to inconsistent retrieval performance for different image queries.\nFinally, we combine both retrieved and generated GPS candidates in\nGeo-verification for location prediction. Experiments on two well-established\ndatasets IM2GPS3k and YFCC4k verify the superiority of G3 compared to other\nstate-of-the-art methods.\n', ""  Planet-scale image geolocalization remains a challenging problem due to the\ndiversity of images originating from anywhere in the world. Although approaches\nbased on vision transformers have made significant progress in geolocalization\naccuracy, success in prior literature is constrained to narrow distributions of\nimages of landmarks, and performance has not generalized to unseen places. We\npresent a new geolocalization system that combines semantic geocell creation,\nmulti-task contrastive pretraining, and a novel loss function. Additionally,\nour work is the first to perform retrieval over location clusters for guess\nrefinements. We train two models for evaluations on street-level data and\ngeneral-purpose image geolocalization; the first model, PIGEON, is trained on\ndata from the game of Geoguessr and is capable of placing over 40% of its\nguesses within 25 kilometers of the target location globally. We also develop a\nbot and deploy PIGEON in a blind experiment against humans, ranking in the top\n0.01% of players. We further challenge one of the world's foremost professional\nGeoguessr players to a series of six matches with millions of viewers, winning\nall six games. Our second model, PIGEOTTO, differs in that it is trained on a\ndataset of images from Flickr and Wikipedia, achieving state-of-the-art results\non a wide range of image geolocalization benchmarks, outperforming the previous\nSOTA by up to 7.7 percentage points on the city accuracy level and up to 38.8\npercentage points on the country level. Our findings suggest that PIGEOTTO is\nthe first image geolocalization model that effectively generalizes to unseen\nplaces and that our approach can pave the way for highly accurate, planet-scale\nimage geolocalization systems. Our code is available on GitHub.\n""] , ['  Geoparsing is the task of estimating the latitude and longitude (coordinates)\nof location expressions in texts. Geoparsing must deal with the ambiguity of\nthe expressions that indicate multiple locations with the same notation. For\nevaluating geoparsing systems, several corpora have been proposed in previous\nwork. However, these corpora are small-scale and suffer from the coverage of\nlocation expressions on general domains. In this paper, we propose Wikipedia\nHyperlink-based Location Linking (WHLL), a novel method to construct a\nlarge-scale corpus for geoparsing from Wikipedia articles. WHLL leverages\nhyperlinks in Wikipedia to annotate multiple location expressions with\ncoordinates. With this method, we constructed the WHLL corpus, a new\nlarge-scale corpus for geoparsing. The WHLL corpus consists of 1.3M articles,\neach containing about 7.8 unique location expressions. 45.6% of location\nexpressions are ambiguous and refer to more than one location with the same\nnotation. In each article, location expressions of the article title and those\nhyperlinks to other articles are assigned with coordinates. By utilizing\nhyperlinks, we can accurately assign location expressions with coordinates even\nwith ambiguous location expressions in the texts. Experimental results show\nthat there remains room for improvement by disambiguating location expressions.\n', ""  Social geolocation is an important problem of predicting the originating\nlocations of social media posts. However, this task is challenging due to the\nneed for a substantial volume of training data, alongside well-annotated\nlabels. These issues are further exacerbated by new or less popular locations\nwith insufficient labels, further leading to an imbalanced dataset. In this\npaper, we propose \\textbf{ContrastGeo}, a \\textbf{Contrast}ive learning\nenhanced framework for few-shot social \\textbf{Geo}location. Specifically, a\nTweet-Location Contrastive learning objective is introduced to align\nrepresentations of tweets and locations within tweet-location pairs. To capture\nthe correlations between tweets and locations, a Tweet-Location Matching\nobjective is further adopted into the framework and refined via an online hard\nnegative mining approach. We also develop three fusion strategies with various\nfusion encoders to better generate joint representations of tweets and\nlocations. Comprehensive experiments on three social media datasets highlight\nContrastGeo's superior performance over several state-of-the-art baselines in\nfew-shot social geolocation.\n"", '  Geospatial Location Embedding (GLE) helps a Large Language Model (LLM)\nassimilate and analyze spatial data. GLE emergence in Geospatial Artificial\nIntelligence (GeoAI) is precipitated by the need for deeper geospatial\nawareness in our complex contemporary spaces and the success of LLMs in\nextracting deep meaning in Generative AI. We searched Google Scholar, Science\nDirect, and arXiv for papers on geospatial location embedding and LLM and\nreviewed articles focused on gaining deeper spatial ""knowing"" through LLMs. We\nscreened 304 titles, 30 abstracts, and 18 full-text papers that reveal four GLE\nthemes - Entity Location Embedding (ELE), Document Location Embedding (DLE),\nSequence Location Embedding (SLE), and Token Location Embedding (TLE).\nSynthesis is tabular and narrative, including a dialogic conversation between\n""Space"" and ""LLM."" Though GLEs aid spatial understanding by superimposing\nspatial data, they emphasize the need to advance in the intricacies of spatial\nmodalities and generalized reasoning. GLEs signal the need for a Spatial\nFoundation/Language Model (SLM) that embeds spatial knowing within the model\narchitecture. The SLM framework advances Spatial Artificial Intelligence\nSystems (SPAIS), establishing a Spatial Vector Space (SVS) that maps to\nphysical space. The resulting spatially imbued Language Model is unique. It\nsimultaneously represents actual space and an AI-capable space, paving the way\nfor AI native geo storage, analysis, and multi-modality as the basis for\nSpatial Artificial Intelligence Systems (SPAIS).\n']",Geospatial Information Retrieval and Analysis,Geospatial Location Analysis and Geoparsing
147,"LiDAR-based 3D Scene Understanding and Perception , Radar and Lidar for Autonomous Driving and Sensing , ""3D Perception in Autonomous Driving with Radar and LiDAR""","['lidar', 'lidars', 'laser', 'radar', 'lasermix', 'fusion', '3d', 'deepipcv2', 'detection', 'clouds'] , ['radar', 'radars', 'lidar', 'targets', 'range', 'doppler', 'target', 'sensing', 'echoes', 'drones'] , ['lidar', 'radar', 'pointnet', 'clouds', '3d', 'camera', 'terrain', 'slam', 'depth', 'mapping']","['  Efficient data utilization is crucial for advancing 3D scene understanding in\nautonomous driving, where reliance on heavily human-annotated LiDAR point\nclouds challenges fully supervised methods. Addressing this, our study extends\ninto semi-supervised learning for LiDAR semantic segmentation, leveraging the\nintrinsic spatial priors of driving scenes and multi-sensor complements to\naugment the efficacy of unlabeled datasets. We introduce LaserMix++, an evolved\nframework that integrates laser beam manipulations from disparate LiDAR scans\nand incorporates LiDAR-camera correspondences to further assist data-efficient\nlearning. Our framework is tailored to enhance 3D scene consistency\nregularization by incorporating multi-modality, including 1) multi-modal\nLaserMix operation for fine-grained cross-sensor interactions; 2)\ncamera-to-LiDAR feature distillation that enhances LiDAR feature learning; and\n3) language-driven knowledge guidance generating auxiliary supervisions using\nopen-vocabulary models. The versatility of LaserMix++ enables applications\nacross LiDAR representations, establishing it as a universally applicable\nsolution. Our framework is rigorously validated through theoretical analysis\nand extensive experiments on popular driving perception datasets. Results\ndemonstrate that LaserMix++ markedly outperforms fully supervised alternatives,\nachieving comparable accuracy with five times fewer annotations and\nsignificantly improving the supervised-only baselines. This substantial\nadvancement underscores the potential of semi-supervised approaches in reducing\nthe reliance on extensive labeled data in LiDAR-based 3D scene understanding\nsystems.\n', '  LiDAR point clouds have become the most common data source in autonomous\ndriving. However, due to the sparsity of point clouds, accurate and reliable\ndetection cannot be achieved in specific scenarios. Because of their\ncomplementarity with point clouds, images are getting increasing attention.\nAlthough with some success, existing fusion methods either perform hard fusion\nor do not fuse in a direct manner. In this paper, we propose a generic 3D\ndetection framework called MMFusion, using multi-modal features. The framework\naims to achieve accurate fusion between LiDAR and images to improve 3D\ndetection in complex scenes. Our framework consists of two separate streams:\nthe LiDAR stream and the camera stream, which can be compatible with any\nsingle-modal feature extraction network. The Voxel Local Perception Module in\nthe LiDAR stream enhances local feature representation, and then the\nMulti-modal Feature Fusion Module selectively combines feature output from\ndifferent streams to achieve better fusion. Extensive experiments have shown\nthat our framework not only outperforms existing benchmarks but also improves\ntheir detection, especially for detecting cyclists and pedestrians on KITTI\nbenchmarks, with strong robustness and generalization capabilities. Hopefully,\nour work will stimulate more research into multi-modal fusion for autonomous\ndriving tasks.\n', ""  With the widespread application of Light Detection and Ranging (LiDAR)\ntechnology in fields such as autonomous driving, robot navigation, and terrain\nmapping, the importance of edge detection in LiDAR images has become\nincreasingly prominent. Traditional edge detection methods often face\nchallenges in accuracy and computational complexity when processing LiDAR\nimages. To address these issues, this study proposes an edge detection method\nfor LiDAR images based on artificial intelligence technology. This paper first\nreviews the current state of research on LiDAR technology and image edge\ndetection, introducing common edge detection algorithms and their applications\nin LiDAR image processing. Subsequently, a deep learning-based edge detection\nmodel is designed and implemented, optimizing the model training process\nthrough preprocessing and enhancement of the LiDAR image dataset. Experimental\nresults indicate that the proposed method outperforms traditional methods in\nterms of detection accuracy and computational efficiency, showing significant\npractical application value. Finally, improvement strategies are proposed for\nthe current method's shortcomings, and the improvements are validated through\nexperiments.\n""] , ['  This paper presents a novel deep-learning-based approach to improve\nlocalizing radar measurements against lidar maps. Although the state of the art\nfor localization is matching lidar data to lidar maps, radar has been\nconsidered as a promising alternative. This is largely due to radar being more\nresilient against adverse weather such as precipitation and heavy fog. To make\nuse of existing high-quality lidar maps, while maintaining performance in\nadverse weather, it is of interest to match radar data to lidar maps. However,\nowing in part to the unique artefacts present in radar measurements,\nradar-lidar localization has struggled to achieve comparable performance to\nlidar-lidar systems, preventing it from being viable for autonomous driving.\nThis work builds on an ICP-based radar-lidar localization system by including a\nlearned preprocessing step that weights radar points based on high-level scan\ninformation. Combining a proven analytical approach with a learned weight\nreduces localization errors in radar-lidar ICP results run on real-world\nautonomous driving data by up to 54.94% in translation and 68.39% in rotation,\nwhile maintaining interpretability and robustness.\n', '  Millimeter-wave (mmWave) radars are indispensable for perception tasks of\nautonomous vehicles, thanks to their resilience in challenging weather\nconditions. Yet, their deployment is often limited by insufficient spatial\nresolution for precise semantic scene interpretation. Classical\nsuper-resolution techniques adapted from optical imaging inadequately address\nthe distinct characteristics of radar signal data. In response, our study\nredefines radar imaging super-resolution as a one-dimensional (1D) signal\nsuper-resolution spectra estimation problem by harnessing the radar signal\nprocessing domain knowledge, introducing innovative data normalization and a\ndomain-informed signal-to-noise ratio (SNR)-guided loss function. Our tailored\ndeep learning network for automotive radar imaging exhibits remarkable\nscalability, parameter efficiency and fast inference speed, alongside enhanced\nperformance in terms of radar imaging quality and resolution. Extensive testing\nconfirms that our SR-SPECNet sets a new benchmark in producing high-resolution\nradar range-azimuth images, outperforming existing methods across varied\nantenna configurations and dataset sizes. Source code and new radar dataset\nwill be made publicly available online.\n', '  Simulation is an invaluable tool for radio-frequency system designers that\nenables rapid prototyping of various algorithms for imaging, target detection,\nclassification, and tracking. However, simulating realistic radar scans is a\nchallenging task that requires an accurate model of the scene, radio frequency\nmaterial properties, and a corresponding radar synthesis function. Rather than\nspecifying these models explicitly, we propose DART - Doppler Aided Radar\nTomography, a Neural Radiance Field-inspired method which uses radar-specific\nphysics to create a reflectance and transmittance-based rendering pipeline for\nrange-Doppler images. We then evaluate DART by constructing a custom data\ncollection platform and collecting a novel radar dataset together with accurate\nposition and instantaneous velocity measurements from lidar-based localization.\nIn comparison to state-of-the-art baselines, DART synthesizes superior radar\nrange-Doppler images from novel views across all datasets and additionally can\nbe used to generate high quality tomographic images.\n'] , ['  Depth estimation is critical in autonomous driving for interpreting 3D scenes\naccurately. Recently, radar-camera depth estimation has become of sufficient\ninterest due to the robustness and low-cost properties of radar. Thus, this\npaper introduces a two-stage, end-to-end trainable Confidence-aware Fusion Net\n(CaFNet) for dense depth estimation, combining RGB imagery with sparse and\nnoisy radar point cloud data. The first stage addresses radar-specific\nchallenges, such as ambiguous elevation and noisy measurements, by predicting a\nradar confidence map and a preliminary coarse depth map. A novel approach is\npresented for generating the ground truth for the confidence map, which\ninvolves associating each radar point with its corresponding object to identify\npotential projection surfaces. These maps, together with the initial radar\ninput, are processed by a second encoder. For the final depth estimation, we\ninnovate a confidence-aware gated fusion mechanism to integrate radar and image\nfeatures effectively, thereby enhancing the reliability of the depth map by\nfiltering out radar noise. Our methodology, evaluated on the nuScenes dataset,\ndemonstrates superior performance, improving upon the current leading model by\n3.2% in Mean Absolute Error (MAE) and 2.7% in Root Mean Square Error (RMSE).\nCode: https://github.com/harborsarah/CaFNet\n', ""  The 3D object detection capabilities in urban environments have been\nenormously improved by recent developments in Light Detection and Range (LiDAR)\ntechnology. This paper presents a novel framework that transforms the detection\nand analysis of 3D objects in traffic scenarios by utilizing the power of\nelevated LiDAR sensors. We are presenting our methodology's remarkable capacity\nto collect complex 3D point cloud data, which allows us to accurately and in\ndetail capture the dynamics of urban traffic. Due to the limitation in\nobtaining real-world traffic datasets, we utilize the simulator to generate 3D\npoint cloud for specific scenarios. To support our experimental analysis, we\nfirstly simulate various 3D point cloud traffic-related objects. Then, we use\nthis dataset as a basis for training and evaluating our 3D object detection\nmodels, in identifying and monitoring both vehicles and pedestrians in\nsimulated urban traffic environments. Next, we fine tune the Point\nVoxel-Region-based Convolutional Neural Network (PV-RCNN) architecture, making\nit more suited to handle and understand the massive volumes of point cloud data\ngenerated by our urban traffic simulations. Our results show the effectiveness\nof the proposed solution in accurately detecting objects in traffic scenes and\nhighlight the role of LiDAR in improving urban safety and advancing intelligent\ntransportation systems.\n"", '  The rapid evolution of deep learning and its integration with autonomous\ndriving systems have led to substantial advancements in 3D perception using\nmultimodal sensors. Notably, radar sensors show greater robustness compared to\ncameras and lidar under adverse weather and varying illumination conditions.\nThis study delves into the often-overlooked yet crucial issue of domain shift\nin 4D radar-based object detection, examining how varying environmental\nconditions, such as different weather patterns and road types, impact 3D object\ndetection performance. Our findings highlight distinct domain shifts across\nvarious weather scenarios, revealing unique dataset sensitivities that\nunderscore the critical role of radar point cloud generation. Additionally, we\ndemonstrate that transitioning between different road types, especially from\nhighways to urban settings, introduces notable domain shifts, emphasizing the\nnecessity for diverse data collection across varied road environments. To the\nbest of our knowledge, this is the first comprehensive analysis of domain shift\neffects on 4D radar-based object detection. We believe this empirical study\ncontributes to understanding the complex nature of domain shifts in radar data\nand suggests paths forward for data collection strategy in the face of\nenvironmental variability.\n']",Sensor Fusion and Perception for Autonomous Driving,"""3D Perception in Autonomous Driving with Radar and LiDAR"""
148,"MIMO Channel Estimation and Decoding in Wireless Communications , Massive MIMO and mmWave for 5G Networks","['channels', 'channel', 'decoding', 'decoders', 'mimo', 'decoder', 'multiplexing', 'communications', 'wireless', 'receiver'] , ['mimo', '5g', 'wireless', 'cellular', 'antennas', 'mmwave', 'transmits', 'pilot', 'antenna', 'channel']","['  Though achieving marvelous progress in various scenarios, existing semantic\ncommunication frameworks mainly consider single-input single-output Gaussian\nchannels or Rayleigh fading channels, neglecting the widely-used multiple-input\nmultiple-output (MIMO) channels, which hinders the application into practical\nsystems. One common solution to combat MIMO fading is to utilize feedback MIMO\nchannel state information (CSI). In this paper, we incorporate MIMO CSI into\nsystem designs from a new perspective and propose the learnable CSI fusion\nsemantic communication (LCFSC) framework, where CSI is treated as side\ninformation by the semantic extractor to enhance the semantic coding. To avoid\nfeature fusion due to abrupt combination of CSI with features, we present a\nnon-invasive CSI fusion multi-head attention module inside the Swin\nTransformer. With the learned attention masking map determined by both source\nand channel states, more robust attention distribution could be generated.\nFurthermore, the percentage of mask elements could be flexibly adjusted by the\nlearnable mask ratio, which is produced based on the conditional variational\ninterference in an unsupervised manner. In this way, CSI-aware semantic coding\nis achieved through learnable CSI fusion masking. Experiment results testify\nthe superiority of LCFSC over traditional schemes and state-of-the-art Swin\nTransformer-based semantic communication frameworks in MIMO fading channels.\n', '  Recently, deep learning (DL) has been emerging as a promising approach for\nchannel estimation and signal detection in wireless communications. The\nmajority of the existing studies investigating the use of DL techniques in this\ndomain focus on analysing channel impulse responses that are generated from\nonly one channel distribution such as additive white Gaussian channel noise and\nRayleigh channels. In practice, to cope with the dynamic nature of the wireless\nchannel, DL methods must be re-trained on newly non-aged collected data which\nis costly, inefficient, and impractical. To tackle this challenge, this paper\nproposes a novel universal deep neural network (Uni-DNN) that can achieve high\ndetection performance in various wireless environments without retraining the\nmodel. In particular, our proposed Uni-DNN model consists of a wireless channel\nclassifier and a signal detector which are constructed by using DNNs. The\nwireless channel classifier enables the signal detector to generalise and\nperform optimally for multiple wireless channel distributions. In addition, to\nfurther improve the signal detection performance of the proposed model,\nconvolutional neural network is employed. Extensive simulations using the\northogonal frequency division multiplexing scheme demonstrate that the bit\nerror rate performance of our proposed solution can outperform conventional\nDL-based approaches as well as least square and minimum mean square error\nchannel estimators in practical low pilot density scenarios.\n', '  In general, reliable communication via multiple-input multiple-output (MIMO)\northogonal frequency division multiplexing (OFDM) requires accurate channel\nestimation at the receiver. The existing literature largely focuses on\ndenoising methods for channel estimation that depend on either (i) channel\nanalysis in the time-domain with prior channel knowledge or (ii) supervised\nlearning techniques which require large pre-labeled datasets for training. To\naddress these limitations, we present a frequency-domain denoising method based\non a reinforcement learning framework that does not need a priori channel\nknowledge and pre-labeled data. Our methodology includes a new successive\nchannel denoising process based on channel curvature computation, for which we\nobtain a channel curvature magnitude threshold to identify unreliable channel\nestimates. Based on this process, we formulate the denoising mechanism as a\nMarkov decision process, where we define the actions through a geometry-based\nchannel estimation update, and the reward function based on a policy that\nreduces mean squared error (MSE). We then resort to Q-learning to update the\nchannel estimates. Numerical results verify that our denoising algorithm can\nsuccessfully mitigate noise in channel estimates. In particular, our algorithm\nprovides a significant improvement over the practical least squares (LS)\nestimation method and provides performance that approaches that of the ideal\nlinear minimum mean square error (LMMSE) estimation with perfect knowledge of\nchannel statistics.\n'] , ['  Massive MIMO is expected to play an important role in the development of 5G\nnetworks. This paper addresses the issue of pilot contamination and scalability\nin massive MIMO systems. The current practice of reusing orthogonal pilot\nsequences in adjacent cells leads to difficulty in differentiating incoming\ninter- and intra-cell pilot sequences. One possible solution is to increase the\nnumber of orthogonal pilot sequences, which results in dedicating more space of\ncoherence block to pilot transmission than data transmission. This, in turn,\nalso hinders the scalability of massive MIMO systems, particularly in\naccommodating a large number of IoT devices within a cell. To overcome these\nchallenges, this paper devises an innovative pilot allocation scheme based on\nthe data transfer patterns of IoT devices. The scheme assigns orthogonal pilot\nsequences to clusters of devices instead of individual devices, allowing\nmultiple devices to utilize the same pilot for periodically transmitting data.\nMoreover, we formulate the pilot assignment problem as a graph coloring problem\nand use the max k-cut graph partitioning approach to overcome the pilot\ncontamination in a multicell massive MIMO system. The proposed scheme\nsignificantly improves the spectral efficiency and enables the scalability of\nmassive MIMO systems; for instance, by using ten orthogonal pilot sequences, we\nare able to accommodate 200 devices with only a 12.5% omission rate.\n', '  Millimeter wave (mmWave) communications can potentially meet the high\ndata-rate requirements of unmanned aerial vehicle (UAV) networks. However, as\nthe prerequisite of mmWave communications, the narrow directional beam tracking\nis very challenging because of the three-dimensional (3D) mobility and attitude\nvariation of UAVs. Aiming to address the beam tracking difficulties, we propose\nto integrate the conformal array (CA) with the surface of each UAV, which\nenables the full spatial coverage and the agile beam tracking in highly dynamic\nUAV mmWave networks. More specifically, the key contributions of our work are\nthree-fold. 1) A new mmWave beam tracking framework is established for the\nCA-enabled UAV mmWave network. 2) A specialized hierarchical codebook is\nconstructed to drive the directional radiating element (DRE)-covered\ncylindrical conformal array (CCA), which contains both the angular beam pattern\nand the subarray pattern to fully utilize the potential of the CA. 3) A\ncodebook-based multiuser beam tracking scheme is proposed, where the Gaussian\nprocess machine learning enabled UAV position/attitude predication is developed\nto improve the beam tracking efficiency in conjunction with the tracking-error\naware adaptive beamwidth control. Simulation results validate the effectiveness\nof the proposed codebook-based beam tracking scheme in the CA-enabled UAV\nmmWave network, and demonstrate the advantages of CA over the conventional\nplanner array in terms of spectrum efficiency and outage probability in the\nhighly dynamic scenarios.\n', '  5G sets the foundation for an era of creativity with its faster speeds,\nincreased data throughput, reduced latency, and enhanced IoT connectivity, all\nenabled by Massive MIMO (M-MIMO) technology. M-MIMO boosts network efficiency\nand enhances user experience by employing intelligent user scheduling. This\npaper presents a user scheduling scheme and pilot assignment strategy designed\nfor IoT devices, emphasizing mitigating pilot contamination, a key obstacle to\nimproving spectral efficiency (SE) and system scalability in M-MIMO networks.\nWe utilize a user clustering-based pilot allocation scheme to boost IoT device\nscalability in M-MIMO systems. Additionally, our smart pilot allocation\nminimizes interference and enhances SE by treating pilot assignment as a graph\ncoloring problem, optimizing it through integer linear programming (ILP).\nRecognizing the computational complexity of ILP, we introduced a binary\nsearch-based heuristic predicated on interference threshold to expedite the\ncomputation, while maintaining a near-optimal solution. The simulation results\nshow a significant decrease in the required pilot overhead (about 17%), and\nsubstantial enhancement in SE (about 8-14%).\n']",Advanced Wireless Communication Technologies,MIMO Channel Estimation and Decoding in Wireless Communications
149,"""Satellite Imagery Analysis for Earth Observation"" , Orbit Prediction and Satellite Tracking","['cropland', 'sensed', 'imagery', 'land', 'satellite', 'crop', 'vegetation', 'cnn', 'earth', 'sensing'] , ['orbit', 'orbiting', 'satellites', 'spacecraft', 'satellite', 'orbits', 'orbital', 'jupiter', 'prediction', 'asteroids']","[""  Earth observation (EO) satellite missions have been providing detailed images\nabout the state of the Earth and its land cover for over 50 years. Long term\nmissions, such as NASA's Landsat, Terra, and Aqua satellites, and more\nrecently, the ESA's Sentinel missions, record images of the entire world every\nfew days. Although single images provide point-in-time data, repeated images of\nthe same area, or satellite image time series (SITS) provide information about\nthe changing state of vegetation and land use. These SITS are useful for\nmodeling dynamic processes and seasonal changes such as plant phenology. They\nhave potential benefits for many aspects of land and natural resource\nmanagement, including applications in agricultural, forest, water, and disaster\nmanagement, urban planning, and mining. However, the resulting satellite image\ntime series (SITS) are complex, incorporating information from the temporal,\nspatial, and spectral dimensions. Therefore, deep learning methods are often\ndeployed as they can analyze these complex relationships. This review presents\na summary of the state-of-the-art methods of modelling environmental,\nagricultural, and other Earth observation variables from SITS data using deep\nlearning methods. We aim to provide a resource for remote sensing experts\ninterested in using deep learning techniques to enhance Earth observation\nmodels with temporal information.\n"", '  We introduce a simple yet effective early fusion method for crop yield\nprediction that handles multiple input modalities with different temporal and\nspatial resolutions. We use high-resolution crop yield maps as ground truth\ndata to train crop and machine learning model agnostic methods at the sub-field\nlevel. We use Sentinel-2 satellite imagery as the primary modality for input\ndata with other complementary modalities, including weather, soil, and DEM\ndata. The proposed method uses input modalities available with global coverage,\nmaking the framework globally scalable. We explicitly highlight the importance\nof input modalities for crop yield prediction and emphasize that the\nbest-performing combination of input modalities depends on region, crop, and\nchosen model.\n', ""  Satellites equipped with optical sensors capture high-resolution imagery,\nproviding valuable insights into various environmental phenomena. In recent\nyears, there has been a surge of research focused on addressing some challenges\nin remote sensing, ranging from water detection in diverse landscapes to the\nsegmentation of mountainous and terrains. Ongoing investigations goals to\nenhance the precision and efficiency of satellite imagery analysis. Especially,\nthere is a growing emphasis on developing methodologies for accurate water body\ndetection, snow and clouds, important for environmental monitoring, resource\nmanagement, and disaster response. Within this context, this paper focus on the\ncloud segmentation from remote sensing imagery. Accurate remote sensing data\nanalysis can be challenging due to the presence of clouds in optical\nsensor-based applications. The quality of resulting products such as\napplications and research is directly impacted by cloud detection, which plays\na key role in the remote sensing data processing pipeline. This paper examines\nseven cutting-edge semantic segmentation and detection algorithms applied to\nclouds identification, conducting a benchmark analysis to evaluate their\narchitectural approaches and identify the most performing ones. To increase the\nmodel's adaptability, critical elements including the type of imagery and the\namount of spectral bands used during training are analyzed. Additionally, this\nresearch tries to produce machine learning algorithms that can perform cloud\nsegmentation using only a few spectral bands, including RGB and RGBN-IR\ncombinations. The model's flexibility for a variety of applications and user\nscenarios is assessed by using imagery from Sentinel-2 and Landsat-8 as\ndatasets. This benchmark can be reproduced using the material from this github\nlink: https://github.com/toelt-llc/cloud_segmentation_comparative.\n""] , ['  The Space Domain Awareness (SDA) community routinely tracks satellites in\norbit by fitting an orbital state to observations made by the Space\nSurveillance Network (SSN). In order to fit such orbits, an accurate model of\nthe forces that are acting on the satellite is required. Over the past several\ndecades, high-quality, physics-based models have been developed for satellite\nstate estimation and propagation. These models are exceedingly good at\nestimating and propagating orbital states for non-maneuvering satellites;\nhowever, there are several classes of anomalous accelerations that a satellite\nmight experience which are not well-modeled, such as satellites that use\nlow-thrust electric propulsion to modify their orbit. Physics-Informed Neural\nNetworks (PINNs) are a valuable tool for these classes of satellites as they\ncombine physics models with Deep Neural Networks (DNNs), which are highly\nexpressive and versatile function approximators. By combining a physics model\nwith a DNN, the machine learning model need not learn astrodynamics, which\nresults in more efficient and effective utilization of machine learning\nresources. This paper details the application of PINNs to estimate the orbital\nstate and a continuous, low-amplitude anomalous acceleration profile for\nsatellites. The PINN is trained to learn the unknown acceleration by minimizing\nthe mean square error of observations. We evaluate the performance of pure\nphysics models with PINNs in terms of their observation residuals and their\npropagation accuracy beyond the fit span of the observations. For a two-day\nsimulation of a GEO satellite using an unmodeled acceleration profile on the\norder of $10^{-8} \\text{ km/s}^2$, the PINN outperformed the best-fit physics\nmodel by orders of magnitude for both observation residuals (123 arcsec vs 1.00\narcsec) as well as propagation accuracy (3860 km vs 164 km after five days).\n', ""  The increasing volume of space objects in Earth's orbit presents a\nsignificant challenge for Space Situational Awareness (SSA). And in particular,\naccurate orbit prediction is crucial to anticipate the position and velocity of\nspace objects, for collision avoidance and space debris mitigation. When\nperforming Orbit Prediction (OP), it is necessary to consider the impact of\nnon-conservative forces, such as atmospheric drag and gravitational\nperturbations, that contribute to uncertainty around the future position of\nspacecraft and space debris alike. Conventional propagator methods like the\nSGP4 inadequately account for these forces, while numerical propagators are\nable to model the forces at a high computational cost. To address these\nlimitations, we propose an orbit prediction algorithm utilizing machine\nlearning. This algorithm forecasts state vectors on a spacecraft using past\npositions and environmental variables like atmospheric density from external\nsources. The orbital data used in the paper is gathered from precision\nephemeris data from the International Laser Ranging Service (ILRS), for the\nperiod of almost a year. We show how the use of machine learning and\ntime-series techniques can produce low positioning errors at a very low\ncomputational cost, thus significantly improving SSA capabilities by providing\nfaster and reliable orbit determination for an ever increasing number of space\nobjects.\n"", ""  The Simplified General Perturbations 4 (SGP4) orbital propagation method is\nwidely used for predicting the positions and velocities of Earth-orbiting\nobjects rapidly and reliably. Despite continuous refinement, SGP models still\nlack the precision of numerical propagators, which offer significantly smaller\nerrors. This study presents dSGP4, a novel differentiable version of SGP4\nimplemented using PyTorch. By making SGP4 differentiable, dSGP4 facilitates\nvarious space-related applications, including spacecraft orbit determination,\nstate conversion, covariance transformation, state transition matrix\ncomputation, and covariance propagation. Additionally, dSGP4's PyTorch\nimplementation allows for embarrassingly parallel orbital propagation across\nbatches of Two-Line Element Sets (TLEs), leveraging the computational power of\nCPUs, GPUs, and advanced hardware for distributed prediction of satellite\npositions at future times. Furthermore, dSGP4's differentiability enables\nintegration with modern machine learning techniques. Thus, we propose a novel\norbital propagation paradigm, ML-dSGP4, where neural networks are integrated\ninto the orbital propagator. Through stochastic gradient descent, this combined\nmodel's inputs, outputs, and parameters can be iteratively refined, surpassing\nSGP4's precision. Neural networks act as identity operators by default,\nadhering to SGP4's behavior. However, dSGP4's differentiability allows\nfine-tuning with ephemeris data, enhancing precision while maintaining\ncomputational speed. This empowers satellite operators and researchers to train\nthe model using specific ephemeris or high-precision numerical propagation\ndata, significantly advancing orbital prediction capabilities.\n""]",Satellite Data Analysis and Space Object Tracking,"""Satellite Imagery Analysis for Earth Observation"""
150,"Indoor Localization using WiFi Fingerprinting , Inertial Sensor-Based Navigation and Tracking","['wifi', 'fingerprinting', 'gps', 'wi', 'wireless', 'fingerprint', 'localization', 'ranging', 'sensing', 'indoor'] , ['gps', 'accelerometer', 'gyroscope', 'sensor', 'sensors', 'tracking', 'inertial', 'magnetometer', 'magnetometers', 'satellite']","[""  The rise of the Internet of Things (IoT) and mobile internet applications has\nspurred interest in location-based services (LBS) for commercial, military, and\nsocial applications. While the global positioning system (GPS) dominates\noutdoor localization, its efficacy wanes indoors due to signal challenges.\nIndoor localization systems leverage wireless technologies like Wi-Fi, ZigBee,\nBluetooth, UWB, selecting based on context. Received signal strength indicator\n(RSSI) technology, known for its accuracy and simplicity, is widely adopted.\nThis study employs machine learning algorithms in three phases: supervised\nregressors, supervised classifiers, and ensemble methods for RSSI-based indoor\nlocalization. Additionally, it introduces a weighted least squares technique\nand pseudo-linear solution approach to address non-linear RSSI measurement\nequations by approximating them with linear equations. An experimental testbed,\nutilizing diverse wireless technologies and anchor nodes, is designed for data\ncollection, employing IoT cloud architectures. Pre-processing involves\ninvestigating filters for data refinement before algorithm training. The study\nemploys machine learning models like linear regression, polynomial regression,\nsupport vector regression, random forest regression, and decision tree\nregressor across various wireless technologies. These models estimate the\ngeographical coordinates of a moving target node, and their performance is\nevaluated using metrics such as accuracy, root mean square errors, precision,\nrecall, sensitivity, coefficient of determinant, and the f1-score. The\nexperiment's outcomes provide insights into the effectiveness of different\nsupervised machine learning techniques in terms of localization accuracy and\nrobustness in indoor environments.\n"", '  Wi-Fi fingerprinting has emerged as the most popular approach to indoor\nlocalization. The use of ML algorithms has greatly improved the localization\nperformance of Wi-Fi fingerprinting, but its success depends on the\navailability of fingerprint databases composed of a large number of RSSIs, the\nMAC addresses of access points, and the other measurement information. However,\nmost fingerprint databases do not reflect well the time varying nature of\nelectromagnetic interferences in complicated modern indoor environment. This\ncould result in significant changes in statistical characteristics of\ntraining/validation and testing datasets, which are often constructed at\ndifferent times, and even the characteristics of the testing datasets could be\ndifferent from those of the data submitted by users during the operation of\nlocalization systems after their deployment. In this paper, we consider the\nimplications of time-varying Wi-Fi fingerprints on indoor localization from a\ndata-centric point of view and discuss the differences between static and\ndynamic databases. As a case study, we have constructed a dynamic database\ncovering three floors of the IR building of XJTLU based on RSSI measurements,\nover 44 days, and investigated the differences between static and dynamic\ndatabases in terms of statistical characteristics and localization performance.\nThe analyses based on variance calculations and Isolation Forest show the\ntemporal shifts in RSSIs, which result in a noticeable trend of the increase in\nthe localization error of a Gaussian process regression model with the maximum\nerror of 6.65 m after 14 days of training without model adjustments. The\nresults of the case study with the XJTLU dynamic database clearly demonstrate\nthe limitations of static databases and the importance of the creation and\nadoption of dynamic databases for future indoor localization research and\nreal-world deployment.\n', '  The recognition of human activities based on WiFi Channel State Information\n(CSI) enables contactless and visual privacy-preserving sensing in indoor\nenvironments. However, poor model generalization, due to varying environmental\nconditions and sensing hardware, is a well-known problem in this space. To\naddress this issue, in this work, data augmentation techniques commonly used in\nimage-based learning are applied to WiFi CSI to investigate their effects on\nmodel generalization performance in cross-scenario and cross-system settings.\nIn particular, we focus on the generalization between line-of-sight (LOS) and\nnon-line-of-sight (NLOS) through-wall scenarios, as well as on the\ngeneralization between different antenna systems, which remains under-explored.\nWe collect and make publicly available a dataset of CSI amplitude spectrograms\nof human activities. Utilizing this data, an ablation study is conducted in\nwhich activity recognition models based on the EfficientNetV2 architecture are\ntrained, allowing us to assess the effects of each augmentation on model\ngeneralization performance. The gathered results show that specific\ncombinations of simple data augmentation techniques applied to CSI amplitude\ndata can significantly improve cross-scenario and cross-system generalization.\n'] , ['  Many Internet of Things applications utilize low-cost, micro,\nelectro-mechanical inertial sensors. A common task is orientation estimation.\nTo tackle such a task, attitude and heading reference system algorithms are\napplied. Relying on the gyroscope readings, the accelerometer readings are used\nto update the attitude angles, and magnetometer measurements are utilized to\nupdate the heading angle. In indoor environments, magnetometers suffer from\ninterference that degrades their performance. This mainly influences\napplications focused on estimating the heading angle like finding the heading\nangle of a closet or fridge door. To circumvent such situations, we propose\nDoorINet, an end-to-end deep-learning framework to calculate the heading angle\nfrom door-mounted, low-cost inertial sensors without using magnetometers. To\nevaluate our approach, we record a unique dataset containing 391 minutes of\naccelerometer and gyroscope measurements and corresponding ground-truth heading\nangle. We show that our proposed approach outperforms commonly used, model\nbased approaches and data-driven methods.\n', '  Accurate alignment of a fixed mobile device equipped with inertial sensors\ninside a moving vehicle is important for navigation, activity recognition, and\nother applications. Accurate estimation of the device mounting angle is\nrequired to rotate the inertial measurement from the sensor frame to the moving\nplatform frame to standardize measurements and improve the performance of the\ntarget task. In this work, a data-driven approach using deep neural networks\n(DNNs) is proposed to learn the yaw mounting angle of a smartphone equipped\nwith an inertial measurement unit (IMU) and strapped to a car. The proposed\nmodel uses only the accelerometer and gyroscope readings from an IMU as input\nand, in contrast to existing solutions, does not require global position inputs\nfrom global navigation satellite systems (GNSS). To train the model in a\nsupervised manner, IMU data is collected for training and validation with the\nsensor mounted at a known yaw mounting angle, and a range of ground truth\nlabels is generated by applying a random rotation in a bounded range to the\nmeasurements. The trained model is tested on data with real rotations showing\nsimilar performance as with synthetic rotations. The trained model is deployed\non an Android device and evaluated in real-time to test the accuracy of the\nestimated yaw mounting angle. The model is shown to find the mounting angle at\nan accuracy of 8 degrees within 5 seconds, and 4 degrees within 27 seconds. An\nexperiment is conducted to compare the proposed model with an existing\noff-the-shelf solution.\n', ""  In this paper, we validate the performance of the a sensor fusion-based\nGlobal Navigation Satellite System (GNSS) spoofing attack detection framework\nfor Autonomous Vehicles (AVs). To collect data, a vehicle equipped with a GNSS\nreceiver, along with Inertial Measurement Unit (IMU) is used. The detection\nframework incorporates two strategies: The first strategy involves comparing\nthe predicted location shift, which is the distance traveled between two\nconsecutive timestamps, with the inertial sensor-based location shift. For this\npurpose, data from low-cost in-vehicle inertial sensors such as the\naccelerometer and gyroscope sensor are fused and fed into a long short-term\nmemory (LSTM) neural network. The second strategy employs a Random-Forest\nsupervised machine learning model to detect and classify turns, distinguishing\nbetween left and right turns using the output from the steering angle sensor.\nIn experiments, two types of spoofing attack models: turn-by-turn and wrong\nturn are simulated. These spoofing attacks are modeled as SQL injection\nattacks, where, upon successful implementation, the navigation system perceives\ninjected spoofed location information as legitimate while being unable to\ndetect legitimate GNSS signals. Importantly, the IMU data remains uncompromised\nthroughout the spoofing attack. To test the effectiveness of the detection\nframework, experiments are conducted in Tuscaloosa, AL, mimicking urban road\nstructures. The results demonstrate the framework's ability to detect various\nsophisticated GNSS spoofing attacks, even including slow position drifting\nattacks. Overall, the experimental results showcase the robustness and efficacy\nof the sensor fusion-based spoofing attack detection approach in safeguarding\nAVs against GNSS spoofing threats.\n""]",Indoor Navigation and Tracking Technologies,Inertial Sensor-Based Navigation and Tracking
151,"Bayesian State Estimation and Filtering , System Identification and Estimation Methods , Off-Policy Evaluation Estimators","['kalmannet', 'kalman', 'filtering', 'estimation', 'bayesian', 'filter', 'state', 'probabilistic', 'gpssm', 'states'] , ['estimators', 'estimate', 'modelic', 'controllers', 'lasso', 'identification', 'systems', 'complexity', 'identifiability', 'controller'] , ['policies', 'policy', 'estimators', 'unbiasedness', 'estimation', 'unbiasedly', 'unbiased', 'evaluation', 'estimate', 'estimator']","[""  Bayesian filtering serves as the mainstream framework of state estimation in\ndynamic systems. Its standard version utilizes total probability rule and\nBayes' law alternatively, where how to define and compute conditional\nprobability is critical to state distribution inference. Previously, the\nconditional probability is assumed to be exactly known, which represents a\nmeasure of the occurrence probability of one event, given the second event. In\nthis paper, we find that by adding an additional event that stipulates an\ninequality condition, we can transform the conditional probability into a\nspecial integration that is analogous to convolution. Based on this\ntransformation, we show that both transition probability and output probability\ncan be generalized to convolutional forms, resulting in a more general\nfiltering framework that we call convolutional Bayesian filtering. This new\nframework encompasses standard Bayesian filtering as a special case when the\ndistance metric of the inequality condition is selected as Dirac delta\nfunction. It also allows for a more nuanced consideration of model mismatch by\nchoosing different types of inequality conditions. For instance, when the\ndistance metric is defined in a distributional sense, the transition\nprobability and output probability can be approximated by simply rescaling them\ninto fractional powers. Under this framework, a robust version of Kalman filter\ncan be constructed by only altering the noise covariance matrix, while\nmaintaining the conjugate nature of Gaussian distributions. Finally, we\nexemplify the effectiveness of our approach by reshaping classic filtering\nalgorithms into convolutional versions, including Kalman filter, extended\nKalman filter, unscented Kalman filter and particle filter.\n"", ""  The research topic is: data-driven Bayesian state estimation with compressed\nmeasurement (BSCM) of model-free process, say for a (causal) tracking\napplication. The dimension of the temporal measurement vector is lower than the\ndimension of the temporal state vector to be estimated. Hence the state\nestimation problem is an underdetermined inverse problem. The state-space-model\n(SSM) of the underlying dynamical process is assumed to be unknown and hence,\nwe use the terminology 'model-free process'. In absence of the SSM, we can not\nemploy traditional model-driven methods like Kalman Filter (KF) and Particle\nFilter (PF) and instead require data-driven methods. We first experimentally\nshow that two existing unsupervised learning-based data-driven methods fail to\naddress the BSCM problem for model-free process; they are data-driven nonlinear\nstate estimation (DANSE) method and deep Markov model (DMM) method. The\nunsupervised learning uses unlabelled data comprised of only noisy\nmeasurements. While DANSE provides a good predictive performance to model the\ntemporal measurement data as time-series, its unsupervised learning lacks a\nregularization for state estimation. We then investigate use of a\nsemi-supervised learning approach, and develop a semi-supervised learning-based\nDANSE method, referred to as SemiDANSE. In the semi-supervised learning, we use\na limited amount of labelled data along-with a large amount of unlabelled data,\nand that helps to bring the desired regularization for BSCM problem in the\nabsence of SSM. The labelled data means pairwise measurement-and-state data.\nUsing three chaotic dynamical systems (or processes) with nonlinear SSMs as\nbenchmark, we show that the data-driven SemiDANSE provides competitive\nperformance for BSCM against three SSM-informed methods - a hybrid method\ncalled KalmanNet, and two traditional model-driven methods called extended KF\nand unscented KF.\n"", '  The problem of system identification for the Kalman filter, relying on the\nexpectation-maximization (EM) procedure to learn the underlying parameters of a\ndynamical system, has largely been studied assuming that observations are\nsampled at equally-spaced time points. However, in many applications this is a\nrestrictive and unrealistic assumption. This paper addresses system\nidentification for the continuous-discrete filter, with the aim of generalizing\nlearning for the Kalman filter by relying on a solution to a continuous-time\nIt\\^o stochastic differential equation (SDE) for the latent state and\ncovariance dynamics. We introduce a novel two-filter, analytical form for the\nposterior with a Bayesian derivation, which yields analytical updates which do\nnot require the forward-pass to be pre-computed. Using this analytical and\nefficient computation of the posterior, we provide an EM procedure which\nestimates the parameters of the SDE, naturally incorporating irregularly\nsampled measurements. Generalizing the learning of latent linear dynamical\nsystems (LDS) to continuous-time may extend the use of the hybrid Kalman filter\nto data which is not regularly sampled or has intermittent missing values, and\ncan extend the power of non-linear system identification methods such as\nswitching LDS (SLDS), which rely on EM for the linear discrete-time Kalman\nfilter as a sub-unit for learning locally linearized behavior of a non-linear\nsystem. We apply the method by learning the parameters of a latent,\nmultivariate Fokker-Planck SDE representing a toggle-switch genetic circuit\nusing biologically realistic parameters, and compare the efficacy of learning\nrelative to the discrete-time Kalman filter as the step-size irregularity and\nspectral-radius of the dynamics-matrix increases.\n'] , [""  Machine Learning (ML) and linear System Identification (SI) have been\nhistorically developed independently. In this paper, we leverage\nwell-established ML tools - especially the automatic differentiation framework\n- to introduce SIMBa, a family of discrete linear multi-step-ahead state-space\nSI methods using backpropagation. SIMBa relies on a novel\nLinear-Matrix-Inequality-based free parametrization of Schur matrices to ensure\nthe stability of the identified model.\n  We show how SIMBa generally outperforms traditional linear state-space SI\nmethods, and sometimes significantly, although at the price of a higher\ncomputational burden. This performance gap is particularly remarkable compared\nto other SI methods with stability guarantees, where the gain is frequently\nabove 25% in our investigations, hinting at SIMBa's ability to simultaneously\nachieve state-of-the-art fitting performance and enforce stability.\nInterestingly, these observations hold for a wide variety of input-output\nsystems and on both simulated and real-world data, showcasing the flexibility\nof the proposed approach. We postulate that this new SI paradigm presents a\ngreat extension potential to identify structured nonlinear models from data,\nand we hence open-source SIMBa on https://github.com/Cemempamoi/simba.\n"", '  The focus of this paper is on linear system identification in the setting\nwhere it is known that the underlying partially-observed linear dynamical\nsystem lies within a finite collection of known candidate models. We first\nconsider the problem of identification from a given trajectory, which in this\nsetting reduces to identifying the index of the true model with high\nprobability. We characterize the finite-time sample complexity of this problem\nby leveraging recent advances in the non-asymptotic analysis of linear\nleast-square methods in the literature. In comparison to the earlier results\nthat assume no prior knowledge of the system, our approach takes advantage of\nthe smaller hypothesis class and leads to the design of a learner with a\ndimension-free sample complexity bound. Next, we consider the switching control\nof linear systems, where there is a candidate controller for each of the\ncandidate models and data is collected through interaction of the system with a\ncollection of potentially destabilizing controllers. We develop a\ndimension-dependent criterion that can detect those destabilizing controllers\nin finite time. By leveraging these results, we propose a data-driven switching\nstrategy that identifies the unknown parameters of the underlying system. We\nthen provide a non-asymptotic analysis of its performance and discuss its\nimplications on the classical method of estimator-based supervisory control.\n', '  We study non-parametric frequency-domain system identification from a\nfinite-sample perspective. We assume an open loop scenario where the excitation\ninput is periodic and consider the Empirical Transfer Function Estimate (ETFE),\nwhere the goal is to estimate the frequency response at certain desired\n(evenly-spaced) frequencies, given input-output samples. We show that under\nsub-Gaussian colored noise (in time-domain) and stability assumptions, the ETFE\nestimates are concentrated around the true values. The error rate is of the\norder of\n$\\mathcal{O}((d_{\\mathrm{u}}+\\sqrt{d_{\\mathrm{u}}d_{\\mathrm{y}}})\\sqrt{M/N_{\\mathrm{tot}}})$,\nwhere $N_{\\mathrm{tot}}$ is the total number of samples, $M$ is the number of\ndesired frequencies, and $d_{\\mathrm{u}},\\,d_{\\mathrm{y}}$ are the dimensions\nof the input and output signals respectively. This rate remains valid for\ngeneral irrational transfer functions and does not require a finite order\nstate-space representation. By tuning $M$, we obtain a\n$N_{\\mathrm{tot}}^{-1/3}$ finite-sample rate for learning the frequency\nresponse over all frequencies in the $ \\mathcal{H}_{\\infty}$ norm. Our result\ndraws upon an extension of the Hanson-Wright inequality to semi-infinite\nmatrices. We study the finite-sample behavior of ETFE in simulations.\n'] , [""  Offline policy evaluation (OPE) allows us to evaluate and estimate a new\nsequential decision-making policy's performance by leveraging historical\ninteraction data collected from other policies. Evaluating a new policy online\nwithout a confident estimate of its performance can lead to costly, unsafe, or\nhazardous outcomes, especially in education and healthcare. Several OPE\nestimators have been proposed in the last decade, many of which have\nhyperparameters and require training. Unfortunately, choosing the best OPE\nalgorithm for each task and domain is still unclear. In this paper, we propose\na new algorithm that adaptively blends a set of OPE estimators given a dataset\nwithout relying on an explicit selection using a statistical procedure. We\nprove that our estimator is consistent and satisfies several desirable\nproperties for policy evaluation. Additionally, we demonstrate that when\ncompared to alternative approaches, our estimator can be used to select\nhigher-performing policies in healthcare and robotics. Our work contributes to\nimproving ease of use for a general-purpose, estimator-agnostic, off-policy\nevaluation framework for offline RL.\n"", '  Off-Policy Evaluation (OPE) aims to assess the effectiveness of\ncounterfactual policies using only offline logged data and is often used to\nidentify the top-k promising policies for deployment in online A/B tests.\nExisting evaluation metrics for OPE estimators primarily focus on the\n""accuracy"" of OPE or that of downstream policy selection, neglecting\nrisk-return tradeoff in the subsequent online policy deployment. To address\nthis issue, we draw inspiration from portfolio evaluation in finance and\ndevelop a new metric, called SharpeRatio@k, which measures the risk-return\ntradeoff of policy portfolios formed by an OPE estimator under varying online\nevaluation budgets (k). We validate our metric in two example scenarios,\ndemonstrating its ability to effectively distinguish between low-risk and\nhigh-risk estimators and to accurately identify the most efficient one.\nEfficiency of an estimator is characterized by its capability to form the most\nadvantageous policy portfolios, maximizing returns while minimizing risks\nduring online deployment, a nuance that existing metrics typically overlook. To\nfacilitate a quick, accurate, and consistent evaluation of OPE via\nSharpeRatio@k, we have also integrated this metric into an open-source\nsoftware, SCOPE-RL (https://github.com/hakuhodo-technologies/scope-rl).\nEmploying SharpeRatio@k and SCOPE-RL, we conduct comprehensive benchmarking\nexperiments on various estimators and RL tasks, focusing on their risk-return\ntradeoff. These experiments offer several interesting directions and\nsuggestions for future OPE research.\n', '  The Off-Policy Evaluation (OPE) problem consists of evaluating the\nperformance of counterfactual policies with data collected by another one. This\nproblem is of utmost importance for various application domains, e.g.,\nrecommendation systems, medical treatments, and many others. To solve the OPE\nproblem, we resort to estimators, which aim to estimate in the most accurate\nway possible the performance that the counterfactual policies would have had if\nthey were deployed in place of the logging policy. In the literature, several\nestimators have been developed, all with different characteristics and\ntheoretical guarantees. Therefore, there is no dominant estimator, and each\nestimator may be the best one for different OPE problems, depending on the\ncharacteristics of the dataset at hand. While the selection of the estimator is\na crucial choice for an accurate OPE, this problem has been widely overlooked\nin the literature. We propose an automated data-driven OPE estimator selection\nmethod based on machine learning. In particular, the core idea we propose in\nthis paper is to create several synthetic OPE tasks and use a machine learning\nmodel trained to predict the best estimator for those synthetic tasks. We\nempirically show how our method is able to generalize to unseen tasks and make\na better estimator selection compared to a baseline method on several\nreal-world datasets, with a computational cost significantly lower than the one\nof the baseline.\n']",State Estimation and System Identification,Bayesian State Estimation and Filtering
152,"Medical Image Registration and Deformation Analysis , Medical Imaging Segmentation and Analysis , ""Medical Image Segmentation and Detection""","['deformation', 'deforming', 'registration', 'pose', 'deformations', 'registrations', 'imaging', 'deformable', 'images', 'anatomy'] , ['mri', 'imaging', 'neuroimaging', 'tomography', 'fmri', 'segmentation', 'dataset', 'deep', 'images', 'brain'] , ['segmentation', 'imaging', 'supervised', 'mri', 'microscopy', 'tomography', 'images', 'datasets', 'unsupervised', 'detection']","[""  Medical image synthesis remains challenging due to misalignment noise during\ntraining. Existing methods have attempted to address this challenge by\nincorporating a registration-guided module. However, these methods tend to\noverlook the task-specific constraints on the synthetic and registration\nmodules, which may cause the synthetic module to still generate spatially\naligned images with misaligned target images during training, regardless of the\nregistration module's function. Therefore, this paper proposes\nregistration-guided consistency and incorporates disentanglement learning for\nmedical image synthesis. The proposed registration-guided consistency\narchitecture fosters task-specificity within the synthetic and registration\nmodules by applying identical deformation fields before and after synthesis,\nwhile enforcing output consistency through an alignment loss. Moreover, the\nsynthetic module is designed to possess the capability of disentangling\nanatomical structures and specific styles across various modalities. An anatomy\nconsistency loss is introduced to further compel the synthetic module to\npreserve geometrical integrity within latent spaces. Experiments conducted on\nboth an in-house abdominal CECT-CT dataset and a publicly available pelvic\nMR-CT dataset have demonstrated the superiority of the proposed method.\n"", '  Image registration (IR) is a process that deforms images to align them with\nrespect to a reference space, making it easier for medical practitioners to\nexamine various medical images in a standardized reference frame, such as\nhaving the same rotation and scale. This document introduces image registration\nusing a simple numeric example. It provides a definition of image registration\nalong with a space-oriented symbolic representation. This review covers various\naspects of image transformations, including affine, deformable, invertible, and\nbidirectional transformations, as well as medical image registration algorithms\nsuch as Voxelmorph, Demons, SyN, Iterative Closest Point, and SynthMorph. It\nalso explores atlas-based registration and multistage image registration\ntechniques, including coarse-fine and pyramid approaches. Furthermore, this\nsurvey paper discusses medical image registration taxonomies, datasets,\nevaluation measures, such as correlation-based metrics, segmentation-based\nmetrics, processing time, and model size. It also explores applications in\nimage-guided surgery, motion tracking, and tumor diagnosis. Finally, the\ndocument addresses future research directions, including the further\ndevelopment of transformers.\n', '  In this work, we propose a novel deformable convolutional pyramid network for\nunsupervised image registration. Specifically, the proposed network enhances\nthe traditional pyramid network by adding an additional shared auxiliary\ndecoder for image pairs. This decoder provides multi-scale high-level feature\ninformation from unblended image pairs for the registration task. During the\nregistration process, we also design a multi-scale feature fusion block to\nextract the most beneficial features for the registration task from both global\nand local contexts. Validation results indicate that this method can capture\ncomplex deformations while achieving higher registration accuracy and\nmaintaining smooth and plausible deformations.\n'] , ['  Computed tomography (CT) segmentation models frequently include classes that\nare not currently supported by magnetic resonance imaging (MRI) segmentation\nmodels. In this study, we show that a simple image inversion technique can\nsignificantly improve the segmentation quality of CT segmentation models on MRI\ndata, by using the TotalSegmentator model, applied to T1-weighted MRI images,\nas example. Image inversion is straightforward to implement and does not\nrequire dedicated graphics processing units (GPUs), thus providing a quick\nalternative to complex deep modality-transfer models for generating\nsegmentation masks for MRI data.\n', '  Cardiac magnetic resonance imaging (MRI) has emerged as a clinically\ngold-standard technique for diagnosing cardiac diseases, thanks to its ability\nto provide diverse information with multiple modalities and anatomical views.\nAccelerated cardiac MRI is highly expected to achieve time-efficient and\npatient-friendly imaging, and then advanced image reconstruction approaches are\nrequired to recover high-quality, clinically interpretable images from\nundersampled measurements. However, the lack of publicly available cardiac MRI\nk-space dataset in terms of both quantity and diversity has severely hindered\nsubstantial technological progress, particularly for data-driven artificial\nintelligence. Here, we provide a standardized, diverse, and high-quality\nCMRxRecon2024 dataset to facilitate the technical development, fair evaluation,\nand clinical transfer of cardiac MRI reconstruction approaches, towards\npromoting the universal frameworks that enable fast and robust reconstructions\nacross different cardiac MRI protocols in clinical practice. To the best of our\nknowledge, the CMRxRecon2024 dataset is the largest and most diverse publicly\navailable cardiac k-space dataset. It is acquired from 330 healthy volunteers,\ncovering commonly used modalities, anatomical views, and acquisition\ntrajectories in clinical cardiac MRI workflows. Besides, an open platform with\ntutorials, benchmarks, and data processing tools is provided to facilitate data\nusage, advanced method development, and fair performance evaluation.\n', ""  Brain disorders are a major challenge to global health, causing millions of\ndeaths each year. Accurate diagnosis of these diseases relies heavily on\nadvanced medical imaging techniques such as Magnetic Resonance Imaging (MRI)\nand Computed Tomography (CT). However, the scarcity of annotated data poses a\nsignificant challenge in deploying machine learning models for medical\ndiagnosis. To address this limitation, deep learning techniques have shown\nconsiderable promise. Domain adaptation techniques enhance a model's ability to\ngeneralize across imaging modalities by transferring knowledge from one domain\n(e.g., CT images) to another (e.g., MRI images). Such cross-modality adaptation\nis essential to improve the ability of models to consistently generalize across\ndifferent imaging modalities. This study collected relevant resources from the\nKaggle website and employed the Maximum Mean Difference (MMD) method - a\npopular domain adaptation method - to reduce the differences between imaging\ndomains. By combining MMD with Convolutional Neural Networks (CNNs), the\naccuracy and utility of the model is obviously enhanced. The excellent\nexperimental results highlight the great potential of data-driven domain\nadaptation techniques to improve diagnostic accuracy and efficiency, especially\nin resource-limited environments. By bridging the gap between different imaging\nmodalities, the study aims to provide clinicians with more reliable diagnostic\ntools.\n""] , ['  Diffusion probabilistic models (DPMs) have exhibited significant\neffectiveness in computer vision tasks, particularly in image generation.\nHowever, their notable performance heavily relies on labelled datasets, which\nlimits their application in medical images due to the associated high-cost\nannotations. Current DPM-related methods for lesion detection in medical\nimaging, which can be categorized into two distinct approaches, primarily rely\non image-level annotations. The first approach, based on anomaly detection,\ninvolves learning reference healthy brain representations and identifying\nanomalies based on the difference in inference results. In contrast, the second\napproach, resembling a segmentation task, employs only the original brain\nmulti-modalities as prior information for generating pixel-level annotations.\nIn this paper, our proposed model - discrepancy distribution medical diffusion\n(DDMD) - for lesion detection in brain MRI introduces a novel framework by\nincorporating distinctive discrepancy features, deviating from the conventional\ndirect reliance on image-level annotations or the original brain modalities. In\nour method, the inconsistency in image-level annotations is translated into\ndistribution discrepancies among heterogeneous samples while preserving\ninformation within homogeneous samples. This property retains pixel-wise\nuncertainty and facilitates an implicit ensemble of segmentation, ultimately\nenhancing the overall detection performance. Thorough experiments conducted on\nthe BRATS2020 benchmark dataset containing multimodal MRI scans for brain\ntumour detection demonstrate the great performance of our approach in\ncomparison to state-of-the-art methods.\n', '  Understanding the morphological structure of medical images and precisely\nsegmenting the region of interest or abnormality is an important task that can\nassist in diagnosis. However, the unique properties of medical imaging make\nclear segmentation difficult, and the high cost and time-consuming task of\nlabeling leads to a coarse-grained representation of ground truth. Facing with\nthese problems, we propose a novel Diffusion Transformer Segmentation (DTS)\nmodel for robust segmentation in the presence of noise. We propose an\nalternative to the dominant Denoising U-Net encoder through experiments\napplying a transformer architecture, which captures global dependency through\nself-attention. Additionally, we propose k-neighbor label smoothing, reverse\nboundary attention, and self-supervised learning with morphology-driven\nlearning to improve the ability to identify complex structures. Our model,\nwhich analyzes the morphological representation of images, shows better results\nthan the previous models in various medical imaging modalities, including CT,\nMRI, and lesion images.\n', '  Image segmentation, the process of partitioning an image into meaningful\nregions, plays a pivotal role in computer vision and medical imaging\napplications. Unsupervised segmentation, particularly in the absence of labeled\ndata, remains a challenging task due to the inter-class similarity and\nvariations in intensity and resolution. In this study, we extract high-level\nfeatures of the input image using pretrained vision transformer. Subsequently,\nthe proposed method leverages the underlying graph structures of the images,\nseeking to discover and delineate meaningful boundaries using graph neural\nnetworks and modularity based optimization criteria without relying on\npre-labeled training data. Experimental results on benchmark datasets\ndemonstrate the effectiveness and versatility of the proposed approach,\nshowcasing competitive performance compared to the state-of-the-art\nunsupervised segmentation methods. This research contributes to the broader\nfield of unsupervised medical imaging and computer vision by presenting an\ninnovative methodology for image segmentation that aligns with real-world\nchallenges. The proposed method holds promise for diverse applications,\nincluding medical imaging, remote sensing, and object recognition, where\nlabeled data may be scarce or unavailable. The github repository of the code is\navailable on [https://github.com/ksgr5566/unseggnet]\n']",Medical Image Analysis,Medical Imaging Segmentation and Analysis
153,Radiology Report Generation and Evaluation,"['radiology', 'radiological', 'radiologists', 'radiologist', 'radiologic', 'multimodal', 'textual', 'reporting', 'medical', 'diagnostic']","[""  GPT-4V's purported strong multimodal abilities raise interests in using it to\nautomate radiology report writing, but there lacks thorough evaluations. In\nthis work, we perform a systematic evaluation of GPT-4V in generating radiology\nreports on two chest X-ray report datasets: MIMIC-CXR and IU X-Ray. We attempt\nto directly generate reports using GPT-4V through different prompting\nstrategies and find that it fails terribly in both lexical metrics and clinical\nefficacy metrics. To understand the low performance, we decompose the task into\ntwo steps: 1) the medical image reasoning step of predicting medical condition\nlabels from images; and 2) the report synthesis step of generating reports from\n(groundtruth) conditions. We show that GPT-4V's performance in image reasoning\nis consistently low across different prompts. In fact, the distributions of\nmodel-predicted labels remain constant regardless of which groundtruth\nconditions are present on the image, suggesting that the model is not\ninterpreting chest X-rays meaningfully. Even when given groundtruth conditions\nin report synthesis, its generated reports are less correct and less\nnatural-sounding than a finetuned LLaMA-2. Altogether, our findings cast doubt\non the viability of using GPT-4V in a radiology workflow.\n"", ""  Evaluating generated radiology reports is crucial for the development of\nradiology AI, but existing metrics fail to reflect the task's clinical\nrequirements. This study proposes a novel evaluation framework using large\nlanguage models (LLMs) to compare radiology reports for assessment. We compare\nthe performance of various LLMs and demonstrate that, when using GPT-4, our\nproposed metric achieves evaluation consistency close to that of radiologists.\nFurthermore, to reduce costs and improve accessibility, making this method\npractical, we construct a dataset using LLM evaluation results and perform\nknowledge distillation to train a smaller model. The distilled model achieves\nevaluation capabilities comparable to GPT-4. Our framework and distilled model\noffer an accessible and efficient evaluation method for radiology report\ngeneration, facilitating the development of more clinically relevant models.\nThe model will be further open-sourced and accessible.\n"", '  The impression section of a radiology report summarizes important radiology\nfindings and plays a critical role in communicating these findings to\nphysicians. However, the preparation of these summaries is time-consuming and\nerror-prone for radiologists. Recently, numerous models for radiology report\nsummarization have been developed. Nevertheless, there is currently no model\nthat can summarize these reports in multiple languages. Such a model could\ngreatly improve future research and the development of Deep Learning models\nthat incorporate data from patients with different ethnic backgrounds. In this\nstudy, the generation of radiology impressions in different languages was\nautomated by fine-tuning a model, publicly available, based on a multilingual\ntext-to-text Transformer to summarize findings available in English,\nPortuguese, and German radiology reports. In a blind test, two board-certified\nradiologists indicated that for at least 70% of the system-generated summaries,\nthe quality matched or exceeded the corresponding human-written summaries,\nsuggesting substantial clinical reliability. Furthermore, this study showed\nthat the multilingual model outperformed other models that specialized in\nsummarizing radiology reports in only one language, as well as models that were\nnot specifically designed for summarizing radiology reports, such as ChatGPT.\n']",Radiology Report Generation and Evaluation,Radiology Report Generation and Evaluation
154,Retinal Imaging and Ophthalmology Analysis,"['retina', 'retinal', 'macular', 'retinopathy', 'ophthalmologists', 'ophthalmology', 'ocular', 'blindness', 'glaucoma', 'supervised']","['  Retinal fundus images play a crucial role in the early detection of eye\ndiseases and, using deep learning approaches, recent studies have even\ndemonstrated their potential for detecting cardiovascular risk factors and\nneurological disorders. However, the impact of technical factors on these\nimages can pose challenges for reliable AI applications in ophthalmology. For\nexample, large fundus cohorts are often confounded by factors like camera type,\nimage quality or illumination level, bearing the risk of learning shortcuts\nrather than the causal relationships behind the image generation process. Here,\nwe introduce a novel population model for retinal fundus images that\neffectively disentangles patient attributes from camera effects, thus enabling\ncontrollable and highly realistic image generation. To achieve this, we propose\na novel disentanglement loss based on distance correlation. Through qualitative\nand quantitative analyses, we demonstrate the effectiveness of this novel loss\nfunction in disentangling the learned subspaces. Our results show that our\nmodel provides a new perspective on the complex relationship between patient\nattributes and technical confounders in retinal fundus image generation.\n', '  Optical coherence tomography (OCT) is a non-invasive imaging technique with\nextensive clinical applications in ophthalmology. OCT enables the visualization\nof the retinal layers, playing a vital role in the early detection and\nmonitoring of retinal diseases. OCT uses the principle of light wave\ninterference to create detailed images of the retinal microstructures, making\nit a valuable tool for diagnosing ocular conditions. This work presents an\nopen-access OCT dataset (OCTDL) comprising over 2000 OCT images labeled\naccording to disease group and retinal pathology. The dataset consists of OCT\nrecords of patients with Age-related Macular Degeneration (AMD), Diabetic\nMacular Edema (DME), Epiretinal Membrane (ERM), Retinal Artery Occlusion (RAO),\nRetinal Vein Occlusion (RVO), and Vitreomacular Interface Disease (VID). The\nimages were acquired with an Optovue Avanti RTVue XR using raster scanning\nprotocols with dynamic scan length and image resolution. Each retinal b-scan\nwas acquired by centering on the fovea and interpreted and cataloged by an\nexperienced retinal specialist. In this work, we applied Deep Learning\nclassification techniques to this new open-access dataset.\n', '  Retinal optical coherence tomography (OCT) images provide crucial insights\ninto the health of the posterior ocular segment. Therefore, the advancement of\nautomated image analysis methods is imperative to equip clinicians and\nresearchers with quantitative data, thereby facilitating informed\ndecision-making. The application of deep learning (DL)-based approaches has\ngained extensive traction for executing these analysis tasks, demonstrating\nremarkable performance compared to labor-intensive manual analyses. However,\nthe acquisition of Retinal OCT images often presents challenges stemming from\nprivacy concerns and the resource-intensive labeling procedures, which\ncontradicts the prevailing notion that DL models necessitate substantial data\nvolumes for achieving superior performance. Moreover, limitations in available\ncomputational resources constrain the progress of high-performance medical\nartificial intelligence, particularly in less developed regions and countries.\nThis paper introduces a novel ensemble learning mechanism designed for\nrecognizing retinal diseases under limited resources (e.g., data, computation).\nThe mechanism leverages insights from multiple pre-trained models, facilitating\nthe transfer and adaptation of their knowledge to Retinal OCT images. This\napproach establishes a robust model even when confronted with limited labeled\ndata, eliminating the need for an extensive array of parameters, as required in\nlearning from scratch. Comprehensive experimentation on real-world datasets\ndemonstrates that the proposed approach can achieve superior performance in\nrecognizing Retinal OCT images, even when dealing with exceedingly restricted\nlabeled datasets. Furthermore, this method obviates the necessity of learning\nextensive-scale parameters, making it well-suited for deployment in\nlow-resource scenarios.\n']",Retinal Imaging and Analysis in Ophthalmology,Retinal Imaging and Ophthalmology Analysis
155,"""Optical Imaging and Spectroscopy for Nanoscale Analysis"" , ""Photonic Computing and Optical Information Processing""","['raman', 'spectroscopy', 'microscopy', 'nanoscale', 'microscope', 'spectroscopic', 'nanostructures', 'optical', 'optics', 'imaging'] , ['photonics', 'photonic', 'optical', 'nanophotonic', 'optics', 'multiplexing', 'laser', 'wavelength', 'waveguide', 'throughput']","['  Phase retrieval, the problem of recovering lost phase information from\nmeasured intensity alone, is an inverse problem that is widely faced in various\nimaging modalities ranging from astronomy to nanoscale imaging. The current\nprocess of phase recovery is iterative in nature. As a result, the image\nformation is time-consuming and computationally expensive, precluding real-time\nimaging. Here, we use 3D nanoscale X-ray imaging as a representative example to\ndevelop a deep learning model to address this phase retrieval problem. We\nintroduce 3D-CDI-NN, a deep convolutional neural network and differential\nprogramming framework trained to predict 3D structure and strain solely from\ninput 3D X-ray coherent scattering data. Our networks are designed to be\n""physics-aware"" in multiple aspects; in that the physics of x-ray scattering\nprocess is explicitly enforced in the training of the network, and the training\ndata are drawn from atomistic simulations that are representative of the\nphysics of the material. We further refine the neural network prediction\nthrough a physics-based optimization procedure to enable maximum accuracy at\nlowest computational cost. 3D-CDI-NN can invert a 3D coherent diffraction\npattern to real-space structure and strain hundreds of times faster than\ntraditional iterative phase retrieval methods, with negligible loss in\naccuracy. Our integrated machine learning and differential programming solution\nto the phase retrieval problem is broadly applicable across inverse problems in\nother application areas.\n', '  Raman spectroscopy, a photonic modality based on the inelastic backscattering\nof coherent light, is a valuable asset to the intraoperative sensing space,\noffering non-ionizing potential and highly-specific molecular fingerprint-like\nspectroscopic signatures that can be used for diagnosis of pathological tissue\nin the dynamic surgical field. Though Raman suffers from weakness in intensity,\nSurface-Enhanced Raman Spectroscopy (SERS), which uses metal nanostructures to\namplify Raman signals, can achieve detection sensitivities that rival\ntraditional photonic modalities. In this study, we outline a robotic Raman\nsystem that can reliably pinpoint the location and boundaries of a tumor\nembedded in healthy tissue, modeled here as a tissue-mimicking phantom with\nselectively infused Gold Nanostar regions. Further, due to the relative dearth\nof collected biological SERS or Raman data, we implement transfer learning to\nachieve 100% validation classification accuracy for Gold Nanostars compared to\nControl Agarose, thus providing a proof-of-concept for Raman-based deep\nlearning training pipelines. We reconstruct a surgical field of 30x60mm in 10.2\nminutes, and achieve 98.2% accuracy, preserving relative measurements between\nfeatures in the phantom. We also achieve an 84.3% Intersection-over-Union\nscore, which is the extent of overlap between the ground truth and predicted\nreconstructions. Lastly, we also demonstrate that the Raman system and\nclassification algorithm do not discern based on sample color, but instead on\npresence of SERS agents. This study provides a crucial step in the translation\nof intelligent Raman systems in intraoperative oncological spaces.\n', '  Accurate detection and analysis of traces of persistent organic pollutants in\nwater is important in many areas, including environmental monitoring and food\nquality control, due to their long environmental stability and potential\nbioaccumulation. While conventional analysis of organic pollutants requires\nexpensive equipment, surface enhanced Raman spectroscopy (SERS) has\ndemonstrated great potential for accurate detection of these contaminants.\nHowever, SERS analytical difficulties, such as spectral preprocessing,\ndenoising, and substrate-based spectral variation, have hindered widespread use\nof the technique. Here, we demonstrate an approach for predicting the\nconcentration of sample pollutants from messy, unprocessed Raman data using\nmachine learning. Frequency domain transform methods, including the Fourier and\nWalsh Hadamard transforms, are applied to sets of Raman spectra of three model\nmicropollutants in water (rhodamine 6G, chlorpyrifos, and triclosan), which are\nthen used to train machine learning algorithms. Using standard machine learning\nmodels, the concentration of sample pollutants are predicted with more than 80\npercent cross-validation accuracy from raw Raman data. cross-validation\naccuracy of 85 percent was achieved using deep learning for a moderately sized\ndataset (100 spectra), and 70 to 80 percent cross-validation accuracy was\nachieved even for very small datasets (50 spectra). Additionally, standard\nmodels were shown to accurately identify characteristic peaks via analysis of\ntheir importance scores. The approach shown here has the potential to be\napplied to facilitate accurate detection and analysis of persistent organic\npollutants by surface-enhanced Raman spectroscopy.\n'] , ['  Among the promising advantages of photonic computing over conventional\ncomputing architectures is the potential to increase computing efficiency\nthrough massive parallelism by using the many degrees of freedom provided by\nphotonics. Here, we numerically demonstrate the simultaneous use of time and\nfrequency (equivalently wavelength) multiplexing to solve three independent\ntasks at the same time on the same photonic circuit. In particular, we consider\na microring-based time-delay reservoir computing (TDRC) scheme that\nsimultaneously solves three tasks: Time-series prediction, classification, and\nwireless channel equalization. The scheme relies on time-division multiplexing\nto avoid the necessity of multiple physical nonlinear nodes, while the tasks\nare parallelized using wavelength division multiplexing (WDM). The input data\nmodulated on each optical channel is mapped to a higher dimensional space by\nthe nonlinear dynamics of the silicon microring cavity. The carrier wavelength\nand input power assigned to each optical channel have a high influence on the\nperformance of its respective task. When all tasks operate under the same\nwavelength/power conditions, our results show that the computing nature of each\ntask is the deciding factor of the level of performance achievable. However, it\nis possible to achieve good performance for all tasks simultaneously by\noptimizing the parameters of each optical channel. The variety of applications\ncovered by the tasks shows the versatility of the proposed photonic TDRC\nscheme. Overall, this work provides insight into the potential of WDM-based\nschemes for improving the computing capabilities of reservoir computing\nschemes.\n', '  Solving partial differential equations (PDEs) numerically often requires huge\ncomputing time, energy cost, and hardware resources in practical applications.\nThis has limited their applications in many scenarios (e.g., autonomous\nsystems, supersonic flows) that have a limited energy budget and require near\nreal-time response. Leveraging optical computing, this paper develops an\non-chip training framework for physics-informed neural networks (PINNs), aiming\nto solve high-dimensional PDEs with fJ/MAC photonic power consumption and\nultra-low latency. Despite the ultra-high speed of optical neural networks,\ntraining a PINN on an optical chip is hard due to (1) the large size of\nphotonic devices, and (2) the lack of scalable optical memory devices to store\nthe intermediate results of back-propagation (BP). To enable realistic optical\nPINN training, this paper presents a scalable method to avoid the BP process.\nWe also employ a tensor-compressed approach to improve the convergence and\nscalability of our optical PINN training. This training framework is designed\nwith tensorized optical neural networks (TONN) for scalable inference\nacceleration and MZI phase-domain tuning for \\textit{in-situ} optimization. Our\nsimulation results of a 20-dim HJB PDE show that our photonic accelerator can\nreduce the number of MZIs by a factor of $1.17\\times 10^3$, with only $1.36$ J\nand $1.15$ s to solve this equation. This is the first real-size optical PINN\ntraining framework that can be applied to solve high-dimensional PDEs.\n', '  Subwavelength photonic structures and metamaterials provide revolutionary\napproaches for controlling light. The inverse design methods proposed for these\nsubwavelength structures are vital to the development of new photonic devices.\nHowever, most of the existing inverse design methods cannot realize direct\nmapping from optical properties to photonic structures but instead rely on\nforward simulation methods to perform iterative optimization. In this work, we\nexploit the powerful generative abilities of artificial intelligence (AI) and\npropose a practical inverse design method based on latent diffusion models. Our\nmethod maps directly the optical properties to structures without the\nrequirement of forward simulation and iterative optimization. Here, the given\noptical properties can work as ""prompts"" and guide the constructed model to\ncorrectly ""draw"" the required photonic structures. Experiments show that our\ndirect mapping-based inverse design method can generate subwavelength photonic\nstructures at high fidelity while following the given optical properties. This\nmay change the method used for optical design and greatly accelerate the\nresearch on new photonic devices.\n']","Optical and Photonic Technologies for Imaging, Sensing, and Computing","""Optical Imaging and Spectroscopy for Nanoscale Analysis"""
156,"""Flow-Guided Localization in Bloodstreams using Nanodevices""","['bloodstream', 'nanodevice', 'nanodevices', 'microvascular', 'bloodstreams', 'macrovascular', 'flow', 'nanotechnology', 'arterial', 'vivo']","[""  Flow-guided localization using in-body nanodevices in the bloodstream is\nexpected to be beneficial for early disease detection, continuous monitoring of\nbiological conditions, and targeted treatment. The nanodevices face size and\npower constraints that produce erroneous raw data for localization purposes.\nOn-body anchors receive this data, and use it to derive the locations of\ndiagnostic events of interest. Different Machine Learning (ML) approaches have\nbeen recently proposed for this task, yet they are currently restricted to a\nreference bloodstream of a resting patient. As such, they are unable to deal\nwith the physical diversity of patients' bloodstreams and cannot provide\ncontinuous monitoring due to changes in individual patient's activities. Toward\naddressing these issues for the current State-of-the-Art (SotA) flow-guided\nlocalization approach based on Graph Neural Networks (GNNs), we propose a\npipeline for GNN adaptation based on individual physiological indicators\nincluding height, weight, and heart rate. Our results indicate that the\nproposed adaptions are beneficial in reconciling the individual differences\nbetween bloodstreams and activities.\n"", ""  Advancements in nanotechnology and material science are paving the way toward\nnanoscale devices that combine sensing, computing, data and energy storage, and\nwireless communication. In precision medicine, these nanodevices show promise\nfor disease diagnostics, treatment, and monitoring from within the patients'\nbloodstreams. Assigning the location of a sensed biological event with the\nevent itself, which is the main proposition of flow-guided in-body nanoscale\nlocalization, would be immensely beneficial from the perspective of precision\nmedicine. The nanoscale nature of the nanodevices and the challenging\nenvironment that the bloodstream represents, result in current flow-guided\nlocalization approaches being constrained in their communication and\nenergy-related capabilities. The communication and energy constraints of the\nnanodevices result in different features of raw data for flow-guided\nlocalization, in turn affecting its performance. An analytical modeling of the\neffects of imperfect communication and constrained energy causing intermittent\noperation of the nanodevices on the raw data produced by the nanodevices would\nbe beneficial. Hence, we propose an analytical model of raw data for\nflow-guided localization, where the raw data is modeled as a function of\ncommunication and energy-related capabilities of the nanodevice. We evaluate\nthe model by comparing its output with the one obtained through the utilization\nof a simulator for objective evaluation of flow-guided localization, featuring\ncomparably higher level of realism. Our results across a number of scenarios\nand heterogeneous performance metrics indicate high similarity between the\nmodel and simulator-generated raw datasets.\n"", ""  Contemporary research advances in nanotechnology and material science are\nrooted in the emergence of nanodevices as a versatile tool that harmonizes\nsensing, computing, wireless communication, data storage, and energy\nharvesting. These devices offer novel pathways for disease diagnostics,\ntreatment, and monitoring within the bloodstreams. Ensuring precise\nlocalization of events of diagnostic interest, which underpins the concept of\nflow-guided in-body nanoscale localization, would provide an added diagnostic\nvalue to the detected events. Raw data generated by the nanodevices is pivotal\nfor this localization and consist of an event detection indicator and the time\nelapsed since the last passage of a nanodevice through the heart. The energy\nconstraints of the nanodevices lead to intermittent operation and unreliable\ncommunication, intrinsically affecting this data. This posits a need for\ncomprehensively modelling the features of this data. These imperfections also\nhave profound implications for the viability of existing flow-guided\nlocalization approaches, which are ill-prepared to address the intricacies of\nthe environment. Our first contribution lies in an analytical model of raw data\nfor flow-guided localization, dissecting how communication and energy\ncapabilities influence the nanodevices' data output. This model acts as a vital\nbridge, reconciling idealized assumptions with practical challenges of\nflow-guided localization. Toward addressing these practical challenges, we also\npresent an integration of Graph Neural Networks (GNNs) into the flow-guided\nlocalization paradigm. GNNs excel in capturing complex dynamic interactions\ninherent to the localization of events sensed by the nanodevices. Our results\nhighlight the potential of GNNs not only to enhance localization accuracy but\nalso extend coverage to encompass the entire bloodstream.\n""]",Flow-Guided Localization of Nanodevices in Bloodstreams,"""Flow-Guided Localization in Bloodstreams using Nanodevices"""
157,"Multiscale Elasticity and Microstructure Modeling , ""Metamaterial Design and CAD Optimization""","['elasticity', 'deformation', 'elastic', 'microstructures', 'neural', 'multiscale', 'microstructure', 'mechanical', 'microscale', 'mechanics'] , ['cad', 'metamaterial', 'metamaterials', 'generative', 'meshing', 'mesh', 'designs', 'neural', 'designing', 'shapes']","['  Multiscale partial differential equations (PDEs) arise in various\napplications, and several schemes have been developed to solve them\nefficiently. Homogenization theory is a powerful methodology that eliminates\nthe small-scale dependence, resulting in simplified equations that are\ncomputationally tractable while accurately predicting the macroscopic response.\nIn the field of continuum mechanics, homogenization is crucial for deriving\nconstitutive laws that incorporate microscale physics in order to formulate\nbalance laws for the macroscopic quantities of interest. However, obtaining\nhomogenized constitutive laws is often challenging as they do not in general\nhave an analytic form and can exhibit phenomena not present on the microscale.\nIn response, data-driven learning of the constitutive law has been proposed as\nappropriate for this task. However, a major challenge in data-driven learning\napproaches for this problem has remained unexplored: the impact of\ndiscontinuities and corner interfaces in the underlying material. These\ndiscontinuities in the coefficients affect the smoothness of the solutions of\nthe underlying equations. Given the prevalence of discontinuous materials in\ncontinuum mechanics applications, it is important to address the challenge of\nlearning in this context; in particular, to develop underpinning theory that\nestablishes the reliability of data-driven methods in this scientific domain.\nThe paper addresses this unexplored challenge by investigating the learnability\nof homogenized constitutive laws for elliptic operators in the presence of such\ncomplexities. Approximation theory is presented, and numerical experiments are\nperformed which validate the theory in the context of learning the solution\noperator defined by the cell problem arising in homogenization for elliptic\nPDEs.\n', '  Accurately modeling the mechanical behavior of materials is crucial for\nnumerous engineering applications. The quality of these models depends directly\non the accuracy of the constitutive law that defines the stress-strain\nrelation. Discovering these constitutive material laws remains a significant\nchallenge, in particular when only material deformation data is available. To\naddress this challenge, unsupervised machine learning methods have been\nproposed. However, existing approaches have several limitations: they either\nfail to ensure that the learned constitutive relations are consistent with\nphysical principles, or they rely on a predefined library of constitutive\nrelations or manually crafted input features. These dependencies require\nsignificant expertise and specialized domain knowledge. Here, we introduce a\nmachine learning approach called uLED, which overcomes the limitations by using\nthe input convex neural network (ICNN) as the surrogate constitutive model. We\nimprove the optimization strategy for training ICNN, allowing it to be trained\nend-to-end using direct strain invariants as input across various materials.\nFurthermore, we utilize the nodal force equilibrium at the internal domain as\nthe training objective, which enables us to learn the constitutive relation\nsolely from temporal displacement recordings. We validate the effectiveness of\nthe proposed method on a diverse range of material laws. We demonstrate that it\nis robust to a significant level of noise and that it converges to the ground\ntruth with increasing data resolution. We also show that the model can be\neffectively trained using a displacement field from a subdomain of the test\nspecimen and that the learned constitutive relation from one material sample is\ntransferable to other samples with different geometries. The developed\nmethodology provides an effective tool for discovering constitutive relations.\n', '  Identifying constitutive parameters in engineering and biological materials,\nparticularly those with intricate geometries and mechanical behaviors, remains\na longstanding challenge. The recent advent of Physics-Informed Neural Networks\n(PINNs) offers promising solutions, but current frameworks are often limited to\nbasic constitutive laws and encounter practical constraints when combined with\nexperimental data. In this paper, we introduce a robust PINN-based framework\ndesigned to identify material parameters for soft materials, specifically those\nexhibiting complex constitutive behaviors, under large deformation in plane\nstress conditions. Distinctively, our model emphasizes training PINNs with\nmulti-modal synthetic experimental datasets consisting of full-field\ndeformation and loading history, ensuring algorithm robustness even with noisy\ndata. Our results reveal that the PINNs framework can accurately identify\nconstitutive parameters of the incompressible Arruda-Boyce model for samples\nwith intricate geometries, maintaining an error below 5%, even with an\nexperimental noise level of 5%. We believe our framework provides a robust\nmodulus identification approach for complex solids, especially for those with\ngeometrical and constitutive complexity.\n'] , ['  Mechanical metamaterials represent an innovative class of artificial\nstructures, distinguished by their extraordinary mechanical characteristics,\nwhich are beyond the scope of traditional natural materials. The use of deep\ngenerative models has become increasingly popular in the design of metamaterial\nunits. The effectiveness of using deep generative models lies in their capacity\nto compress complex input data into a simplified, lower-dimensional latent\nspace, while also enabling the creation of novel optimal designs through\nsampling within this space. However, the design process does not take into\naccount the effect of model uncertainty due to data sparsity or the effect of\ninput data uncertainty due to inherent randomness in the data. This might lead\nto the generation of undesirable structures with high sensitivity to the\nuncertainties in the system. To address this issue, a novel uncertainty-aware\ndeep learning framework-based robust design approach is proposed for the design\nof metamaterial units with optimal target properties. The proposed approach\nutilizes the probabilistic nature of the deep learning framework and quantifies\nboth aleatoric and epistemic uncertainties associated with surrogate-based\ndesign optimization. We demonstrate that the proposed design approach is\ncapable of designing high-performance metamaterial units with high reliability.\nTo showcase the effectiveness of the proposed design approach, a\nsingle-objective design optimization problem and a multi-objective design\noptimization problem are presented. The optimal robust designs obtained are\nvalidated by comparing them to the designs obtained from the topology\noptimization method as well as the designs obtained from a deterministic deep\nlearning framework-based design optimization where none of the uncertainties in\nthe system are explicitly considered.\n', '  Geometric Deep Learning techniques have become a transformative force in the\nfield of Computer-Aided Design (CAD), and have the potential to revolutionize\nhow designers and engineers approach and enhance the design process. By\nharnessing the power of machine learning-based methods, CAD designers can\noptimize their workflows, save time and effort while making better informed\ndecisions, and create designs that are both innovative and practical. The\nability to process the CAD designs represented by geometric data and to analyze\ntheir encoded features enables the identification of similarities among diverse\nCAD models, the proposition of alternative designs and enhancements, and even\nthe generation of novel design alternatives. This survey offers a comprehensive\noverview of learning-based methods in computer-aided design across various\ncategories, including similarity analysis and retrieval, 2D and 3D CAD model\nsynthesis, and CAD generation from point clouds. Additionally, it provides a\ncomplete list of benchmark datasets and their characteristics, along with\nopen-source codes that have propelled research in this domain. The final\ndiscussion delves into the challenges prevalent in this field, followed by\npotential future research directions in this rapidly evolving field.\n', ""  CAD (Computer-Aided Design) plays a crucial role in mechanical industry,\nwhere large numbers of similar-shaped CAD parts are often created. Efficiently\nreusing these parts is key to reducing design and production costs for\nenterprises. Retrieval systems are vital for achieving CAD reuse, but the\ncomplex shapes of CAD models are difficult to accurately describe using text or\nkeywords, making traditional retrieval methods ineffective. While existing\nrepresentation learning approaches have been developed for CAD, manually\nlabeling similar samples in these methods is expensive. Additionally, CAD\nmodels' unique parameterized data structure presents challenges for applying\nexisting 3D shape representation learning techniques directly. In this work, we\npropose GC-CAD, a self-supervised contrastive graph neural network-based method\nfor mechanical CAD retrieval that directly models parameterized CAD raw files.\nGC-CAD consists of two key modules: structure-aware representation learning and\ncontrastive graph learning framework. The method leverages graph neural\nnetworks to extract both geometric and topological information from CAD models,\ngenerating feature representations. We then introduce a simple yet effective\ncontrastive graph learning framework approach, enabling the model to train\nwithout manual labels and generate retrieval-ready representations.\nExperimental results on four datasets including human evaluation demonstrate\nthat the proposed method achieves significant accuracy improvements and up to\n100 times efficiency improvement over the baseline methods.\n""]",Materials Modeling and Design Optimization,Multiscale Elasticity and Microstructure Modeling
158,Additive Manufacturing and Laser-Based Fabrication,"['machining', 'fabrication', 'manufacturing', 'welding', 'laser', 'machined', 'cad', 'fusion', 'porosity', 'machine']","['  Additive manufacturing, especially laser powder bed fusion (L-PBF), is widely\nused for fabricating metal parts with intricate geometries. However, parts\nproduced via L-PBF suffer from varied surface roughness which affects the\ndynamic or fatigue properties. Accurate prediction of fatigue properties as a\nfunction of surface roughness is a critical requirement for qualifying L-PBF\nparts. In this work, an analytical methodology is put forth to predict the\nfatigue life of L-PBF components having heterogeneous surface roughness.\nThirty-six Hastelloy X specimens are printed using L-PBF followed by\nindustry-standard heat treatment procedures. Half of these specimens are built\nwith as-printed gauge sections and the other half is printed as cylinders from\nwhich fatigue specimens are extracted via machining. Specimens are printed in a\nvertical orientation and an orientation 30 degree from the vertical axis. The\nsurface roughness of the specimens is measured using computed tomography and\nparameters such as the maximum valley depth are used to build an extreme value\ndistribution. Fatigue testing is conducted at an isothermal condition of\n500-degree F. It is observed that the rough specimens fail much earlier\ncompared to the machined specimens due to the deep valleys present on the\nsurfaces of the former ones. The valleys act as notches leading to high strain\nlocalization. Following this observation, a functional relationship is\nformulated analytically that considers surface valleys as notches and\ncorrelates the strain localization around those notches with fatigue life,\nusing the Coffin-Manson-Basquin and Ramberg-Osgood equation. In conclusion, the\nproposed analytical model successfully predicts the fatigue life of L-PBF\nspecimens at an elevated temperature undergoing different strain loadings.\n', '  Metal additive manufacturing is gaining broad interest and increased use in\nthe industrial and academic fields. However, the quantification and\ncommercialization of standard parts usually require extensive experiments and\nexpensive post-characterization, which impedes the rapid development and\nadaptation of metal AM technologies. In this work, a similarity-based\nacceleration (S-acceleration) method for design of experiments is developed to\nreduce the time and costs associated with unveiling process-property (porosity\ndefects) relationships during manufacturing. With S-acceleration, part semantic\nfeatures from machine-setting parameters and physics-effects informed\ncharacteristics are explored for measuring mutual part similarities. A\nuser-defined simplification rate of experiments is proposed to purposely remove\nredundant parts before conducting experiments printing without sacrificing\ninformation gain as original full factorial experiment design. This\nS-acceleration design of experiments is demonstrated on a Concept Laser M2\nmachine for the experimental plan of modeling relationships between process\nparameters and part porosity defects. The printed part has 2 mm diameter by 4\nmm tall pin geometry considering variations in build location and orientation,\nlaser settings and powder feedstock are held constant. In total, 242 parts are\nmeasured to create a ground truth data set of porosity levels by using X-ray\ntomography microscopy. The S-acceleration method is assessed for performance\nconsidering 40%, 50%, and 60% of user-defined experiment simplification rates.\nThe repeated experiments are removed without ignoring the minority experiments\noutlier, assuring a similar process-property relation in the original\nexperiment plan. The experiment number is significantly reduced based on part\nsimilarity with minimal compromise of model accuracy and obtained knowledge.\n', '  A digital twin (DT), with the components of a physics-based model, a\ndata-driven model, and a machine learning (ML) enabled efficient surrogate,\nbehaves as a virtual twin of the real-world physical process. In terms of Laser\nPowder Bed Fusion (L-PBF) based additive manufacturing (AM), a DT can predict\nthe current and future states of the melt pool and the resulting defects\ncorresponding to the input laser parameters, evolve itself by assimilating\nin-situ sensor data, and optimize the laser parameters to mitigate defect\nformation. In this paper, we present a deep neural operator enabled\ncomputational framework of the DT for closed-loop feedback control of the L-PBF\nprocess. This is accomplished by building a high-fidelity computational model\nto accurately represent the melt pool states, an efficient surrogate model to\napproximate the melt pool solution field, followed by an physics-based\nprocedure to extract information from the computed melt pool simulation that\ncan further be correlated to the defect quantities of interest (e.g., surface\nroughness). In particular, we leverage the data generated from the\nhigh-fidelity physics-based model and train a series of Fourier neural operator\n(FNO) based ML models to effectively learn the relation between the input laser\nparameters and the corresponding full temperature field of the melt pool.\nSubsequently, a set of physics-informed variables such as the melt pool\ndimensions and the peak temperature can be extracted to compute the resulting\ndefects. An optimization algorithm is then exercised to control laser input and\nminimize defects. On the other hand, the constructed DT can also evolve with\nthe physical twin via offline finetuning and online material calibration.\nFinally, a probabilistic framework is adopted for uncertainty quantification.\nThe developed DT is envisioned to guide the AM process and facilitate\nhigh-quality manufacturing.\n']",Additive Manufacturing and Laser-Based Fabrication Techniques,Additive Manufacturing and Laser-Based Fabrication
159,"Efficient Modern Hopfield Models for Memory Retrieval , Deep Hashing for Image Retrieval","['hopfield', 'memory', 'retrieval', 'sparse', 'hop', '_softmax', 'efficient', 'sparsemap', 'storage', 'hop82'] , ['hashing', 'imagenet', 'hash', 'cnn', 'cnns', 'retrieval', 'convolutional', 'images', 'features', 'embedding']","['  We investigate the computational limits of the memory retrieval dynamics of\nmodern Hopfield models from the fine-grained complexity analysis. Our key\ncontribution is the characterization of a phase transition behavior in the\nefficiency of all possible modern Hopfield models based on the norm of\npatterns. Specifically, we establish an upper bound criterion for the norm of\ninput query patterns and memory patterns. Only below this criterion,\nsub-quadratic (efficient) variants of the modern Hopfield model exist, assuming\nthe Strong Exponential Time Hypothesis (SETH). To showcase our theory, we\nprovide a formal example of efficient constructions of modern Hopfield models\nusing low-rank approximation when the efficient criterion holds. This includes\na derivation of a lower bound on the computational time, scaling linearly with\n$\\max\\{$# of stored memory patterns, length of input query sequence$\\}$. In\naddition, we prove its memory retrieval error bound and exponential memory\ncapacity.\n', '  We propose a two-stage memory retrieval dynamics for modern Hopfield models,\ntermed $\\mathtt{U\\text{-}Hop}$, with enhanced memory capacity. Our key\ncontribution is a learnable feature map $\\Phi$ which transforms the Hopfield\nenergy function into kernel space. This transformation ensures convergence\nbetween the local minima of energy and the fixed points of retrieval dynamics\nwithin the kernel space. Consequently, the kernel norm induced by $\\Phi$ serves\nas a novel similarity measure. It utilizes the stored memory patterns as\nlearning data to enhance memory capacity across all modern Hopfield models.\nSpecifically, we accomplish this by constructing a separation loss\n$\\mathcal{L}_\\Phi$ that separates the local minima of kernelized energy by\nseparating stored memory patterns in kernel space. Methodologically,\n$\\mathtt{U\\text{-}Hop}$ memory retrieval process consists of: (Stage I)\nminimizing separation loss for a more uniform memory (local minimum)\ndistribution, followed by (Stage II) standard Hopfield energy minimization for\nmemory retrieval. This results in a significant reduction of possible\nmetastable states in the Hopfield energy function, thus enhancing memory\ncapacity by preventing memory confusion. Empirically, with real-world datasets,\nwe demonstrate that $\\mathtt{U\\text{-}Hop}$ outperforms all existing modern\nHopfield models and state-of-the-art similarity measures, achieving substantial\nimprovements in both associative memory retrieval and deep learning tasks. Code\nis available at https://github.com/MAGICS-LAB/UHop ; future updates are on\narXiv:2404.03827\n', '  We present a nonparametric construction for deep learning compatible modern\nHopfield models and utilize this framework to debut an efficient variant. Our\nkey contribution stems from interpreting the memory storage and retrieval\nprocesses in modern Hopfield models as a nonparametric regression problem\nsubject to a set of query-memory pairs. Crucially, our framework not only\nrecovers the known results from the original dense modern Hopfield model but\nalso fills the void in the literature regarding efficient modern Hopfield\nmodels, by introducing \\textit{sparse-structured} modern Hopfield models with\nsub-quadratic complexity. We establish that this sparse model inherits the\nappealing theoretical properties of its dense analogue -- connection with\ntransformer attention, fixed point convergence and exponential memory capacity\n-- even without knowing details of the Hopfield energy function. Additionally,\nwe showcase the versatility of our framework by constructing a family of modern\nHopfield models as extensions, including linear, random masked, top-$K$ and\npositive random feature modern Hopfield models. Empirically, we validate the\nefficacy of our framework in both synthetic and realistic settings.\n'] , ['  Deep hashing techniques have emerged as the predominant approach for\nefficient image retrieval. Traditionally, these methods utilize pre-trained\nconvolutional neural networks (CNNs) such as AlexNet and VGG-16 as feature\nextractors. However, the increasing complexity of datasets poses challenges for\nthese backbone architectures in capturing meaningful features essential for\neffective image retrieval. In this study, we explore the efficacy of employing\nhigh-resolution features learned through state-of-the-art techniques for image\nretrieval tasks. Specifically, we propose a novel methodology that utilizes\nHigh-Resolution Networks (HRNets) as the backbone for the deep hashing task,\ntermed High-Resolution Hashing Network (HHNet). Our approach demonstrates\nsuperior performance compared to existing methods across all tested benchmark\ndatasets, including CIFAR-10, NUS-WIDE, MS COCO, and ImageNet. This performance\nimprovement is more pronounced for complex datasets, which highlights the need\nto learn high-resolution features for intricate image retrieval tasks.\nFurthermore, we conduct a comprehensive analysis of different HRNet\nconfigurations and provide insights into the optimal architecture for the deep\nhashing task\n', ""  Previous knowledge distillation (KD) methods mostly focus on compressing\nnetwork architectures, which is not thorough enough in deployment as some costs\nlike transmission bandwidth and imaging equipment are related to the image\nsize. Therefore, we propose Pixel Distillation that extends knowledge\ndistillation into the input level while simultaneously breaking architecture\nconstraints. Such a scheme can achieve flexible cost control for deployment, as\nit allows the system to adjust both network architecture and image quality\naccording to the overall requirement of resources. Specifically, we first\npropose an input spatial representation distillation (ISRD) mechanism to\ntransfer spatial knowledge from large images to student's input module, which\ncan facilitate stable knowledge transfer between CNN and ViT. Then, a\nTeacher-Assistant-Student (TAS) framework is further established to disentangle\npixel distillation into the model compression stage and input compression\nstage, which significantly reduces the overall complexity of pixel distillation\nand the difficulty of distilling intermediate knowledge. Finally, we adapt\npixel distillation to object detection via an aligned feature for preservation\n(AFP) strategy for TAS, which aligns output dimensions of detectors at each\nstage by manipulating features and anchors of the assistant. Comprehensive\nexperiments on image classification and object detection demonstrate the\neffectiveness of our method. Code is available at\nhttps://github.com/gyguo/PixelDistillation.\n"", '  Unsupervised semantic hashing has emerged as an indispensable technique for\nfast image search, which aims to convert images into binary hash codes without\nrelying on labels. Recent advancements in the field demonstrate that employing\nlarge-scale backbones (e.g., ViT) in unsupervised semantic hashing models can\nyield substantial improvements. However, the inference delay has become\nincreasingly difficult to overlook. Knowledge distillation provides a means for\npractical model compression to alleviate this delay. Nevertheless, the\nprevailing knowledge distillation approaches are not explicitly designed for\nsemantic hashing. They ignore the unique search paradigm of semantic hashing,\nthe inherent necessities of the distillation process, and the property of hash\ncodes. In this paper, we propose an innovative Bit-mask Robust Contrastive\nknowledge Distillation (BRCD) method, specifically devised for the distillation\nof semantic hashing models. To ensure the effectiveness of two kinds of search\nparadigms in the context of semantic hashing, BRCD first aligns the semantic\nspaces between the teacher and student models through a contrastive knowledge\ndistillation objective. Additionally, to eliminate noisy augmentations and\nensure robust optimization, a cluster-based method within the knowledge\ndistillation process is introduced. Furthermore, through a bit-level analysis,\nwe uncover the presence of redundancy bits resulting from the bit independence\nproperty. To mitigate these effects, we introduce a bit mask mechanism in our\nknowledge distillation objective. Finally, extensive experiments not only\nshowcase the noteworthy performance of our BRCD method in comparison to other\nknowledge distillation methods but also substantiate the generality of our\nmethods across diverse semantic hashing models and backbones. The code for BRCD\nis available at https://github.com/hly1998/BRCD.\n']",Efficient Models for Information Retrieval,Efficient Modern Hopfield Models for Memory Retrieval
160,"Cloud Computing Resource Management and Optimization , Query Optimization in Database Management Systems , Information Retrieval and Query Optimization","['cloud', 'virtualization', 'scheduling', 'workloads', 'workload', 'microservices', 'microservice', 'supercomputing', 'service', 'aws'] , ['queries', 'dbmss', 'dbms', 'optimizers', 'databases', 'sql', 'optimizer', 'database', 'query', 'rdbms'] , ['retrieval', 'queries', 'sparql', 'search', 'annotations', 'retrievers', 'litsearch', 'semantic', 'retriever', 'query']","['  The paragraph is grammatically correct and logically coherent. It discusses\nthe importance of mobile terminal cloud computing migration technology in\nmeeting the demands of evolving computer and cloud computing technologies. It\nemphasizes the need for efficient data access and storage, as well as the\nutilization of cloud computing migration technology to prevent additional time\ndelays. The paragraph also highlights the contributions of cloud computing\nmigration technology to expanding cloud computing services. Additionally, it\nacknowledges the role of virtualization as a fundamental capability of cloud\ncomputing while emphasizing that cloud computing and virtualization are not\ninherently interconnected. Finally, it introduces machine learning-based\nvirtual machine migration optimization and dynamic resource allocation as a\ncritical research direction in cloud computing, citing the limitations of\nstatic rules or manual settings in traditional cloud computing environments.\nOverall, the paragraph effectively communicates the importance of machine\nlearning technology in addressing resource allocation and virtual machine\nmigration challenges in cloud computing.\n', '  With the continuous expansion of the scale of cloud computing applications,\nartificial intelligence technologies such as Deep Learning and Reinforcement\nLearning have gradually become the key tools to solve the automated task\nscheduling of large-scale cloud computing systems. Aiming at the complexity and\nreal-time requirement of task scheduling in large-scale cloud computing system,\nthis paper proposes an automatic task scheduling scheme based on deep learning\nand reinforcement learning. Firstly, the deep learning technology is used to\nmonitor and predict the parameters in the cloud computing system in real time\nto obtain the system status information. Then, combined with reinforcement\nlearning algorithm, the task scheduling strategy is dynamically adjusted\naccording to the real-time system state and task characteristics to achieve the\noptimal utilization of system resources and the maximum of task execution\nefficiency. This paper verifies the effectiveness and performance advantages of\nthe proposed scheme in experiments, and proves the potential and application\nprospect of deep learning and reinforcement learning in automatic task\nscheduling in large-scale cloud computing systems.\n', '  In recent years, cloud computing has been widely used. Cloud computing refers\nto the centralized computing resources, users through the access to the\ncentralized resources to complete the calculation, the cloud computing center\nwill return the results of the program processing to the user. Cloud computing\nis not only for individual users, but also for enterprise users. By purchasing\na cloud server, users do not have to buy a large number of computers, saving\ncomputing costs. According to a report by China Economic News Network, the\nscale of cloud computing in China has reached 209.1 billion yuan. At present,\nthe more mature cloud service providers in China are Ali Cloud, Baidu Cloud,\nHuawei Cloud and so on. Therefore, this paper proposes an innovative approach\nto solve complex problems in cloud computing resource scheduling and management\nusing machine learning optimization techniques. Through in-depth study of\nchallenges such as low resource utilization and unbalanced load in the cloud\nenvironment, this study proposes a comprehensive solution, including\noptimization methods such as deep learning and genetic algorithm, to improve\nsystem performance and efficiency, and thus bring new breakthroughs and\nprogress in the field of cloud computing resource management.Rational\nallocation of resources plays a crucial role in cloud computing. In the\nresource allocation of cloud computing, the cloud computing center has limited\ncloud resources, and users arrive in sequence. Each user requests the cloud\ncomputing center to use a certain number of cloud resources at a specific time.\n'] , ['  Query optimizers in relational database management systems (RDBMSs) search\nfor execution plans expected to be optimal for a given queries. They use\nparameter estimates, often inaccurate, and make assumptions that may not hold\nin practice. Consequently, they may select execution plans that are suboptimal\nat runtime, when these estimates and assumptions are not valid, which may\nresult in poor query performance. Therefore, query optimizers do not\nsufficiently support robust query optimization. Recent years have seen a surge\nof interest in using machine learning (ML) to improve efficiency of data\nsystems and reduce their maintenance overheads, with promising results obtained\nin the area of query optimization in particular. In this paper, inspired by\nthese advancements, and based on several years of experience of IBM Db2 in this\njourney, we propose Robust Optimization of Queries, (Roq), a holistic framework\nthat enables robust query optimization based on a risk-aware learning approach.\nRoq includes a novel formalization of the notion of robustness in the context\nof query optimization and a principled approach for its quantification and\nmeasurement based on approximate probabilistic ML. It also includes novel\nstrategies and algorithms for query plan evaluation and selection. Roq also\nincludes a novel learned cost model that is designed to predict query execution\ncost and the associated risks and performs query optimization accordingly. We\ndemonstrate experimentally that Roq provides significant improvements to robust\nquery optimization compared to the state-of-the-art.\n', ""  Query optimization in relational database management systems (DBMSs) is\ncritical for fast query processing. The query optimizer relies on precise\nselectivity and cost estimates to effectively optimize queries prior to\nexecution. While this strategy is effective for relational DBMSs, it is not\nsufficient for DBMSs tailored for processing machine learning (ML) queries. In\nML-centric DBMSs, query optimization is challenging for two reasons. First, the\nperformance bottleneck of the queries shifts to user-defined functions (UDFs)\nthat often wrap around deep learning models, making it difficult to accurately\nestimate UDF statistics without profiling the query. This leads to inaccurate\nstatistics and sub-optimal query plans. Second, the optimal query plan for ML\nqueries is data-dependent, necessitating DBMSs to adapt the query plan on the\nfly during execution. So, a static query plan is not sufficient for such\nqueries.\n  In this paper, we present Hydro, an ML-centric DBMS that utilizes adaptive\nquery processing (AQP) for efficiently processing ML queries. Hydro is designed\nto quickly evaluate UDF-based query predicates by ensuring optimal predicate\nevaluation order and improving the scalability of UDF execution. By integrating\nAQP, Hydro continuously monitors UDF statistics, routes data to predicates in\nan optimal order, and dynamically allocates resources for evaluating\npredicates. We demonstrate Hydro's efficacy through four illustrative use\ncases, delivering up to 11.52x speedup over a baseline system.\n"", '  Modern database systems rely on cost-based query optimizers to come up with\ngood execution plans for input queries. Such query optimizers rely on cost\nmodels to estimate the costs of candidate query execution plans. A cost model\nrepresents a function from a set of cost units to query execution cost, where\neach cost unit specifies the unit cost of executing a certain type of query\nprocessing operation (such as table scan or join). These cost units are\ntraditionally viewed as constants, whose values only depend on the platform\nconfiguration where the database system runs on top of but are invariant for\nqueries processed by the database system. In this paper, we challenge this\nclassic view by thinking of these cost units as variables instead. We show\nthat, by varying the cost-unit values one can obtain query plans that\nsignificantly outperform the default query plans returned by the query\noptimizer when viewing the cost units as constants. We term this cost-unit\ntuning process ""query tuning"" (QT) and show that it is similar to the\nwell-known hyper-parameter optimization (HPO) problem in AutoML. As a result,\nany state-of-the-art HPO technologies can be applied to QT. We study the QT\nproblem in the context of anytime tuning, which is desirable in practice by\nconstraining the total time spent on QT within a given budget -- we call this\nproblem budget-aware query tuning. We further extend our study from tuning a\nsingle query to tuning a workload with multiple queries, and we call this\ngeneralized problem budget-aware workload tuning (WT), which aims for\nminimizing the execution time of the entire workload. WT is more challenging as\none needs to further prioritize individual query tuning within the given time\nbudget. We propose solutions to both QT and WT and experimental evaluation\nusing both benchmark and real workloads demonstrates the efficacy of our\nproposed solutions.\n'] , ['  Information retrieval models that aim to search for the documents relevant to\nthe given query have shown many successes, which have been applied to diverse\ntasks. However, the query provided by the user is oftentimes very short, which\nchallenges the retrievers to correctly fetch relevant documents. To tackle\nthis, existing studies have proposed expanding the query with a couple of\nadditional (user-related) features related to the query. Yet, they may be\nsuboptimal to effectively augment the query, though there is plenty of\ninformation available to augment it in a relational database. Motivated by\nthis, we present a novel retrieval framework called Database-Augmented Query\nrepresentation (DAQu), which augments the original query with various\n(query-related) metadata across multiple tables. In addition, as the number of\nfeatures in the metadata can be very large and there is no order among them, we\nencode them with our graph-based set encoding strategy, which considers\nhierarchies of features in the database without order. We validate DAQu in\ndiverse retrieval scenarios that can incorporate metadata from the relational\ndatabase, demonstrating that ours significantly enhances overall retrieval\nperformance, compared to existing query augmentation methods.\n', ""  CIS is a prominent area in IR which focuses on developing interactive\nknowledge assistants. These systems must adeptly comprehend the user's\ninformation requirements within the conversational context and retrieve the\nrelevant information. To this aim, the existing approaches model the user's\ninformation needs by generating a single query rewrite or a single\nrepresentation of the query in the query space embedding. However, to answer\ncomplex questions, a single query rewrite or representation is often\nineffective. To address this, a system needs to do reasoning over multiple\npassages. In this work, we propose using a generate-then-retrieve approach to\nimprove the passage retrieval performance for complex user queries. In this\napproach, we utilize large language models (LLMs) to (i) generate an initial\nanswer to the user's information need by doing reasoning over the context of\nthe conversation, and (ii) ground this answer to the collection. Based on the\nexperiments, our proposed approach significantly improves the retrieval\nperformance on TREC iKAT 23, TREC CAsT 20 and 22 datasets, under various\nsetups. Also, we show that grounding the LLM's answer requires more than one\nsearchable query, where an average of 3 queries outperforms human rewrites.\n"", ""  Query rewriting is a crucial technique for passage retrieval in open-domain\nconversational question answering (CQA). It decontexualizes conversational\nqueries into self-contained questions suitable for off-the-shelf retrievers.\nExisting methods attempt to incorporate retriever's preference during the\ntraining of rewriting models. However, these approaches typically rely on\nextensive annotations such as in-domain rewrites and/or relevant passage\nlabels, limiting the models' generalization and adaptation capabilities. In\nthis paper, we introduce AdaQR ($\\textbf{Ada}$ptive $\\textbf{Q}$uery\n$\\textbf{R}$ewriting), a framework for training query rewriting models with\nlimited rewrite annotations from seed datasets and completely no passage label.\nOur approach begins by fine-tuning compact large language models using only\n~$10\\%$ of rewrite annotations from the seed dataset training split. The models\nare then utilized to generate rewrite candidates for each query instance. A\nnovel approach is then proposed to assess retriever's preference for these\ncandidates by the probability of answers conditioned on the conversational\nquery by marginalizing the Top-$K$ passages. This serves as the reward for\noptimizing the rewriter further using Direct Preference Optimization (DPO), a\nprocess free of rewrite and retrieval annotations. Experimental results on four\nopen-domain CQA datasets demonstrate that AdaQR not only enhances the in-domain\ncapabilities of the rewriter with limited annotation requirement, but also\nadapts effectively to out-of-domain datasets.\n""]",Optimization and Management of Computing Resources and Information Systems,Query Optimization in Database Management Systems
161,"Vehicular Edge Computing and Task Offloading Optimization , ""Edge AI and Mobile Networks"" , ""IoT Monitoring and Control with Edge Computing and Machine Learning""","['offloading', 'vehicular', 'edge', 'congestion', 'scheduling', 'traffic', 'networks', 'vehicles', 'throughput', 'network'] , ['edge', 'cloud', 'iot', 'bandwidth', 'mobile', 'devices', 'gaisnet', 'network', 'throughput', 'ai'] , ['iot', 'iotds', 'sensor', 'rscnet', 'cloud', 'sensing', 'sensors', 'networks', 'network', 'classification']","['  Vehicular edge computing (VEC) is an emerging technology that enables\nvehicles to perform high-intensity tasks by executing tasks locally or\noffloading them to nearby edge devices. However, obstacles such as buildings\nmay degrade the communications and incur communication interruptions, and thus\nthe vehicle may not meet the requirement for task offloading. Reconfigurable\nintelligent surfaces (RIS) is introduced to support vehicle communication and\nprovide an alternative communication path. The system performance can be\nimproved by flexibly adjusting the phase-shift of the RIS. For RIS-assisted VEC\nsystem where tasks arrive randomly, we design a control scheme that considers\noffloading power, local power allocation and phase-shift optimization. To solve\nthis non-convex problem, we propose a new deep reinforcement learning (DRL)\nframework that employs modified multi-agent deep deterministic policy gradient\n(MADDPG) approach to optimize the power allocation for vehicle users (VUs) and\nblock coordinate descent (BCD) algorithm to optimize the phase-shift of the\nRIS. Simulation results show that our proposed scheme outperforms the\ncentralized deep deterministic policy gradient (DDPG) scheme and random scheme.\n', '  Computational offloading has become an enabling component for edge\nintelligence in mobile and smart devices. Existing offloading schemes mainly\nfocus on mobile devices and servers, while ignoring the potential network\ncongestion caused by tasks from multiple mobile devices, especially in wireless\nmulti-hop networks. To fill this gap, we propose a low-overhead,\ncongestion-aware distributed task offloading scheme by augmenting a distributed\ngreedy framework with graph-based machine learning. In simulated wireless\nmulti-hop networks with 20-110 nodes and a resource allocation scheme based on\nshortest path routing and contention-based link scheduling, our approach is\ndemonstrated to be effective in reducing congestion or unstable queues under\nthe context-agnostic baseline, while improving the execution latency over local\ncomputing.\n', '  With the increasing demand for multiple applications on internet of vehicles.\nIt requires vehicles to carry out multiple computing tasks in real time.\nHowever, due to the insufficient computing capability of vehicles themselves,\noffloading tasks to vehicular edge computing (VEC) servers and allocating\ncomputing resources to tasks becomes a challenge. In this paper, a multi task\ndigital twin (DT) VEC network is established. By using DT to develop offloading\nstrategies and resource allocation strategies for multiple tasks of each\nvehicle in a single slot, an optimization problem is constructed. To solve it,\nwe propose a multi-agent reinforcement learning method on the task offloading\nand resource allocation. Numerous experiments demonstrate that our method is\neffective compared to other benchmark algorithms.\n'] , ['  On-device large language models (LLMs), referring to running LLMs on edge\ndevices, have raised considerable interest owing to their superior privacy,\nreduced latency, and bandwidth saving. Nonetheless, the capabilities of\non-device LLMs are intrinsically constrained by the limited capacity of edge\ndevices compared to the much more powerful cloud centers. To bridge the gap\nbetween cloud-based and on-device AI, mobile edge intelligence (MEI) presents a\nviable solution to this problem by provisioning AI capabilities within the edge\nof mobile networks with improved privacy and latency relative to cloud\ncomputing. MEI sits between on-device AI and cloud-based AI, featuring wireless\ncommunications and more powerful computing resources than end devices. This\narticle provides a contemporary survey on harnessing MEI for LLMs. We first\ncover the preliminaries of LLMs, starting with LLMs and MEI, followed by\nresource-efficient LLM techniques. We then illustrate several killer\napplications to demonstrate the need for deploying LLMs at the network edge and\npresent an architectural overview of MEI for LLMs (MEI4LLM). Subsequently, we\ndelve into various aspects of MEI4LLM, extensively covering edge LLM caching\nand delivery, edge LLM training, and edge LLM inference. Finally, we identify\nfuture research opportunities. We aim to inspire researchers in the field to\nleverage mobile edge computing to facilitate LLM deployment in close proximity\nto users, thereby unleashing the potential of LLMs across various privacy- and\ndelay-sensitive applications.\n', ""  Big Artificial Intelligence (AI) models have emerged as a crucial element in\nvarious intelligent applications at the edge, such as voice assistants in smart\nhomes and autonomous robotics in smart factories. Training big AI models, e.g.,\nfor personalized fine-tuning and continual model refinement, poses significant\nchallenges to edge devices due to the inherent conflict between limited\ncomputing resources and intensive workload associated with training. Despite\nthe constraints of on-device training, traditional approaches usually resort to\naggregating training data and sending it to a remote cloud for centralized\ntraining. Nevertheless, this approach is neither sustainable, which strains\nlong-range backhaul transmission and energy-consuming datacenters, nor safely\nprivate, which shares users' raw data with remote infrastructures. To address\nthese challenges, we alternatively observe that prevalent edge environments\nusually contain a diverse collection of trusted edge devices with untapped idle\nresources, which can be leveraged for edge training acceleration. Motivated by\nthis, in this article, we propose collaborative edge training, a novel training\nmechanism that orchestrates a group of trusted edge devices as a resource pool\nfor expedited, sustainable big AI model training at the edge. As an initial\nstep, we present a comprehensive framework for building collaborative edge\ntraining systems and analyze in-depth its merits and sustainable scheduling\nchoices following its workflow. To further investigate the impact of its\nparallelism design, we empirically study a case of four typical parallelisms\nfrom the perspective of energy demand with realistic testbeds. Finally, we\ndiscuss open challenges for sustainable collaborative edge training to point to\nfuture directions of edge-centric big AI model training.\n"", '  Artificial intelligence (AI) technologies have emerged as pivotal enablers\nacross a multitude of industries largely due to their significant resurgence\nover the past decade. The transformative power of AI is primarily derived from\nthe utilization of deep neural networks (DNNs), which require extensive data\nfor training and substantial computational resources for processing.\nConsequently, DNN models are typically trained and deployed on resource-rich\ncloud servers. However, due to potential latency issues associated with cloud\ncommunications, deep learning (DL) workflows are increasingly being\ntransitioned to wireless edge networks in proximity to end-user devices (EUDs).\nThis shift is designed to support latency-sensitive applications and has given\nrise to a new paradigm of edge AI, which will play a critical role in upcoming\nsixth-generation (6G) networks to support ubiquitous AI applications. Despite\nits considerable potential, edge AI faces substantial challenges, mostly due to\nthe dichotomy between the resource limitations of wireless edge networks and\nthe resource-intensive nature of DL. Specifically, the acquisition of\nlarge-scale data, as well as the training and inference processes of DNNs, can\nrapidly deplete the battery energy of EUDs. This necessitates an\nenergy-conscious approach to edge AI to ensure both optimal and sustainable\nperformance. In this paper, we present a contemporary survey on green edge AI.\nWe commence by analyzing the principal energy consumption components of edge AI\nsystems to identify the fundamental design principles of green edge AI. Guided\nby these principles, we then explore energy-efficient design methodologies for\nthe three critical tasks in edge AI systems, including training data\nacquisition, edge training, and edge inference. Finally, we underscore\npotential future research directions to further enhance the energy efficiency\nof edge AI.\n'] , ['  This article explores how to drive intelligent iot monitoring and control\nthrough cloud computing and machine learning. As iot and the cloud continue to\ngenerate large and diverse amounts of data as sensor devices in the network,\nthe collected data is sent to the cloud for statistical analysis, prediction,\nand data analysis to achieve business objectives. However, because the cloud\ncomputing model is limited by distance, it can be problematic in environments\nwhere the quality of the Internet connection is not ideal for critical\noperations. Therefore, edge computing, as a distributed computing architecture,\nmoves the location of processing applications, data and services from the\ncentral node of the network to the logical edge node of the network to reduce\nthe dependence on cloud processing and analysis of data, and achieve near-end\ndata processing and analysis. The combination of iot and edge computing can\nreduce latency, improve efficiency, and enhance security, thereby driving the\ndevelopment of intelligent systems. The paper also introduces the development\nof iot monitoring and control technology, the application of edge computing in\niot monitoring and control, and the role of machine learning in data analysis\nand fault detection. Finally, the application and effect of intelligent\nInternet of Things monitoring and control system in industry, agriculture,\nmedical and other fields are demonstrated through practical cases and\nexperimental studies.\n', ""  Deep learning models are increasingly deployed on edge Internet of Things\n(IoT) devices. However, these models typically operate under supervised\nconditions and fail to recognize unseen classes different from training. To\naddress this, zero-shot learning (ZSL) aims to classify data of unseen classes\nwith the help of semantic information. Foundation models (FMs) trained on\nweb-scale data have shown impressive ZSL capability in natural language\nprocessing and visual understanding. However, leveraging FMs' generalized\nknowledge for zero-shot IoT sensing using signals such as mmWave, IMU, and\nWi-Fi has not been fully investigated. In this work, we align the IoT data\nembeddings with the semantic embeddings generated by an FM's text encoder for\nzero-shot IoT sensing. To utilize the physics principles governing the\ngeneration of IoT sensor signals to derive more effective prompts for semantic\nembedding extraction, we propose to use cross-attention to combine a learnable\nsoft prompt that is optimized automatically on training data and an auxiliary\nhard prompt that encodes domain knowledge of the IoT sensing task. To address\nthe problem of IoT embeddings biasing to seen classes due to the lack of unseen\nclass data during training, we propose using data augmentation to synthesize\nunseen class IoT data for fine-tuning the IoT feature extractor and embedding\nprojector. We evaluate our approach on multiple IoT sensing tasks. Results show\nthat our approach achieves superior open-set detection and generalized\nzero-shot learning performance compared with various baselines. Our code is\navailable at https://github.com/schrodingho/FM\\_ZSL\\_IoT.\n"", ""  The Internet of Things (IoT) network integrating billions of smart physical\ndevices embedded with sensors, software, and communication technologies is a\ncritical and rapidly expanding component of our modern world. The IoT ecosystem\nprovides a rich source of real-world modalities such as motion, thermal,\ngeolocation, imaging, depth, sensors, and audio to recognize the states of\nhumans and physical objects. Machine learning presents a rich opportunity to\nautomatically process IoT data at scale, enabling efficient inference for\nunderstanding human wellbeing, controlling physical devices, and\ninterconnecting smart cities. To realize this potential, we introduce IoT-LM,\nan open-source large multisensory language model tailored for the IoT\necosystem. IoT-LM is enabled by two technical contributions: the first is\nMultiIoT, the most expansive unified IoT dataset to date, encompassing over\n1.15 million samples from 12 modalities and 8 tasks prepared for multisensory\npre-training and instruction-tuning. The second is a new multisensory multitask\nadapter layer to condition pre-trained large language models on multisensory\nIoT data. Not only does IoT-LM yield substantial improvements on 8 supervised\nIoT classification tasks, but it also demonstrates new interactive\nquestion-answering, reasoning, and dialog capabilities conditioned on IoT\nsensors. We release IoT-LM's data sources and new multisensory language\nmodeling framework.\n""]",Edge Computing and Artificial Intelligence for IoT and Mobile Networks,"""Edge AI and Mobile Networks"""
162,"""Verilog Generation and Hardware Design Automation"" , ""AI/ML Hardware Accelerators and Optimizations"" , ""Compute-in-Memory Architectures and Compilation Techniques""","['verilog', 'programmable', 'programming', 'hardware', 'verilogeval', 'generate', 'tools', 'automation', 'vhdl', 'veribug'] , ['parallelization', 'fpgas', 'cnns', 'fpga', 'throughput', 'accelerator', 'accelerators', 'hardware', 'memory', 'microarchitecture'] , ['sram', 'memory', 'fpgas', 'fpga', 'hardware', 'cpus', 'cim', 'gpu', 'gpus', 'cpu']","['  Large Language Models (LLMs) have recently shown promise in streamlining\nhardware design processes by encapsulating vast amounts of domain-specific\ndata. In addition, they allow users to interact with the design processes\nthrough natural language instructions, thus making hardware design more\naccessible to developers. However, effectively leveraging LLMs in hardware\ndesign necessitates providing domain-specific data during inference (e.g.,\nthrough in-context learning), fine-tuning, or pre-training. Unfortunately,\nexisting publicly available hardware datasets are often limited in size,\ncomplexity, or detail, which hinders the effectiveness of LLMs in hardware\ndesign tasks. To address this issue, we first propose a set of criteria for\ncreating high-quality hardware datasets that can effectively enhance\nLLM-assisted hardware design. Based on these criteria, we propose a\nMulti-Grained-Verilog (MG-Verilog) dataset, which encompasses descriptions at\nvarious levels of detail and corresponding code samples. To benefit the broader\nhardware design community, we have developed an open-source infrastructure that\nfacilitates easy access, integration, and extension of the dataset to meet\nspecific project needs. Furthermore, to fully exploit the potential of the\nMG-Verilog dataset, which varies in complexity and detail, we introduce a\nbalanced fine-tuning scheme. This scheme serves as a unique use case to\nleverage the diverse levels of detail provided by the dataset. Extensive\nexperiments demonstrate that the proposed dataset and fine-tuning scheme\nconsistently improve the performance of LLMs in hardware design tasks.\n', '  Natural language interfaces have exhibited considerable potential in the\nautomation of Verilog generation derived from high-level specifications through\nthe utilization of large language models, garnering significant attention.\nNevertheless, this paper elucidates that visual representations contribute\nessential contextual information critical to design intent for hardware\narchitectures possessing spatial complexity, potentially surpassing the\nefficacy of natural-language-only inputs. Expanding upon this premise, our\npaper introduces an open-source benchmark for multi-modal generative models\ntailored for Verilog synthesis from visual-linguistic inputs, addressing both\nsingular and complex modules. Additionally, we introduce an open-source visual\nand natural language Verilog query language framework to facilitate efficient\nand user-friendly multi-modal queries. To evaluate the performance of the\nproposed multi-modal hardware generative AI in Verilog generation tasks, we\ncompare it with a popular method that relies solely on natural language. Our\nresults demonstrate a significant accuracy improvement in the multi-modal\ngenerated Verilog compared to queries based solely on natural language. We hope\nto reveal a new approach to hardware design in the large-hardware-design-model\nera, thereby fostering a more diversified and productive approach to hardware\ndesign.\n', '  Recent advances in large language models have demonstrated their potential\nfor automated generation of hardware description language (HDL) code from\nhigh-level prompts. Researchers have utilized fine-tuning to enhance the\nability of these large language models (LLMs) in the field of Chip Design.\nHowever, the lack of Verilog data hinders further improvement in the quality of\nVerilog generation by LLMs. Additionally, the absence of a Verilog and\nElectronic Design Automation (EDA) script data augmentation framework\nsignificantly increases the time required to prepare the training dataset for\nLLM trainers. This paper proposes an automated design-data augmentation\nframework, which generates high-volume and high-quality natural language\naligned with Verilog and EDA scripts. For Verilog generation, it translates\nVerilog files to an abstract syntax tree and then maps nodes to natural\nlanguage with a predefined template. For Verilog repair, it uses predefined\nrules to generate the wrong verilog file and then pairs EDA Tool feedback with\nthe right and wrong verilog file. For EDA Script generation, it uses existing\nLLM(GPT-3.5) to obtain the description of the Script. To evaluate the\neffectiveness of our data augmentation method, we finetune Llama2-13B and\nLlama2-7B models using the dataset generated by our augmentation framework. The\nresults demonstrate a significant improvement in the Verilog generation tasks\nwith LLMs. Moreover, the accuracy of Verilog generation surpasses that of the\ncurrent state-of-the-art open-source Verilog generation model, increasing from\n58.8% to 70.6% with the same benchmark. Our 13B model (ChipGPT-FT) has a pass\nrate improvement compared with GPT-3.5 in Verilog generation and outperforms in\nEDA script (i.e., SiliconCompiler) generation with only 200 EDA script data.\n'] , ['  The relentless advancement of artificial intelligence (AI) and machine\nlearning (ML) applications necessitates the development of specialized hardware\naccelerators capable of handling the increasing complexity and computational\ndemands. Traditional computing architectures, based on the von Neumann model,\nare being outstripped by the requirements of contemporary AI/ML algorithms,\nleading to a surge in the creation of accelerators like the Graphcore\nIntelligence Processing Unit (IPU), Sambanova Reconfigurable Dataflow Unit\n(RDU), and enhanced GPU platforms. These hardware accelerators are\ncharacterized by their innovative data-flow architectures and other design\noptimizations that promise to deliver superior performance and energy\nefficiency for AI/ML tasks.\n  This research provides a preliminary evaluation and comparison of these\ncommercial AI/ML accelerators, delving into their hardware and software design\nfeatures to discern their strengths and unique capabilities. By conducting a\nseries of benchmark evaluations on common DNN operators and other AI/ML\nworkloads, we aim to illuminate the advantages of data-flow architectures over\nconventional processor designs and offer insights into the performance\ntrade-offs of each platform. The findings from our study will serve as a\nvaluable reference for the design and performance expectations of research\nprototypes, thereby facilitating the development of next-generation hardware\naccelerators tailored for the ever-evolving landscape of AI/ML applications.\nThrough this analysis, we aspire to contribute to the broader understanding of\ncurrent accelerator technologies and to provide guidance for future innovations\nin the field.\n', ""  With the recent growth in demand for large-scale deep neural networks,\ncompute in-memory (CiM) has come up as a prominent solution to alleviate\nbandwidth and on-chip interconnect bottlenecks that constrain Von-Neuman\narchitectures. However, the construction of CiM hardware poses a challenge as\nany specific memory hierarchy in terms of cache sizes and memory bandwidth at\ndifferent interfaces may not be ideally matched to any neural network's\nattributes such as tensor dimension and arithmetic intensity, thus leading to\nsuboptimal and under-performing systems. Despite the success of neural\narchitecture search (NAS) techniques in yielding efficient sub-networks for a\ngiven hardware metric budget (e.g., DNN execution time or latency), it assumes\nthe hardware configuration to be frozen, often yielding sub-optimal\nsub-networks for a given budget. In this paper, we present CiMNet, a framework\nthat jointly searches for optimal sub-networks and hardware configurations for\nCiM architectures creating a Pareto optimal frontier of downstream task\naccuracy and execution metrics (e.g., latency). The proposed framework can\ncomprehend the complex interplay between a sub-network's performance and the\nCiM hardware configuration choices including bandwidth, processing element\nsize, and memory size. Exhaustive experiments on different model architectures\nfrom both CNN and Transformer families demonstrate the efficacy of the CiMNet\nin finding co-optimized sub-networks and CiM hardware configurations.\nSpecifically, for similar ImageNet classification accuracy as baseline ViT-B,\noptimizing only the model architecture increases performance (or reduces\nworkload execution time) by 1.7x while optimizing for both the model\narchitecture and hardware configuration increases it by 3.1x.\n"", '  Deep neural networks (DNNs) have been widely used in many artificial\nintelligence (AI) tasks. However, deploying them brings significant challenges\ndue to the huge cost of memory, energy, and computation. To address these\nchallenges, researchers have developed various model compression techniques\nsuch as model quantization and model pruning. Recently, there has been a surge\nin research of compression methods to achieve model efficiency while retaining\nthe performance. Furthermore, more and more works focus on customizing the DNN\nhardware accelerators to better leverage the model compression techniques. In\naddition to efficiency, preserving security and privacy is critical for\ndeploying DNNs. However, the vast and diverse body of related works can be\noverwhelming. This inspires us to conduct a comprehensive survey on recent\nresearch toward the goal of high-performance, cost-efficient, and safe\ndeployment of DNNs. Our survey first covers the mainstream model compression\ntechniques such as model quantization, model pruning, knowledge distillation,\nand optimizations of non-linear operations. We then introduce recent advances\nin designing hardware accelerators that can adapt to efficient model\ncompression approaches. Additionally, we discuss how homomorphic encryption can\nbe integrated to secure DNN deployment. Finally, we discuss several issues,\nsuch as hardware evaluation, generalization, and integration of various\ncompression approaches. Overall, we aim to provide a big picture of efficient\nDNNs, from algorithm to hardware accelerators and security perspectives.\n'] , ['  The demand for efficient machine learning (ML) accelerators is growing\nrapidly, driving the development of novel computing concepts such as resistive\nrandom access memory (RRAM)-based tiled computing-in-memory (CIM)\narchitectures. CIM allows to compute within the memory unit, resulting in\nfaster data processing and reduced power consumption. Efficient compiler\nalgorithms are essential to exploit the potential of tiled CIM architectures.\nWhile conventional ML compilers focus on code generation for CPUs, GPUs, and\nother von Neumann architectures, adaptations are needed to cover CIM\narchitectures. Cross-layer scheduling is a promising approach, as it enhances\nthe utilization of CIM cores, thereby accelerating computations. Although\nsimilar concepts are implicitly used in previous work, there is a lack of clear\nand quantifiable algorithmic definitions for cross-layer scheduling for tiled\nCIM architectures. To close this gap, we present CLSA-CIM, a cross-layer\nscheduling algorithm for tiled CIM architectures. We integrate CLSA-CIM with\nexisting weight-mapping strategies and compare performance against\nstate-of-the-art (SOTA) scheduling algorithms. CLSA-CIM improves the\nutilization by up to 17.9 x , resulting in an overall speedup increase of up to\n29.2 x compared to SOTA.\n', '  In recent years, various computing-in-memory (CIM) processors have been\npresented, showing superior performance over traditional architectures. To\nunleash the potential of various CIM architectures, such as device precision,\ncrossbar size, and crossbar number, it is necessary to develop compilation\ntools that are fully aware of the CIM architectural details and implementation\ndiversity. However, due to the lack of architectural support in current popular\nopen-source compiling stacks, existing CIM designs either manually deploy\nnetworks or build their own compilers, which is time-consuming and\nlabor-intensive. Although some works expose the specific CIM device programming\ninterfaces to compilers, they are often bound to a fixed CIM architecture,\nlacking the flexibility to support the CIM architectures with different\ncomputing granularity. On the other hand, existing compilation works usually\nconsider the scheduling of limited operation types (such as crossbar-bound\nmatrix-vector multiplication). Unlike conventional processors, CIM accelerators\nare featured by their diverse architecture, circuit, and device, which cannot\nbe simply abstracted by a single level if we seek to fully explore the\nadvantages brought by CIM. Therefore, we propose CIM-MLC, a universal\nmulti-level compilation framework for general CIM architectures. We first\nestablish a general hardware abstraction for CIM architectures and computing\nmodes to represent various CIM accelerators. Based on the proposed abstraction,\nCIM-MLC can compile tasks onto a wide range of CIM accelerators having\ndifferent devices, architectures, and programming interfaces. More importantly,\ncompared with existing compilation work, CIM-MLC can explore the mapping and\nscheduling strategies across multiple architectural tiers, which form a\ntractable yet effective design space, to achieve better scheduling and\ninstruction generation results.\n', '  Compute-in-memory (CiM) has emerged as a highly energy efficient solution for\nperforming matrix multiplication during Machine Learning (ML) inference.\nHowever, integrating compute in memory poses key questions, such as 1) What\ntype of CiM to use: Given a multitude of CiM design characteristics,\ndetermining their suitability from architecture perspective is needed. 2) When\nto use CiM: ML inference includes workloads with a variety of memory and\ncompute requirements, making it difficult to identify when CiM is more\nbeneficial. 3) Where to integrate CiM: Each memory level has different\nbandwidth and capacity, creating different data reuse opportunities for CiM\nintegration.\n  To answer such questions regarding on-chip CiM integration for accelerating\nML workloads, we use an analytical architecture evaluation methodology where we\ntailor the dataflow mapping. The mapping algorithm aims to achieve highest\nweight reuse and reduced data movements for a given CiM prototype and workload.\nOur experiments show that CiM integrated memory improves energy efficiency by\nup to 3.4x and throughput by up to 15.6x compared to tensor-core-like baseline\narchitecture, with INT-8 precision under iso-area constraints. We believe the\nproposed work provides insights into what type of CiM to use, and when and\nwhere to optimally integrate it in the cache hierarchy for efficient matrix\nmultiplication.\n']",Hardware Design and Acceleration for AI and ML Applications,"""AI/ML Hardware Accelerators and Optimizations"""
163,"GPU Scheduling in Heterogeneous Clusters , Learning-Augmented Scheduling Algorithms","['gpus', 'scheduling', 'gpu', 'cluster', 'clusters', 'scheduler', 'processors', 'schedulers', 'schedulability', 'schedule'] , ['scheduling', 'prediction', 'knapsack', 'algorithms', 'optimal', 'predictive', 'optimally', 'predictions', 'schedules', 'optimization']","['  GPU-based heterogeneous architectures are now commonly used in HPC clusters.\nDue to their architectural simplicity specialized for data-level parallelism,\nGPUs can offer much higher computational throughput and memory bandwidth than\nCPUs in the same generation do. However, as the available resources in GPUs\nhave increased exponentially over the past decades, it has become increasingly\ndifficult for a single program to fully utilize them. As a consequence, the\nindustry has started supporting several resource partitioning features in order\nto improve the resource utilization by co-scheduling multiple programs on the\nsame GPU die at the same time. Driven by the technological trend, this paper\nfocuses on hierarchical resource partitioning on modern GPUs, and as an\nexample, we utilize a combination of two different features available on recent\nNVIDIA GPUs in a hierarchical manner: MPS (Multi-Process Service), a\nfiner-grained logical partitioning; and MIG (Multi-Instance GPU), a\ncoarse-grained physical partitioning. We propose a method for comprehensively\nco-optimizing the setup of hierarchical partitioning and the selection of\nco-scheduling groups from a given set of jobs, based on reinforcement learning\nusing their profiles. Our thorough experimental results demonstrate that our\napproach can successfully set up job concurrency, partitioning, and\nco-scheduling group selections simultaneously. This results in a maximum\nthroughput improvement by a factor of 1.87 compared to the time-sharing\nscheduling.\n', '  Joint consideration of scheduling and adaptive parallelism offers great\nopportunities for improving the training efficiency of large models on\nheterogeneous GPU clusters. However, integrating adaptive parallelism into a\ncluster scheduler expands the cluster scheduling space. The new space is the\nproduct of the original scheduling space and the parallelism exploration space\nof adaptive parallelism (also a product of pipeline, data, and tensor\nparallelism). The exponentially enlarged scheduling space and ever-changing\noptimal parallelism plan from adaptive parallelism together result in the\ncontradiction between low-overhead and accurate performance data acquisition\nfor efficient cluster scheduling. This paper presents Crius, a training system\nfor efficiently scheduling multiple large models with adaptive parallelism in a\nheterogeneous cluster. Crius proposes a novel scheduling granularity called\nCell. It represents a job with deterministic resources and pipeline stages. The\nexploration space of Cell is shrunk to the product of only data and tensor\nparallelism, thus exposing the potential for accurate and low-overhead\nperformance estimation. Crius then accurately estimates Cells and efficiently\nschedules training jobs. When a Cell is selected as a scheduling choice, its\nrepresented job runs with the optimal parallelism plan explored. Experimental\nresults show that Crius reduces job completion time by up to 48.9% and\nschedules large models with up to 1.49x cluster throughput improvement.\n', '  Training large-scale models relies on a vast number of computing resources.\nFor example, training the GPT-4 model (1.8 trillion parameters) requires 25000\nA100 GPUs . It is a challenge to build a large-scale cluster with one type of\nGPU-accelerator. Using multiple types of GPU-accelerators to construct a\nlarge-scale cluster is an effective way to solve the problem of insufficient\nhomogeneous GPU-accelerators. However, the existing distributed training\nsystems for large-scale models only support homogeneous GPU-accelerators, not\nsupport heterogeneous GPU-accelerators. To address the problem, this paper\nproposes a distributed training system with hybrid parallelism, HETHUB, for\nlarge-scale models, which supports heterogeneous cluster, including AMD, Nvidia\nGPU and other types of GPU-accelerators . It introduces a distributed unified\ncommunicator to realize the communication between heterogeneous\nGPU-accelerators, a distributed performance predictor, and an automatic\nparallel planner to develop and train models efficiently with heterogeneous\nGPU-accelerators. Compared to the distributed training system with homogeneous\nGPU-accelerators, our system can support six combinations of heterogeneous\nGPU-accelerators. We train the Llama-140B model on a heterogeneous cluster with\n768 GPU-accelerators(128 AMD and 640 GPU-accelerator A). The experiment results\nshow that the optimal performance of our system in the heterogeneous cluster\nhas achieved up to 97.49% of the theoretical upper bound performance.\n'] , ['  The non-clairvoyant scheduling problem has gained new interest within\nlearning-augmented algorithms, where the decision-maker is equipped with\npredictions without any quality guarantees. In practical settings, access to\npredictions may be reduced to specific instances, due to cost or data\nlimitations. Our investigation focuses on scenarios where predictions for only\n$B$ job sizes out of $n$ are available to the algorithm. We first establish\nnear-optimal lower bounds and algorithms in the case of perfect predictions.\nSubsequently, we present a learning-augmented algorithm satisfying the\nrobustness, consistency, and smoothness criteria, and revealing a novel\ntradeoff between consistency and smoothness inherent in the scenario with a\nrestricted number of predictions.\n', '  This paper develops learning-augmented algorithms for energy trading in\nvolatile electricity markets. The basic problem is to sell (or buy) $k$ units\nof energy for the highest revenue (lowest cost) over uncertain time-varying\nprices, which can framed as a classic online search problem in the literature\nof competitive analysis. State-of-the-art algorithms assume no knowledge about\nfuture market prices when they make trading decisions in each time slot, and\naim for guaranteeing the performance for the worst-case price sequence. In\npractice, however, predictions about future prices become commonly available by\nleveraging machine learning. This paper aims to incorporate machine-learned\npredictions to design competitive algorithms for online search problems. An\nimportant property of our algorithms is that they achieve performances\ncompetitive with the offline algorithm in hindsight when the predictions are\naccurate (i.e., consistency) and also provide worst-case guarantees when the\npredictions are arbitrarily wrong (i.e., robustness). The proposed algorithms\nachieve the Pareto-optimal trade-off between consistency and robustness, where\nno other algorithms for online search can improve on the consistency for a\ngiven robustness. Further, we extend the basic online search problem to a more\ngeneral inventory management setting that can capture storage-assisted energy\ntrading in electricity markets. In empirical evaluations using traces from\nreal-world applications, our learning-augmented algorithms improve the average\nempirical performance compared to benchmark algorithms, while also providing\nimproved worst-case performance.\n', '  An important goal of modern scheduling systems is to efficiently manage power\nusage. In energy-efficient scheduling, the operating system controls the speed\nat which a machine is processing jobs with the dual objective of minimizing\nenergy consumption and optimizing the quality of service cost of the resulting\nschedule. Since machine-learned predictions about future requests can often be\nlearned from historical data, a recent line of work on learning-augmented\nalgorithms aims to achieve improved performance guarantees by leveraging\npredictions. In particular, for energy-efficient scheduling, Bamas et. al.\n[BamasMRS20] and Antoniadis et. al. [antoniadis2021novel] designed algorithms\nwith predictions for the energy minimization with deadlines problem and\nachieved an improved competitive ratio when the prediction error is small while\nalso maintaining worst-case bounds even when the prediction error is\narbitrarily large.\n  In this paper, we consider a general setting for energy-efficient scheduling\nand provide a flexible learning-augmented algorithmic framework that takes as\ninput an offline and an online algorithm for the desired energy-efficient\nscheduling problem. We show that, when the prediction error is small, this\nframework gives improved competitive ratios for many different energy-efficient\nscheduling problems, including energy minimization with deadlines, while also\nmaintaining a bounded competitive ratio regardless of the prediction error.\nFinally, we empirically demonstrate that this framework achieves an improved\nperformance on real and synthetic datasets.\n']",Advanced Scheduling Techniques for Efficient Resource Utilization,GPU Scheduling in Heterogeneous Clusters
164,"Quality-Diversity Optimization in Dynamic Environments , Supply Chain Resilience and Optimization","['reinforcement', 'learning', 'optimizing', 'exploration', 'reward', 'rewards', 'autonomous', 'optimization', 'optimisation', 'strategies'] , ['forecasting', 'inventory', 'forecast', 'forecasts', 'predicting', 'prediction', 'supply', 'planning', 'analytics', 'suppliers']","['  Evolutionary search via the quality-diversity (QD) paradigm can discover\nhighly performing solutions in different behavioural niches, showing\nconsiderable potential in complex real-world scenarios such as evolutionary\nrobotics. Yet most QD methods only tackle static tasks that are fixed over\ntime, which is rarely the case in the real world. Unlike noisy environments,\nwhere the fitness of an individual changes slightly at every evaluation,\ndynamic environments simulate tasks where external factors at unknown and\nirregular intervals alter the performance of the individual with a severity\nthat is unknown a priori. Literature on optimisation in dynamic environments is\nextensive, yet such environments have not been explored in the context of QD\nsearch. This paper introduces a novel and generalisable Dynamic QD methodology\nthat aims to keep the archive of past solutions updated in the case of\nenvironment changes. Secondly, we present a novel characterisation of dynamic\nenvironments that can be easily applied to well-known benchmarks, with minor\ninterventions to move them from a static task to a dynamic one. Our Dynamic QD\nintervention is applied on MAP-Elites and CMA-ME, two powerful QD algorithms,\nand we test the dynamic variants on different dynamic tasks.\n', '  Training generally capable agents that thoroughly explore their environment\nand learn new and diverse skills is a long-term goal of robot learning. Quality\nDiversity Reinforcement Learning (QD-RL) is an emerging research area that\nblends the best aspects of both fields -- Quality Diversity (QD) provides a\nprincipled form of exploration and produces collections of behaviorally diverse\nagents, while Reinforcement Learning (RL) provides a powerful performance\nimprovement operator enabling generalization across tasks and dynamic\nenvironments. Existing QD-RL approaches have been constrained to sample\nefficient, deterministic off-policy RL algorithms and/or evolution strategies,\nand struggle with highly stochastic environments. In this work, we, for the\nfirst time, adapt on-policy RL, specifically Proximal Policy Optimization\n(PPO), to the Differentiable Quality Diversity (DQD) framework and propose\nadditional improvements over prior work that enable efficient optimization and\ndiscovery of novel skills on challenging locomotion tasks. Our new algorithm,\nProximal Policy Gradient Arborescence (PPGA), achieves state-of-the-art\nresults, including a 4x improvement in best reward over baselines on the\nchallenging humanoid domain.\n', '  In the past few years, a considerable amount of research has been dedicated\nto the exploitation of previous learning experiences and the design of Few-shot\nand Meta Learning approaches, in problem domains ranging from Computer Vision\nto Reinforcement Learning based control. A notable exception, where to the best\nof our knowledge, little to no effort has been made in this direction is\nQuality-Diversity (QD) optimization. QD methods have been shown to be effective\ntools in dealing with deceptive minima and sparse rewards in Reinforcement\nLearning. However, they remain costly due to their reliance on inherently\nsample inefficient evolutionary processes. We show that, given examples from a\ntask distribution, information about the paths taken by optimization in\nparameter space can be leveraged to build a prior population, which when used\nto initialize QD methods in unseen environments, allows for few-shot\nadaptation. Our proposed method does not require backpropagation. It is simple\nto implement and scale, and furthermore, it is agnostic to the underlying\nmodels that are being trained. Experiments carried in both sparse and dense\nreward settings using robotic manipulation and navigation benchmarks show that\nit considerably reduces the number of generations that are required for QD\noptimization in these environments.\n'] , ['  Recent pandemics have highlighted vulnerabilities in our global economic\nsystems, especially supply chains. Possible future pandemic raises a dilemma\nfor businesses owners between short-term profitability and long-term supply\nchain resilience planning. In this study, we propose a novel agent-based\nsimulation model integrating extended Susceptible-Infected-Recovered (SIR)\nepidemiological model and supply and demand economic model to evaluate supply\nchain resilience strategies during pandemics. Using this model, we explore a\nrange of supply chain resilience strategies under pandemic scenarios using in\nsilico experiments. We find that a balanced approach to supply chain resilience\nperforms better in both pandemic and non-pandemic times compared to extreme\nstrategies, highlighting the importance of preparedness in the form of a better\nsupply chain resilience. However, our analysis shows that the exact supply\nchain resilience strategy is hard to obtain for each firm and is relatively\nsensitive to the exact profile of the pandemic and economic state at the\nbeginning of the pandemic. As such, we used a machine learning model that uses\nthe agent-based simulation to estimate a near-optimal supply chain resilience\nstrategy for a firm. The proposed model offers insights for policymakers and\nbusinesses to enhance supply chain resilience in the face of future pandemics,\ncontributing to understanding the trade-offs between short-term gains and\nlong-term sustainability in supply chain management before and during\npandemics.\n', '  This study tackles the complexities of global supply chains, which are\nincreasingly vulnerable to disruptions caused by port congestion, material\nshortages, and inflation. To address these challenges, we explore the\napplication of machine learning methods, which excel in predicting and\noptimizing solutions based on large datasets. Our focus is on enhancing supply\nchain security through fraud detection, maintenance prediction, and material\nbackorder forecasting. We introduce an automated machine learning framework\nthat streamlines data analysis, model construction, and hyperparameter\noptimization for these tasks. By automating these processes, our framework\nimproves the efficiency and effectiveness of supply chain security measures.\nOur research identifies key factors that influence machine learning\nperformance, including sampling methods, categorical encoding, feature\nselection, and hyperparameter optimization. We demonstrate the importance of\nconsidering these factors when applying machine learning to supply chain\nchallenges. Traditional mathematical programming models often struggle to cope\nwith the complexity of large-scale supply chain problems. Our study shows that\nmachine learning methods can provide a viable alternative, particularly when\ndealing with extensive datasets and complex patterns. The automated machine\nlearning framework presented in this study offers a novel approach to supply\nchain security, contributing to the existing body of knowledge in the field.\nIts comprehensive automation of machine learning processes makes it a valuable\ncontribution to the domain of supply chain management.\n', ""  Successful supply chain optimization must mitigate imbalances between supply\nand demand over time. While accurate demand prediction is essential for supply\nplanning, it alone does not suffice. The key to successful supply planning for\noptimal and viable execution lies in maximizing predictability for both demand\nand supply throughout an execution horizon. Therefore, enhancing the accuracy\nof supply predictions is imperative to create an attainable supply plan that\nmatches demand without overstocking or understocking. However, in complex\nsupply chain networks with numerous nodes and edges, accurate supply\npredictions are challenging due to dynamic node interactions, cascading supply\ndelays, resource availability, production and logistic capabilities.\nConsequently, supply executions often deviate from their initial plans. To\naddress this, we present the Graph-based Supply Prediction (GSP) probabilistic\nmodel. Our attention-based graph neural network (GNN) model predicts supplies,\ninventory, and imbalances using graph-structured historical data, demand\nforecasting, and original supply plan inputs. The experiments, conducted using\nhistorical data from a global consumer goods company's large-scale supply\nchain, demonstrate that GSP significantly improves supply and inventory\nprediction accuracy, potentially offering supply plan corrections to optimize\nexecutions.\n""]",Optimization and Learning in Complex Systems,Supply Chain Resilience and Optimization
165,"Contract Theory and Incentive Design , ""Optimizing Ride-Sharing and Taxi Dispatch Systems""","['contracts', 'optimal', 'incentive', 'learns', 'agents', 'bandit', 'incentives', 'contract', 'reward', 'agent'] , ['ridesharing', 'taxis', 'passengers', 'taxi', 'intercity', 'passenger', 'routes', 'trips', 'dispatching', 'vehicles']","[""  We study a repeated contracting setting in which a Principal adaptively\nchooses amongst $k$ Agents at each of $T$ rounds. The Agents are non-myopic,\nand so a mechanism for the Principal induces a $T$-round extensive form game\namongst the Agents. We give several results aimed at understanding an\nunder-explored aspect of contract theory -- the game induced when choosing an\nAgent to contract with. First, we show that this game admits a pure-strategy\n\\emph{non-responsive} equilibrium amongst the Agents -- informally an\nequilibrium in which the Agent's actions depend on the history of realized\nstates of nature, but not on the history of each other's actions, and so avoids\nthe complexities of collusion and threats. Next, we show that if the Principal\nselects Agents using a \\emph{monotone} bandit algorithm, then for any concave\ncontract, in any such equilibrium, the Principal obtains no regret to\ncontracting with the best Agent in hindsight -- not just given their realized\nactions, but also to the counterfactual world in which they had offered a\nguaranteed $T$-round contract to the best Agent in hindsight, which would have\ninduced a different sequence of actions. Finally, we show that if the Principal\nselects Agents using a monotone bandit algorithm which guarantees no\nswap-regret, then the Principal can additionally offer only limited liability\ncontracts (in which the Agent never needs to pay the Principal) while getting\nno-regret to the counterfactual world in which she offered a linear contract to\nthe best Agent in hindsight -- despite the fact that linear contracts are not\nlimited liability. We instantiate this theorem by demonstrating the existence\nof a monotone no swap-regret bandit algorithm, which to our knowledge has not\npreviously appeared in the literature.\n"", ""  We study principal-agent problems in which a principal commits to an\noutcome-dependent payment scheme -- called contract -- in order to induce an\nagent to take a costly, unobservable action leading to favorable outcomes. We\nconsider a generalization of the classical (single-round) version of the\nproblem in which the principal interacts with the agent by committing to\ncontracts over multiple rounds. The principal has no information about the\nagent, and they have to learn an optimal contract by only observing the outcome\nrealized at each round. We focus on settings in which the size of the agent's\naction space is small. We design an algorithm that learns an\napproximately-optimal contract with high probability in a number of rounds\npolynomial in the size of the outcome space, when the number of actions is\nconstant. Our algorithm solves an open problem by Zhu et al.[2022]. Moreover,\nit can also be employed to provide a $\\tilde{\\mathcal{O}}(T^{4/5})$ regret\nbound in the related online learning setting in which the principal aims at\nmaximizing their cumulative utility, thus considerably improving\npreviously-known regret bounds.\n"", ""  Generalized principal-agent problems, including Stackelberg games, contract\ndesign, and Bayesian persuasion, are a class of economic problems where an\nagent best responds to a principal's committed strategy. We study repeated\ngeneralized principal-agent problems under the assumption that the principal\ndoes not have commitment power and the agent uses algorithms to learn to\nrespond to the principal. We reduce this problem to a one-shot generalized\nprincipal-agent problem with an approximately-best-responding agent. Using this\nreduction, we show that: (1) if the agent uses contextual no-regret learning\nalgorithms, then the principal can guarantee a utility that is at least the\nprincipal's optimal utility in the classic non-learning model minus the square\nroot of the agent's regret; (2) if the agent uses contextual no-swap-regret\nlearning algorithms, then the principal cannot obtain any utility more than the\noptimal utility in the non-learning model plus the agent's swap regret. But (3)\nif the agent uses mean-based learning algorithms (which can be no-regret but\nnot no-swap-regret), then the principal can do significantly better than the\nnon-learning model. These general results not only refine previous results in\nStackelberg games and contract design with learning agents but also lead to new\nresults for Bayesian persuasion with a learning agent.\n""] , [""  Large events such as conferences, concerts and sports games, often cause\nsurges in demand for ride services that are not captured in average demand\npatterns, posing unique challenges for routing algorithms. We propose a\nlearning framework for an autonomous fleet of taxis that leverages event data\nfrom the internet to predict demand surges and generate cooperative routing\npolicies. We achieve this through a combination of two major components: (i) a\ndemand prediction framework that uses textual event information in the form of\nevents' descriptions and reviews to predict event-driven demand surges over\nstreet intersections, and (ii) a scalable multiagent reinforcement learning\nframework that leverages demand predictions and uses one-agent-at-a-time\nrollout combined with limited sampling certainty equivalence to learn\nintersection-level routing policies. For our experimental results we consider\nreal NYC ride share data for the year 2022 and information for more than 2000\nevents across 300 unique venues in Manhattan. We test our approach with a fleet\nof 100 taxis on a map with 2235 street intersections. Our experimental results\ndemonstrate that our method learns routing policies that reduce wait time\noverhead per serviced request by 25% to 75%, while picking up 1% to 4% more\nrequests than other model-based RL frameworks and classical methods in\noperations research.\n"", '  Mobility-on-demand (MoD) systems consist of a fleet of shared vehicles that\ncan be hailed for one-way point-to-point trips. The total distance driven by\nthe vehicles and the fleet size can be reduced by employing ridesharing, i.e.,\nby assigning multiple passengers to one vehicle. However, finding the optimal\npassenger-vehicle assignment in an MoD system is a hard combinatorial problem.\nIn this work, we demonstrate how the VGA method, a recently proposed systematic\nmethod for ridesharing, can be used to compute the optimal passenger-vehicle\nassignments and corresponding vehicle routes in a massive-scale MoD system. In\ncontrast to existing works, we solve all passenger-vehicle assignment problems\nto optimality, regularly dealing with instances containing thousands of\nvehicles and passengers. Moreover, to examine the impact of using optimal\nridesharing assignments, we compare the performance of an MoD system that uses\noptimal assignments against an MoD system that uses assignments computed using\ninsertion heuristic and against an MoD system that uses no ridesharing. We\nfound that the system that uses optimal ridesharing assignments subject to the\nmaximum travel delay of 4 minutes reduces the vehicle distance driven by 57 %\ncompared to an MoD system without ridesharing. Furthermore, we found that the\noptimal assignments result in a 20 % reduction in vehicle distance driven and 5\n% lower average passenger travel delay compared to a system that uses insertion\nheuristic.\n', ""  The emergence of on-demand ride pooling services allows each vehicle to serve\nmultiple passengers at a time, thus increasing drivers' income and enabling\npassengers to travel at lower prices than taxi/car on-demand services (only one\npassenger can be assigned to a car at a time like UberX and Lyft). Although\non-demand ride pooling services can bring so many benefits, ride pooling\nservices need a well-defined matching strategy to maximize the benefits for all\nparties (passengers, drivers, aggregation companies and environment), in which\nthe regional dispatching of vehicles has a significant impact on the matching\nand revenue. Existing algorithms often only consider revenue maximization,\nwhich makes it difficult for requests with unusual distribution to get a ride.\nHow to increase revenue while ensuring a reasonable assignment of requests\nbrings a challenge to ride pooling service companies (aggregation companies).\nIn this paper, we propose a framework for vehicle dispatching for ride pooling\ntasks, which splits the city into discrete dispatching regions and uses the\nreinforcement learning (RL) algorithm to dispatch vehicles in these regions. We\nalso consider the mutual information (MI) between vehicle and order\ndistribution as the intrinsic reward of the RL algorithm to improve the\ncorrelation between their distributions, thus ensuring the possibility of\ngetting a ride for unusually distributed requests. In experimental results on a\nreal-world taxi dataset, we demonstrate that our framework can significantly\nincrease revenue up to an average of 3\\% over the existing best on-demand ride\npooling method.\n""]",Optimization of Resource Allocation and Incentives in Dynamic Systems,Contract Theory and Incentive Design
166,Optimal Power Flow in Power Grids,"['powerflownet', 'powerflowmultinet', 'networks', 'powergraph', 'grid', 'safepowergraph', 'grids', 'flow', 'blackout', 'optimal']","['  A DC OPF surrogate modeling framework is developed for Monte Carlo (MC)\nsampling-based risk quantification in power grid operation. MC simulation\nnecessitates solving a large number of DC OPF problems corresponding to the\nsamples of stochastic grid variables (power demand and renewable generation),\nwhich is computationally prohibitive. Computationally inexpensive surrogates of\nOPF provide an attractive alternative for expedited MC simulation. Graph neural\nnetwork (GNN) surrogates of DC OPF, which are especially suitable to\ngraph-structured data, are employed in this work. Previously developed DC OPF\nsurrogate models have focused on accurate operational decision-making and not\non risk quantification. Here, risk quantification-specific aspects of DC OPF\nsurrogate evaluation is the main focus. To this end, the proposed GNN\nsurrogates are evaluated using realistic joint probability distributions,\nquantification of their risk estimation accuracy, and investigation of their\ngeneralizability. Four synthetic grids (Case118, Case300, Case1354pegase, and\nCase2848rte) are used for surrogate model performance evaluation. It is shown\nthat the GNN surrogates are sufficiently accurate for predicting the\n(bus-level, branch-level and system-level) grid state and enable fast as well\nas accurate operational risk quantification for power grids. The article thus\ndevelops tools for fast reliability and risk quantification in real-world power\ngrids using GNN-based surrogates.\n', '  Optimal Power Flow (OPF) refers to a wide range of related optimization\nproblems with the goal of operating power systems efficiently and securely. In\nthe simplest setting, OPF determines how much power to generate in order to\nminimize costs while meeting demand for power and satisfying physical and\noperational constraints. In even the simplest case, power grid operators use\napproximations of the AC-OPF problem because solving the exact problem is\nprohibitively slow with state-of-the-art solvers. These approximations\nsacrifice accuracy and operational feasibility in favor of speed. This\ntrade-off leads to costly ""uplift payments"" and increased carbon emissions,\nespecially for large power grids. In the present work, we train a deep learning\nsystem (CANOS) to predict near-optimal solutions (within 1% of the true AC-OPF\ncost) without compromising speed (running in as little as 33--65 ms).\nImportantly, CANOS scales to realistic grid sizes with promising empirical\nresults on grids containing as many as 10,000 buses. Finally, because CANOS is\na Graph Neural Network, it is robust to changes in topology. We show that CANOS\nis accurate across N-1 topological perturbations of a base grid typically used\nin security-constrained analysis. This paves the way for more efficient\noptimization of more complex OPF problems which alter grid connectivity such as\nunit commitment, topology optimization and security-constrained OPF.\n', '  The AC optimal power flow (AC-OPF) problem is essential for power system\noperations, but its non-convex nature makes it challenging to solve. A widely\nused simplification is the linearized DC optimal power flow (DC-OPF) problem,\nwhich can be solved to global optimality, but whose optimal solution is always\ninfeasible in the original AC-OPF problem. Recently, neural networks (NN) have\nbeen introduced for solving the AC-OPF problem at significantly faster\ncomputation times. However, these methods necessitate extensive datasets, are\ndifficult to train, and are often viewed as black boxes, leading to resistance\nfrom operators who prefer more transparent and interpretable solutions. In this\npaper, we introduce a novel learning-based approach that merges simplicity and\ninterpretability, providing a bridge between traditional approximation methods\nand black-box learning techniques. Our approach not only provides transparency\nfor operators but also achieves competitive accuracy. Numerical results across\nvarious power networks demonstrate that our method provides accuracy comparable\nto, and often surpassing, that of neural networks, particularly when training\ndatasets are limited.\n']",Optimization Methods for Power Grid Management,Optimal Power Flow in Power Grids
167,"Game Theory and Strategic Decision Making , ""Mechanism Design for Strategic Markets and Games"" , Strategic Classification and Learnability","['games', 'strategy', 'strategic', 'game', 'equilibria', 'optimality', 'information', 'subgames', 'equilibrium', 'subgame'] , ['optimal', 'allocation', 'equilibrium', 'incentive', 'bandits', 'equilibria', 'auctions', 'games', 'markets', 'bandit'] , ['learnability', 'classification', 'prediction', 'optimal', 'deterministic', 'classifier', 'adversary', 'randomized', 'bandit', 'complexity']","['  We study zero-sum differential games with state constraints and one-sided\ninformation, where the informed player (Player 1) has a categorical payoff type\nunknown to the uninformed player (Player 2). The goal of Player 1 is to\nminimize his payoff without violating the constraints, while that of Player 2\nis to violate the state constraints if possible, or to maximize the payoff\notherwise. One example of the game is a man-to-man matchup in football. Without\nstate constraints, Cardaliaguet (2007) showed that the value of such a game\nexists and is convex to the common belief of players. Our theoretical\ncontribution is an extension of this result to games with state constraints and\nthe derivation of the primal and dual subdynamic principles necessary for\ncomputing behavioral strategies. Different from existing works that are\nconcerned about the scalability of no-regret learning in games with discrete\ndynamics, our study reveals the underlying structure of strategies for belief\nmanipulation resulting from information asymmetry and state constraints. This\nstructure will be necessary for scalable learning on games with continuous\nactions and long time windows. We use a simplified football game to demonstrate\nthe utility of this work, where we reveal player positions and belief states in\nwhich the attacker should (or should not) play specific random deceptive moves\nto take advantage of information asymmetry, and compute how the defender should\nrespond.\n', '  While Nash equilibrium has emerged as the central game-theoretic solution\nconcept, many important games contain several Nash equilibria and we must\ndetermine how to select between them in order to create real strategic agents.\nSeveral Nash equilibrium refinement concepts have been proposed and studied for\nsequential imperfect-information games, the most prominent being trembling-hand\nperfect equilibrium, quasi-perfect equilibrium, and recently one-sided\nquasi-perfect equilibrium. These concepts are robust to certain arbitrarily\nsmall mistakes, and are guaranteed to always exist; however, we argue that\nneither of these is the correct concept for developing strong agents in\nsequential games of imperfect information. We define a new equilibrium\nrefinement concept for extensive-form games called observable perfect\nequilibrium in which the solution is robust over trembles in\npublicly-observable action probabilities (not necessarily over all action\nprobabilities that may not be observable by opposing players). Observable\nperfect equilibrium correctly captures the assumption that the opponent is\nplaying as rationally as possible given mistakes that have been observed (while\nprevious solution concepts do not). We prove that observable perfect\nequilibrium is always guaranteed to exist, and demonstrate that it leads to a\ndifferent solution than the prior extensive-form refinements in no-limit poker.\nWe expect observable perfect equilibrium to be a useful equilibrium refinement\nconcept for modeling many important imperfect-information games of interest in\nartificial intelligence.\n', ""  We introduce Boolean Observation Games, a subclass of multi-player finite\nstrategic games with incomplete information and qualitative objectives. In\nBoolean observation games, each player is associated with a finite set of\npropositional variables of which only it can observe the value, and it controls\nwhether and to whom it can reveal that value. It does not control the given,\nfixed, value of variables. Boolean observation games are a generalization of\nBoolean games, a well-studied subclass of strategic games but with complete\ninformation, and wherein each player controls the value of its variables.\n  In Boolean observation games, player goals describe multi-agent knowledge of\nvariables. As in classical strategic games, players choose their strategies\nsimultaneously and therefore observation games capture aspects of both\nimperfect and incomplete information. They require reasoning about sets of\noutcomes given sets of indistinguishable valuations of variables. An outcome\nrelation between such sets determines what the Nash equilibria are. We present\nvarious outcome relations, including a qualitative variant of ex-post\nequilibrium. We identify conditions under which, given an outcome relation,\nNash equilibria are guaranteed to exist. We also study the complexity of\nchecking for the existence of Nash equilibria and of verifying if a strategy\nprofile is a Nash equilibrium. We further study the subclass of Boolean\nobservation games with `knowing whether' goal formulas, for which the\nsatisfaction does not depend on the value of variables. We show that each such\nBoolean observation game corresponds to a Boolean game and vice versa, by a\ndifferent correspondence, and that both correspondences are precise in terms of\nexistence of Nash equilibria.\n""] , [""  Two-sided matching markets have been widely studied in the literature due to\ntheir rich applications. Since participants are usually uncertain about their\npreferences, online algorithms have recently been adopted to learn them through\niterative interactions. An existing work initiates the study of this problem in\na many-to-one setting with responsiveness. However, their results are far from\noptimal and lack guarantees of incentive compatibility. We first extend an\nexisting algorithm for the one-to-one setting to this more general setting and\nshow it achieves a near-optimal bound for player-optimal regret. Nevertheless,\ndue to the substantial requirement for collaboration, a single player's\ndeviation could lead to a huge increase in its own cumulative rewards and a\nlinear regret for others. In this paper, we aim to enhance the regret bound in\nmany-to-one markets while ensuring incentive compatibility. We first propose\nthe adaptively explore-then-deferred-acceptance (AETDA) algorithm for\nresponsiveness setting and derive an upper bound for player-optimal stable\nregret while demonstrating its guarantee of incentive compatibility. To the\nbest of our knowledge, it constitutes the first polynomial player-optimal\nguarantee in matching markets that offers such robust assurances without known\n$\\Delta$, where $\\Delta$ is some preference gap among players and arms. We also\nconsider broader substitutable preferences, one of the most general conditions\nto ensure the existence of a stable matching and cover responsiveness. We\ndevise an online DA (ODA) algorithm and establish an upper bound for the\nplayer-pessimal stable regret for this setting.\n"", ""  We consider non-cooperative facility location games where both facilities and\nclients act strategically and heavily influence each other. This contrasts\nestablished game-theoretic facility location models with non-strategic clients\nthat simply select the closest opened facility. In our model, every facility\nlocation has a set of attracted clients and each client has a set of shopping\nlocations and a weight that corresponds to her spending capacity. Facility\nagents selfishly select a location for opening their facility to maximize the\nattracted total spending capacity, whereas clients strategically decide how to\ndistribute their spending capacity among the opened facilities in their\nshopping range. We focus on a natural client behavior similar to classical load\nbalancing: our selfish clients aim for a distribution that minimizes their\nmaximum waiting times for getting serviced, where a facility's waiting time\ncorresponds to its total attracted client weight.\n  We show that subgame perfect equilibria exist and give almost tight constant\nbounds on the Price of Anarchy and the Price of Stability, which even hold for\na broader class of games with arbitrary client behavior. Since facilities and\nclients influence each other, it is crucial for the facilities to anticipate\nthe selfish clients' behavior when selecting their location. For this, we\nprovide an efficient algorithm that also implies an efficient check for\nequilibrium. Finally, we show that computing a socially optimal facility\nplacement is NP-hard and that this result holds for all feasible client weight\ndistributions.\n"", '  We study a non-cooperative two-sided facility location game in which\nfacilities and clients behave strategically. This is in contrast to many other\nfacility location games in which clients simply visit their closest facility.\nFacility agents select a location on a graph to open a facility to attract as\nmuch purchasing power as possible, while client agents choose which facilities\nto patronize by strategically distributing their purchasing power in order to\nminimize their total waiting time. Here, the waiting time of a facility depends\non its received total purchasing power. We show that our client stage is an\natomic splittable congestion game, which implies existence, uniqueness and\nefficient computation of a client equilibrium. Therefore, facility agents can\nefficiently predict client behavior and make strategic decisions accordingly.\nDespite that, we prove that subgame perfect equilibria do not exist in all\ninstances of this game and that their existence is NP-hard to decide. On the\npositive side, we provide a simple and efficient algorithm to compute\n3-approximate subgame perfect equilibria.\n'] , ['  In contrast with standard classification tasks, strategic classification\ninvolves agents strategically modifying their features in an effort to receive\nfavorable predictions. For instance, given a classifier determining loan\napproval based on credit scores, applicants may open or close their credit\ncards to fool the classifier. The learning goal is to find a classifier robust\nagainst strategic manipulations. Various settings, based on what and when\ninformation is known, have been explored in strategic classification. In this\nwork, we focus on addressing a fundamental question: the learnability gaps\nbetween strategic classification and standard learning.\n  We essentially show that any learnable class is also strategically learnable:\nwe first consider a fully informative setting, where the manipulation structure\n(which is modeled by a manipulation graph $G^\\star$) is known and during\ntraining time the learner has access to both the pre-manipulation data and\npost-manipulation data. We provide nearly tight sample complexity and regret\nbounds, offering significant improvements over prior results. Then, we relax\nthe fully informative setting by introducing two natural types of uncertainty.\nFirst, following Ahmadi et al. (2023), we consider the setting in which the\nlearner only has access to the post-manipulation data. We improve the results\nof Ahmadi et al. (2023) and close the gap between mistake upper bound and lower\nbound raised by them. Our second relaxation of the fully informative setting\nintroduces uncertainty to the manipulation structure. That is, we assume that\nthe manipulation graph is unknown but belongs to a known class of graphs. We\nprovide nearly tight bounds on the learning complexity in various unknown\nmanipulation graph settings. Notably, our algorithm in this setting is of\nindependent interest and can be applied to other problems such as multi-label\nlearning.\n', ""  We study the problem of online binary classification in settings where\nstrategic agents can modify their observable features to receive a positive\nclassification. We model the set of feasible manipulations by a directed graph\nover the feature space, and assume the learner only observes the manipulated\nfeatures instead of the original ones. We introduce the Strategic Littlestone\nDimension, a new combinatorial measure that captures the joint complexity of\nthe hypothesis class and the manipulation graph. We demonstrate that it\ncharacterizes the instance-optimal mistake bounds for deterministic learning\nalgorithms in the realizable setting. We also achieve improved regret in the\nagnostic setting by a refined agnostic-to-realizable reduction that accounts\nfor the additional challenge of not observing agents' original features.\nFinally, we relax the assumption that the learner knows the manipulation graph,\ninstead assuming their knowledge is captured by a family of graphs. We derive\nregret bounds in both the realizable setting where all agents manipulate\naccording to the same graph within the graph family, and the agnostic setting\nwhere the manipulation graphs are chosen adversarially and not consistently\nmodeled by a single graph in the family.\n"", ""  We study the problem of online binary classification where strategic agents\ncan manipulate their observable features in predefined ways, modeled by a\nmanipulation graph, in order to receive a positive classification. We show this\nsetting differs in fundamental ways from non-strategic online classification.\nFor instance, whereas in the non-strategic case, a mistake bound of $\\ln|H|$ is\nachievable via the halving algorithm when the target function belongs to a\nknown class $H$, we show that no deterministic algorithm can achieve a mistake\nbound $o(\\Delta)$ in the strategic setting, where $\\Delta$ is the maximum\ndegree of the manipulation graph (even when $|H|=O(\\Delta)$). We obtain an\nalgorithm achieving mistake bound $O(\\Delta\\ln|H|)$. We also extend this to the\nagnostic setting and obtain an algorithm with a $\\Delta$ multiplicative regret,\nand we show no deterministic algorithm can achieve $o(\\Delta)$ multiplicative\nregret.\n  Next, we study two randomized models based on whether the random choices are\nmade before or after agents respond, and show they exhibit fundamental\ndifferences. In the first model, at each round the learner deterministically\nchooses a probability distribution over classifiers inducing expected values on\neach vertex (probabilities of being classified as positive), which the\nstrategic agents respond to. We show that any learner in this model has to\nsuffer linear regret. On the other hand, in the second model, while the\nadversary who selects the next agent must respond to the learner's probability\ndistribution over classifiers, the agent then responds to the actual hypothesis\nclassifier drawn from this distribution. Surprisingly, we show this model is\nmore advantageous to the learner, and we design randomized algorithms that\nachieve sublinear regret bounds against both oblivious and adaptive\nadversaries.\n""]",Game Theory and Strategic Decision Making,Game Theory and Strategic Decision Making
168,"Sports Analytics and Performance Optimization , Artificial Intelligence in Games and Gaming , Mean-Field Games and Nash Equilibria","['soccernet', 'esports', 'sports', 'soccer', 'sport', 'athletes', 'soccerrag', 'players', 'athlete', 'matches'] , ['games', 'gameplay', 'chess', 'ai', 'game', 'strategy', 'gaming', 'poker', 'hearthstone', 'play'] , ['optimal', 'reinforcement', 'games', 'equilibrium', 'equilibria', 'nash', 'markov', 'agents', 'game', 'complexity']","['  In this paper, we present a novel sequential team selection model in soccer.\nSpecifically, we model the stochastic process of player injury and\nunavailability using player-specific information learned from real-world soccer\ndata. Monte-Carlo Tree Search is used to select teams for games that optimise\nlong-term team performance across a soccer season by reasoning over player\ninjury probability. We validate our approach compared to benchmark solutions\nfor the 2018/19 English Premier League season. Our model achieves similar\nseason expected points to the benchmark whilst reducing first-team injuries by\n~13% and the money inefficiently spent on injured players by ~11% -\ndemonstrating the potential to reduce costs and improve player welfare in\nreal-world soccer teams.\n', '  This paper represents an analysis on the momentum of tennis match. And due to\nGeneralization performance of it, it can be helpful in constructing a system to\npredict the result of sports game and analyze the performance of player based\non the Technical statistics. We First use hidden markov models to predict the\nmomentum which is defined as the performance of players. Then we use Xgboost to\nprove the significance of momentum. Finally we use LightGBM to evaluate the\nperformance of our model and use SHAP feature importance ranking and weight\nanalysis to find the key points that affect the performance of players.\n', '  Understanding sports is crucial for the advancement of Natural Language\nProcessing (NLP) due to its intricate and dynamic nature. Reasoning over\ncomplex sports scenarios has posed significant challenges to current NLP\ntechnologies which require advanced cognitive capabilities. Toward addressing\nthe limitations of existing benchmarks on sports understanding in the NLP\nfield, we extensively evaluated mainstream large language models for various\nsports tasks. Our evaluation spans from simple queries on basic rules and\nhistorical facts to complex, context-specific reasoning, leveraging strategies\nfrom zero-shot to few-shot learning, and chain-of-thought techniques. In\naddition to unimodal analysis, we further assessed the sports reasoning\ncapabilities of mainstream video language models to bridge the gap in\nmultimodal sports understanding benchmarking. Our findings highlighted the\ncritical challenges of sports understanding for NLP. We proposed a new\nbenchmark based on a comprehensive overview of existing sports datasets and\nprovided extensive error analysis which we hope can help identify future\nresearch priorities in this field.\n'] , ['  Games have been the perfect test-beds for artificial intelligence research\nfor the characteristics that widely exist in real-world scenarios. Learning and\noptimisation, decision making in dynamic and uncertain environments, game\ntheory, planning and scheduling, design and education are common research areas\nshared between games and real-world problems. Numerous open-source games or\ngame-based environments have been implemented for studying artificial\nintelligence. In addition to single- or multi-player, collaborative or\nadversarial games, there has also been growing interest in implementing\nplatforms for creative design in recent years. Those platforms provide ideal\nbenchmarks for exploring and comparing artificial intelligence ideas and\ntechniques. This paper reviews the games and game-based platforms for\nartificial intelligence research, provides guidance on matching particular\ntypes of artificial intelligence with suitable games for testing and matching\nparticular needs in games with suitable artificial intelligence techniques,\ndiscusses the research trend induced by the evolution of those games and\nplatforms, and gives an outlook.\n', '  Creating and evaluating games manually is an arduous and laborious task.\nProcedural content generation can aid by creating game artifacts, but usually\nnot an entire game. Evolutionary game design, which combines evolutionary\nalgorithms with automated playtesting, has been used to create novel board\ngames with simple equipment; however, the original approach does not include\ncomplex tabletop games with dice, cards, and maps. This work proposes an\nextension of the approach for tabletop games, evaluating the process by\ngenerating variants of Risk, a military strategy game where players must\nconquer map territories to win. We achieved this using a genetic algorithm to\nevolve the chosen parameters, as well as a rules-based agent to test the games\nand a variety of quality criteria to evaluate the new variations generated. Our\nresults show the creation of new variations of the original game with smaller\nmaps, resulting in shorter matches. Also, the variants produce more balanced\nmatches, maintaining the usual drama. We also identified limitations in the\nprocess, where, in many cases, where the objective function was correctly\npursued, but the generated games were nearly trivial. This work paves the way\ntowards promising research regarding the use of evolutionary game design beyond\nclassic board games.\n', '  Poker is in the family of imperfect information games unlike other games such\nas chess, connect four, etc which are perfect information game instead. While\nmany perfect information games have been solved, no non-trivial imperfect\ninformation game has been solved to date. This makes poker a great test bed for\nArtificial Intelligence research. In this paper we firstly compare Game theory\noptimal poker to Exploitative poker. Secondly, we discuss the intricacies of\nabstraction techniques, betting models, and specific strategies employed by\nsuccessful poker bots like Tartanian[1] and Pluribus[6]. Thirdly, we also\nexplore 2-player vs multi-player games and the limitations that come when\nplaying with more players. Finally, this paper discusses the role of machine\nlearning and theoretical approaches in developing winning strategies and\nsuggests future directions for this rapidly evolving field.\n'] , ['  Mean Field Games (MFGs) have the ability to handle large-scale multi-agent\nsystems, but learning Nash equilibria in MFGs remains a challenging task. In\nthis paper, we propose a deep reinforcement learning (DRL) algorithm that\nachieves population-dependent Nash equilibrium without the need for averaging\nor sampling from history, inspired by Munchausen RL and Online Mirror Descent.\nThrough the design of an additional inner-loop replay buffer, the agents can\neffectively learn to achieve Nash equilibrium from any distribution, mitigating\ncatastrophic forgetting. The resulting policy can be applied to various initial\ndistributions. Numerical experiments on four canonical examples demonstrate our\nalgorithm has better convergence properties than SOTA algorithms, in particular\na DRL version of Fictitious Play for population-dependent policies.\n', '  A fundamental shortcoming of the concept of Nash equilibrium is its\ncomputational intractability: approximating Nash equilibria in normal-form\ngames is PPAD-hard. In this paper, inspired by the ideas of smoothed analysis,\nwe introduce a relaxed variant of Nash equilibrium called $\\sigma$-smooth Nash\nequilibrium, for a smoothness parameter $\\sigma$. In a $\\sigma$-smooth Nash\nequilibrium, players only need to achieve utility at least as high as their\nbest deviation to a $\\sigma$-smooth strategy, which is a distribution that does\nnot put too much mass (as parametrized by $\\sigma$) on any fixed action. We\ndistinguish two variants of $\\sigma$-smooth Nash equilibria: strong\n$\\sigma$-smooth Nash equilibria, in which players are required to play\n$\\sigma$-smooth strategies under equilibrium play, and weak $\\sigma$-smooth\nNash equilibria, where there is no such requirement.\n  We show that both weak and strong $\\sigma$-smooth Nash equilibria have\nsuperior computational properties to Nash equilibria: when $\\sigma$ as well as\nan approximation parameter $\\epsilon$ and the number of players are all\nconstants, there is a constant-time randomized algorithm to find a weak\n$\\epsilon$-approximate $\\sigma$-smooth Nash equilibrium in normal-form games.\nIn the same parameter regime, there is a polynomial-time deterministic\nalgorithm to find a strong $\\epsilon$-approximate $\\sigma$-smooth Nash\nequilibrium in a normal-form game. These results stand in contrast to the\noptimal algorithm for computing $\\epsilon$-approximate Nash equilibria, which\ncannot run in faster than quasipolynomial-time. We complement our upper bounds\nby showing that when either $\\sigma$ or $\\epsilon$ is an inverse polynomial,\nfinding a weak $\\epsilon$-approximate $\\sigma$-smooth Nash equilibria becomes\ncomputationally intractable.\n', '  Reinforcement learning for multi-agent games has attracted lots of attention\nrecently. However, given the challenge of solving Nash equilibria for large\npopulation games, existing works with guaranteed polynomial complexities either\nfocus on variants of zero-sum and potential games, or aim at solving (coarse)\ncorrelated equilibria, or require access to simulators, or rely on certain\nassumptions that are hard to verify. This work proposes MF-OML (Mean-Field\nOccupation-Measure Learning), an online mean-field reinforcement learning\nalgorithm for computing approximate Nash equilibria of large population\nsequential symmetric games. MF-OML is the first fully polynomial multi-agent\nreinforcement learning algorithm for provably solving Nash equilibria (up to\nmean-field approximation gaps that vanish as the number of players $N$ goes to\ninfinity) beyond variants of zero-sum and potential games. When evaluated by\nthe cumulative deviation from Nash equilibria, the algorithm is shown to\nachieve a high probability regret bound of $\\tilde{O}(M^{3/4}+N^{-1/2}M)$ for\ngames with the strong Lasry-Lions monotonicity condition, and a regret bound of\n$\\tilde{O}(M^{11/12}+N^{- 1/6}M)$ for games with only the Lasry-Lions\nmonotonicity condition, where $M$ is the total number of episodes and $N$ is\nthe number of agents of the game. As a byproduct, we also obtain the first\ntractable globally convergent computational algorithm for computing approximate\nNash equilibria of monotone mean-field games.\n']",Artificial Intelligence in Sports and Games,Artificial Intelligence in Games and Gaming
169,"Auction Bidding Strategies and Mechanisms , Voting Systems and Electoral Manipulation","['auction', 'bidding', 'auctions', 'bids', 'bid', 'bidders', 'bandit', 'optimal', 'pricing', 'bidder'] , ['gerrymandering', 'redistricting', 'gerrymanderer', 'candidates', 'elections', 'voting', 'candidate', 'electoral', 'ensemble', 'voters']","['  Learning to bid in repeated first-price auctions is a fundamental problem at\nthe interface of game theory and machine learning, which has seen a recent\nsurge in interest due to the transition of display advertising to first-price\nauctions. In this work, we propose a novel concave formulation for\npure-strategy bidding in first-price auctions, and use it to analyze natural\nGradient-Ascent-based algorithms for this problem. Importantly, our analysis\ngoes beyond regret, which was the typical focus of past work, and also accounts\nfor the strategic backdrop of online-advertising markets where bidding\nalgorithms are deployed -- we provide the first guarantees of\nstrategic-robustness and incentive-compatibility for Gradient Ascent.\n  Concretely, we show that our algorithms achieve $O(\\sqrt{T})$ regret when the\nhighest competing bids are generated adversarially, and show that no online\nalgorithm can do better. We further prove that the regret reduces to $O(\\log\nT)$ when the competition is stationary and stochastic, which drastically\nimproves upon the previous best of $O(\\sqrt{T})$. Moving beyond regret, we show\nthat a strategic seller cannot exploit our algorithms to extract more revenue\non average than is possible under the optimal mechanism. Finally, we prove that\nour algorithm is also incentive compatible -- it is a (nearly) dominant\nstrategy for the buyer to report her values truthfully to the algorithm as a\nwhole. Altogether, these guarantees make our algorithms the first to\nsimultaneously achieve both optimal regret and strategic-robustness.\n', ""  Advertisers increasingly use automated bidding to optimize their ad campaigns\non online advertising platforms. Autobidding optimizes an advertiser's\nobjective subject to various constraints, e.g. average ROI and budget\nconstraints. In this paper, we study the problem of designing online\nautobidding algorithms to optimize value subject to ROI and budget constraints\nwhen the platform is running any mixture of first and second price auction.\n  We consider the following stochastic setting: There is an item for sale in\neach of $T$ rounds. In each round, buyers submit bids and an auction is run to\nsell the item. We focus on one buyer, possibly with budget and ROI constraints.\nWe assume that the buyer's value and the highest competing bid are drawn i.i.d.\nfrom some unknown (joint) distribution in each round. We design a low-regret\nbidding algorithm that satisfies the buyer's constraints. Our benchmark is the\nobjective value achievable by the best possible Lipschitz function that maps\nvalues to bids, which is rich enough to best respond to many different\ncorrelation structures between value and highest competing bid. Our main result\nis an algorithm with full information feedback that guarantees a near-optimal\n$\\tilde O(\\sqrt T)$ regret with respect to the best Lipschitz function. Our\nresult applies to a wide range of auctions, most notably any mixture of first\nand second price auctions (price is a convex combination of the first and\nsecond price). In addition, our result holds for both value-maximizing buyers\nand quasi-linear utility-maximizing buyers.\n  We also study the bandit setting, where we show an $\\Omega(T^{2/3})$ lower\nbound on the regret for first-price auctions, showing a large disparity between\nthe full information and bandit settings. We also design an algorithm with\n$\\tilde O(T^{3/4})$ regret, when the value distribution is known and is\nindependent of the highest competing bid.\n"", ""  We consider repeated multi-unit auctions with uniform pricing, which are\nwidely used in practice for allocating goods such as carbon licenses. In each\nround, $K$ identical units of a good are sold to a group of buyers that have\nvaluations with diminishing marginal returns. The buyers submit bids for the\nunits, and then a price $p$ is set per unit so that all the units are sold. We\nconsider two variants of the auction, where the price is set to the $K$-th\nhighest bid and $(K+1)$-st highest bid, respectively.\n  We analyze the properties of this auction in both the offline and online\nsettings. In the offline setting, we consider the problem that one player $i$\nis facing: given access to a data set that contains the bids submitted by\ncompetitors in past auctions, find a bid vector that maximizes player $i$'s\ncumulative utility on the data set. We design a polynomial time algorithm for\nthis problem, by showing it is equivalent to finding a maximum-weight path on a\ncarefully constructed directed acyclic graph.\n  In the online setting, the players run learning algorithms to update their\nbids as they participate in the auction over time. Based on our offline\nalgorithm, we design efficient online learning algorithms for bidding. The\nalgorithms have sublinear regret, under both full information and bandit\nfeedback structures. We complement our online learning algorithms with regret\nlower bounds.\n  Finally, we analyze the quality of the equilibria in the worst case through\nthe lens of the core solution concept in the game among the bidders. We show\nthat the $(K+1)$-st price format is susceptible to collusion among the bidders;\nmeanwhile, the $K$-th price format does not have this issue.\n""] , ['  By classic results in social choice theory, any reasonable preferential\nvoting method sometimes gives individuals an incentive to report an insincere\npreference. The extent to which different voting methods are more or less\nresistant to such strategic manipulation has become a key consideration for\ncomparing voting methods. Here we measure resistance to manipulation by whether\nneural networks of varying sizes can learn to profitably manipulate a given\nvoting method in expectation, given different types of limited information\nabout how other voters will vote. We trained over 70,000 neural networks of 26\nsizes to manipulate against 8 different voting methods, under 6 types of\nlimited information, in committee-sized elections with 5-21 voters and 3-6\ncandidates. We find that some voting methods, such as Borda, are highly\nmanipulable by networks with limited information, while others, such as Instant\nRunoff, are not, despite being quite profitably manipulated by an ideal\nmanipulator with full information. For the two probability models for elections\nthat we use, the overall least manipulable of the 8 methods we study are\nCondorcet methods, namely Minimax and Split Cycle.\n', '  We study the computational complexity of the map redistricting problem\n(gerrymandering). Mathematically, the electoral district designer\n(gerrymanderer) attempts to partition a weighted graph into $k$ connected\ncomponents (districts) such that its candidate (party) wins as many districts\nas possible. Prior work has principally concerned the special cases where the\ngraph is a path or a tree. Our focus concerns the realistic case where the\ngraph is planar. We prove that the gerrymandering problem is solvable in\npolynomial time in $\\lambda$-outerplanar graphs, when the number of candidates\nand $\\lambda$ are constants and the vertex weights (voting weights) are\npolynomially bounded. In contrast, the problem is NP-complete in general planar\ngraphs even with just two candidates. This motivates the study of approximation\nalgorithms for gerrymandering planar graphs. However, when the number of\ncandidates is large, we prove it is hard to distinguish between instances where\nthe gerrymanderer cannot win a single district and instances where the\ngerrymanderer can win at least one district. This immediately implies that the\nredistricting problem is inapproximable in polynomial time in planar graphs,\nunless P=NP. This conclusion appears terminal for the design of good\napproximation algorithms -- but it is not. The inapproximability bound can be\ncircumvented as it only applies when the maximum number of districts the\ngerrymanderer can win is extremely small, say one. Indeed, for a fixed number\nof candidates, our main result is that there is a constant factor approximation\nalgorithm for redistricting unweighted planar graphs, provided the optimal\nvalue is a large enough constant.\n', ""  Role mining is a technique used to derive a role-based authorization policy\nfrom an existing policy. Given a set of users $U$, a set of permissions $P$ and\na user-permission authorization relation $\\mahtit{UPA}\\subseteq U\\times P$, a\nrole mining algorithm seeks to compute a set of roles $R$, a user-role\nauthorization relation $\\mathit{UA}\\subseteq U\\times R$ and a permission-role\nauthorization relation $\\mathit{PA}\\subseteq R\\times P$, such that the\ncomposition of $\\mathit{UA}$ and $\\mathit{PA}$ is close (in some appropriate\nsense) to $\\mathit{UPA}$.\n  In this paper, we first introduce the Generalized Noise Role Mining problem\n(GNRM) -- a generalization of the MinNoise Role Mining problem -- which we\nbelieve has considerable practical relevance. Extending work of Fomin et al.,\nwe show that GNRM is fixed parameter tractable, with parameter $r + k$, where\n$r$ is the number of roles in the solution and $k$ is the number of\ndiscrepancies between $\\mathit{UPA}$ and the relation defined by the\ncomposition of $\\mathit{UA}$ and $\\mathit{PA}$. We further introduce a\nbi-objective optimization variant of GNRM, where we wish to minimize both $r$\nand $k$ subject to upper bounds $r\\le \\bar{r}$ and $k\\le \\bar{k}$, where\n$\\bar{r}$ and $\\bar{k}$ are constants. We show that the Pareto front of this\nbi-objective optimization problem (BO-GNRM) can be computed in fixed-parameter\ntractable time with parameter $\\bar{r}+\\bar{k}$.\n  We then report the results of our experimental work using the integer\nprogramming solver Gurobi to solve instances of BO-GNRM. Our key findings are\nthat (a) we obtained strong support that Gurobi's performance is\nfixed-parameter tractable, (b) our results suggest that our techniques may be\nuseful for role mining in practice, based on our experiments in the context of\nthree well-known real-world authorization policies.\n""]",Mechanisms and Strategies in Auctions and Voting Systems,Auction Bidding Strategies and Mechanisms
170,"""Agent-Based Modeling of Financial Markets"" , Ontology-based Modelling of Agents and Services","['markets', 'agents', 'market', 'traders', 'agent', 'trading', 'trader', 'investors', 'liquidity', 'finance'] , ['ontology', 'ontological', 'agents', 'owl', 'agent', 'semantics', 'services', 'netlogo', 'infrastructures', 'modelling']","[""  We consider the dynamics and the interactions of multiple reinforcement\nlearning optimal execution trading agents interacting with a reactive\nAgent-Based Model (ABM) of a financial market in event time. The model\nrepresents a market ecology with 3-trophic levels represented by: optimal\nexecution learning agents, minimally intelligent liquidity takers, and fast\nelectronic liquidity providers. The optimal execution agent classes include\nbuying and selling agents that can either use a combination of limit orders and\nmarket orders, or only trade using market orders. The reward function\nexplicitly balances trade execution slippage against the penalty of not\nexecuting the order timeously. This work demonstrates how multiple competing\nlearning agents impact a minimally intelligent market simulation as functions\nof the number of agents, the size of agents' initial orders, and the state\nspaces used for learning. We use phase space plots to examine the dynamics of\nthe ABM, when various specifications of learning agents are included. Further,\nwe examine whether the inclusion of optimal execution agents that can learn is\nable to produce dynamics with the same complexity as empirical data. We find\nthat the inclusion of optimal execution agents changes the stylised facts\nproduced by ABM to conform more with empirical data, and are a necessary\ninclusion for ABMs investigating market micro-structure. However, including\nexecution agents to chartist-fundamentalist-noise ABMs is insufficient to\nrecover the complexity observed in empirical data.\n"", '  Investors and regulators can greatly benefit from a realistic market\nsimulator that enables them to anticipate the consequences of their decisions\nin real markets. However, traditional rule-based market simulators often fall\nshort in accurately capturing the dynamic behavior of market participants,\nparticularly in response to external market impact events or changes in the\nbehavior of other participants. In this study, we explore an agent-based\nsimulation framework employing reinforcement learning (RL) agents. We present\nthe implementation details of these RL agents and demonstrate that the\nsimulated market exhibits realistic stylized facts observed in real-world\nmarkets. Furthermore, we investigate the behavior of RL agents when confronted\nwith external market impacts, such as a flash crash. Our findings shed light on\nthe effectiveness and adaptability of RL-based agents within the simulation,\noffering insights into their response to significant market events.\n', '  We present a novel agent-based approach to simulating an over-the-counter\n(OTC) financial market in which trades are intermediated solely by market\nmakers and agent visibility is constrained to a network topology. Dynamics,\nsuch as changes in price, result from agent-level interactions that\nubiquitously occur via market maker agents acting as liquidity providers. Two\nadditional agents are considered: trend investors use a deep convolutional\nneural network paired with a deep Q-learning framework to inform trading\ndecisions by analysing price history; and value investors use a static\nprice-target to determine their trade directions and sizes. We demonstrate that\nour novel inclusion of a network topology with market makers facilitates\nexplorations into various market structures. First, we present the model and an\noverview of its mechanics. Second, we validate our findings via comparison to\nthe real-world: we demonstrate a fat-tailed distribution of price changes,\nauto-correlated volatility, a skew negatively correlated to market maker\npositioning, predictable price-history patterns and more. Finally, we\ndemonstrate that our network-based model can lend insights into the effect of\nmarket-structure on price-action. For example, we show that markets with\nsparsely connected intermediaries can have a critical point of fragmentation,\nbeyond which the market forms distinct clusters and arbitrage becomes rapidly\npossible between the prices of different market makers. A discussion is\nprovided on future work that would be beneficial.\n'] , ['  In this paper an agent-based simulation is developed in order to evaluate an\nAmI scenario based on agents. Many AmI applications are implemented through\nagents but they are not compared to any other existing alternative in order to\nevaluate the relative benefits of using them. The proposal simulation\nenvironment developed in Netlogo analyse such benefits using two evaluation\ncriteria: First, measuring agent satisfaction of different types of desires\nalong the execution. Second, measuring time savings obtained through a correct\nuse of context information.\n  So, here, a previously suggested agent architecture, an ontology and a\n12-steps protocol to provide AmI services in airports, is evaluated using a\nNetLogo simulation environment. The present work uses a NetLogo model\nconsidering scalability problems of this application domain but using FIPA and\nBDI extensions to be coherent with our previous works and our previous JADE\nimplementation of them.\n  The NetLogo model presented simulates an airport with agent users passing\nthrough several zones located in a specific order in a map: passport controls,\ncheck-in counters of airline companies, boarding gates, different types of\nshopping. Although initial data in simulations are generated randomly, and the\nmodel is just an approximation of real-world airports, the definition of this\ncase of use of Ambient Intelligence through NetLogo agents opens an interesting\nway to evaluate the benefits of using Ambient Intelligence, which is a\nsignificant contribution to the final development of them.\n', ""  A critical function of an organization is to foster the level of integration\n(coordination and cooperation) necessary to achieve its objectives. The need to\ncoordinate and motivation to cooperate emerges from the myriad dependencies\nbetween an organization's members and their work. Therefore, to reason about\nsolutions to coordination and cooperation problems requires a robust\nrepresentation that includes the underlying dependencies. We find that such a\nrepresentation remains missing from formal organizational models, and we\nleverage semantics to bridge this gap. Drawing on well-established\norganizational research and our extensive fieldwork with one of North America's\nlargest municipalities, (1) we introduce an ontology, formalized in first-order\nlogic, that operationalizes concepts like outcome, reward, and epistemic\ndependence, and their links to potential integration risks; and (2) present\nreal-world applications of this ontology to analyze and support integration in\ncomplex government infrastructure projects. Our ontology is implemented and\nvalidated in both Z3 and OWL. Key features of our model include inferable\ndependencies, explainable coordination and cooperation risks, and actionable\ninsights on how dependency structures within an organization can be altered to\nmitigate the risks. Conceptualizing real-world challenges like incentive\nmisalignment, free-riding, and subgoal optimization in terms of dependency\nstructures, our semantics-based approach represents a novel method for\nmodelling and enhancing coordination and cooperation. Integrated within a\ndecision-support system, our model may serve as an impactful aid for\norganizational design and effectiveness. More broadly, our approach underscores\nthe transformative potential of semantics in deriving tangible, real-world\nvalue from existing organization theory.\n"", '  In this contribution we extend an ontology for modelling agents and their\ninteractions, called Ontology for Agents, Systems, and Integration of Services\n(in short, OASIS), with conditionals and ontological smart contracts (in short,\nOSCs). OSCs are ontological representations of smart contracts that allow to\nestablish responsibilities and authorizations among agents and set agreements,\nwhereas conditionals allow one to restrict and limit agent interactions, define\nactivation mechanisms that trigger agent actions, and define constraints and\ncontract terms on OSCs. Conditionals and OSCs, as defined in OASIS, are applied\nto extend with ontological capabilities digital public ledgers such as the\nblockchain and smart contracts implemented on it. We will also sketch the\narchitecture of a framework based on the OASIS definition of OSCs that exploits\nthe Ethereum platform and the Interplanetary File System.\n']",Agent-Based Modeling and Ontological Representations in Finance and Complex Systems,"""Agent-Based Modeling of Financial Markets"""
171,Multi-Agent Path Finding (MAPF),"['pathfinding', 'agents', 'planning', 'agent', 'paths', 'planner', 'robots', 'algorithms', 'robotics', 'robot']","['  Multi-Agent Path Finding (MAPF) involves determining paths for multiple\nagents to travel simultaneously and collision-free through a shared area toward\ngiven goal locations. This problem is computationally complex, especially when\ndealing with large numbers of agents, as is common in realistic applications\nlike autonomous vehicle coordination. Finding an optimal solution is often\ncomputationally infeasible, making the use of approximate, suboptimal\nalgorithms essential. Adding to the complexity, agents might act in a\nself-interested and strategic way, possibly misrepresenting their goals to the\nMAPF algorithm if it benefits them. Although the field of mechanism design\noffers tools to align incentives, using these tools without careful\nconsideration can fail when only having access to approximately optimal\noutcomes. In this work, we introduce the problem of scalable mechanism design\nfor MAPF and propose three strategyproof mechanisms, two of which even use\napproximate MAPF algorithms. We test our mechanisms on realistic MAPF domains\nwith problem sizes ranging from dozens to hundreds of agents. We find that they\nimprove welfare beyond a simple baseline.\n', '  Multi-agent path finding (MAPF) is the problem of finding paths for multiple\nagents such that they do not collide. This problem manifests in numerous\nreal-world applications such as controlling transportation robots in automated\nwarehouses, moving characters in video games, and coordinating self-driving\ncars in intersections. Finding optimal solutions to MAPF is NP-Hard, yet modern\noptimal solvers can scale to hundreds of agents and even thousands in some\ncases. Different solvers employ different approaches, and there is no single\nstate-of-the-art approach for all problems. Furthermore, there are no clear,\nprovable, guidelines for choosing when each optimal MAPF solver to use. Prior\nwork employed Algorithm Selection (AS) techniques to learn such guidelines from\npast data. A major challenge when employing AS for choosing an optimal MAPF\nalgorithm is how to encode the given MAPF problem. Prior work either used\nhand-crafted features or an image representation of the problem. We explore\ngraph-based encodings of the MAPF problem and show how they can be used\non-the-fly with a modern graph embedding algorithm called FEATHER. Then, we\nshow how this encoding can be effectively joined with existing encodings,\nresulting in a novel AS method we call MAPF Algorithm selection via Graph\nembedding (MAG). An extensive experimental evaluation of MAG on several MAPF\nalgorithm selection tasks reveals that it is either on-par or significantly\nbetter than existing methods.\n', '  Multi-Agent Path Finding (MAPF) is a fundamental problem in robotics that\nasks us to compute collision-free paths for a team of agents, all moving across\na shared map. Although many works appear on this topic, all current algorithms\nstruggle as the number of agents grows. The principal reason is that existing\napproaches typically plan free-flow optimal paths, which creates congestion. To\ntackle this issue, we propose a new approach for MAPF where agents are guided\nto their destination by following congestion-avoiding paths. We evaluate the\nidea in two large-scale settings: one-shot MAPF, where each agent has a single\ndestination, and lifelong MAPF, where agents are continuously assigned new\ndestinations. Empirically, we report large improvements in solution quality for\none-short MAPF and in overall throughput for lifelong MAPF.\n']",Multi-Agent Path Planning and Coordination,Multi-Agent Path Finding (MAPF)
172,"Social Network Influence and Community Dynamics , Influence Estimation in Machine Learning , Influence Maximization in Complex Networks","['networks', 'communities', 'social', 'influence', 'network', 'community', 'socially', 'friends', 'cohesion', 'distrust'] , ['influence', 'influencer', 'influential', 'influence_analysis_papers', 'annotated', 'attribution', 'predictions', 'evaluations', 'gradient', 'outlier'] , ['networks', 'nodes', 'influence', 'graphs', 'influential', 'maximization', 'spreading', 'node', 'cascades', 'cascade']","[""  This paper revises previous work and introduces changes in spatio-temporal\nscales. The paper presents a model that includes layers A and B with varying\ndegrees of forgetting and dependence over time. We also model changes in\ndependence and forgetting in layers A, A', B, and B' under certain conditions.\nIn addition, to discuss the formation of opinion clusters that have reinforcing\nor obstructive behaviors of forgetting and dependence and are conservative or\nbrainwashing or detoxifying and less prone to filter bubbling, new clusters C\nand D that recommend, obstruct, block, or incite forgetting and dependence over\ntime are Introduction. This introduction allows us to test hypotheses regarding\nthe expansion of opinions in two dimensions over time and space, the state of\ndevelopment of opinion space, and the expansion of public opinion. Challenges\nin consensus building will be highlighted, emphasizing the dynamic nature of\nopinions and the need to consider factors such as dissent, distrust, and media\ninfluence. The paper proposes an extended framework that incorporates trust,\ndistrust, and media influence into the consensus building model. We introduce\nnetwork analysis using dimerizing as a method to gain deeper insights. In this\ncontext, we discuss network clustering, media influence, and consensus\nbuilding. The location and distribution of dimers will be analyzed to gain\ninsight into the structure and dynamics of the network. Dimertiling has been\napplied in various fields other than network analysis, such as physics and\nsociology. The paper concludes by emphasizing the importance of diverse\nperspectives, network analysis, and influential entities in consensus building.\nIt also introduces torus-based visualizations that aid in understanding complex\nnetwork structures.\n"", '  This note considers an innovative interdisciplinary methodology that bridges\nthe gap between the fundamental principles of quantum mechanics applied to the\nstudy of materials such as tellurium nanoparticles (TeNPs) and graphene and the\ncomplex dynamics of social systems. The basis for this approach lies in the\nmetaphorical parallels drawn between the structural features of TeNPs and\ngraphene and the behavioral patterns of social groups in the face of\nmisinformation. TeNPs exhibit unique properties such as the strengthening of\ncovalent bonds within telluric chains and the disruption of secondary structure\nleading to the separation of these chains. This is analogous to increased\ncohesion within social groups and disruption of information flow between\ndifferent subgroups, respectively. . Similarly, the outstanding properties of\ngraphene, such as high electrical conductivity, strength, and flexibility,\nprovide additional aspects for understanding the resilience and adaptability of\nsocial structures in response to external stimuli such as fake news. This\nresearch note proposes a novel metaphorical framework for analyzing the spread\nof fake news within social groups, analogous to the structural features of\ntelluric nanoparticles (TeNPs). We investigate how the strengthening of\ncovalent bonds within TeNPs reflects the strengthening of social cohesion in\ngroups that share common beliefs and values. This paper is partially an attempt\nto utilize ""Generative AI"" and was written with educational intent. There are\ncurrently no plans for it to become a peer-reviewed paper.\n', ""  As people's opinions change, their social networks typically coevolve with\nthem. People are often more susceptible to influence by people with similar\nopinions than by people with dissimilar opinions. In a bounded-confidence model\n(BCM) of opinion dynamics, interacting individuals influence each other through\ndyadic influence if and only if their opinions are sufficiently similar to each\nother. We introduce `neighborhood BCMs' (NBCMs) that include both the usual\ndyadic influence and a transitive influence, which models the effect of friends\nof a friend when determining whether or not an interaction with a friend\ninfluences an individual. In this transitive influence, an individual's opinion\nis influenced by a neighbor when, on average, the opinions of the neighbor's\nneighbors are sufficiently similar to their own opinion. We formulate\nneighborhood Deffuant--Weisbuch (NDW) and neighborhood Hegselmann--Krause (NHK)\nBCMs. We simulate our NDW model on time-independent networks and observe\ninteresting opinion states that cannot occur in an associated baseline DW\nmodel. We also simulate our NDW model on adaptive networks that coevolve with\nopinions by changing its structure through `transitive homophily'. An\nindividual that breaks a tie to one of its neighbors and then rewires that tie\nto a new individual, with a preference for individuals with a mean neighbor\nopinion that is closer to that individual's opinion. We explore how the\nqualitative opinion dynamics and network properties of our time-independent and\nadaptive NDWM models change as we adjust the relative proportions of dyadic and\ntransitive influence. Finally, we study a two-layer opinion--disease model in\nwhich we couple our NDW model with disease spread through a shared adaptive\nnetwork that can change both on the opinion layer and on the disease layer and\nwe examine how the opinion dynamics affect disease spread.\n""] , ['  Large-scale black-box models have become ubiquitous across numerous\napplications. Understanding the influence of individual training data sources\non predictions made by these models is crucial for improving their\ntrustworthiness. Current influence estimation techniques involve computing\ngradients for every training point or repeated training on different subsets.\nThese approaches face obvious computational challenges when scaled up to large\ndatasets and models.\n  In this paper, we introduce and explore the Mirrored Influence Hypothesis,\nhighlighting a reciprocal nature of influence between training and test data.\nSpecifically, it suggests that evaluating the influence of training data on\ntest predictions can be reformulated as an equivalent, yet inverse problem:\nassessing how the predictions for training samples would be altered if the\nmodel were trained on specific test samples. Through both empirical and\ntheoretical validations, we demonstrate the wide applicability of our\nhypothesis. Inspired by this, we introduce a new method for estimating the\ninfluence of training data, which requires calculating gradients for specific\ntest samples, paired with a forward pass for each training point. This approach\ncan capitalize on the common asymmetry in scenarios where the number of test\nsamples under concurrent examination is much smaller than the scale of the\ntraining dataset, thus gaining a significant improvement in efficiency compared\nto existing approaches.\n  We demonstrate the applicability of our method across a range of scenarios,\nincluding data attribution in diffusion models, data leakage detection,\nanalysis of memorization, mislabeled data detection, and tracing behavior in\nlanguage models. Our code will be made available at\nhttps://github.com/ruoxi-jia-group/Forward-INF.\n', ""  Good models require good training data. For overparameterized deep models,\nthe causal relationship between training data and model predictions is\nincreasingly opaque and poorly understood. Influence analysis partially\ndemystifies training's underlying interactions by quantifying the amount each\ntraining instance alters the final model. Measuring the training data's\ninfluence exactly can be provably hard in the worst case; this has led to the\ndevelopment and use of influence estimators, which only approximate the true\ninfluence. This paper provides the first comprehensive survey of training data\ninfluence analysis and estimation. We begin by formalizing the various, and in\nplaces orthogonal, definitions of training data influence. We then organize\nstate-of-the-art influence analysis methods into a taxonomy; we describe each\nof these methods in detail and compare their underlying assumptions, asymptotic\ncomplexities, and overall strengths and weaknesses. Finally, we propose future\nresearch directions to make influence analysis more useful in practice as well\nas more theoretically and empirically sound. A curated, up-to-date list of\nresources related to influence analysis is available at\nhttps://github.com/ZaydH/influence_analysis_papers.\n"", '  Influence functions serve as crucial tools for assessing sample influence in\nmodel interpretation, subset training set selection, noisy label detection, and\nmore. By employing the first-order Taylor extension, influence functions can\nestimate sample influence without the need for expensive model retraining.\nHowever, applying influence functions directly to deep models presents\nchallenges, primarily due to the non-convex nature of the loss function and the\nlarge size of model parameters. This difficulty not only makes computing the\ninverse of the Hessian matrix costly but also renders it non-existent in some\ncases. Various approaches, including matrix decomposition, have been explored\nto expedite and approximate the inversion of the Hessian matrix, with the aim\nof making influence functions applicable to deep models. In this paper, we\nrevisit a specific, albeit naive, yet effective approximation method known as\nTracIn. This method substitutes the inverse of the Hessian matrix with an\nidentity matrix. We provide deeper insights into why this simple approximation\nmethod performs well. Furthermore, we extend its applications beyond measuring\nmodel utility to include considerations of fairness and robustness. Finally, we\nenhance TracIn through an ensemble strategy. To validate its effectiveness, we\nconduct experiments on synthetic data and extensive evaluations on noisy label\ndetection, sample selection for large language model fine-tuning, and defense\nagainst adversarial attacks.\n'] , [""  The identification of a seed set to maximize information spread in a network\nis crucial, a concept known as Influence Maximization (IM). Elegant IM\nalgorithms could naturally extend to cases where each node is equipped with\nspecific weight, referred to as individual effect, to measure the node's\nimportance. Prevailing literature has typically assumed that the individual\neffect remains constant during the cascade process. However, this assumption is\nnot always feasible, as the individual effect of each node is primarily\nevaluated by the difference between the outputs in the activated and\nnon-activated states, with one of these states always being unobservable after\npropagation. Moreover, the individual effect is sensitive to the environmental\ninformation provided by surrounding nodes. To address these challenges, we\nextend the consideration of IM to a broader scenario involving general networks\nwith dynamic node individual effects, leveraging causality techniques. In our\npaper, we address this through the development of a Causal Influence\nMaximization (CauIM) algorithm. Theoretically, for CauIM, we present the\ngeneralized lower bound of influence spread and provide robustness analysis.\nEmpirically, in synthetic and real-world experiments, we demonstrate the\neffectiveness and robustness of CauIM, along with a novel acceleration\ntechnique.\n"", '  Since the structure of complex networks is often unknown, we may identify the\nmost influential seed nodes by exploring only a part of the underlying network,\ngiven a small budget for node queries. We propose IM-META, a solution to\ninfluence maximization (IM) in networks with unknown topology by retrieving\ninformation from queries and node metadata. Since using such metadata is not\nwithout risk due to the noisy nature of metadata and uncertainties in\nconnectivity inference, we formulate a new IM problem that aims to find both\nseed nodes and queried nodes. In IM-META, we develop an effective method that\niteratively performs three steps: 1) we learn the relationship between\ncollected metadata and edges via a Siamese neural network, 2) we select a\nnumber of inferred confident edges to construct a reinforced graph, and 3) we\nidentify the next node to query by maximizing the inferred influence spread\nusing our topology-aware ranking strategy. Through experimental evaluation of\nIM-META on four real-world datasets, we demonstrate a) the speed of network\nexploration via node queries, b) the effectiveness of each module, c) the\nsuperiority over benchmark methods, d) the robustness to more difficult\nsettings, e) the hyperparameter sensitivity, and f) the scalability.\n', '  Influence maximization (IM) is the problem of identifying a limited number of\ninitial influential users within a social network to maximize the number of\ninfluenced users. However, previous research has mostly focused on individual\ninformation propagation, neglecting the simultaneous and interactive\ndissemination of multiple information items. In reality, when users encounter a\npiece of information, such as a smartphone product, they often associate it\nwith related products in their minds, such as earphones or computers from the\nsame brand. Additionally, information platforms frequently recommend related\ncontent to users, amplifying this cascading effect and leading to multiplex\ninfluence diffusion.\n  This paper first formulates the Multiplex Influence Maximization (Multi-IM)\nproblem using multiplex diffusion models with an information association\nmechanism. In this problem, the seed set is a combination of influential users\nand information. To effectively manage the combinatorial complexity, we propose\nGraph Bayesian Optimization for Multi-IM (GBIM). The multiplex diffusion\nprocess is thoroughly investigated using a highly effective global kernelized\nattention message-passing module. This module, in conjunction with Bayesian\nlinear regression (BLR), produces a scalable surrogate model. A data\nacquisition module incorporating the exploration-exploitation trade-off is\ndeveloped to optimize the seed set further. Extensive experiments on synthetic\nand real-world datasets have proven our proposed framework effective. The code\nis available at https://github.com/zirui-yuan/GBIM.\n']",Influence and Information Diffusion in Complex Networks,Influence Maximization in Complex Networks
173,"Automated Machine Learning (AutoML) Adoption and Perception , Technology Acceptance and Adoption in Various Domains","['automl', 'automate', 'automated', 'automating', 'automation', 'autorecsys', 'autommlab', 'automatically', 'autorul', 'expertise'] , ['automated', 'willingness', 'technology', 'tools', 'collusion', 'detection', 'auditing', 'awareness', 'service', 'reviewers']","['  Background. Due to the widespread adoption of Artificial Intelligence (AI)\nand Machine Learning (ML) for building software applications, companies are\nstruggling to recruit employees with a deep understanding of such technologies.\nIn this scenario, AutoML is soaring as a promising solution to fill the AI/ML\nskills gap since it promises to automate the building of end-to-end AI/ML\npipelines that would normally be engineered by specialized team members. Aims.\nDespite the growing interest and high expectations, there is a dearth of\ninformation about the extent to which AutoML is currently adopted by teams\ndeveloping AI/ML-enabled systems and how it is perceived by practitioners and\nresearchers. Method. To fill these gaps, in this paper, we present a\nmixed-method study comprising a benchmark of 12 end-to-end AutoML tools on two\nSE datasets and a user survey with follow-up interviews to further our\nunderstanding of AutoML adoption and perception. Results. We found that AutoML\nsolutions can generate models that outperform those trained and optimized by\nresearchers to perform classification tasks in the SE domain. Also, our\nfindings show that the currently available AutoML solutions do not live up to\ntheir names as they do not equally support automation across the stages of the\nML development workflow and for all the team members. Conclusions. We derive\ninsights to inform the SE research community on how AutoML can facilitate their\nactivities and tool builders on how to design the next generation of AutoML\ntechnologies.\n', ""  Automated machine learning (AutoML) was formed around the fundamental\nobjectives of automatically and efficiently configuring machine learning (ML)\nworkflows, aiding the research of new ML algorithms, and contributing to the\ndemocratization of ML by making it accessible to a broader audience. Over the\npast decade, commendable achievements in AutoML have primarily focused on\noptimizing predictive performance. This focused progress, while substantial,\nraises questions about how well AutoML has met its broader, original goals. In\nthis position paper, we argue that a key to unlocking AutoML's full potential\nlies in addressing the currently underexplored aspect of user interaction with\nAutoML systems, including their diverse roles, expectations, and expertise. We\nenvision a more human-centered approach in future AutoML research, promoting\nthe collaborative design of ML systems that tightly integrates the\ncomplementary strengths of human expertise and AutoML methodologies.\n"", '  Automated machine learning (AutoML) is envisioned to make ML techniques\naccessible to ordinary users. Recent work has investigated the role of humans\nin enhancing AutoML functionality throughout a standard ML workflow. However,\nit is also critical to understand how users adopt existing AutoML solutions in\ncomplex, real-world settings from a holistic perspective. To fill this gap,\nthis study conducted semi-structured interviews of AutoML users (N=19) focusing\non understanding (1) the limitations of AutoML encountered by users in their\nreal-world practices, (2) the strategies users adopt to cope with such\nlimitations, and (3) how the limitations and workarounds impact their use of\nAutoML. Our findings reveal that users actively exercise user agency to\novercome three major challenges arising from customizability, transparency, and\nprivacy. Furthermore, users make cautious decisions about whether and how to\napply AutoML on a case-by-case basis. Finally, we derive design implications\nfor developing future AutoML solutions.\n'] , ['  Public acceptance of conditionally automated vehicles is a crucial step in\nthe realization of smart cities. Prior research in Europe has shown that the\nfactors of hedonic motivation, social influence, and performance expectancy, in\ndecreasing order of importance, influence acceptance. Moreover, a generally\npositive acceptance of the technology was reported. However, there is a lack of\ninformation regarding the public acceptance of conditionally automated vehicles\nin the United States. In this study, we carried out a web-based experiment\nwhere participants were provided information regarding the technology and then\ncompleted a questionnaire on their perceptions. The collected data was analyzed\nusing PLS-SEM to examine the factors that may lead to public acceptance of the\ntechnology in the United States. Our findings showed that social influence,\nperformance expectancy, effort expectancy, hedonic motivation, and facilitating\nconditions determine conditionally automated vehicle acceptance. Additionally,\ncertain factors were found to influence the perception of how useful the\ntechnology is, the effort required to use it, and the facilitating conditions\nfor its use. By integrating the insights gained from this study, stakeholders\ncan better facilitate the adoption of autonomous vehicle technology,\ncontributing to safer, more efficient, and user-friendly transportation systems\nin the future that help realize the vision of the smart city.\n', '  A major threat to the peer-review systems of computer science conferences is\nthe existence of ""collusion rings"" between reviewers. In such collusion rings,\nreviewers who have also submitted their own papers to the conference work\ntogether to manipulate the conference\'s paper assignment, with the aim of being\nassigned to review each other\'s papers. The most straightforward way that\ncolluding reviewers can manipulate the paper assignment is by indicating their\ninterest in each other\'s papers through strategic paper bidding. One potential\napproach to solve this important problem would be to detect the colluding\nreviewers from their manipulated bids, after which the conference can take\nappropriate action. While prior work has developed effective techniques to\ndetect other kinds of fraud, no research has yet established that detecting\ncollusion rings is even possible. In this work, we tackle the question of\nwhether it is feasible to detect collusion rings from the paper bidding. To\nanswer this question, we conduct empirical analysis of two realistic conference\nbidding datasets, including evaluations of existing algorithms for fraud\ndetection in other applications. We find that collusion rings can achieve\nconsiderable success at manipulating the paper assignment while remaining\nhidden from detection: for example, in one dataset, undetected colluders are\nable to achieve assignment to up to 30% of the papers authored by other\ncolluders. In addition, when 10 colluders bid on all of each other\'s papers, no\ndetection algorithm outputs a group of reviewers with more than 31% overlap\nwith the true colluders. These results suggest that collusion cannot be\neffectively detected from the bidding using popular existing tools,\ndemonstrating the need to develop more complex detection algorithms as well as\nthose that leverage additional metadata (e.g., reviewer-paper text-similarity\nscores).\n', ""  The detrimental effects of air pollutants on human health have prompted\nincreasing concerns regarding indoor air quality (IAQ). The emergence of\ndigital health interventions and citizen science initiatives has provided new\navenues for raising awareness, improving IAQ, and promoting behavioural\nchanges. The Technology Acceptance Model (TAM) offers a theoretical framework\nto understand user acceptance and adoption of IAQ technology. This paper\npresents a case study using the COM-B model and Internet of Things (IoT)\ntechnology to design a human-centred digital visualisation platform, leading to\nbehavioural changes and improved IAQ. The study also investigates users'\nacceptance and adoption of the technology, focusing on their experiences,\nexpectations, and the impact on IAQ. Integrating IAQ sensing, digital\nhealth-related interventions, citizen science, and the TAM model offers\nopportunities to address IAQ challenges, enhance public health, and foster\nsustainable indoor environments. The analytical results show that factors such\nas human behaviour, indoor activities, and awareness play crucial roles in\nshaping IAQ.\n""]",Technology Adoption and Acceptance in Various Domains,Technology Acceptance and Adoption in Various Domains
174,"Click-Through Rate Prediction in Advertising , E-commerce Search Relevance Modeling","['clicks', 'advertiser', 'recommender', 'ctr', 'feature', 'advertisers', 'prediction', 'ads', 'advertising', 'click'] , ['retrieval', 'intents', 'search', 'intent', 'queryner', 'relevance', 'queries', 'pagerank', 'ranking', 'matching']","['  Click-through rate (CTR) prediction is widely used in academia and industry.\nMost CTR tasks fall into a feature embedding \\& feature interaction paradigm,\nwhere the accuracy of CTR prediction is mainly improved by designing practical\nfeature interaction structures. However, recent studies have argued that the\nfixed feature embedding learned only through the embedding layer limits the\nperformance of existing CTR models. Some works apply extra modules on top of\nthe embedding layer to dynamically refine feature representations in different\ninstances, making it effective and easy to integrate with existing CTR methods.\nDespite the promising results, there is a lack of a systematic review and\nsummarization of this new promising direction on the CTR task. To fill this\ngap, we comprehensively summarize and define a new module, namely\n\\textbf{feature refinement} (FR) module, that can be applied between feature\nembedding and interaction layers. We extract 14 FR modules from previous works,\nincluding instances where the FR module was proposed but not clearly defined or\nexplained. We fully assess the effectiveness and compatibility of existing FR\nmodules through comprehensive and extensive experiments with over 200 augmented\nmodels and over 4,000 runs for more than 15,000 GPU hours. The results offer\ninsightful guidelines for researchers, and all benchmarking code and\nexperimental results are open-sourced. In addition, we present a new\narchitecture of assigning independent FR modules to separate sub-networks for\nparallel CTR models, as opposed to the conventional method of inserting a\nshared FR module on top of the embedding layer. Our approach is also supported\nby comprehensive experiments demonstrating its effectiveness.\n', '  Multi-Task Learning (MTL) plays a crucial role in real-world advertising\napplications such as recommender systems, aiming to achieve robust\nrepresentations while minimizing resource consumption. MTL endeavors to\nsimultaneously optimize multiple tasks to construct a unified model serving\ndiverse objectives. In online advertising systems, tasks like Click-Through\nRate (CTR) and Conversion Rate (CVR) are often treated as MTL problems\nconcurrently. However, it has been overlooked that a conversion ($y_{cvr}=1$)\nnecessitates a preceding click ($y_{ctr}=1$). In other words, while certain CTR\ntasks are associated with corresponding conversions, others lack such\nassociations. Moreover, the likelihood of noise is significantly higher in CTR\ntasks where conversions do not occur compared to those where they do, and\nexisting methods lack the ability to differentiate between these two scenarios.\nIn this study, exposure labels corresponding to conversions are regarded as\ndefinitive indicators, and a novel task-specific loss is introduced by\ncalculating a \\textbf{p}air\\textbf{wise} \\textbf{r}anking (PWiseR) loss between\nmodel predictions, manifesting as pairwise ranking loss, to encourage the model\nto rely more on them. To demonstrate the effect of the proposed loss function,\nexperiments were conducted on different MTL and Single-Task Learning (STL)\nmodels using four distinct public MTL datasets, namely Alibaba FR, NL, US, and\nCCP, along with a proprietary industrial dataset. The results indicate that our\nproposed loss function outperforms the BCE loss function in most cases in terms\nof the AUC metric.\n', '  Click-Through Rate (CTR) prediction holds a pivotal place in online\nadvertising and recommender systems since CTR prediction performance directly\ninfluences the overall satisfaction of the users and the revenue generated by\ncompanies. Even so, CTR prediction is still an active area of research since it\ninvolves accurately modelling the preferences of users based on sparse and\nhigh-dimensional features where the higher-order interactions of multiple\nfeatures can lead to different outcomes.\n  Most CTR prediction models have relied on a single fusion and interaction\nlearning strategy. The few CTR prediction models that have utilized multiple\ninteraction modelling strategies have treated each interaction to be\nself-contained. In this paper, we propose a novel model named STEC that reaps\nthe benefits of multiple interaction learning approaches in a single unified\narchitecture. Additionally, our model introduces residual connections from\ndifferent orders of interactions which boosts the performance by allowing lower\nlevel interactions to directly affect the predictions. Through extensive\nexperiments on four real-world datasets, we demonstrate that STEC outperforms\nexisting state-of-the-art approaches for CTR prediction thanks to its greater\nexpressive capabilities.\n'] , [""  Semantic relevance calculation is crucial for e-commerce search engines, as\nit ensures that the items selected closely align with customer intent.\nInadequate attention to this aspect can detrimentally affect user experience\nand engagement. Traditional text-matching techniques are prevalent but often\nfail to capture the nuances of search intent accurately, so neural networks now\nhave become a preferred solution to processing such complex text matching.\nExisting methods predominantly employ representation-based architectures, which\nstrike a balance between high traffic capacity and low latency. However, they\nexhibit significant shortcomings in generalization and robustness when compared\nto interaction-based architectures. In this work, we introduce a robust\ninteraction-based modeling paradigm to address these shortcomings. It\nencompasses 1) a dynamic length representation scheme for expedited inference,\n2) a professional terms recognition method to identify subjects and core\nattributes from complex sentence structures, and 3) a contrastive adversarial\ntraining protocol to bolster the model's robustness and matching capabilities.\nExtensive offline evaluations demonstrate the superior robustness and\neffectiveness of our approach, and online A/B testing confirms its ability to\nimprove relevance in the same exposure position, resulting in more clicks and\nconversions. To the best of our knowledge, this method is the first\ninteraction-based approach for large e-commerce search relevance calculation.\nNotably, we have deployed it for the entire search traffic on alibaba.com, the\nlargest B2B e-commerce platform in the world.\n"", ""  Text matching systems have become a fundamental service in most searching\nplatforms. For instance, they are responsible for matching user queries to\nrelevant candidate items, or rewriting the user-input query to a pre-selected\nhigh-performing one for a better search experience. In practice, both the\nqueries and items often contain multiple attributes, such as the category of\nthe item and the location mentioned in the query, which represent condensed key\ninformation that is helpful for matching. However, most of the existing works\ndownplay the effectiveness of attributes by integrating them into text\nrepresentations as supplementary information. Hence, in this work, we focus on\nexploring the relationship between the attributes from two sides. Since\nattributes from two ends are often not aligned in terms of number and type, we\npropose to exploit the benefit of attributes by multiple-intent modeling. The\nintents extracted from attributes summarize the diverse needs of queries and\nprovide rich content of items, which are more refined and abstract, and can be\naligned for paired inputs. Concretely, we propose a multi-intent\nattribute-aware matching model (MIM), which consists of three main components:\nattribute-aware encoder, multi-intent modeling, and intent-aware matching. In\nthe attribute-aware encoder, the text and attributes are weighted and processed\nthrough a scaled attention mechanism with regard to the attributes' importance.\nAfterward, the multi-intent modeling extracts intents from two ends and aligns\nthem. Herein, we come up with a distribution loss to ensure the learned intents\nare diverse but concentrated, and a kullback-leibler divergence loss that\naligns the learned intents. Finally, in the intent-aware matching, the intents\nare evaluated by a self-supervised masking task, and then incorporated to\noutput the final matching result.\n"", ""  Text relevance or text matching of query and product is an essential\ntechnique for the e-commerce search system to ensure that the displayed\nproducts can match the intent of the query. Many studies focus on improving the\nperformance of the relevance model in search system. Recently, pre-trained\nlanguage models like BERT have achieved promising performance on the text\nrelevance task. While these models perform well on the offline test dataset,\nthere are still obstacles to deploy the pre-trained language model to the\nonline system as their high latency. The two-tower model is extensively\nemployed in industrial scenarios, owing to its ability to harmonize performance\nwith computational efficiency. Regrettably, such models present an opaque\n``black box'' nature, which prevents developers from making special\noptimizations. In this paper, we raise deep Bag-of-Words (DeepBoW) model, an\nefficient and interpretable relevance architecture for Chinese e-commerce. Our\napproach proposes to encode the query and the product into the sparse BoW\nrepresentation, which is a set of word-weight pairs. The weight means the\nimportant or the relevant score between the corresponding word and the raw\ntext. The relevance score is measured by the accumulation of the matched word\nbetween the sparse BoW representation of the query and the product. Compared to\npopular dense distributed representation that usually suffers from the drawback\nof black-box, the most advantage of the proposed representation model is highly\nexplainable and interventionable, which is a superior advantage to the\ndeployment and operation of online search engines. Moreover, the online\nefficiency of the proposed model is even better than the most efficient inner\nproduct form of dense representation ...\n""]",Online Advertising and E-commerce Search Optimization,Click-Through Rate Prediction in Advertising
175,"""Job Market Analysis and Resume Optimization"" , ""Customer Behavior in Retail and E-commerce"" , Uplift Modeling for Marketing Strategies","['resumes', 'resume', 'hiring', 'jobs', 'recruitment', 'workforce', 'employment', 'applicants', 'skills', 'occupations'] , ['ecommerce', 'retailing', 'retail', 'customers', 'purchases', 'marketing', 'promotions', 'customer', 'willingness', 'categories'] , ['uplift', 'coupon', 'coupons', 'promotion', 'roi', 'modeling', 'discounts', 'sales', 'revenue', 'marketing']","['  [Abridged Abstract]\n  Recent technological advances underscore labor market dynamics, yielding\nsignificant consequences for employment prospects and increasing job vacancy\ndata across platforms and languages. Aggregating such data holds potential for\nvaluable insights into labor market demands, new skills emergence, and\nfacilitating job matching for various stakeholders. However, despite prevalent\ninsights in the private sector, transparent language technology systems and\ndata for this domain are lacking. This thesis investigates Natural Language\nProcessing (NLP) technology for extracting relevant information from job\ndescriptions, identifying challenges including scarcity of training data, lack\nof standardized annotation guidelines, and shortage of effective extraction\nmethods from job ads. We frame the problem, obtaining annotated data, and\nintroducing extraction methodologies. Our contributions include job description\ndatasets, a de-identification dataset, and a novel active learning algorithm\nfor efficient model training. We propose skill extraction using weak\nsupervision, a taxonomy-aware pre-training methodology adapting multilingual\nlanguage models to the job market domain, and a retrieval-augmented model\nleveraging multiple skill extraction datasets to enhance overall performance.\nFinally, we ground extracted information within a designated taxonomy.\n', '  A reliable resume-job matching system helps a company find suitable\ncandidates from a pool of resumes, and helps a job seeker find relevant jobs\nfrom a list of job posts. However, since job seekers apply only to a few jobs,\ninteraction records in resume-job datasets are sparse. Different from many\nprior work that use complex modeling techniques, we tackle this sparsity\nproblem using data augmentations and a simple contrastive learning approach.\nConFit first creates an augmented resume-job dataset by paraphrasing specific\nsections in a resume or a job post. Then, ConFit uses contrastive learning to\nfurther increase training samples from $B$ pairs per batch to $O(B^2)$ per\nbatch. We evaluate ConFit on two real-world datasets and find it outperforms\nprior methods (including BM25 and OpenAI text-ada-002) by up to 19% and 31%\nabsolute in nDCG@10 for ranking jobs and ranking resumes, respectively.\n', ""  Crafting the ideal, job-specific resume is a challenging task for many job\napplicants, especially for early-career applicants. While it is highly\nrecommended that applicants tailor their resume to the specific role they are\napplying for, manually tailoring resumes to job descriptions and role-specific\nrequirements is often (1) extremely time-consuming, and (2) prone to human\nerrors. Furthermore, performing such a tailoring step at scale while applying\nto several roles may result in a lack of quality of the edited resumes. To\ntackle this problem, in this demo paper, we propose ResumeFlow: a Large\nLanguage Model (LLM) aided tool that enables an end user to simply provide\ntheir detailed resume and the desired job posting, and obtain a personalized\nresume specifically tailored to that specific job posting in the matter of a\nfew seconds. Our proposed pipeline leverages the language understanding and\ninformation extraction capabilities of state-of-the-art LLMs such as OpenAI's\nGPT-4 and Google's Gemini, in order to (1) extract details from a job\ndescription, (2) extract role-specific details from the user-provided resume,\nand then (3) use these to refine and generate a role-specific resume for the\nuser. Our easy-to-use tool leverages the user-chosen LLM in a completely\noff-the-shelf manner, thus requiring no fine-tuning. We demonstrate the\neffectiveness of our tool via a video demo and propose novel task-specific\nevaluation metrics to control for alignment and hallucination. Our tool is\navailable at https://job-aligned-resume.streamlit.app.\n""] , [""  Predicting human decision-making under risk and uncertainty represents a\nquintessential challenge that spans economics, psychology, and related\ndisciplines. Despite decades of research effort, no model can be said to\naccurately describe and predict human choice even for the most stylized tasks\nlike choice between lotteries. Here, we introduce BEAST Gradient Boosting\n(BEAST-GB), a novel hybrid model that synergizes behavioral theories,\nspecifically the model BEAST, with machine learning techniques. First, we show\nthe effectiveness of BEAST-GB by describing CPC18, an open competition for\nprediction of human decision making under risk and uncertainty, in which\nBEAST-GB won. Second, we show that it achieves state-of-the-art performance on\nthe largest publicly available dataset of human risky choice, outperforming\npurely data-driven neural networks, indicating the continued relevance of BEAST\ntheoretical insights in the presence of large data. Third, we demonstrate\nBEAST-GB's superior predictive power in an ensemble of choice experiments in\nwhich the BEAST model alone falters, underscoring the indispensable role of\nmachine learning in interpreting complex idiosyncratic behavioral data.\nFinally, we show BEAST-GB also displays robust domain generalization\ncapabilities as it effectively predicts choice behavior in new experimental\ncontexts that it was not trained on. These results confirm the potency of\ncombining domain-specific theoretical frameworks with machine learning,\nunderscoring a methodological advance with broad implications for modeling\ndecisions in diverse environments.\n"", '  Recently, peoples awareness of online purchases has significantly risen. This\nhas given rise to online retail platforms and the need for a better\nunderstanding of customer purchasing behaviour. Retail companies are pressed\nwith the need to deal with a high volume of customer purchases, which requires\nsophisticated approaches to perform more accurate and efficient customer\nsegmentation. Customer segmentation is a marketing analytical tool that aids\ncustomer-centric service and thus enhances profitability. In this paper, we aim\nto develop a customer segmentation model to improve decision-making processes\nin the retail market industry. To achieve this, we employed a UK-based online\nretail dataset obtained from the UCI machine learning repository. The retail\ndataset consists of 541,909 customer records and eight features. Our study\nadopted the RFM (recency, frequency, and monetary) framework to quantify\ncustomer values. Thereafter, we compared several state-of-the-art (SOTA)\nclustering algorithms, namely, K-means clustering, the Gaussian mixture model\n(GMM), density-based spatial clustering of applications with noise (DBSCAN),\nagglomerative clustering, and balanced iterative reducing and clustering using\nhierarchies (BIRCH). The results showed the GMM outperformed other approaches,\nwith a Silhouette Score of 0.80.\n', ""  Problem definition. In retailing, discrete choice models (DCMs) are commonly\nused to capture the choice behavior of customers when offered an assortment of\nproducts. When estimating DCMs using transaction data, flexible models (such as\nmachine learning models or nonparametric models) are typically not\ninterpretable and hard to estimate, while tractable models (such as the\nmultinomial logit model) tend to misspecify the complex behavior represeted in\nthe data. Methodology/results. In this study, we use a forest of binary\ndecision trees to represent DCMs. This approach is based on random forests, a\npopular machine learning algorithm. The resulting model is interpretable: the\ndecision trees can explain the decision-making process of customers during the\npurchase. We show that our approach can predict the choice probability of any\nDCM consistently and thus never suffers from misspecification. Moreover, our\nalgorithm predicts assortments unseen in the training data. The mechanism and\nerrors can be theoretically analyzed. We also prove that the random forest can\nrecover preference rankings of customers thanks to the splitting criterion such\nas the Gini index and information gain ratio. Managerial implications. The\nframework has unique practical advantages. It can capture customers' behavioral\npatterns such as irrationality or sequential searches when purchasing a\nproduct. It handles nonstandard formats of training data that result from\naggregation. It can measure product importance based on how frequently a random\ncustomer would make decisions depending on the presence of the product. It can\nalso incorporate price information and customer features. Our numerical\nexperiments using synthetic and real data show that using random forests to\nestimate customer choices can outperform existing methods.\n""] , [""  Uplift modeling is a technique used to predict the effect of a treatment\n(e.g., discounts) on an individual's response. Although several methods have\nbeen proposed for multi-valued treatment, they are extended from binary\ntreatment methods. There are still some limitations. Firstly, existing methods\ncalculate uplift based on predicted responses, which may not guarantee a\nconsistent uplift distribution between treatment and control groups. Moreover,\nthis may cause cumulative errors for multi-valued treatment. Secondly, the\nmodel parameters become numerous with many prediction heads, leading to reduced\nefficiency. To address these issues, we propose a novel \\underline{M}ulti-gate\n\\underline{M}ixture-of-Experts based \\underline{M}ulti-valued\n\\underline{T}reatment \\underline{N}etwork (M$^3$TN). M$^3$TN consists of two\ncomponents: 1) a feature representation module with Multi-gate\nMixture-of-Experts to improve the efficiency; 2) a reparameterization module by\nmodeling uplift explicitly to improve the effectiveness. We also conduct\nextensive experiments to demonstrate the effectiveness and efficiency of our\nM$^3$TN.\n"", '  Uplift modeling, vital in online marketing, seeks to accurately measure the\nimpact of various strategies, such as coupons or discounts, on different users\nby predicting the Individual Treatment Effect (ITE). In an e-commerce setting,\nuser behavior follows a defined sequential chain, including impression, click,\nand conversion. Marketing strategies exert varied uplift effects at each stage\nwithin this chain, impacting metrics like click-through and conversion rate.\nDespite its utility, existing research has neglected to consider the inter-task\nacross all stages impacts within a specific treatment and has insufficiently\nutilized the treatment information, potentially introducing substantial bias\ninto subsequent marketing decisions. We identify these two issues as the\nchain-bias problem and the treatment-unadaptive problem. This paper introduces\nthe Entire Chain UPlift method with context-enhanced learning (ECUP), devised\nto tackle these issues. ECUP consists of two primary components: 1) the Entire\nChain-Enhanced Network, which utilizes user behavior patterns to estimate ITE\nthroughout the entire chain space, models the various impacts of treatments on\neach task, and integrates task prior information to enhance context awareness\nacross all stages, capturing the impact of treatment on different tasks, and 2)\nthe Treatment-Enhanced Network, which facilitates fine-grained treatment\nmodeling through bit-level feature interactions, thereby enabling adaptive\nfeature adjustment. Extensive experiments on public and industrial datasets\nvalidate ECUPs effectiveness. Moreover, ECUP has been deployed on the Meituan\nfood delivery platform, serving millions of daily active users, with the\nrelated dataset released for future research.\n', '  Uplift modeling has been widely employed in online marketing by predicting\nthe response difference between the treatment and control groups, so as to\nidentify the sensitive individuals toward interventions like coupons or\ndiscounts. Compared with traditional \\textit{conversion uplift modeling},\n\\textit{revenue uplift modeling} exhibits higher potential due to its direct\nconnection with the corporate income. However, previous works can hardly handle\nthe continuous long-tail response distribution in revenue uplift modeling.\nMoreover, they have neglected to optimize the uplift ranking among different\nindividuals, which is actually the core of uplift modeling. To address such\nissues, in this paper, we first utilize the zero-inflated lognormal (ZILN) loss\nto regress the responses and customize the corresponding modeling network,\nwhich can be adapted to different existing uplift models. Then, we study the\nranking-related uplift modeling error from the theoretical perspective and\npropose two tighter error bounds as the additional loss terms to the\nconventional response regression loss. Finally, we directly model the uplift\nranking error for the entire population with a listwise uplift ranking loss.\nThe experiment results on offline public and industrial datasets validate the\neffectiveness of our method for revenue uplift modeling. Furthermore, we\nconduct large-scale experiments on a prominent online fintech marketing\nplatform, Tencent FiT, which further demonstrates the superiority of our method\nin real-world applications.\n']",Business and Marketing Analytics,Uplift Modeling for Marketing Strategies
176,"Cross-Domain Recommendation Systems , ""Personalized Recommendation Systems with Multi-modal Embeddings"" , Scalable Embeddings for Recommender Systems","['recommender', 'recommendation', 'personalized', 'collaborative', 'domains', 'cdsr', 'interests', 'domain', 'cdr', 'hypergraph'] , ['recommender', 'embeddings', 'personalized', 'embedding', 'recommendation', 'recommendations', 'ranking', 'items', 'modality', 'similarity'] , ['embeddings', 'embedding', 'recommender', 'memory', 'recommendations', 'factorization', 'recommendation', 'storage', 'gpu', 'items']","[""  Cross-domain recommendation (CDR) has been proven as a promising way to\ntackle the user cold-start problem, which aims to make recommendations for\nusers in the target domain by transferring the user preference derived from the\nsource domain. Traditional CDR studies follow the embedding and mapping (EMCDR)\nparadigm, which transfers user representations from the source to target domain\nby learning a user-shared mapping function, neglecting the user-specific\npreference. Recent CDR studies attempt to learn user-specific mapping functions\nin meta-learning paradigm, which regards each user's CDR as an individual task,\nbut neglects the preference correlations among users, limiting the beneficial\ninformation for user representations. Moreover, both of the paradigms neglect\nthe explicit user-item interactions from both domains during the mapping\nprocess. To address the above issues, this paper proposes a novel CDR framework\nwith neural process (NP), termed as CDRNP. Particularly, it develops the\nmeta-learning paradigm to leverage user-specific preference, and further\nintroduces a stochastic process by NP to capture the preference correlations\namong the overlapping and cold-start users, thus generating more powerful\nmapping functions by mapping the user-specific preference and common preference\ncorrelations to a predictive probability distribution. In addition, we also\nintroduce a preference remainer to enhance the common preference from the\noverlapping users, and finally devises an adaptive conditional decoder with\npreference modulation to make prediction for cold-start users with items in the\ntarget domain. Experimental results demonstrate that CDRNP outperforms previous\nSOTA methods in three real-world CDR scenarios.\n"", '  Cross-domain recommendation (CDR) extends conventional recommender systems by\nleveraging user-item interactions from dense domains to mitigate data sparsity\nand the cold start problem. While CDR offers substantial potential for\nenhancing recommendation performance, most existing CDR methods suffer from\nsensitivity to the ratio of overlapping users and intrinsic discrepancy between\nsource and target domains. To overcome these limitations, in this work, we\nexplore the application of graph signal processing (GSP) in CDR scenarios. We\npropose CGSP, a unified CDR framework based on GSP, which employs a\ncross-domain similarity graph constructed by flexibly combining target-only\nsimilarity and source-bridged similarity. By processing personalized graph\nsignals computed for users from either the source or target domain, our\nframework effectively supports both inter-domain and intra-domain\nrecommendations. Our empirical evaluation demonstrates that CGSP consistently\noutperforms various encoder-based CDR approaches in both intra-domain and\ninter-domain recommendation scenarios, especially when the ratio of overlapping\nusers is low, highlighting its significant practical implication in real-world\napplications.\n', ""  Cross-Domain Recommendation (CDR) is a promising paradigm inspired by\ntransfer learning to solve the cold-start problem in recommender systems.\nExisting state-of-the-art CDR methods train an explicit mapping function to\ntransfer the cold-start users from a data-rich source domain to a target\ndomain. However, a limitation of these methods is that the mapping function is\ntrained on overlapping users across domains, while only a small number of\noverlapping users are available for training. By visualizing the loss landscape\nof the existing CDR model, we find that training on a small number of\noverlapping users causes the model to converge to sharp minima, leading to poor\ngeneralization. Based on this observation, we leverage loss-geometry-based\nmachine learning approach and propose a novel CDR method called Sharpness-Aware\nCDR (SCDR). Our proposed method simultaneously optimizes recommendation loss\nand loss sharpness, leading to better generalization with theoretical\nguarantees. Empirical studies on real-world datasets demonstrate that SCDR\nsignificantly outperforms the other CDR models for cold-start recommendation\ntasks, while concurrently enhancing the model's robustness to adversarial\nattacks.\n""] , ['  With the rapid development of online multimedia services, especially in\ne-commerce platforms, there is a pressing need for personalised recommendation\nsystems that can effectively encode the diverse multi-modal content associated\nwith each item. However, we argue that existing multi-modal recommender systems\ntypically use isolated processes for both feature extraction and modality\nmodelling. Such isolated processes can harm the recommendation performance.\nFirstly, an isolated extraction process underestimates the importance of\neffective feature extraction in multi-modal recommendations, potentially\nincorporating non-relevant information, which is harmful to item\nrepresentations. Second, an isolated modality modelling process produces\ndisjointed embeddings for item modalities due to the individual processing of\neach modality, which leads to a suboptimal fusion of user/item representations\nfor effective user preferences prediction. We hypothesise that the use of a\nunified model for addressing both aforementioned isolated processes will enable\nthe consistent extraction and cohesive fusion of joint multi-modal features,\nthereby enhancing the effectiveness of multi-modal recommender systems. In this\npaper, we propose a novel model, called Unified Multi-modal Graph Transformer\n(UGT), which firstly leverages a multi-way transformer to extract aligned\nmulti-modal features from raw data for top-k recommendation. Subsequently, we\nbuild a unified graph neural network in our UGT model to jointly fuse the\nuser/item representations with their corresponding multi-modal features. Using\nthe graph transformer architecture of our UGT model, we show that the UGT model\ncan achieve significant effectiveness gains, especially when jointly optimised\nwith the commonly-used multi-modal recommendation losses.\n', '  Recommendation systems, as widely implemented nowadays on various platforms,\nrecommend relevant items to users based on their preferences. The classical\nmethods which rely on user-item interaction matrices has limitations,\nespecially in scenarios where there is a lack of interaction data for new\nitems. Knowledge graph (KG)-based recommendation systems have emerged as a\npromising solution. However, most KG-based methods adopt node embeddings, which\ndo not provide personalized recommendations for different users and cannot\ngeneralize well to the new items. To address these limitations, we propose\nKnowledge-enhanced User-Centric subgraph Network (KUCNet), a subgraph learning\napproach with graph neural network (GNN) for effective recommendation. KUCNet\nconstructs a U-I subgraph for each user-item pair that captures both the\nhistorical information of user-item interactions and the side information\nprovided in KG. An attention-based GNN is designed to encode the U-I subgraphs\nfor recommendation. Considering efficiency, the pruned user-centric computation\ngraph is further introduced such that multiple U-I subgraphs can be\nsimultaneously computed and that the size can be pruned by Personalized\nPageRank. Our proposed method achieves accurate, efficient, and interpretable\nrecommendations especially for new items. Experimental results demonstrate the\nsuperiority of KUCNet over state-of-the-art KG-based and collaborative\nfiltering (CF)-based methods.\n', '  The explainability of recommendation systems is crucial for enhancing user\ntrust and satisfaction. Leveraging large language models (LLMs) offers new\nopportunities for comprehensive recommendation logic generation. However, in\nexisting related studies, fine-tuning LLM models for recommendation tasks\nincurs high computational costs and alignment issues with existing systems,\nlimiting the application potential of proven proprietary/closed-source LLM\nmodels, such as GPT-4. In this work, our proposed effective strategy LANE\naligns LLMs with online recommendation systems without additional LLMs tuning,\nreducing costs and improving explainability. This innovative approach addresses\nkey challenges in integrating language models with recommendation systems while\nfully utilizing the capabilities of powerful proprietary models. Specifically,\nour strategy operates through several key components: semantic embedding, user\nmulti-preference extraction using zero-shot prompting, semantic alignment, and\nexplainable recommendation generation using Chain of Thought (CoT) prompting.\nBy embedding item titles instead of IDs and utilizing multi-head attention\nmechanisms, our approach aligns the semantic features of user preferences with\nthose of candidate items, ensuring coherent and user-aligned recommendations.\nSufficient experimental results including performance comparison, questionnaire\nvoting, and visualization cases prove that our method can not only ensure\nrecommendation performance, but also provide easy-to-understand and reasonable\nrecommendation logic.\n'] , ['  Recommender systems typically represent users and items by learning their\nembeddings, which are usually set to uniform dimensions and dominate the model\nparameters. However, real-world recommender systems often operate in streaming\nrecommendation scenarios, where the number of users and items continues to\ngrow, leading to substantial storage resource consumption for these embeddings.\nAlthough a few methods attempt to mitigate this by employing embedding size\nsearch strategies to assign different embedding dimensions in streaming\nrecommendations, they assume that the embedding size grows with the frequency\nof users/items, which eventually still exceeds the predefined memory budget\nover time. To address this issue, this paper proposes to learn Scalable\nLightweight Embeddings for streaming recommendation, called SCALL, which can\nadaptively adjust the embedding sizes of users/items within a given memory\nbudget over time. Specifically, we propose to sample embedding sizes from a\nprobabilistic distribution, with the guarantee to meet any predefined memory\nbudget. By fixing the memory budget, the proposed embedding size sampling\nstrategy can increase and decrease the embedding sizes in accordance to the\nfrequency of the corresponding users or items. Furthermore, we develop a\nreinforcement learning-based search paradigm that models each state with mean\npooling to keep the length of the state vectors fixed, invariant to the\nchanging number of users and items. As a result, the proposed method can\nprovide embedding sizes to unseen users and items. Comprehensive empirical\nevaluations on two public datasets affirm the advantageous effectiveness of our\nproposed method.\n', '  At the heart of contemporary recommender systems (RSs) are latent factor\nmodels that provide quality recommendation experience to users. These models\nuse embedding vectors, which are typically of a uniform and fixed size, to\nrepresent users and items. As the number of users and items continues to grow,\nthis design becomes inefficient and hard to scale. Recent lightweight embedding\nmethods have enabled different users and items to have diverse embedding sizes,\nbut are commonly subject to two major drawbacks. Firstly, they limit the\nembedding size search to optimizing a heuristic balancing the recommendation\nquality and the memory complexity, where the trade-off coefficient needs to be\nmanually tuned for every memory budget requested. The implicitly enforced\nmemory complexity term can even fail to cap the parameter usage, making the\nresultant embedding table fail to meet the memory budget strictly. Secondly,\nmost solutions, especially reinforcement learning based ones derive and\noptimize the embedding size for each each user/item on an instance-by-instance\nbasis, which impedes the search efficiency. In this paper, we propose Budgeted\nEmbedding Table (BET), a novel method that generates table-level actions (i.e.,\nembedding sizes for all users and items) that is guaranteed to meet\npre-specified memory budgets. Furthermore, by leveraging a set-based action\nformulation and engaging set representation learning, we present an innovative\naction search strategy powered by an action fitness predictor that efficiently\nevaluates each table-level action. Experiments have shown state-of-the-art\nperformance on two real-world datasets when BET is paired with three popular\nrecommender models under different memory budgets.\n', ""  Recommender models are commonly used to suggest relevant items to a user for\ne-commerce and online advertisement-based applications. These models use\nmassive embedding tables to store numerical representation of items' and users'\ncategorical variables (memory intensive) and employ neural networks (compute\nintensive) to generate final recommendations. Training these large-scale\nrecommendation models is evolving to require increasing data and compute\nresources. The highly parallel neural networks portion of these models can\nbenefit from GPU acceleration however, large embedding tables often cannot fit\nin the limited-capacity GPU device memory. Hence, this paper deep dives into\nthe semantics of training data and obtains insights about the feature access,\ntransfer, and usage patterns of these models. We observe that, due to the\npopularity of certain inputs, the accesses to the embeddings are highly skewed\nwith a few embedding entries being accessed up to 10000x more. This paper\nleverages this asymmetrical access pattern to offer a framework, called FAE,\nand proposes a hot-embedding aware data layout for training recommender models.\nThis layout utilizes the scarce GPU memory for storing the highly accessed\nembeddings, thus reduces the data transfers from CPU to GPU. At the same time,\nFAE engages the GPU to accelerate the executions of these hot embedding\nentries. Experiments on production-scale recommendation models with real\ndatasets show that FAE reduces the overall training time by 2.3x and 1.52x in\ncomparison to XDL CPU-only and XDL CPU-GPU execution while maintaining baseline\naccuracy\n""]",Recommender Systems and Personalization Techniques,Scalable Embeddings for Recommender Systems
177,"Recommender Systems with Advanced Techniques , ""Recommender System Poisoning Attacks and Defenses"" , Deconfounding in Recommender Systems , Recommender Systems and Personalized Recommendations , Explainable Recommender Systems and Predictions","['recommender', 'recommenders', 'personalized', 'conversational', 'recommendation', 'recommendations', 'factorization', 'embeddings', 'attention', 'ranking'] , ['malicious', 'recommender', 'adversary', 'adversarial', 'recommenders', 'attacks', 'vulnerabilities', 'threats', 'attacker', 'attackers'] , ['confounders', 'confounder', 'recommender', 'recommending', 'bias', 'confounding', 'biases', 'recommendation', 'recommendations', 'causald'] , ['recommender', 'recommendation', 'recommendations', 'personalized', 'popularity', 'preference', 'spotify', 'preferences', 'playlists', 'rankings'] , ['recommender', 'recommenders', 'explanations', 'ai', 'predictability', 'explainability', 'visualizations', 'personalised', 'suggesting', 'predictable']","[""  Recently, large language models (LLMs) have shown great potential in\nrecommender systems, either improving existing recommendation models or serving\nas the backbone. However, there exists a large semantic gap between LLMs and\nrecommender systems, since items to be recommended are often indexed by\ndiscrete identifiers (item ID) out of the LLM's vocabulary. In essence, LLMs\ncapture language semantics while recommender systems imply collaborative\nsemantics, making it difficult to sufficiently leverage the model capacity of\nLLMs for recommendation. To address this challenge, in this paper, we propose a\nnew LLM-based recommendation model called LC-Rec, which can better integrate\nlanguage and collaborative semantics for recommender systems. Our approach can\ndirectly generate items from the entire item set for recommendation, without\nrelying on candidate items. Specifically, we make two major contributions in\nour approach. For item indexing, we design a learning-based vector quantization\nmethod with uniform semantic mapping, which can assign meaningful and\nnon-conflicting IDs (called item indices) for items. For alignment tuning, we\npropose a series of specially designed tuning tasks to enhance the integration\nof collaborative semantics in LLMs. Our fine-tuning tasks enforce LLMs to\ndeeply integrate language and collaborative semantics (characterized by the\nlearned item indices), so as to achieve an effective adaptation to recommender\nsystems. Extensive experiments demonstrate the effectiveness of our method,\nshowing that our approach can outperform a number of competitive baselines\nincluding traditional recommenders and existing LLM-based recommenders. Our\ncode is available at https://github.com/RUCAIBox/LC-Rec/.\n"", '  Conversational recommender system (CRS), which combines the techniques of\ndialogue system and recommender system, has obtained increasing interest\nrecently. In contrast to traditional recommender system, it learns the user\npreference better through interactions (i.e. conversations), and then further\nboosts the recommendation performance. However, existing studies on CRS ignore\nto address the relationship among attributes, users, and items effectively,\nwhich might lead to inappropriate questions and inaccurate recommendations. In\nthis view, we propose a knowledge graph based conversational recommender system\n(referred as KG-CRS). Specifically, we first integrate the user-item graph and\nitem-attribute graph into a dynamic graph, i.e., dynamically changing during\nthe dialogue process by removing negative items or attributes. We then learn\ninformative embedding of users, items, and attributes by also considering\npropagation through neighbors on the graph. Extensive experiments on three real\ndatasets validate the superiority of our method over the state-of-the-art\napproaches in terms of both the recommendation and conversation tasks.\n', '  Addressing the challenges related to data sparsity, cold-start problems, and\ndiversity in recommendation systems is both crucial and demanding. Many current\nsolutions leverage knowledge graphs to tackle these issues by combining both\nitem-based and user-item collaborative signals. A common trend in these\napproaches focuses on improving ranking performance at the cost of escalating\nmodel complexity, reducing diversity, and complicating the task. It is\nessential to provide recommendations that are both personalized and diverse,\nrather than solely relying on achieving high rank-based performance, such as\nClick-through Rate, Recall, etc. In this paper, we propose a hybrid multi-task\nlearning approach, training on user-item and item-item interactions. We apply\nitem-based contrastive learning on descriptive text, sampling positive and\nnegative pairs based on item metadata. Our approach allows the model to better\nunderstand the relationships between entities within the knowledge graph by\nutilizing semantic information from text. It leads to more accurate, relevant,\nand diverse user recommendations and a benefit that extends even to cold-start\nusers who have few interactions with items. We perform extensive experiments on\ntwo widely used datasets to validate the effectiveness of our approach. Our\nfindings demonstrate that jointly training user-item interactions and\nitem-based signals using synopsis text is highly effective. Furthermore, our\nresults provide evidence that item-based contrastive learning enhances the\nquality of entity embeddings, as indicated by metrics such as uniformity and\nalignment.\n'] , [""  Modern recommender systems (RS) have profoundly enhanced user experience\nacross digital platforms, yet they face significant threats from poisoning\nattacks. These attacks, aimed at manipulating recommendation outputs for\nunethical gains, exploit vulnerabilities in RS through injecting malicious data\nor intervening model training. This survey presents a unique perspective by\nexamining these threats through the lens of an attacker, offering fresh\ninsights into their mechanics and impacts. Concretely, we detail a systematic\npipeline that encompasses four stages of a poisoning attack: setting attack\ngoals, assessing attacker capabilities, analyzing victim architecture, and\nimplementing poisoning strategies. The pipeline not only aligns with various\nattack tactics but also serves as a comprehensive taxonomy to pinpoint focuses\nof distinct poisoning attacks. Correspondingly, we further classify defensive\nstrategies into two main categories: poisoning data filtering and robust\ntraining from the defender's perspective. Finally, we highlight existing\nlimitations and suggest innovative directions for further exploration in this\nfield.\n"", ""  To make room for privacy and efficiency, the deployment of many recommender\nsystems is experiencing a shift from central servers to personal devices, where\nthe federated recommender systems (FedRecs) and decentralized collaborative\nrecommender systems (DecRecs) are arguably the two most representative\nparadigms. While both leverage knowledge (e.g., gradients) sharing to\nfacilitate learning local models, FedRecs rely on a central server to\ncoordinate the optimization process, yet in DecRecs, the knowledge sharing\ndirectly happens between clients. Knowledge sharing also opens a backdoor for\nmodel poisoning attacks, where adversaries disguise themselves as benign\nclients and disseminate polluted knowledge to achieve malicious goals like\npromoting an item's exposure rate. Although research on such poisoning attacks\nprovides valuable insights into finding security loopholes and corresponding\ncountermeasures, existing attacks mostly focus on FedRecs, and are either\ninapplicable or ineffective for DecRecs. Compared with FedRecs where the\ntampered information can be universally distributed to all clients once\nuploaded to the cloud, each adversary in DecRecs can only communicate with\nneighbor clients of a small size, confining its impact to a limited range. To\nfill the gap, we present a novel attack method named Poisoning with Adaptive\nMalicious Neighbors (PAMN). With item promotion in top-K recommendation as the\nattack objective, PAMN effectively boosts target items' ranks with several\nadversaries that emulate benign clients and transfers adaptively crafted\ngradients conditioned on each adversary's neighbors. Moreover, with the\nvulnerabilities of DecRecs uncovered, a dedicated defensive mechanism based on\nuser-level gradient clipping with sparsified updating is proposed. Extensive\nexperiments demonstrate the effectiveness of the poisoning attack and the\nrobustness of our defensive mechanism.\n"", ""  Recommender systems have become an integral part of online services to help\nusers locate specific information in a sea of data. However, existing studies\nshow that some recommender systems are vulnerable to poisoning attacks,\nparticularly those that involve learning schemes. A poisoning attack is where\nan adversary injects carefully crafted data into the process of training a\nmodel, with the goal of manipulating the system's final recommendations. Based\non recent advancements in artificial intelligence, such attacks have gained\nimportance recently. While numerous countermeasures to poisoning attacks have\nbeen developed, they have not yet been systematically linked to the properties\nof the attacks. Consequently, assessing the respective risks and potential\nsuccess of mitigation strategies is difficult, if not impossible. This survey\naims to fill this gap by primarily focusing on poisoning attacks and their\ncountermeasures. This is in contrast to prior surveys that mainly focus on\nattacks and their detection methods. Through an exhaustive literature review,\nwe provide a novel taxonomy for poisoning attacks, formalise its dimensions,\nand accordingly organise 30+ attacks described in the literature. Further, we\nreview 40+ countermeasures to detect and/or prevent poisoning attacks,\nevaluating their effectiveness against specific types of attacks. This\ncomprehensive survey should serve as a point of reference for protecting\nrecommender systems against poisoning attacks. The article concludes with a\ndiscussion on open issues in the field and impactful directions for future\nresearch. A rich repository of resources associated with poisoning attacks is\navailable at https://github.com/tamlhp/awesome-recsys-poisoning.\n""] , ['  Recommender models aim to capture user preferences from historical feedback\nand then predict user-specific feedback on candidate items. However, the\npresence of various unmeasured confounders causes deviations between the user\npreferences in the historical feedback and the true preferences, resulting in\nmodels not meeting their expected performance. Existing debias models either\n(1) specific to solving one particular bias or (2) directly obtain auxiliary\ninformation from user historical feedback, which cannot identify whether the\nlearned preferences are true user preferences or mixed with unmeasured\nconfounders. Moreover, we find that the former recommender system is not only a\nsuccessor to unmeasured confounders but also acts as an unmeasured confounder\naffecting user preference modeling, which has always been neglected in previous\nstudies. To this end, we incorporate the effect of the former recommender\nsystem and treat it as a proxy for all unmeasured confounders. We propose a\nnovel framework, Separating and Learning Latent Confounders For Recommendation\n(SLFR), which obtains the representation of unmeasured confounders to identify\nthe counterfactual feedback by disentangling user preferences and unmeasured\nconfounders, then guides the target model to capture the true preferences of\nusers. Extensive experiments in five real-world datasets validate the\nadvantages of our method.\n', ""  In recent years, dual-target Cross-Domain Recommendation (CDR) has been\nproposed to capture comprehensive user preferences in order to ultimately\nenhance the recommendation accuracy in both data-richer and data-sparser\ndomains simultaneously. However, in addition to users' true preferences, the\nuser-item interactions might also be affected by confounders (e.g., free\nshipping, sales promotion). As a result, dual-target CDR has to meet two\nchallenges: (1) how to effectively decouple observed confounders, including\nsingle-domain confounders and cross-domain confounders, and (2) how to preserve\nthe positive effects of observed confounders on predicted interactions, while\neliminating their negative effects on capturing comprehensive user preferences.\nTo address the above two challenges, we propose a Causal Deconfounding\nframework via Confounder Disentanglement for dual-target Cross-Domain\nRecommendation, called CD2CDR. In CD2CDR, we first propose a confounder\ndisentanglement module to effectively decouple observed single-domain and\ncross-domain confounders. We then propose a causal deconfounding module to\npreserve the positive effects of such observed confounders and eliminate their\nnegative effects via backdoor adjustment, thereby enhancing the recommendation\naccuracy in each domain. Extensive experiments conducted on five real-world\ndatasets demonstrate that CD2CDR significantly outperforms the state-of-the-art\nmethods.\n"", ""  Recommender systems suffer from confounding biases when there exist\nconfounders affecting both item features and user feedback (e.g., like or not).\nExisting causal recommendation methods typically assume confounders are fully\nobserved and measured, forgoing the possible existence of hidden confounders in\nreal applications. For instance, product quality is a confounder since\naffecting both item prices and user ratings, but is hidden for the third-party\ne-commerce platform due to the difficulty of large-scale quality inspection;\nignoring it could result in the bias effect of over-recommending high-price\nitems. This work analyzes and addresses the problem from a causal perspective.\nThe key lies in modeling the causal effect of item features on a user's\nfeedback. To mitigate hidden confounding effects, it is compulsory but\nchallenging to estimate the causal effect without measuring the confounder.\nTowards this goal, we propose a Hidden Confounder Removal (HCR) framework that\nleverages front-door adjustment to decompose the causal effect into two partial\neffects, according to the mediators between item features and user feedback.\nThe partial effects are independent from the hidden confounder and\nidentifiable. During training, HCR performs multi-task learning to infer the\npartial effects from historical interactions. We instantiate HCR for two\nscenarios and conduct experiments on three real-world datasets. Empirical\nresults show that the HCR framework provides more accurate recommendations,\nespecially for less-active users. We will release the code once accepted.\n""] , [""  Recommender systems usually learn user interests from various user behaviors,\nincluding clicks and post-click behaviors (e.g., like and favorite). However,\nthese behaviors inevitably exhibit popularity bias, leading to some unfairness\nissues: 1) for items with similar quality, more popular ones get more exposure;\nand 2) even worse the popular items with lower popularity might receive more\nexposure. Existing work on mitigating popularity bias blindly eliminates the\nbias and usually ignores the effect of item quality. We argue that the\nrelationships between different user behaviors (e.g., conversion rate) actually\nreflect the item quality. Therefore, to handle the unfairness issues, we\npropose to mitigate the popularity bias by considering multiple user behaviors.\n  In this work, we examine causal relationships behind the interaction\ngeneration procedure in multi-behavior recommendation. Specifically, we find\nthat: 1) item popularity is a confounder between the exposed items and users'\npost-click interactions, leading to the first unfairness; and 2) some hidden\nconfounders (e.g., the reputation of item producers) affect both item\npopularity and quality, resulting in the second unfairness. To alleviate these\nconfounding issues, we propose a causal framework to estimate the causal\neffect, which leverages backdoor adjustment to block the backdoor paths caused\nby the confounders. In the inference stage, we remove the negative effect of\npopularity and utilize the good effect of quality for recommendation.\nExperiments on two real-world datasets validate the effectiveness of our\nproposed framework, which enhances fairness without sacrificing recommendation\naccuracy.\n"", ""  Recommender systems can automatically recommend users with items that they\nprobably like. The goal of them is to model the user-item interaction by\neffectively representing the users and items. Existing methods have primarily\nlearned the user's preferences and item's features with vectorized embeddings,\nand modeled the user's general preferences to items by the interaction of them.\nIn fact, users have their specific preferences to item attributes and different\npreferences are usually related. Therefore, exploring the fine-grained\npreferences as well as modeling the relationships among user's different\npreferences could improve the recommendation performance. Toward this end, we\npropose a dual preference distribution learning framework (DUPLE), which aims\nto jointly learn a general preference distribution and a specific preference\ndistribution for a given user, where the former corresponds to the user's\ngeneral preference to items and the latter refers to the user's specific\npreference to item attributes. Notably, the mean vector of each Gaussian\ndistribution can capture the user's preferences, and the covariance matrix can\nlearn their relationship. Moreover, we can summarize a preferred attribute\nprofile for each user, depicting his/her preferred item attributes. We then can\nprovide the explanation for each recommended item by checking the overlap\nbetween its attributes and the user's preferred attribute profile. Extensive\nquantitative and qualitative experiments on six public datasets demonstrate the\neffectiveness and explainability of the DUPLE method.\n"", ""  Recommendation systems are widespread, and through customized\nrecommendations, promise to match users with options they will like. To that\nend, data on engagement is collected and used. Most recommendation systems are\nranking-based, where they rank and recommend items based on their predicted\nengagement. However, the engagement signals are often only a crude proxy for\nutility, as data on the latter is rarely collected or available. This paper\nexplores the following question: By optimizing for measurable proxies, are\nrecommendation systems at risk of significantly under-delivering on utility? If\nso, how can one improve utility which is seldom measured? To study these\nquestions, we introduce a model of repeated user consumption in which, at each\ninteraction, users select between an outside option and the best option from a\nrecommendation set. Our model accounts for user heterogeneity, with the\nmajority preferring ``popular'' content, and a minority favoring ``niche''\ncontent. The system initially lacks knowledge of individual user preferences\nbut can learn them through observations of users' choices over time. Our\ntheoretical and numerical analysis demonstrate that optimizing for engagement\ncan lead to significant utility losses. Instead, we propose a utility-aware\npolicy that initially recommends a mix of popular and niche content. As the\nplatform becomes more forward-looking, our utility-aware policy achieves the\nbest of both worlds: near-optimal utility and near-optimal engagement\nsimultaneously. Our study elucidates an important feature of recommendation\nsystems; given the ability to suggest multiple items, one can perform\nsignificant exploration without incurring significant reductions in engagement.\nBy recommending high-risk, high-reward items alongside popular items, systems\ncan enhance discovery of high utility items without significantly affecting\nengagement.\n""] , [""  Recent years have witnessed a rapid growth of recommender systems, providing\nsuggestions in numerous applications with potentially high social impact, such\nas health or justice. Meanwhile, in Europe, the upcoming AI Act mentions\n\\emph{transparency} as a requirement for critical AI systems in order to\n``mitigate the risks to fundamental rights''. Post-hoc explanations seamlessly\nalign with this goal and extensive literature on the subject produced several\nforms of such objects, graphs being one of them. Early studies in visualization\ndemonstrated the graphs' ability to improve user understanding, positioning\nthem as potentially ideal explanations. However, it remains unclear how\ngraph-based explanations compare to other explanation designs. In this work, we\naim to determine the effectiveness of graph-based explanations in improving\nusers' perception of AI-based recommendations using a mixed-methods approach.\nWe first conduct a qualitative study to collect users' requirements for graph\nexplanations. We then run a larger quantitative study in which we evaluate the\ninfluence of various explanation designs, including enhanced graph-based ones,\non aspects such as understanding, usability and curiosity toward the AI system.\nWe find that users perceive graph-based explanations as more usable than\ndesigns involving feature importance. However, we also reveal that textual\nexplanations lead to higher objective understanding than graph-based designs.\nMost importantly, we highlight the strong contrast between participants'\nexpressed preferences for graph design and their actual ratings using it, which\nare lower compared to textual design. These findings imply that meeting\nstakeholders' expressed preferences might not alone guarantee ``good''\nexplanations. Therefore, crafting hybrid designs successfully balancing social\nexpectations with downstream performance emerges as a significant challenge.\n"", '  Providing system-generated explanations for recommendations represents an\nimportant step towards transparent and trustworthy recommender systems.\nExplainable recommender systems provide a human-understandable rationale for\ntheir outputs. Over the last two decades, explainable recommendation has\nattracted much attention in the recommender systems research community. This\npaper aims to provide a comprehensive review of research efforts on visual\nexplanation in recommender systems. More concretely, we systematically review\nthe literature on explanations in recommender systems based on four dimensions,\nnamely explanation goal, explanation scope, explanation style, and explanation\nformat. Recognizing the importance of visualization, we approach the\nrecommender system literature from the angle of explanatory visualizations,\nthat is using visualizations as a display style of explanation. As a result, we\nderive a set of guidelines that might be constructive for designing explanatory\nvisualizations in recommender systems and identify perspectives for future work\nin this field. The aim of this review is to help recommendation researchers and\npractitioners better understand the potential of visually explainable\nrecommendation research and to support them in the systematic design of visual\nexplanations in current and future recommender systems.\n', ""  In recent years, predicting mobile app usage has become increasingly\nimportant for areas like app recommendation, user behaviour analysis, and\nmobile resource management. Existing models, however, struggle with the\nheterogeneous nature of contextual data and the user cold start problem. This\nstudy introduces a novel prediction model, Mobile App Prediction Leveraging\nLarge Language Model Embeddings (MAPLE), which employs Large Language Models\n(LLMs) and installed app similarity to overcome these challenges. MAPLE\nutilises the power of LLMs to process contextual data and discern intricate\nrelationships within it effectively. Additionally, we explore the use of\ninstalled app similarity to address the cold start problem, facilitating the\nmodelling of user preferences and habits, even for new users with limited\nhistorical data. In essence, our research presents MAPLE as a novel, potent,\nand practical approach to app usage prediction, making significant strides in\nresolving issues faced by existing models. MAPLE stands out as a comprehensive\nand effective solution, setting a new benchmark for more precise and\npersonalised app usage predictions. In tests on two real-world datasets, MAPLE\nsurpasses contemporary models in both standard and cold start scenarios. These\noutcomes validate MAPLE's capacity for precise app usage predictions and its\nresilience against the cold start problem. This enhanced performance stems from\nthe model's proficiency in capturing complex temporal patterns and leveraging\ncontextual information. As a result, MAPLE can potentially improve personalised\nmobile app usage predictions and user experiences markedly.\n""]",Advances in Recommender Systems,Recommender Systems and Personalized Recommendations
178,"Fairness in Machine Learning Systems , Fairness in Recommender Systems , Fairness in Graph Neural Networks , Fairness in Learning to Rank Models , Algorithmic Recourse and Fairness","['fairness', 'unfairness', 'discrimination', 'discriminatory', 'bias', 'unfair', 'biases', 'equalized', 'classifiers', 'classifier'] , ['fairness', 'unfairness', 'recommender', 'unfair', 'biases', 'discrimination', 'incentives', 'rankings', 'personalization', 'equitable'] , ['fairness', 'graphpar', 'bias', 'discriminatory', 'networks', 'graphgini', 'biases', 'unfair', 'unfairness', 'fairgb'] , ['ranking', 'rankings', 'rank', 'fairness', 'bias', 'merit', 'unfair', 'utility', 'predictors', 'redistribution'] , ['recourses', 'recourse', 'algorithmic', 'unfairness', 'algorithms', 'decisions', 'predictive', 'bias', 'allocation', 'fairness']","['  While significant advancements have been made in the field of fair machine\nlearning, the majority of studies focus on scenarios where the decision model\noperates on a static population. In this paper, we study fairness in dynamic\nsystems where sequential decisions are made. Each decision may shift the\nunderlying distribution of features or user behavior. We model the dynamic\nsystem through a Markov Decision Process (MDP). By acknowledging that\ntraditional fairness notions and long-term fairness are distinct requirements\nthat may not necessarily align with one another, we propose an algorithmic\nframework to integrate various fairness considerations with reinforcement\nlearning using both pre-processing and in-processing approaches. Three case\nstudies show that our method can strike a balance between traditional fairness\nnotions, long-term fairness, and utility.\n', '  Training supervised machine learning systems with a fairness loss can improve\nprediction fairness across different demographic groups. However, doing so\nrequires demographic annotations for training data, without which we cannot\nproduce debiased classifiers for most tasks. Drawing inspiration from transfer\nlearning methods, we investigate whether we can utilize demographic data from a\nrelated task to improve the fairness of a target task. We adapt a single-task\nfairness loss to a multi-task setting to exploit demographic labels from a\nrelated task in debiasing a target task and demonstrate that demographic\nfairness objectives transfer fairness within a multi-task framework.\nAdditionally, we show that this approach enables intersectional fairness by\ntransferring between two datasets with different single-axis demographics. We\nexplore different data domains to show how our loss can improve fairness\ndomains and tasks.\n', '  With the introduction of machine learning in high-stakes decision making,\nensuring algorithmic fairness has become an increasingly important problem to\nsolve. In response to this, many mathematical definitions of fairness have been\nproposed, and a variety of optimisation techniques have been developed, all\ndesigned to maximise a defined notion of fairness. However, fair solutions are\nreliant on the quality of the training data, and can be highly sensitive to\nnoise. Recent studies have shown that robustness (the ability for a model to\nperform well on unseen data) plays a significant role in the type of strategy\nthat should be used when approaching a new problem and, hence, measuring the\nrobustness of these strategies has become a fundamental problem. In this work,\nwe therefore propose a new criterion to measure the robustness of various\nfairness optimisation strategies - the robustness ratio. We conduct multiple\nextensive experiments on five bench mark fairness data sets using three of the\nmost popular fairness strategies with respect to four of the most popular\ndefinitions of fairness. Our experiments empirically show that fairness methods\nthat rely on threshold optimisation are very sensitive to noise in all the\nevaluated data sets, despite mostly outperforming other methods. This is in\ncontrast to the other two methods, which are less fair for low noise scenarios\nbut fairer for high noise ones. To the best of our knowledge, we are the first\nto quantitatively evaluate the robustness of fairness optimisation strategies.\nThis can potentially can serve as a guideline in choosing the most suitable\nfairness strategy for various data sets.\n'] , ['  User-side group fairness is crucial for modern recommender systems, aiming to\nalleviate performance disparities among user groups defined by sensitive\nattributes like gender, race, or age. In the ever-evolving landscape of\nuser-item interactions, continual adaptation to newly collected data is crucial\nfor recommender systems to stay aligned with the latest user preferences.\nHowever, we observe that such continual adaptation often exacerbates\nperformance disparities. This necessitates a thorough investigation into\nuser-side fairness in dynamic recommender systems, an area that has been\nunexplored in the literature. This problem is challenging due to distribution\nshifts, frequent model updates, and non-differentiability of ranking metrics.\nTo our knowledge, this paper presents the first principled study on ensuring\nuser-side fairness in dynamic recommender systems. We start with theoretical\nanalyses on fine-tuning v.s. retraining, showing that the best practice is\nincremental fine-tuning with restart. Guided by our theoretical analyses, we\npropose FAir Dynamic rEcommender (FADE), an end-to-end fine-tuning framework to\ndynamically ensure user-side fairness over time. To overcome the\nnon-differentiability of recommendation metrics in the fairness loss, we\nfurther introduce Differentiable Hit (DH) as an improvement over the recent\nNeuralNDCG method, not only alleviating its gradient vanishing issue but also\nachieving higher efficiency. Besides that, we also address the instability\nissue of the fairness loss by leveraging the competing nature between the\nrecommendation loss and the fairness loss. Through extensive experiments on\nreal-world datasets, we demonstrate that FADE effectively and efficiently\nreduces performance disparities with little sacrifice in the overall\nrecommendation performance.\n', '  Efforts in the recommendation community are shifting from the sole emphasis\non utility to considering beyond-utility factors, such as fairness and\nrobustness. Robustness of recommendation models is typically linked to their\nability to maintain the original utility when subjected to attacks. Limited\nresearch has explored the robustness of a recommendation model in terms of\nfairness, e.g., the parity in performance across groups, under attack\nscenarios. In this paper, we aim to assess the robustness of graph-based\nrecommender systems concerning fairness, when exposed to attacks based on\nedge-level perturbations. To this end, we considered four different fairness\noperationalizations, including both consumer and provider perspectives.\nExperiments on three datasets shed light on the impact of perturbations on the\ntargeted fairness notion, uncovering key shortcomings in existing evaluation\nprotocols for robustness. As an example, we observed perturbations affect\nconsumer fairness on a higher extent than provider fairness, with alarming\nunfairness for the former. Source code:\nhttps://github.com/jackmedda/CPFairRobust\n', ""  In large-scale recommendation systems, the vast array of items makes it\ninfeasible to obtain accurate user preferences for each product, resulting in a\ncommon issue of missing labels. Typically, only items previously recommended to\nusers have associated ground truth data. Although there is extensive research\non fairness concerning fully observed user-item interactions, the challenge of\nfairness in scenarios with missing labels remains underexplored. Previous\nmethods often treat these samples missing labels as negative, which can\nsignificantly deviate from the ground truth fairness metrics. Our study\naddresses this gap by proposing a novel method employing a small randomized\ntraffic to estimate fairness metrics accurately. We present theoretical bounds\nfor the estimation error of our fairness metric and support our findings with\nempirical evidence on real data. Our numerical experiments on synthetic and\nTikTok's real-world data validate our theory and show the efficiency and\neffectiveness of our novel methods. To the best of our knowledge, we are the\nfirst to emphasize the necessity of random traffic in dataset collection for\nrecommendation fairness, the first to publish a fairness-related dataset from\nTikTok and to provide reliable estimates of fairness metrics in the context of\nlarge-scale recommendation systems with missing labels.\n""] , ['  Fairness-aware graph learning has gained increasing attention in recent\nyears. Nevertheless, there lacks a comprehensive benchmark to evaluate and\ncompare different fairness-aware graph learning methods, which blocks\npractitioners from choosing appropriate ones for broader real-world\napplications. In this paper, we present an extensive benchmark on ten\nrepresentative fairness-aware graph learning methods. Specifically, we design a\nsystematic evaluation protocol and conduct experiments on seven real-world\ndatasets to evaluate these methods from multiple perspectives, including group\nfairness, individual fairness, the balance between different fairness criteria,\nand computational efficiency. Our in-depth analysis reveals key insights into\nthe strengths and limitations of existing methods. Additionally, we provide\npractical guidance for applying fairness-aware graph learning methods in\napplications. To the best of our knowledge, this work serves as an initial step\ntowards comprehensively understanding representative fairness-aware graph\nlearning methods to facilitate future advancements in this area.\n', '  Graph Neural Networks (GNNs) have been widely used for various types of graph\ndata processing and analytical tasks in different domains. Training GNNs over\ncentralized graph data can be infeasible due to privacy concerns and regulatory\nrestrictions. Thus, federated learning (FL) becomes a trending solution to\naddress this challenge in a distributed learning paradigm. However, as GNNs may\ninherit historical bias from training data and lead to discriminatory\npredictions, the bias of local models can be easily propagated to the global\nmodel in distributed settings. This poses a new challenge in mitigating bias in\nfederated GNNs. To address this challenge, we propose $\\text{F}^2$GNN, a Fair\nFederated Graph Neural Network, that enhances group fairness of federated GNNs.\nAs bias can be sourced from both data and learning algorithms, $\\text{F}^2$GNN\naims to mitigate both types of bias under federated settings. First, we provide\ntheoretical insights on the connection between data bias in a training graph\nand statistical fairness metrics of the trained GNN models. Based on the\ntheoretical analysis, we design $\\text{F}^2$GNN which contains two key\ncomponents: a fairness-aware local model update scheme that enhances group\nfairness of the local models on the client side, and a fairness-weighted global\nmodel update scheme that takes both data bias and fairness metrics of local\nmodels into consideration in the aggregation process. We evaluate\n$\\text{F}^2$GNN empirically versus a number of baseline methods, and\ndemonstrate that $\\text{F}^2$GNN outperforms these baselines in terms of both\nfairness and model accuracy.\n', '  Graph neural networks (GNNs) have emerged as a powerful tool for analyzing\nand learning from complex data structured as graphs, demonstrating remarkable\neffectiveness in various applications, such as social network analysis,\nrecommendation systems, and drug discovery. However, despite their impressive\nperformance, the fairness problem has increasingly gained attention as a\ncrucial aspect to consider. Existing research in graph learning focuses on\neither group fairness or individual fairness. However, since each concept\nprovides unique insights into fairness from distinct perspectives, integrating\nthem into a fair graph neural network system is crucial. To the best of our\nknowledge, no study has yet to comprehensively tackle both individual and group\nfairness simultaneously. In this paper, we propose a new concept of individual\nfairness within groups and a novel framework named Fairness for Group and\nIndividual (FairGI), which considers both group fairness and individual\nfairness within groups in the context of graph learning. FairGI employs the\nsimilarity matrix of individuals to achieve individual fairness within groups,\nwhile leveraging adversarial learning to address group fairness in terms of\nboth Equal Opportunity and Statistical Parity. The experimental results\ndemonstrate that our approach not only outperforms other state-of-the-art\nmodels in terms of group fairness and individual fairness within groups, but\nalso exhibits excellent performance in population-level individual fairness,\nwhile maintaining comparable prediction accuracy.\n'] , ['  Learning to Rank (LTR) is one of the most widely used machine learning\napplications. It is a key component in platforms with profound societal\nimpacts, including job search, healthcare information retrieval, and social\nmedia content feeds. Conventional LTR models have been shown to produce biases\nresults, stimulating a discourse on how to address the disparities introduced\nby ranking systems that solely prioritize user relevance. However, while\nseveral models of fair learning to rank have been proposed, they suffer from\ndeficiencies either in accuracy or efficiency, thus limiting their\napplicability to real-world ranking platforms. This paper shows how\nefficiently-solvable fair ranking models, based on the optimization of Ordered\nWeighted Average (OWA) functions, can be integrated into the training loop of\nan LTR model to achieve favorable balances between fairness, user utility, and\nruntime efficiency. In particular, this paper is the first to show how to\nbackpropagate through constrained optimizations of OWA objectives, enabling\ntheir use in integrated prediction and decision models.\n', '  Stochastic learning to rank (LTR) is a recent branch in the LTR field that\nconcerns the optimization of probabilistic ranking models. Their probabilistic\nbehavior enables certain ranking qualities that are impossible with\ndeterministic models. For example, they can increase the diversity of displayed\ndocuments, increase fairness of exposure over documents, and better balance\nexploitation and exploration through randomization. A core difficulty in LTR is\ngradient estimation, for this reason, existing stochastic LTR methods have been\nlimited to differentiable ranking models (e.g., neural networks). This is in\nstark contrast with the general field of LTR where Gradient Boosted Decision\nTrees (GBDTs) have long been considered the state-of-the-art. In this work, we\naddress this gap by introducing the first stochastic LTR method for GBDTs. Our\nmain contribution is a novel estimator for the second-order derivatives, i.e.,\nthe Hessian matrix, which is a requirement for effective GBDTs. To efficiently\ncompute both the first and second-order derivatives simultaneously, we\nincorporate our estimator into the existing PL-Rank framework, which was\noriginally designed for first-order derivatives only. Our experimental results\nindicate that stochastic LTR without the Hessian has extremely poor\nperformance, whilst the performance is competitive with the current\nstate-of-the-art with our estimated Hessian. Thus, through the contribution of\nour novel Hessian estimation method, we have successfully introduced GBDTs to\nstochastic LTR.\n', '  Learning to Rank (LTR) methods are vital in online economies, affecting users\nand item providers. Fairness in LTR models is crucial to allocate exposure\nproportionally to item relevance. Widely used deterministic LTR models can lead\nto unfair exposure distribution, especially when items with the same relevance\nreceive slightly different ranking scores. Stochastic LTR models, incorporating\nthe Plackett-Luce (PL) ranking model, address fairness issues but suffer from\nhigh training cost. In addition, they cannot provide guarantees on the utility\nor fairness, which can lead to dramatic degraded utility when optimized for\nfairness. To overcome these limitations, we propose Inference-time Stochastic\nRanking with Risk Control (ISRR), a novel method that performs stochastic\nranking at inference time with guanranteed utility or fairness given pretrained\nscoring functions from deterministic or stochastic LTR models. Comprehensive\nexperimental results on three widely adopted datasets demonstrate that our\nproposed method achieves utility and fairness comparable to existing stochastic\nranking methods with much lower computational cost. In addition, results verify\nthat our method provides finite-sample guarantee on utility and fairness. This\nadvancement represents a significant contribution to the field of stochastic\nranking and fair LTR with promising real-world applications.\n'] , ['  Algorithmic recourse provides explanations that help users overturn an\nunfavorable decision by a machine learning system. But so far very little\nattention has been paid to whether providing recourse is beneficial or not. We\nintroduce an abstract learning-theoretic framework that compares the risks\n(i.e., expected losses) for classification with and without algorithmic\nrecourse. This allows us to answer the question of when providing recourse is\nbeneficial or harmful at the population level. Surprisingly, we find that there\nare many plausible scenarios in which providing recourse turns out to be\nharmful, because it pushes users to regions of higher class uncertainty and\ntherefore leads to more mistakes. We further study whether the party deploying\nthe classifier has an incentive to strategize in anticipation of having to\nprovide recourse, and we find that sometimes they do, to the detriment of their\nusers. Providing algorithmic recourse may therefore also be harmful at the\nsystemic level. We confirm our theoretical findings in experiments on simulated\nand real-world data. All in all, we conclude that the current concept of\nalgorithmic recourse is not reliably beneficial, and therefore requires\nrethinking.\n', '  With the growing use of machine learning (ML) models in critical domains such\nas finance and healthcare, the need to offer recourse for those adversely\naffected by the decisions of ML models has become more important; individuals\nought to be provided with recommendations on actions to take for improving\ntheir situation and thus receiving a favorable decision. Prior work on\nsequential algorithmic recourse -- which recommends a series of changes --\nfocuses on action feasibility and uses the proximity of feature changes to\ndetermine action costs. However, the uncertainties of feature changes and the\nrisk of higher than average costs in recourse have not been considered. It is\nundesirable if a recourse could (with some probability) result in a worse\nsituation from which recovery requires an extremely high cost. It is essential\nto incorporate risks when computing and evaluating recourse. We call the\nrecourse computed with such risk considerations as Safe Algorithmic Recourse\n(SafeAR). The objective is to empower people to choose a recourse based on\ntheir risk tolerance. In this work, we discuss and show how existing recourse\ndesiderata can fail to capture the risk of higher costs. We present a method to\ncompute recourse policies that consider variability in cost and connect\nalgorithmic recourse literature with risk-sensitive reinforcement learning. We\nalso adopt measures ""Value at Risk"" and ""Conditional Value at Risk"" from the\nfinancial literature to summarize risk concisely. We apply our method to two\nreal-world datasets and compare policies with different risk-aversion levels\nusing risk measures and recourse desiderata (sparsity and proximity).\n', ""  Algorithmic recourse -- providing recommendations to those affected\nnegatively by the outcome of an algorithmic system on how they can take action\nand change that outcome -- has gained attention as a means of giving persons\nagency in their interactions with artificial intelligence (AI) systems. Recent\nwork has shown that even if an AI decision-making classifier is ``fair''\n(according to some reasonable criteria), recourse itself may be unfair due to\ndifferences in the initial circumstances of individuals, compounding\ndisparities for marginalized populations and requiring them to exert more\neffort than others. There is a need to define more methods and metrics for\nevaluating fairness in recourse that span a range of normative views of the\nworld, and specifically those that take into account time. Time is a critical\nelement in recourse because the longer it takes an individual to act, the more\nthe setting may change due to model or data drift.\n  This paper seeks to close this research gap by proposing two notions of\nfairness in recourse that are in normative alignment with substantive equality\nof opportunity, and that consider time. The first considers the (often\nrepeated) effort individuals exert per successful recourse event, and the\nsecond considers time per successful recourse event. Building upon an\nagent-based framework for simulating recourse, this paper demonstrates how much\neffort is needed to overcome disparities in initial circumstances. We then\nproposes an intervention to improve the fairness of recourse by rewarding\neffort, and compare it to existing strategies.\n""]",Fairness in Artificial Intelligence and Machine Learning,Fairness in Machine Learning Systems
179,"Causal Discovery and Inference from Data , Counterfactual Explanations for Machine Learning Models , Counterfactual Inference and Causal Modeling , Causal Inference and Counterfactual Reasoning , Counterfactual Image Explanations","['causal', 'causality', 'causally', 'confounders', 'unobserved', 'inference', 'discovery', 'observational', 'data', 'inferring'] , ['counterfactuals', 'counterfactual', 'explanations', 'classifier', 'robustness', 'explaining', 'interpretable', 'predictions', 'robust', 'algorithmic'] , ['counterfactual_interventions', 'counterfactuals', 'counterfactual', 'causality', 'causal', 'causation', 'inference', 'outcomes', 'consistency', 'interventions'] , ['counterfactuals', 'counterfactual', 'causality', 'causal', 'counterfactually', 'inference', 'reasoning', 'interventions', 'interventional', 'discrimination'] , ['generative', 'counterfactuals', 'counterfactual', 'images', 'causal', 'leveraging', 'causaldiffae', 'explanations', 'classifiers', 'inference']","['  The ability to understand causality from data is one of the major milestones\nof human-level intelligence. Causal Discovery (CD) algorithms can identify the\ncause-effect relationships among the variables of a system from related\nobservational data with certain assumptions. Over the years, several methods\nhave been developed primarily based on the statistical properties of data to\nuncover the underlying causal mechanism. In this study, we present an extensive\ndiscussion on the methods designed to perform causal discovery from both\nindependent and identically distributed (I.I.D.) data and time series data. For\nthis purpose, we first introduce the common terminologies used in causal\ndiscovery literature and then provide a comprehensive discussion of the\nalgorithms designed to identify causal relations in different settings. We\nfurther discuss some of the benchmark datasets available for evaluating the\nalgorithmic performance, off-the-shelf tools or software packages to perform\ncausal discovery readily, and the common metrics used to evaluate these\nmethods. We also evaluate some widely used causal discovery algorithms on\nmultiple benchmark datasets and compare their performances. Finally, we\nconclude by discussing the research challenges and the applications of causal\ndiscovery algorithms in multiple areas of interest.\n', ""  Causal discovery from observational data holds great promise, but existing\nmethods rely on strong assumptions about the underlying causal structure, often\nrequiring full observability of all relevant variables. We tackle these\nchallenges by leveraging the score function $\\nabla \\log p(X)$ of observed\nvariables for causal discovery and propose the following contributions. First,\nwe generalize the existing results of identifiability with the score to\nadditive noise models with minimal requirements on the causal mechanisms.\nSecond, we establish conditions for inferring causal relations from the score\neven in the presence of hidden variables; this result is two-faced: we\ndemonstrate the score's potential as an alternative to conditional independence\ntests to infer the equivalence class of causal graphs with hidden variables,\nand we provide the necessary conditions for identifying direct causes in latent\nvariable models. Building on these insights, we propose a flexible algorithm\nfor causal discovery across linear, nonlinear, and latent variable models,\nwhich we empirically validate.\n"", '  This paper proposes techniques to enhance the performance of non-causal\nmodels for causal inference using data from randomized experiments. In domains\nlike advertising, customer retention, and precision medicine, non-causal models\nthat predict outcomes under no intervention are often used to score individuals\nand rank them according to the expected effectiveness of an intervention (e.g,\nan ad, a retention incentive, a nudge). However, these scores may not perfectly\ncorrespond to intervention effects due to the inherent non-causal nature of the\nmodels. To address this limitation, we propose causal fine-tuning and effect\ncalibration, two techniques that leverage experimental data to refine the\noutput of non-causal models for different causal tasks, including effect\nestimation, effect ordering, and effect classification. They are underpinned by\ntwo key advantages. First, they can effectively integrate the predictive\ncapabilities of general non-causal models with the requirements of a causal\ntask in a specific context, allowing decision makers to support diverse causal\napplications with a ""foundational"" scoring model. Second, through simulations\nand an empirical example, we demonstrate that they can outperform the\nalternative of building a causal-effect model from scratch, particularly when\nthe available experimental data is limited and the non-causal scores already\ncapture substantial information about the relative sizes of causal effects.\nOverall, this research underscores the practical advantages of combining\nexperimental data with non-causal models to support causal applications.\n'] , ['  Counterfactual explanations describe how to modify a feature vector in order\nto flip the outcome of a trained classifier. Obtaining robust counterfactual\nexplanations is essential to provide valid algorithmic recourse and meaningful\nexplanations. We study the robustness of explanations of randomized ensembles,\nwhich are always subject to algorithmic uncertainty even when the training data\nis fixed. We formalize the generation of robust counterfactual explanations as\na probabilistic problem and show the link between the robustness of ensemble\nmodels and the robustness of base learners. We develop a practical method with\ngood empirical performance and support it with theoretical guarantees for\nensembles of convex base learners. Our results show that existing methods give\nsurprisingly low robustness: the validity of naive counterfactuals is below\n$50\\%$ on most data sets and can fall to $20\\%$ on problems with many features.\nIn contrast, our method achieves high robustness with only a small increase in\nthe distance from counterfactual explanations to their initial observations.\n', '  The accuracy and understandability of bank failure prediction models are\ncrucial. While interpretable models like logistic regression are favored for\ntheir explainability, complex models such as random forest, support vector\nmachines, and deep learning offer higher predictive performance but lower\nexplainability. These models, known as black boxes, make it difficult to derive\nactionable insights. To address this challenge, using counterfactual\nexplanations is suggested. These explanations demonstrate how changes in input\nvariables can alter the model output and suggest ways to mitigate bank failure\nrisk. The key challenge lies in selecting the most effective method for\ngenerating useful counterfactuals, which should demonstrate validity,\nproximity, sparsity, and plausibility. The paper evaluates several\ncounterfactual generation methods: WhatIf, Multi Objective, and Nearest\nInstance Counterfactual Explanation, and also explores resampling methods like\nundersampling, oversampling, SMOTE, and the cost sensitive approach to address\ndata imbalance in bank failure prediction in the US. The results indicate that\nthe Nearest Instance Counterfactual Explanation method yields higher quality\ncounterfactual explanations, mainly using the cost sensitive approach. Overall,\nthe Multi Objective Counterfactual and Nearest Instance Counterfactual\nExplanation methods outperform others regarding validity, proximity, and\nsparsity metrics, with the cost sensitive approach providing the most desirable\ncounterfactual explanations. These findings highlight the variability in the\nperformance of counterfactual generation methods across different balancing\nstrategies and machine learning models, offering valuable strategies to enhance\nthe utility of black box bank failure prediction models.\n', ""  In the past decade, we have experienced a massive boom in the usage of\ndigital solutions in higher education. Due to this boom, large amounts of data\nhave enabled advanced data analysis methods to support learners and examine\nlearning processes. One of the dominant research directions in learning\nanalytics is predictive modeling of learners' success using various machine\nlearning methods. To build learners' and teachers' trust in such methods and\nsystems, exploring the methods and methodologies that enable relevant\nstakeholders to deeply understand the underlying machine-learning models is\nnecessary. In this context, counterfactual explanations from explainable\nmachine learning tools are promising. Several counterfactual generation methods\nhold much promise, but the features must be actionable and causal to be\neffective. Thus, obtaining which counterfactual generation method suits the\nstudent success prediction models in terms of desiderata, stability, and\nrobustness is essential. Although a few studies have been published in recent\nyears on the use of counterfactual explanations in educational sciences, they\nhave yet to discuss which counterfactual generation method is more suitable for\nthis problem. This paper analyzed the effectiveness of commonly used\ncounterfactual generation methods, such as WhatIf Counterfactual Explanations,\nMulti-Objective Counterfactual Explanations, and Nearest Instance\nCounterfactual Explanations after balancing. This contribution presents a case\nstudy using the Open University Learning Analytics dataset to demonstrate the\npractical usefulness of counterfactual explanations. The results illustrate the\nmethod's effectiveness and describe concrete steps that could be taken to alter\nthe model's prediction.\n""] , ['  The capacity to address counterfactual ""what if"" inquiries is crucial for\nunderstanding and making use of causal influences. Traditional counterfactual\ninference, under Pearls\' counterfactual framework, typically depends on having\naccess to or estimating a structural causal model. Yet, in practice, this\ncausal model is often unknown and might be challenging to identify. Hence, this\npaper aims to perform reliable counterfactual inference based solely on\nobservational data and the (learned) qualitative causal structure, without\nnecessitating a predefined causal model or even direct estimations of\nconditional distributions. To this end, we establish a novel connection between\ncounterfactual inference and quantile regression and show that counterfactual\ninference can be reframed as an extended quantile regression problem. Building\non this insight, we propose a practical framework for efficient and effective\ncounterfactual inference implemented with neural networks under a bi-level\noptimization scheme. The proposed approach enhances the capacity to generalize\nestimated counterfactual outcomes to unseen data, thereby providing an upper\nbound on the generalization error. Furthermore, empirical evidence demonstrates\nits superior statistical efficiency in comparison to existing methods.\nEmpirical results conducted on multiple datasets offer compelling support for\nour theoretical assertions.\n', ""  In the field of causal modeling, potential outcomes (PO) and structural\ncausal models (SCMs) stand as the predominant frameworks. However, these\nframeworks face notable challenges in practically modeling counterfactuals,\nformalized as parameters of the joint distribution of potential outcomes.\nCounterfactual reasoning holds paramount importance in contemporary\ndecision-making processes, especially in scenarios that demand personalized\nincentives based on the joint values of $(Y(0), Y(1))$. This paper begins with\nan investigation of the PO and SCM frameworks for modeling counterfactuals.\nThrough the analysis, we identify an inherent model capacity limitation, termed\nas the ``degenerative counterfactual problem'', emerging from the consistency\nrule that is the cornerstone of both frameworks. To address this limitation, we\nintroduce a novel \\textit{distribution-consistency} assumption, and in\nalignment with it, we propose the Distribution-consistency Structural Causal\nModels (DiscoSCMs) offering enhanced capabilities to model counterfactuals. To\nconcretely reveal the enhanced model capacity, we introduce a new identifiable\ncausal parameter, \\textit{the probability of consistency}, which holds\npractical significance within DiscoSCM alone, showcased with a personalized\nincentive example. Furthermore, we provide a comprehensive set of theoretical\nresults about the ``Ladder of Causation'' within the DiscoSCM framework. We\nhope it opens new avenues for future research of counterfactual modeling,\nultimately enhancing our understanding of causality and its real-world\napplications.\n"", '  Accurate estimation of counterfactual outcomes in high-dimensional data is\ncrucial for decision-making and understanding causal relationships and\nintervention outcomes in various domains, including healthcare, economics, and\nsocial sciences. However, existing methods often struggle to generate accurate\nand consistent counterfactuals, particularly when the causal relationships are\ncomplex. We propose a novel framework that incorporates causal mechanisms and\ndiffusion models to generate high-quality counterfactual samples guided by\ncausal representation. Our approach introduces a novel, theoretically grounded\ntraining and sampling process that enables the model to consistently generate\naccurate counterfactual high-dimensional data under multiple intervention\nsteps. Experimental results on various synthetic and real benchmarks\ndemonstrate the proposed approach outperforms state-of-the-art methods in\ngenerating accurate and high-quality counterfactuals, using different\nevaluation metrics.\n'] , [""  The framework of Pearl's Causal Hierarchy (PCH) formalizes three types of\nreasoning: observational, interventional, and counterfactual, that reflect the\nprogressive sophistication of human thought regarding causation. We investigate\nthe computational complexity aspects of reasoning in this framework focusing\nmainly on satisfiability problems expressed in probabilistic and causal\nlanguages across the PCH. That is, given a system of formulas in the standard\nprobabilistic and causal languages, does there exist a model satisfying the\nformulas? The resulting complexity changes depending on the level of the\nhierarchy as well as the operators allowed in the formulas (addition,\nmultiplication, or marginalization). We focus on formulas involving\nmarginalization that are widely used in probabilistic and causal inference, but\nwhose complexity issues are still little explored. Our main contribution are\nthe exact computational complexity results showing that linear languages\n(allowing addition and marginalization) yield NP^PP-, PSPACE-, and\nNEXP-complete satisfiability problems, depending on the level of the PCH.\nMoreover, we prove that the problem for the full language (allowing\nadditionally multiplication) is complete for the class succ$\\exists$R for\nlanguages on the highest, counterfactual level, which extends previous results\nfor the lower levels of the PCH. Finally, we consider constrained models that\nare restricted to a given Bayesian network, a Directed Acyclic Graph structure,\nor a small polynomial size. The complexity of languages on the interventional\nlevel is increased to the complexity of counterfactual languages without such a\nconstraint, that is, linear languages become NEXP-complete. On the other hand,\nthe complexity on the counterfactual level does not change. The constraint on\nthe size reduces the complexity of the interventional and counterfactual\nlanguages to NEXP-complete.\n"", '  We present counterfactual situation testing (CST), a causal data mining\nframework for detecting discrimination in classifiers. CST aims to answer in an\nactionable and meaningful way the intuitive question ""what would have been the\nmodel outcome had the individual, or complainant, been of a different protected\nstatus?"" It extends the legally-grounded situation testing of Thanh et al.\n(2011) by operationalizing the notion of fairness given the difference using\ncounterfactual reasoning. For any complainant, we find and compare similar\nprotected and non-protected instances in the dataset used by the classifier to\nconstruct a control and test group, where a difference between the decision\noutcomes of the two groups implies potential individual discrimination. Unlike\nsituation testing, which builds both groups around the complainant, we build\nthe test group on the complainant\'s counterfactual generated using causal\nknowledge. The counterfactual is intended to reflect how the protected\nattribute when changed affects the seemingly neutral attributes used by the\nclassifier, which is taken for granted in many frameworks for discrimination.\nUnder CST, we compare similar individuals within each group but dissimilar\nindividuals across both groups due to the possible difference between the\ncomplainant and its counterfactual. Evaluating our framework on two\nclassification scenarios, we show that it uncovers a greater number of cases\nthan situation testing, even when the classifier satisfies the counterfactual\nfairness condition of Kusner et al. (2017).\n', ""  Causal inference has recently garnered significant interest among recommender\nsystem (RS) researchers due to its ability to dissect cause-and-effect\nrelationships and its broad applicability across multiple fields. It offers a\nframework to model the causality in recommender systems like confounding\neffects and deal with counterfactual problems such as offline policy evaluation\nand data augmentation. Although there are already some valuable surveys on\ncausal recommendations, they typically classify approaches based on the\npractical issues faced in RS, a classification that may disperse and fragment\nthe unified causal theories. Considering RS researchers' unfamiliarity with\ncausality, it is necessary yet challenging to comprehensively review relevant\nstudies from a coherent causal theoretical perspective, thereby facilitating a\ndeeper integration of causal inference in RS. This survey provides a systematic\nreview of up-to-date papers in this area from a causal theory standpoint and\ntraces the evolutionary development of RS methods within the same causal\nstrategy. Firstly, we introduce the fundamental concepts of causal inference as\nthe basis of the following review. Subsequently, we propose a novel\ntheory-driven taxonomy, categorizing existing methods based on the causal\ntheory employed - namely, those based on the potential outcome framework, the\nstructural causal model, and general counterfactuals. The review then delves\ninto the technical details of how existing methods apply causal inference to\naddress particular recommender issues. Finally, we highlight some promising\ndirections for future research in this field. Representative papers and\nopen-source resources will be progressively available at\nhttps://github.com/Chrissie-Law/Causal-Inference-for-Recommendation.\n""] , [""  In the realm of Artificial Intelligence (AI), the importance of Explainable\nArtificial Intelligence (XAI) is increasingly recognized, particularly as AI\nmodels become more integral to our lives. One notable single-instance XAI\napproach is counterfactual explanation, which aids users in comprehending a\nmodel's decisions and offers guidance on altering these decisions. Specifically\nin the context of image classification models, effective image counterfactual\nexplanations can significantly enhance user understanding. This paper\nintroduces a novel method for computing feature importance within the feature\nspace of a black-box model. By employing information fusion techniques, our\nmethod maximizes the use of data to address feature counterfactual explanations\nin the feature space. Subsequently, we utilize an image generation model to\ntransform these feature counterfactual explanations into image counterfactual\nexplanations. Our experiments demonstrate that the counterfactual explanations\ngenerated by our method closely resemble the original images in both pixel and\nfeature spaces. Additionally, our method outperforms established baselines,\nachieving impressive experimental results.\n"", '  Counterfactual image editing is an important task in generative AI, which\nasks how an image would look if certain features were different. The current\nliterature on the topic focuses primarily on changing individual features while\nremaining silent about the causal relationships between these features, as\npresent in the real world. In this paper, we formalize the counterfactual image\nediting task using formal language, modeling the causal relationships between\nlatent generative factors and images through a special type of model called\naugmented structural causal models (ASCMs). Second, we show two fundamental\nimpossibility results: (1) counterfactual editing is impossible from i.i.d.\nimage samples and their corresponding labels alone; (2) even when the causal\nrelationships between the latent generative factors and images are available,\nno guarantees regarding the output of the model can be provided. Third, we\npropose a relaxation for this challenging problem by approximating\nnon-identifiable counterfactual distributions with a new family of\ncounterfactual-consistent estimators. This family exhibits the desirable\nproperty of preserving features that the user cares about across both factual\nand counterfactual worlds. Finally, we develop an efficient algorithm to\ngenerate counterfactual images by leveraging neural causal models.\n', ""  In this paper, we propose leveraging causal generative learning as an\ninterpretable tool for explaining image classifiers. Specifically, we present a\ngenerative counterfactual inference approach to study the influence of visual\nfeatures (i.e., pixels) as well as causal factors through generative learning.\nTo this end, we first uncover the most influential pixels on a classifier's\ndecision by varying the value of a causal attribute via counterfactual\ninference and computing both Shapely and contrastive explanations for\ncounterfactual images with these different attribute values. We then establish\na Monte-Carlo mechanism using the generator of a causal generative model in\norder to adapt Shapley explainers to produce feature importances for the\nhuman-interpretable attributes of a causal dataset in the case where a\nclassifier has been trained exclusively on the images of the dataset. Finally,\nwe present optimization methods for creating counterfactual explanations of\nclassifiers by means of counterfactual inference, proposing straightforward\napproaches for both differentiable and arbitrary classifiers. We exploit the\nMorpho-MNIST causal dataset as a case study for exploring our proposed methods\nfor generating counterfacutl explantions. We employ visual explanation methods\nfrom OmnixAI open source toolkit to compare them with our proposed methods. By\nemploying quantitative metrics to measure the interpretability of\ncounterfactual explanations, we find that our proposed methods of\ncounterfactual explanation offer more interpretable explanations compared to\nthose generated from OmnixAI. This finding suggests that our methods are\nwell-suited for generating highly interpretable counterfactual explanations on\ncausal datasets.\n""]",Causal Analysis and Counterfactual Reasoning,Counterfactual Inference and Causal Modeling
180,"Galaxy Properties and Cosmology with AI and Spectroscopy , Astroinformatics for Sustainable Development","['galaxies', 'galactic', 'galaxy', 'astronomical', 'astronomy', 'astrophysical', 'cosmology', 'cosmic', 'cosmological', 'quasars'] , ['astroinformatics', 'astronomy', 'astronomical', 'astrophysical', 'astropt', 'galaxies', 'astrophysics', 'astrollama', 'telescopes', 'universetbd']","[""  It has been recently shown that a powerful way to constrain cosmological\nparameters from galaxy redshift surveys is to train graph neural networks to\nperform field-level likelihood-free inference without imposing cuts on scale.\nIn particular, de Santi et al. (2023) developed models that could accurately\ninfer the value of $\\Omega_{\\rm m}$ from catalogs that only contain the\npositions and radial velocities of galaxies that are robust to uncertainties in\nastrophysics and subgrid models. However, observations are affected by many\neffects, including 1) masking, 2) uncertainties in peculiar velocities and\nradial distances, and 3) different galaxy selections. Moreover, observations\nonly allow us to measure redshift, intertwining galaxies' radial positions and\nvelocities. In this paper we train and test our models on galaxy catalogs,\ncreated from thousands of state-of-the-art hydrodynamic simulations run with\ndifferent codes from the CAMELS project, that incorporate these observational\neffects. We find that, although the presence of these effects degrades the\nprecision and accuracy of the models, and increases the fraction of catalogs\nwhere the model breaks down, the fraction of galaxy catalogs where the model\nperforms well is over 90 %, demonstrating the potential of these models to\nconstrain cosmological parameters even when applied to real data.\n"", '  Modern spectroscopic surveys can only target a small fraction of the vast\namount of photometrically cataloged sources in wide-field surveys. Here, we\nreport the development of a generative AI method capable of predicting optical\ngalaxy spectra from photometric broad-band images alone. This method draws from\nthe latest advances in diffusion models in combination with contrastive\nnetworks. We pass multi-band galaxy images into the architecture to obtain\noptical spectra. From these, robust values for galaxy properties can be derived\nwith any methods in the spectroscopic toolbox, such as standard population\nsynthesis techniques and Lick indices. When trained and tested on 64x64-pixel\nimages from the Sloan Digital Sky Survey, the global bimodality of star-forming\nand quiescent galaxies in photometric space is recovered, as well as a\nmass-metallicity relation of star-forming galaxies. The comparison between the\nobserved and the artificially created spectra shows good agreement in overall\nmetallicity, age, Dn4000, stellar velocity dispersion, and E(B-V) values.\nPhotometric redshift estimates of our generative algorithm can compete with\nother current, specialized deep-learning techniques. Moreover, this work is the\nfirst attempt in the literature to infer velocity dispersion from photometric\nimages. Additionally, we can predict the presence of an active galactic nucleus\nup to an accuracy of 82%. With our method, scientifically interesting galaxy\nproperties, normally requiring spectroscopic inputs, can be obtained in future\ndata sets from large-scale photometric surveys alone. The spectra prediction\nvia AI can further assist in creating realistic mock catalogs.\n', '  We present AstroCLIP, a single, versatile model that can embed both galaxy\nimages and spectra into a shared, physically meaningful latent space. These\nembeddings can then be used - without any model fine-tuning - for a variety of\ndownstream tasks including (1) accurate in-modality and cross-modality semantic\nsimilarity search, (2) photometric redshift estimation, (3) galaxy property\nestimation from both images and spectra, and (4) morphology classification. Our\napproach to implementing AstroCLIP consists of two parts. First, we embed\ngalaxy images and spectra separately by pretraining separate transformer-based\nimage and spectrum encoders in self-supervised settings. We then align the\nencoders using a contrastive loss. We apply our method to spectra from the Dark\nEnergy Spectroscopic Instrument and images from its corresponding Legacy\nImaging Survey. Overall, we find remarkable performance on all downstream\ntasks, even relative to supervised baselines. For example, for a task like\nphotometric redshift prediction, we find similar performance to a\nspecifically-trained ResNet18, and for additional tasks like physical property\nestimation (stellar mass, age, metallicity, and sSFR), we beat this supervised\nbaseline by 19\\% in terms of $R^2$. We also compare our results to a\nstate-of-the-art self-supervised single-modal model for galaxy images, and find\nthat our approach outperforms this benchmark by roughly a factor of two on\nphotometric redshift estimation and physical property prediction in terms of\n$R^2$, while remaining roughly in-line in terms of morphology classification.\nUltimately, our approach represents the first cross-modal self-supervised model\nfor galaxies, and the first self-supervised transformer-based architectures for\ngalaxy images and spectra.\n'] , ['  Policy Brief on ""Long Term Space Data and Informatics Needs"", distilled from\nthe corresponding panel that was part of the discussions during S20 Policy\nWebinar on Astroinformatics for Sustainable Development held on 6-7 July 2023.\n  Persistent space data gathering, retention, transmission, and analysis play a\npivotal role in deepening our grasp of the Universe and fostering the\nachievement of global sustainable development goals. Long-term data storage and\ncuration is crucial not only to make the wide range of burgeoning data sets\navailable to the global science community, but also to stabilize those data\nsets, enabling new science in the future to analyse long-term trends over\nunprecedented time spans. In addition to this, over the long-term, the\nimperative to store all data on the ground should be ameliorated by use of\nspace-based data stores --maintained and seen to be as reliable as any other\ndata archive. This concept is sometimes referred to as Memory of the Sky.\nStoring the data must be accompanied by the ability to analyse them. Several\nconcepts covered below acknowledge roots and inspiration based in the Virtual\nObservatory effort. Within this policy document, we delve into the complexities\nsurrounding the long-term utilization of space data and informatics, shedding\nlight on the challenges and opportunities inherent in this endeavour. Further,\nwe present a series of pragmatic recommendations designed to address these\nchallenges proactively.\n  The policy webinar took place during the G20 presidency in India (2023). A\nsummary based on the seven panels can be found here: arxiv:2401.04623.\n', '  Policy Brief on ""AstroInformatics, Recommendations for Global Collaboration"",\ndistilled from panel discussions during S20 Policy Webinar on Astroinformatics\nfor Sustainable Development held on 6-7 July 2023.\n  The deliberations encompassed a wide array of topics, including broad\nastroinformatics, sky surveys, large-scale international initiatives, global\ndata repositories, space-related data, regional and international collaborative\nefforts, as well as workforce development within the field. These discussions\ncomprehensively addressed the current status, notable achievements, and the\nmanifold challenges that the field of astroinformatics currently confronts.\n  The G20 nations present a unique opportunity due to their abundant human and\ntechnological capabilities, coupled with their widespread geographical\nrepresentation. Leveraging these strengths, significant strides can be made in\nvarious domains. These include, but are not limited to, the advancement of STEM\neducation and workforce development, the promotion of equitable resource\nutilization, and contributions to fields such as Earth Science and Climate\nScience.\n  We present a concise overview, followed by specific recommendations that\npertain to both ground-based and space data initiatives. Our team remains\nreadily available to furnish further elaboration on any of these proposals as\nrequired. Furthermore, we anticipate further engagement during the upcoming G20\npresidencies in Brazil (2024) and South Africa (2025) to ensure the continued\ndiscussion and realization of these objectives.\n  The policy webinar took place during the G20 presidency in India (2023).\nNotes based on the seven panels will be separately published.\n', '  Policy Brief on ""Global Data in Astronomy: Challenges and Opportunities"",\ndistilled from the corresponding panel that was part of the discussions during\nS20 Policy Webinar on Astroinformatics for Sustainable Development held on 6-7\nJuly 2023.\n  Astronomy is increasingly becoming a data-driven science. Advances in our\nunderstanding of the physical mechanisms at work in the Universe require\nbuilding ever-more sensitive telescopes to gather observations of the cosmos to\ntest and advance our theoretical models of how the universe works. To confront\nthe observed data with our theoretical models we require data hosting,\narchiving and storage and high-performance computing resources to run the\ntheoretical calculations and compare our simulated and observed universe. We\nalso require the sophisticated development of highly skilled human resources.\nNewer large projects are often run through international collaborations and\npartnerships, driving a need for \'open science\' and collaborative structure\nacross national boundaries. While astronomical data are useful scientifically,\nthe data do not come with the same ethical/privacy-related restrictions as\nmedical/biological data. Moreover, the ability to use data for new scientific\nanalysis extends and expands the impact and reach of scientific surveys -- this\nis a strength that national funding agencies should capitalize on. We discuss\nthe management and analysis of such large volumes of data and the corresponding\nsignificant challenges that require policy-level preparations.\n  The policy webinar took place during the G20 presidency in India (2023). A\nsummary based on the seven panels can be found here: arxiv:2401.04623.\n']",Astroinformatics and Cosmology,Galaxy Properties and Cosmology with AI and Spectroscopy
181,Solar Flare Prediction using Magnetograms,"['magnetosphere', 'solar', 'flare', 'geomagnetic', 'flares', 'solarcnn', 'sun', 'observatory', 'thermosphere', 'magnetogram']","[""  Image super-resolution has been an important subject in image processing and\nrecognition. Here, we present an attention-aided convolutional neural network\n(CNN) for solar image super-resolution. Our method, named SolarCNN, aims to\nenhance the quality of line-of-sight (LOS) magnetograms of solar active regions\n(ARs) collected by the Michelson Doppler Imager (MDI) on board the Solar and\nHeliospheric Observatory (SOHO). The ground-truth labels used for training\nSolarCNN are the LOS magnetograms collected by the Helioseismic and Magnetic\nImager (HMI) on board the Solar Dynamics Observatory (SDO). Solar ARs consist\nof strong magnetic fields in which magnetic energy can suddenly be released to\nproduce extreme space weather events, such as solar flares, coronal mass\nejections, and solar energetic particles. SOHO/MDI covers Solar Cycle 23, which\nis stronger with more eruptive events than Cycle 24. Enhanced SOHO/MDI\nmagnetograms allow for better understanding and forecasting of violent events\nof space weather. Experimental results show that SolarCNN improves the quality\nof SOHO/MDI magnetograms in terms of the structural similarity index measure\n(SSIM), Pearson's correlation coefficient (PCC), and the peak signal-to-noise\nratio (PSNR).\n"", '  Solar flares, especially C, M, and X class, pose significant risks to\nsatellite operations, communication systems, and power grids. We present a\nnovel approach for predicting extreme solar flares using HMI intensitygrams and\nmagnetograms. By detecting sunspots from intensitygrams and extracting magnetic\nfield patches from magnetograms, we train a Residual Network (ResNet) to\nclassify extreme class flares. Our model demonstrates high accuracy, offering a\nrobust tool for predicting extreme solar flares and improving space weather\nforecasting. Additionally, we show that HMI magnetograms provide more useful\ndata for deep learning compared to other SDO AIA images by better capturing\nfeatures critical for predicting flare magnitudes. This study underscores the\nimportance of identifying magnetic fields in solar flare prediction, marking a\nsignificant advancement in solar activity prediction with practical\nimplications for mitigating space weather impacts.\n', ""  In this dataset we provide a comprehensive collection of magnetograms (images\nquantifying the strength of the magnetic field) from the National Aeronautics\nand Space Administration's (NASA's) Solar Dynamics Observatory (SDO). The\ndataset incorporates data from three sources and provides SDO Helioseismic and\nMagnetic Imager (HMI) magnetograms of solar active regions (regions of large\nmagnetic flux, generally the source of eruptive events) as well as labels of\ncorresponding flaring activity. This dataset will be useful for image analysis\nor solar physics research related to magnetic structure, its evolution over\ntime, and its relation to solar flares. The dataset will be of interest to\nthose researchers investigating automated solar flare prediction methods,\nincluding supervised and unsupervised machine learning (classical and deep),\nbinary and multi-class classification, and regression. This dataset is a\nminimally processed, user configurable dataset of consistently sized images of\nsolar active regions that can serve as a benchmark dataset for solar flare\nprediction research.\n""]",Solar Flare Prediction using Magnetograms,Solar Flare Prediction using Magnetograms
182,"Electricity Demand Forecasting , Solar Energy Forecasting and Prediction , Energy Forecasting in Buildings , ""Energy Market Forecasting and Bidding Strategies""","['forecasting', 'forecast', 'forecasts', 'prediction', 'electricity', 'microgrids', 'microgrid', 'demand', 'appliance', 'renewable'] , ['forecast', 'solar', 'forecasting', 'forecasts', 'meteorological', 'photovoltaic', 'predicting', 'prediction', 'predict', 'renewable'] , ['energy', 'buildings', 'forecasting', 'buildingsbench', 'predicting', 'forecast', 'prediction', 'hvac', 'predict', 'forecasts'] , ['forecasting', 'forecast', 'forecasts', 'prediction', 'bidding', 'schedulers', 'predictions', 'trading', 'markets', 'bids']","[""  Trading on the day-ahead electricity markets requires accurate information\nabout the realization of electricity prices and the uncertainty attached to the\npredictions. Deriving accurate forecasting models presents a difficult task due\nto the day-ahead price's non-stationarity resulting from changing market\nconditions, e.g., due to changes resulting from the energy crisis in 2021. We\npresent a probabilistic forecasting approach for day-ahead electricity prices\nusing the fully data-driven deep generative model called normalizing flow. Our\nmodeling approach generates full-day scenarios of day-ahead electricity prices\nbased on conditional features such as residual load forecasts. Furthermore, we\npropose extended feature sets of prior realizations and a periodic retraining\nscheme that allows the normalizing flow to adapt to the changing conditions of\nmodern electricity markets. Our results highlight that the normalizing flow\ngenerates high-quality scenarios that reproduce the true price distribution and\nyield accurate forecasts. Additionally, our analysis highlights how our\nimprovements towards adaptations in changing regimes allow the normalizing flow\nto adapt to changing market conditions and enable continued sampling of\nhigh-quality day-ahead price scenarios.\n"", '  The precise forecasting of electricity demand also referred to as load\nforecasting, is essential for both planning and managing a power system. It is\ncrucial for many tasks, including choosing which power units to commit to,\nmaking plans for future power generation capacity, enhancing the power network,\nand controlling electricity consumption. As Bangladesh is a developing country,\nthe electricity infrastructure is critical for economic growth and employment\nin this country. Accurate forecasting of electricity demand is crucial for\nensuring that this country has a reliable and sustainable electricity supply to\nmeet the needs of its growing population and economy. The complex and nonlinear\nbehavior of such energy systems inhibits the creation of precise algorithms.\nWithin this context, this paper aims to propose a hybrid model of Convolutional\nNeural Network (CNN) and stacked Bidirectional Long-short Term Memory (BiLSTM)\narchitecture to perform an accurate short-term forecast of the electricity\ndemand of Dhaka city. Short-term forecasting is ordinarily done to anticipate\nload for the following few hours to a few weeks. Normalization techniques have\nbeen also investigated because of the sensitivity of these models towards the\ninput range. The proposed approach produced the best prediction results in\ncomparison to the other benchmark models (LSTM, CNN- BiLSTM and CNN-LSTM) used\nin the study, with MAPE 1.64%, MSE 0.015, RMSE 0.122 and MAE 0.092. The result\nof the proposed model also outperformed some of the existing works on\nload-forecasting.\n', ""  The flexibility in electricity consumption and production in communities of\nresidential buildings, including those with renewable energy sources and energy\nstorage (a.k.a., prosumers), can effectively be utilized through the\nadvancement of short-term demand response mechanisms. It is known that\nflexibility can further be increased if demand response is performed at the\nlevel of communities of prosumers, since aggregated groups can better\ncoordinate electricity consumption. However, the effectiveness of such\nshort-term optimization is highly dependent on the accuracy of electricity load\nforecasts both for each building as well as for the whole community. Structural\nvariations in the electricity load profile can be associated with different\nexogenous factors, such as weather conditions, calendar information and day of\nthe week, as well as user behavior. In this paper, we review a wide range of\nelectricity load forecasting techniques, that can provide significant\nassistance in optimizing load consumption in prosumer communities. We present\nand test artificial intelligence (AI) powered short-term load forecasting\nmethodologies that operate with black-box time series models, such as\nFacebook's Prophet and Long Short-term Memory (LSTM) models; season-based\nSARIMA and smoothing Holt-Winters models; and empirical regression-based models\nthat utilize domain knowledge. The integration of weather forecasts into\ndata-driven time series forecasts is also tested. Results show that the\ncombination of persistent and regression terms (adapted to the load forecasting\ntask) achieves the best forecast accuracy.\n""] , ['  Weather forecasts from numerical weather prediction models play a central\nrole in solar energy forecasting, where a cascade of physics-based models is\nused in a model chain approach to convert forecasts of solar irradiance to\nsolar power production, using additional weather variables as auxiliary\ninformation. Ensemble weather forecasts aim to quantify uncertainty in the\nfuture development of the weather, and can be used to propagate this\nuncertainty through the model chain to generate probabilistic solar energy\npredictions. However, ensemble prediction systems are known to exhibit\nsystematic errors, and thus require post-processing to obtain accurate and\nreliable probabilistic forecasts. The overarching aim of our study is to\nsystematically evaluate different strategies to apply post-processing methods\nin model chain approaches: Not applying any post-processing at all;\npost-processing only the irradiance predictions before the conversion;\npost-processing only the solar power predictions obtained from the model chain;\nor applying post-processing in both steps. In a case study based on a benchmark\ndataset for the Jacumba solar plant in the U.S., we develop statistical and\nmachine learning methods for post-processing ensemble predictions of global\nhorizontal irradiance and solar power generation. Further, we propose a neural\nnetwork-based model for direct solar power forecasting that bypasses the model\nchain. Our results indicate that post-processing substantially improves the\nsolar power generation forecasts, in particular when post-processing is applied\nto the power predictions. The machine learning methods for post-processing\nyield slightly better probabilistic forecasts, and the direct forecasting\napproach performs comparable to the post-processing strategies.\n', ""  This project presents an extension to the GraphCast model, a state-of-the-art\ngraph neural network (GNN) for global weather forecasting, by integrating solar\nenergy production forecasting capabilities. The proposed approach leverages the\nweather forecasts generated by GraphCast and trains a neural network model to\npredict the ratio of actual solar output to potential solar output based on\nvarious weather conditions. The model architecture consists of an input layer\ncorresponding to weather features (temperature, humidity, dew point, wind\nspeed, rain, barometric pressure, and altitude), two hidden layers with ReLU\nactivations, and an output layer predicting solar radiation. The model is\ntrained using a mean absolute error loss function and Adam optimizer. The\nresults demonstrate the model's effectiveness in accurately predicting solar\nradiation, with its convergence behavior, decreasing training loss, and\naccurate prediction of solar radiation patterns suggesting successful learning\nof the underlying relationships between weather conditions and solar radiation.\nThe integration of solar energy production forecasting with GraphCast offers\nvaluable insights for the renewable energy sector, enabling better planning and\ndecision-making based on expected solar energy production. Future work could\nexplore further model refinements, incorporation of additional weather\nvariables, and extension to other renewable energy sources.\n"", '  The challenges in applications of solar energy lies in its intermittency and\ndependency on meteorological parameters such as; solar radiation, ambient\ntemperature, rainfall, wind-speed etc., and many other physical parameters like\ndust accumulation etc. Hence, it is important to estimate the amount of solar\nphotovoltaic (PV) power generation for a specific geographical location.\nMachine learning (ML) models have gained importance and are widely used for\nprediction of solar power plant performance. In this paper, the impact of\nweather parameters on solar PV power generation is estimated by several\nEnsemble ML (EML) models like Bagging, Boosting, Stacking, and Voting for the\nfirst time. The performance of chosen ML algorithms is validated by field\ndataset of a 10kWp solar PV power plant in Eastern India region. Furthermore, a\ncomplete test-bed framework has been designed for data mining as well as to\nselect appropriate learning models. It also supports feature selection and\nreduction for dataset to reduce space and time complexity of the learning\nmodels. The results demonstrate greater prediction accuracy of around 96% for\nStacking and Voting EML models. The proposed work is a generalized one and can\nbe very useful for predicting the performance of large-scale solar PV power\nplants also.\n'] , ['  Energy prediction in buildings plays a crucial role in effective energy\nmanagement. Precise predictions are essential for achieving optimal energy\nconsumption and distribution within the grid. This paper introduces a Long\nShort-Term Memory (LSTM) model designed to forecast building energy consumption\nusing historical energy data, occupancy patterns, and weather conditions. The\nLSTM model provides accurate short, medium, and long-term energy predictions\nfor residential and commercial buildings compared to existing prediction\nmodels. We compare our LSTM model with established prediction methods,\nincluding linear regression, decision trees, and random forest. Encouragingly,\nthe proposed LSTM model emerges as the superior performer across all metrics.\nIt demonstrates exceptional prediction accuracy, boasting the highest R2 score\nof 0.97 and the most favorable mean absolute error (MAE) of 0.007. An\nadditional advantage of our developed model is its capacity to achieve\nefficient energy consumption forecasts even when trained on a limited dataset.\nWe address concerns about overfitting (variance) and underfitting (bias)\nthrough rigorous training and evaluation on real-world data. In summary, our\nresearch contributes to energy prediction by offering a robust LSTM model that\noutperforms alternative methods and operates with remarkable efficiency,\ngeneralizability, and reliability.\n', '  Buildings energy efficiency is a widely researched topic, which is rapidly\ngaining popularity due to rising environmental concerns and the need for energy\nindependence. In Northern Europe heating energy alone accounts for up to 70\npercent of the total building energy consumption. Industry 4.0 technologies\nsuch as IoT, big data, cloud computing and machine learning, along with the\ncreation of predictive and proactive digital twins, can help to reduce this\nnumber. However, buildings thermal dynamics is a very complex process that\ndepends on many variables. As a result, commonly used physics-based white box\nmodels are time-consuming and require vast expertise. On the contrary, black\nbox forecasting models, which rely primarily on building energy consumption\ndata, lack fundamental insights and hinder re-use. In this study we propose an\narchitecture to facilitate grey box modelling of building thermal dynamics\nwhile integrating real time IoT data with 3D representation of buildings. The\narchitecture is validated in a case study creating a digital twin platform that\nenables users to define the thermal dynamics of buildings based on physical\nlaws and real data, thus facilitating informed decision making for the best\nheating energy optimization strategy. Also, the created user interface enables\nstakeholders such as facility managers, energy providers or governing bodies to\nanalyse, compare and evaluate buildings thermal dynamics without extensive\nexpertise or time resources.\n', '  In the context of global sustainability, buildings are significant consumers\nof energy, emphasizing the necessity for innovative strategies to enhance\nefficiency and reduce environmental impact. This research leverages extensive\nraw data from building infrastructures to uncover energy consumption patterns\nand devise strategies for optimizing resource use. We investigate the factors\ninfluencing energy efficiency and cost reduction in buildings, utilizing Lasso\nRegression, Decision Tree, and Random Forest models for accurate energy use\nforecasting. Our study delves into the factors affecting energy utilization,\nfocusing on primary fuel and electrical energy, and discusses the potential for\nsubstantial cost savings and environmental benefits. Significantly, we apply\nmetaheuristic techniques to enhance the Decision Tree algorithm, resulting in\nimproved predictive precision. This enables a more nuanced understanding of the\ncharacteristics of buildings with high and low energy efficiency potential. Our\nfindings offer practical insights for reducing energy consumption and\noperational costs, contributing to the broader goals of sustainable development\nand cleaner production. By identifying key drivers of energy use in buildings,\nthis study provides a valuable framework for policymakers and industry\nstakeholders to implement cleaner and more sustainable energy practices.\n'] , ['  This paper presents an integrated model for bidding energy storage in\nday-ahead and real-time markets to maximize profits. We show that in integrated\ntwo-stage bidding, the real-time bids are independent of day-ahead settlements,\nwhile the day-ahead bids should be based on predicted real-time prices. We\nutilize a transformer-based model for real-time price prediction, which\ncaptures complex dynamical patterns of real-time prices, and use the result for\nday-ahead bidding design. For real-time bidding, we utilize a long short-term\nmemory-dynamic programming hybrid real-time bidding model. We train and test\nour model with historical data from New York State, and our results showed that\nthe integrated system achieved promising results of almost a 20\\% increase in\nprofit compared to only bidding in real-time markets, and at the same time\nreducing the risk in terms of the number of days with negative profits.\n', '  Efficiently integrating renewable resources into electricity markets is vital\nfor addressing the challenges of matching real-time supply and demand while\nreducing the significant energy wastage resulting from curtailments. To address\nthis challenge effectively, the incorporation of storage devices can enhance\nthe reliability and efficiency of the grid, improving market liquidity and\nreducing price volatility. In short-term electricity markets, participants\nnavigate numerous options, each presenting unique challenges and opportunities,\nunderscoring the critical role of the trading strategy in maximizing profits.\nThis study delves into the optimization of day-ahead and balancing market\ntrading, leveraging quantile-based forecasts. Employing three trading\napproaches with practical constraints, our research enhances forecast\nassessment, increases trading frequency, and employs flexible timestamp orders.\nOur findings underscore the profit potential of simultaneous participation in\nboth day-ahead and balancing markets, especially with larger battery storage\nsystems; despite increased costs and narrower profit margins associated with\nhigher-volume trading, the implementation of high-frequency strategies plays a\nsignificant role in maximizing profits and addressing market challenges.\nFinally, we modelled four commercial battery storage systems and evaluated\ntheir economic viability through a scenario analysis, with larger batteries\nshowing a shorter return on investment.\n', '  Large penetration of renewable energy sources (RESs) brings huge uncertainty\ninto the electricity markets. While existing deterministic market clearing\nfails to accommodate the uncertainty, the recently proposed stochastic market\nclearing struggles to achieve desirable market properties. In this work, we\npropose a value-oriented forecasting approach, which tactically determines the\nRESs generation that enters the day-ahead market. With such a forecast, the\nexisting deterministic market clearing framework can be maintained, and the\nday-ahead and real-time overall operation cost is reduced. At the training\nphase, the forecast model parameters are estimated to minimize expected\nday-ahead and real-time overall operation costs, instead of minimizing forecast\nerrors in a statistical sense. Theoretically, we derive the exact form of the\nloss function for training the forecast model that aligns with such a goal. For\nmarket clearing modeled by linear programs, this loss function is a piecewise\nlinear function. Additionally, we derive the analytical gradient of the loss\nfunction with respect to the forecast, which inspires an efficient training\nstrategy. A numerical study shows our forecasts can bring significant benefits\nof the overall cost reduction to deterministic market clearing, compared to\nquality-oriented forecasting approach.\n']",Energy Forecasting and Management,Electricity Demand Forecasting
183,"""Machine Learning for Weather Forecasting and Climate Prediction"" , Climate Change Analysis using NLP , ""Spatiotemporal Weather Forecasting and Climate Prediction""","['forecast', 'forecasts', 'forecasting', 'meteorological', 'weather', 'climate', 'ensembles', 'prediction', 'predictions', 'precipitation'] , ['climate', 'climateq', 'climatebert', 'nlp', 'climateli', 'climatepolicyradar', 'ecoverse', 'corpus', 'narratives', 'policymakers'] , ['forecast', 'forecasts', 'forecasting', 'meteorological', 'predicting', 'spatiotemporal', 'predict', 'climate', 'prediction', 'weather']","['  General circulation models (GCMs) are the foundation of weather and climate\nprediction. GCMs are physics-based simulators which combine a numerical solver\nfor large-scale dynamics with tuned representations for small-scale processes\nsuch as cloud formation. Recently, machine learning (ML) models trained on\nreanalysis data achieved comparable or better skill than GCMs for deterministic\nweather forecasting. However, these models have not demonstrated improved\nensemble forecasts, or shown sufficient stability for long-term weather and\nclimate simulations. Here we present the first GCM that combines a\ndifferentiable solver for atmospheric dynamics with ML components, and show\nthat it can generate forecasts of deterministic weather, ensemble weather and\nclimate on par with the best ML and physics-based methods. NeuralGCM is\ncompetitive with ML models for 1-10 day forecasts, and with the European Centre\nfor Medium-Range Weather Forecasts ensemble prediction for 1-15 day forecasts.\nWith prescribed sea surface temperature, NeuralGCM can accurately track climate\nmetrics such as global mean temperature for multiple decades, and climate\nforecasts with 140 km resolution exhibit emergent phenomena such as realistic\nfrequency and trajectories of tropical cyclones. For both weather and climate,\nour approach offers orders of magnitude computational savings over conventional\nGCMs. Our results show that end-to-end deep learning is compatible with tasks\nperformed by conventional GCMs, and can enhance the large-scale physical\nsimulations that are essential for understanding and predicting the Earth\nsystem.\n', ""  Operational numerical weather prediction systems consist of three fundamental\ncomponents: the global observing system for data collection, data assimilation\nfor generating initial conditions, and the forecasting model to predict future\nweather conditions. While NWP have undergone a quiet revolution, with forecast\nskills progressively improving over the past few decades, their advancement has\nslowed due to challenges such as high computational costs and the complexities\nassociated with assimilating an increasing volume of observational data and\nmanaging finer spatial grids. Advances in machine learning offer an alternative\npath towards more efficient and accurate weather forecasts. The rise of machine\nlearning based weather forecasting models has also spurred the development of\nmachine learning based DA models or even purely machine learning based weather\nforecasting systems. This paper introduces FuXi Weather, an end-to-end machine\nlearning based weather forecasting system. FuXi Weather employs specialized\ndata preprocessing and multi-modal data fusion techniques to integrate\ninformation from diverse sources under all-sky conditions, including microwave\nsounders from 3 polar-orbiting satellites and radio occultation data from\nGlobal Navigation Satellite System. Operating on a 6-hourly DA and forecasting\ncycle, FuXi Weather independently generates robust and accurate 10-day global\nweather forecasts at a spatial resolution of 0.25\\textdegree. It surpasses the\nEuropean Centre for Medium-range Weather Forecasts high-resolution forecasts in\nterms of predictability, extending the skillful forecast lead times for several\nkey weather variables such as the geopotential height at 500 hPa from 9.25 days\nto 9.5 days. The system's high computational efficiency and robust performance,\neven with limited observations, demonstrates its potential as a promising\nalternative to traditional NWP systems.\n"", ""  Weather forecasts are fundamentally uncertain, so predicting the range of\nprobable weather scenarios is crucial for important decisions, from warning the\npublic about hazardous weather, to planning renewable energy use. Here, we\nintroduce GenCast, a probabilistic weather model with greater skill and speed\nthan the top operational medium-range weather forecast in the world, the\nEuropean Centre for Medium-Range Forecasts (ECMWF)'s ensemble forecast, ENS.\nUnlike traditional approaches, which are based on numerical weather prediction\n(NWP), GenCast is a machine learning weather prediction (MLWP) method, trained\non decades of reanalysis data. GenCast generates an ensemble of stochastic\n15-day global forecasts, at 12-hour steps and 0.25 degree latitude-longitude\nresolution, for over 80 surface and atmospheric variables, in 8 minutes. It has\ngreater skill than ENS on 97.4% of 1320 targets we evaluated, and better\npredicts extreme weather, tropical cyclones, and wind power production. This\nwork helps open the next chapter in operational weather forecasting, where\ncritical weather-dependent decisions are made with greater accuracy and\nefficiency.\n""] , [""  Understanding the multifaceted effects of climate change across diverse\ngeographic locations is crucial for timely adaptation and the development of\neffective mitigation strategies. As the volume of scientific literature on this\ntopic continues to grow exponentially, manually reviewing these documents has\nbecome an immensely challenging task. Utilizing Natural Language Processing\n(NLP) techniques to analyze this wealth of information presents an efficient\nand scalable solution. By gathering extensive amounts of peer-reviewed articles\nand studies, we can extract and process critical information about the effects\nof climate change in specific regions. We employ BERT (Bidirectional Encoder\nRepresentations from Transformers) for Named Entity Recognition (NER), which\nenables us to efficiently identify specific geographies within the climate\nliterature. This, in turn, facilitates location-specific analyses. We conduct\nregion-specific climate trend analyses to pinpoint the predominant themes or\nconcerns related to climate change within a particular area, trace the temporal\nprogression of these identified issues, and evaluate their frequency, severity,\nand potential development over time. These in-depth examinations of\nlocation-specific climate data enable the creation of more customized\npolicy-making, adaptation, and mitigation strategies, addressing each region's\nunique challenges and providing more effective solutions rooted in data-driven\ninsights. This approach, founded on a thorough exploration of scientific texts,\noffers actionable insights to a wide range of stakeholders, from policymakers\nto engineers to environmentalists. By proactively understanding these impacts,\nsocieties are better positioned to prepare, allocate resources wisely, and\ndesign tailored strategies to cope with future climate conditions, ensuring a\nmore resilient future for all.\n"", ""  Climate change poses grave challenges, demanding widespread understanding and\nlow-carbon lifestyle awareness. Large language models (LLMs) offer a powerful\ntool to address this crisis, yet comprehensive evaluations of their\nclimate-crisis knowledge are lacking. This paper proposes an automated\nevaluation framework to assess climate-crisis knowledge within LLMs. We adopt a\nhybrid approach for data acquisition, combining data synthesis and manual\ncollection, to compile a diverse set of questions encompassing various aspects\nof climate change. Utilizing prompt engineering based on the compiled\nquestions, we evaluate the model's knowledge by analyzing its generated\nanswers. Furthermore, we introduce a comprehensive set of metrics to assess\nclimate-crisis knowledge, encompassing indicators from 10 distinct\nperspectives. These metrics provide a multifaceted evaluation, enabling a\nnuanced understanding of the LLMs' climate crisis comprehension. The\nexperimental results demonstrate the efficacy of our proposed method. In our\nevaluation utilizing diverse high-performing LLMs, we discovered that while\nLLMs possess considerable climate-related knowledge, there are shortcomings in\nterms of timeliness, indicating a need for continuous updating and refinement\nof their climate-related content.\n"", ""  In this study, we propose a methodology to extract, index, and visualize\n``climate change narratives'' (stories about the connection between causal and\nconsequential events related to climate change). We use two natural language\nprocessing methods, BERT (Bidirectional Encoder Representations from\nTransformers) and causal extraction, to textually analyze newspaper articles on\nclimate change to extract ``climate change narratives.'' The novelty of the\nmethodology could extract and quantify the causal relationships assumed by the\nnewspaper's writers. Looking at the extracted climate change narratives over\ntime, we find that since 2018, an increasing number of narratives suggest the\nimpact of the development of climate change policy discussion and the\nimplementation of climate change-related policies on corporate behaviors,\nmacroeconomics, and price dynamics. We also observed the recent emergence of\nnarratives focusing on the linkages between climate change-related policies and\nmonetary policy. Furthermore, there is a growing awareness of the negative\nimpacts of natural disasters (e.g., abnormal weather and severe floods) related\nto climate change on economic activities, and this issue might be perceived as\na new challenge for companies and governments. The methodology of this study is\nexpected to be applied to a wide range of fields, as it can analyze causal\nrelationships among various economic topics, including analysis of inflation\nexpectation or monetary policy communication strategy.\n""] , ['  Accurate wind speed and direction forecasting is paramount across many\nsectors, spanning agriculture, renewable energy generation, and bushfire\nmanagement. However, conventional forecasting models encounter significant\nchallenges in precisely predicting wind conditions at high spatial resolutions\nfor individual locations or small geographical areas (< 20 km2) and capturing\nmedium to long-range temporal trends and comprehensive spatio-temporal\npatterns. This study focuses on a spatial temporal approach for high-resolution\ngridded wind forecasting at the height of 3 and 10 metres across large areas of\nthe Southwest of Western Australia to overcome these challenges. The model\nutilises the data that covers a broad geographic area and harnesses a diverse\narray of meteorological factors, including terrain characteristics, air\npressure, 10-metre wind forecasts from the European Centre for Medium-Range\nWeather Forecasts, and limited observation data from sparsely distributed\nweather stations (such as 3-metre wind profiles, humidity, and temperature),\nthe model demonstrates promising advancements in wind forecasting accuracy and\nreliability across the entire region of interest. This paper shows the\npotential of our machine learning model for wind forecasts across various\nprediction horizons and spatial coverage. It can help facilitate more informed\ndecision-making and enhance resilience across critical sectors.\n', '  Satellite images have become increasingly valuable for modelling regional\nclimate change effects. Earth surface forecasting represents one such task that\nintegrates satellite images with meteorological data to capture the joint\nevolution of regional climate change effects. However, understanding the\ncomplex relationship between specific meteorological variables and land surface\nevolution poses a significant challenge. In light of this challenge, our paper\nintroduces a pipeline that integrates principles from both perturbation-based\nexplainability techniques like LIME and global marginal explainability\ntechniques like PDP, besides addressing the constraints of using such\ntechniques when applying them to high-dimensional spatiotemporal deep models.\nThe proposed pipeline simplifies the undertaking of diverse investigative\nanalyses, such as marginal sensitivity analysis, marginal correlation analysis,\nlag analysis, etc., on complex land surface forecasting models In this study we\nutilised Convolutional Long Short-Term Memory (ConvLSTM) as the surface\nforecasting model and did analyses on the Normalized Difference Vegetation\nIndex (NDVI) of the surface forecasts, since meteorological variables like\ntemperature, pressure, and precipitation significantly influence it. The study\narea encompasses various regions in Europe. Our analyses show that\nprecipitation exhibits the highest sensitivity in the study area, followed by\ntemperature and pressure. Pressure has little to no direct effect on NDVI.\nAdditionally, interesting nonlinear correlations between meteorological\nvariables and NDVI have been uncovered.\n', '  The majority of real-world processes are spatiotemporal, and the data\ngenerated by them exhibits both spatial and temporal evolution. Weather is one\nof the most essential processes in this domain, and weather forecasting has\nbecome a crucial part of our daily routine. Weather data analysis is considered\nthe most complex and challenging task. Although numerical weather prediction\nmodels are currently state-of-the-art, they are resource-intensive and\ntime-consuming. Numerous studies have proposed time series-based models as a\nviable alternative to numerical forecasts. Recent research in the area of time\nseries analysis indicates significant advancements, particularly regarding the\nuse of state-space-based models (white box) and, more recently, the integration\nof machine learning and deep neural network-based models (black box). The most\nfamous examples of such models are RNNs and transformers. These models have\ndemonstrated remarkable results in the field of time-series analysis and have\ndemonstrated effectiveness in modelling temporal correlations. It is crucial to\ncapture both temporal and spatial correlations for a spatiotemporal process, as\nthe values at nearby locations and time affect the values of a spatiotemporal\nprocess at a specific point. This self-contained paper explores various\nregional data-driven weather forecasting methods, i.e., forecasting over\nmultiple latitude-longitude points (matrix-shaped spatial grid) to capture\nspatiotemporal correlations. The results showed that spatiotemporal prediction\nmodels reduced computational costs while improving accuracy. In particular, the\nproposed tensor train dynamic mode decomposition-based forecasting model has\ncomparable accuracy to the state-of-the-art models without the need for\ntraining. We provide convincing numerical experiments to show that the proposed\napproach is practical.\n']",Climate and Weather Modeling and Prediction using Machine Learning and NLP,"""Machine Learning for Weather Forecasting and Climate Prediction"""
184,"Traffic Flow Prediction with Spatiotemporal Networks , Spatiotemporal Forecasting with Graph Neural Networks","['traffic', 'spatiotemporal', 'transportation', 'networks', 'forecast', 'citynet', 'prediction', 'forecasting', 'predicting', 'congestion'] , ['forecast', 'forecasting', 'networks', 'spatiotemporal', 'graphrl', 'neural', 'prediction', 'graph', 'network', 'temporal']","['  As a core technology of Intelligent Transportation System, traffic flow\nprediction has a wide range of applications. The fundamental challenge in\ntraffic flow prediction is to effectively model the complex spatial-temporal\ndependencies in traffic data. Spatial-temporal Graph Neural Network (GNN)\nmodels have emerged as one of the most promising methods to solve this problem.\nHowever, GNN-based models have three major limitations for traffic prediction:\ni) Most methods model spatial dependencies in a static manner, which limits the\nability to learn dynamic urban traffic patterns; ii) Most methods only consider\nshort-range spatial information and are unable to capture long-range spatial\ndependencies; iii) These methods ignore the fact that the propagation of\ntraffic conditions between locations has a time delay in traffic systems. To\nthis end, we propose a novel Propagation Delay-aware dynamic long-range\ntransFormer, namely PDFormer, for accurate traffic flow prediction.\nSpecifically, we design a spatial self-attention module to capture the dynamic\nspatial dependencies. Then, two graph masking matrices are introduced to\nhighlight spatial dependencies from short- and long-range views. Moreover, a\ntraffic delay-aware feature transformation module is proposed to empower\nPDFormer with the capability of explicitly modeling the time delay of spatial\ninformation propagation. Extensive experimental results on six real-world\npublic traffic datasets show that our method can not only achieve\nstate-of-the-art performance but also exhibit competitive computational\nefficiency. Moreover, we visualize the learned spatial-temporal attention map\nto make our model highly interpretable.\n', '  Spatio-temporal forecasting of traffic flow data represents a typical problem\nin the field of machine learning, impacting urban traffic management systems.\nTraditional statistical and machine learning methods cannot adequately handle\nboth the temporal and spatial dependencies in these complex traffic flow\ndatasets. A prevalent approach in the field is to combine graph convolutional\nnetworks and multi-head attention mechanisms for spatio-temporal processing.\nThis paper proposes a wavelet-based temporal attention model, namely a\nwavelet-based dynamic spatio-temporal aware graph neural network (W-DSTAGNN),\nfor tackling the traffic forecasting problem. Benchmark experiments using\nseveral statistical metrics confirm that our proposal efficiently captures\nspatio-temporal correlations and outperforms ten state-of-the-art models on\nthree different real-world traffic datasets. Our proposed ensemble data-driven\nmethod can handle dynamic temporal and spatial dependencies and make long-term\nforecasts in an efficient manner.\n', ""  Robust prediction of citywide traffic flows at different time periods plays a\ncrucial role in intelligent transportation systems. While previous work has\nmade great efforts to model spatio-temporal correlations, existing methods\nstill suffer from two key limitations: i) Most models collectively predict all\nregions' flows without accounting for spatial heterogeneity, i.e., different\nregions may have skewed traffic flow distributions. ii) These models fail to\ncapture the temporal heterogeneity induced by time-varying traffic patterns, as\nthey typically model temporal correlations with a shared parameterized space\nfor all time periods. To tackle these challenges, we propose a novel\nSpatio-Temporal Self-Supervised Learning (ST-SSL) traffic prediction framework\nwhich enhances the traffic pattern representations to be reflective of both\nspatial and temporal heterogeneity, with auxiliary self-supervised learning\nparadigms. Specifically, our ST-SSL is built over an integrated module with\ntemporal and spatial convolutions for encoding the information across space and\ntime. To achieve the adaptive spatio-temporal self-supervised learning, our\nST-SSL first performs the adaptive augmentation over the traffic flow graph\ndata at both attribute- and structure-levels. On top of the augmented traffic\ngraph, two SSL auxiliary tasks are constructed to supplement the main traffic\nprediction task with spatial and temporal heterogeneity-aware augmentation.\nExperiments on four benchmark datasets demonstrate that ST-SSL consistently\noutperforms various state-of-the-art baselines. Since spatio-temporal\nheterogeneity widely exists in practical datasets, the proposed framework may\nalso cast light on other spatial-temporal applications. Model implementation is\navailable at https://github.com/Echo-Ji/ST-SSL.\n""] , [""  Spatial-temporal forecasting systems play a crucial role in addressing\nnumerous real-world challenges. In this paper, we investigate the potential of\naddressing spatial-temporal forecasting problems using general time series\nforecasting models, i.e., models that do not leverage the spatial relationships\namong the nodes. We propose a all-Multi-Layer Perceptron (all-MLP) time series\nforecasting architecture called RPMixer. The all-MLP architecture was chosen\ndue to its recent success in time series forecasting benchmarks. Furthermore,\nour method capitalizes on the ensemble-like behavior of deep neural networks,\nwhere each individual block within the network behaves like a base learner in\nan ensemble model, particularly when identity mapping residual connections are\nincorporated. By integrating random projection layers into our model, we\nincrease the diversity among the blocks' outputs, thereby improving the overall\nperformance of the network. Extensive experiments conducted on the largest\nspatial-temporal forecasting benchmark datasets demonstrate that the proposed\nmethod outperforms alternative methods, including both spatial-temporal graph\nmodels and general forecasting models.\n"", '  Time series forecasting is essential for our daily activities and precise\nmodeling of the complex correlations and shared patterns among multiple time\nseries is essential for improving forecasting performance. Spatial-Temporal\nGraph Neural Networks (STGNNs) are widely used in multivariate time series\nforecasting tasks and have achieved promising performance on multiple\nreal-world datasets for their ability to model the underlying complex spatial\nand temporal dependencies. However, existing studies have mainly focused on\ndatasets comprising only a few hundred sensors due to the heavy computational\ncost and memory cost of spatial-temporal GNNs. When applied to larger datasets,\nthese methods fail to capture the underlying complex spatial dependencies and\nexhibit limited scalability and performance. To this end, we present a Scalable\nAdaptive Graph Diffusion Forecasting Network (SAGDFN) to capture complex\nspatial-temporal correlation for large-scale multivariate time series and\nthereby, leading to exceptional performance in multivariate time series\nforecasting tasks. The proposed SAGDFN is scalable to datasets of thousands of\nnodes without the need of prior knowledge of spatial correlation. Extensive\nexperiments demonstrate that SAGDFN achieves comparable performance with\nstate-of-the-art baselines on one real-world dataset of 207 nodes and\noutperforms all state-of-the-art baselines by a significant margin on three\nreal-world datasets of 2000 nodes.\n', ""  Predicting Remaining Useful Life (RUL) plays a crucial role in the\nprognostics and health management of industrial systems that involve a variety\nof interrelated sensors. Given a constant stream of time series sensory data\nfrom such systems, deep learning models have risen to prominence at identifying\ncomplex, nonlinear temporal dependencies in these data. In addition to the\ntemporal dependencies of individual sensors, spatial dependencies emerge as\nimportant correlations among these sensors, which can be naturally modelled by\na temporal graph that describes time-varying spatial relationships. However,\nthe majority of existing studies have relied on capturing discrete snapshots of\nthis temporal graph, a coarse-grained approach that leads to loss of temporal\ninformation. Moreover, given the variety of heterogeneous sensors, it becomes\nvital that such inherent heterogeneity is leveraged for RUL prediction in\ntemporal sensor graphs. To capture the nuances of the temporal and spatial\nrelationships and heterogeneous characteristics in an interconnected graph of\nsensors, we introduce a novel model named Temporal and Heterogeneous Graph\nNeural Networks (THGNN). Specifically, THGNN aggregates historical data from\nneighboring nodes to accurately capture the temporal dynamics and spatial\ncorrelations within the stream of sensor data in a fine-grained manner.\nMoreover, the model leverages Feature-wise Linear Modulation (FiLM) to address\nthe diversity of sensor types, significantly improving the model's capacity to\nlearn the heterogeneity in the data sources. Finally, we have validated the\neffectiveness of our approach through comprehensive experiments. Our empirical\nfindings demonstrate significant advancements on the N-CMAPSS dataset,\nachieving improvements of up to 19.2% and 31.6% in terms of two different\nevaluation metrics over state-of-the-art methods.\n""]",Spatiotemporal Forecasting and Prediction in Transportation Systems,Traffic Flow Prediction with Spatiotemporal Networks
185,"Time Series Forecasting with Deep Learning , Stock Price Forecasting with LSTM Networks , Time Series Analysis and Forecasting","['forecasting', 'forecasts', 'forecast', 'lstm', 'prediction', 'predictive', 'seasonal', 'future', 'attention', 'models'] , ['forecasting', 'lstm', 'stocks', 'predicting', 'stock', 'investing', 'prediction', 'stockformer', 'predict', 'stockgpt'] , ['imputation', 'forecasting', 'datasets', 'data', 'spatiotemporal', 'recurrent', 'monitoring', 'incomplete', 'temporal', 'ito']","['  The rapid development of time series forecasting research has brought many\ndeep learning-based modules in this field. However, despite the increasing\namount of new forecasting architectures, it is still unclear if we have\nleveraged the full potential of these existing modules within a properly\ndesigned architecture. In this work, we propose a novel hierarchical neural\narchitecture search approach for time series forecasting tasks. With the design\nof a hierarchical search space, we incorporate many architecture types designed\nfor forecasting tasks and allow for the efficient combination of different\nforecasting architecture modules. Results on long-term-time-series-forecasting\ntasks show that our approach can search for lightweight high-performing\nforecasting architectures across different forecasting tasks.\n', '  Large language models (LLMs) are being applied to time series tasks,\nparticularly time series forecasting. However, are language models actually\nuseful for time series? After a series of ablation studies on three recent and\npopular LLM-based time series forecasting methods, we find that removing the\nLLM component or replacing it with a basic attention layer does not degrade the\nforecasting results -- in most cases the results even improved. We also find\nthat despite their significant computational cost, pretrained LLMs do no better\nthan models trained from scratch, do not represent the sequential dependencies\nin time series, and do not assist in few-shot settings. Additionally, we\nexplore time series encoders and reveal that patching and attention structures\nperform similarly to state-of-the-art LLM-based forecasters.\n', ""  Large language models (LLMs) have been applied in many fields and have\ndeveloped rapidly in recent years. As a classic machine learning task, time\nseries forecasting has recently been boosted by LLMs. Recent works treat large\nlanguage models as \\emph{zero-shot} time series reasoners without further\nfine-tuning, which achieves remarkable performance. However, there are some\nunexplored research problems when applying LLMs for time series forecasting\nunder the zero-shot setting. For instance, the LLMs' preferences for the input\ntime series are less understood. In this paper, by comparing LLMs with\ntraditional time series forecasting models, we observe many interesting\nproperties of LLMs in the context of time series forecasting. First, our study\nshows that LLMs perform well in predicting time series with clear patterns and\ntrends, but face challenges with datasets lacking periodicity. This observation\ncan be explained by the ability of LLMs to recognize the underlying period\nwithin datasets, which is supported by our experiments. In addition, the input\nstrategy is investigated, and it is found that incorporating external knowledge\nand adopting natural language paraphrases substantially improve the predictive\nperformance of LLMs for time series. Overall, our study contributes insight\ninto LLMs' advantages and limitations in time series forecasting under\ndifferent conditions.\n""] , [""  Navigating the intricate landscape of financial markets requires adept\nforecasting of stock price movements. This paper delves into the potential of\nLong Short-Term Memory (LSTM) networks for predicting stock dynamics, with a\nfocus on discerning nuanced rise and fall patterns. Leveraging a dataset from\nthe New York Stock Exchange (NYSE), the study incorporates multiple features to\nenhance LSTM's capacity in capturing complex patterns. Visualization of key\nattributes, such as opening, closing, low, and high prices, aids in unraveling\nsubtle distinctions crucial for comprehensive market understanding. The\nmeticulously crafted LSTM input structure, inspired by established guidelines,\nincorporates both price and volume attributes over a 25-day time step, enabling\nthe model to capture temporal intricacies. A comprehensive methodology,\nincluding hyperparameter tuning with Grid Search, Early Stopping, and Callback\nmechanisms, leads to a remarkable 53% improvement in predictive accuracy. The\nstudy concludes with insights into model robustness, contributions to financial\nforecasting literature, and a roadmap for real-time stock market prediction.\nThe amalgamation of LSTM networks, strategic hyperparameter tuning, and\ninformed feature selection presents a potent framework for advancing the\naccuracy of stock price predictions, contributing substantively to financial\ntime series forecasting discourse.\n"", '  Predicting a fast and accurate model for stock price forecasting is been a\nchallenging task and this is an active area of research where it is yet to be\nfound which is the best way to forecast the stock price. Machine learning, deep\nlearning and statistical analysis techniques are used here to get the accurate\nresult so the investors can see the future trend and maximize the return of\ninvestment in stock trading. This paper will review many deep learning\nalgorithms for stock price forecasting. We use a record of s&p 500 index data\nfor training and testing. The survey motive is to check various deep learning\nand statistical model techniques for stock price forecasting that are Moving\nAverages, ARIMA which are statistical techniques and LSTM, RNN, CNN, and FULL\nCNN which are deep learning models. It will discuss various models, including\nthe Auto regression integration moving average model, the Recurrent neural\nnetwork model, the long short-term model which is the type of RNN used for long\ndependency for data, the convolutional neural network model, and the full\nconvolutional neural network model, in terms of error calculation or percentage\nof accuracy that how much it is accurate which measures by the function like\nRoot mean square error, mean absolute error, mean squared error. The model can\nbe used to predict the stock price by checking the low MAE value as lower the\nMAE value the difference between the predicting and the actual value will be\nless and this model will predict the price more accurately than other models.\n', '  One of the most enticing research areas is the stock market, and projecting\nstock prices may help investors profit by making the best decisions at the\ncorrect time. Deep learning strategies have emerged as a critical technique in\nthe field of the financial market. The stock market is impacted due to two\naspects, one is the geo-political, social and global events on the bases of\nwhich the price trends could be affected. Meanwhile, the second aspect purely\nfocuses on historical price trends and seasonality, allowing us to forecast\nstock prices. In this paper, our aim is to focus on the second aspect and build\na model that predicts future prices with minimal errors. In order to provide\nbetter prediction results of stock price, we propose a new model named Long\nShort-Term Memory (LSTM) with Sequential Self-Attention Mechanism (LSTM-SSAM).\nFinally, we conduct extensive experiments on the three stock datasets: SBIN,\nHDFCBANK, and BANKBARODA. The experimental results prove the effectiveness and\nfeasibility of the proposed model compared to existing models. The experimental\nfindings demonstrate that the root-mean-squared error (RMSE), and R-square (R2)\nevaluation indicators are giving the best results.\n'] , ['  The intricate nature of time series data analysis benefits greatly from the\ndistinct advantages offered by time and frequency domain representations. While\nthe time domain is superior in representing local dependencies, particularly in\nnon-periodic series, the frequency domain excels in capturing global\ndependencies, making it ideal for series with evident periodic patterns. To\ncapitalize on both of these strengths, we propose ATFNet, an innovative\nframework that combines a time domain module and a frequency domain module to\nconcurrently capture local and global dependencies in time series data.\nSpecifically, we introduce Dominant Harmonic Series Energy Weighting, a novel\nmechanism for dynamically adjusting the weights between the two modules based\non the periodicity of the input time series. In the frequency domain module, we\nenhance the traditional Discrete Fourier Transform (DFT) with our Extended DFT,\ndesigned to address the challenge of discrete frequency misalignment.\nAdditionally, our Complex-valued Spectrum Attention mechanism offers a novel\napproach to discern the intricate relationships between different frequency\ncombinations. Extensive experiments across multiple real-world datasets\ndemonstrate that our ATFNet framework outperforms current state-of-the-art\nmethods in long-term time series forecasting.\n', ""  We introduce a novel modeling approach for time series imputation and\nforecasting, tailored to address the challenges often encountered in real-world\ndata, such as irregular samples, missing data, or unaligned measurements from\nmultiple sensors. Our method relies on a continuous-time-dependent model of the\nseries' evolution dynamics. It leverages adaptations of conditional, implicit\nneural representations for sequential data. A modulation mechanism, driven by a\nmeta-learning algorithm, allows adaptation to unseen samples and extrapolation\nbeyond observed time-windows for long-term predictions. The model provides a\nhighly flexible and unified framework for imputation and forecasting tasks\nacross a wide range of challenging scenarios. It achieves state-of-the-art\nperformance on classical benchmarks and outperforms alternative time-continuous\nmodels.\n"", '  In real-world scenarios like traffic and energy, massive time-series data\nwith missing values and noises are widely observed, even sampled irregularly.\nWhile many imputation methods have been proposed, most of them work with a\nlocal horizon, which means models are trained by splitting the long sequence\ninto batches of fit-sized patches. This local horizon can make models ignore\nglobal trends or periodic patterns. More importantly, almost all methods assume\nthe observations are sampled at regular time stamps, and fail to handle complex\nirregular sampled time series arising from different applications. Thirdly,\nmost existing methods are learned in an offline manner. Thus, it is not\nsuitable for many applications with fast-arriving streaming data. To overcome\nthese limitations, we propose BayOTIDE: Bayesian Online Multivariate Time\nseries Imputation with functional decomposition. We treat the multivariate time\nseries as the weighted combination of groups of low-rank temporal factors with\ndifferent patterns. We apply a group of Gaussian Processes (GPs) with different\nkernels as functional priors to fit the factors. For computational efficiency,\nwe further convert the GPs into a state-space prior by constructing an\nequivalent stochastic differential equation (SDE), and developing a scalable\nalgorithm for online inference. The proposed method can not only handle\nimputation over arbitrary time stamps, but also offer uncertainty\nquantification and interpretability for the downstream application. We evaluate\nour method on both synthetic and real-world datasets.We release the code at\n{https://github.com/xuangu-fang/BayOTIDE}\n']",Time Series Analysis and Prediction,Time Series Forecasting with Deep Learning
186,"Reservoir Computing and Chaotic Time Series Prediction , Hydrological Modeling and Flood Forecasting","['reservoir', 'reservoirs', 'neural', 'rnn', 'lstm', 'chaotic', 'forecasting', 'predicting', 'computing', 'memory'] , ['hydrological', 'hydrology', 'hydrologic', 'flooding', 'floods', 'forecasting', 'runoff', 'flood', 'catchment', 'catchments']","['  A reservoir computer is a type of dynamical system arranged to do\ncomputation. Typically, a reservoir computer is constructed by connecting a\nlarge number of nonlinear nodes in a network that includes recurrent\nconnections. In order to achieve accurate results, the reservoir usually\ncontains hundreds to thousands of nodes. This high dimensionality makes it\ndifficult to analyze the reservoir computer using tools from dynamical systems\ntheory. Additionally, the need to create and connect large numbers of nonlinear\nnodes makes it difficult to design and build analog reservoir computers that\ncan be faster and consume less power than digital reservoir computers. We\ndemonstrate here that a reservoir computer may be divided into two parts; a\nsmall set of nonlinear nodes (the reservoir), and a separate set of\ntime-shifted reservoir output signals. The time-shifted output signals serve to\nincrease the rank and memory of the reservoir computer, and the set of\nnonlinear nodes may create an embedding of the input dynamical system. We use\nthis time-shifting technique to obtain excellent performance from an\nopto-electronic delay-based reservoir computer with only a small number of\nvirtual nodes. Because only a few nonlinear nodes are required, construction of\na reservoir computer becomes much easier, and delay-based reservoir computers\ncan operate at much higher speeds.\n', '  Photonic reservoir computing has been successfully utilized in time-series\nprediction as the need for hardware implementations has increased. Prediction\nof chaotic time series remains a significant challenge, an area where the\nconventional reservoir computing framework encounters limitations of prediction\naccuracy. We introduce an attention mechanism to the reservoir computing model\nin the output stage. This attention layer is designed to prioritize distinct\nfeatures and temporal sequences, thereby substantially enhancing the prediction\naccuracy. Our results show that a photonic reservoir computer enhanced with the\nattention mechanism exhibits improved prediction capabilities for smaller\nreservoirs. These advancements highlight the transformative possibilities of\nreservoir computing for practical applications where accurate prediction of\nchaotic time series is crucial.\n', '  Reservoir computing is a form of machine learning that utilizes nonlinear\ndynamical systems to perform complex tasks in a cost-effective manner when\ncompared to typical neural networks. Many recent advancements in reservoir\ncomputing, in particular quantum reservoir computing, make use of reservoirs\nthat are inherently stochastic. However, the theoretical justification for\nusing these systems has not yet been well established. In this paper, we\ninvestigate the universality of stochastic reservoir computers, in which we use\na stochastic system for reservoir computing using the probabilities of each\nreservoir state as the readout instead of the states themselves. In stochastic\nreservoir computing, the number of distinct states of the entire reservoir\ncomputer can potentially scale exponentially with the size of the reservoir\nhardware, offering the advantage of compact device size. We prove that classes\nof stochastic echo state networks, and therefore the class of all stochastic\nreservoir computers, are universal approximating classes. We also investigate\nthe performance of two practical examples of stochastic reservoir computers in\nclassification and chaotic time series prediction. While shot noise is a\nlimiting factor in the performance of stochastic reservoir computing, we show\nsignificantly improved performance compared to a deterministic reservoir\ncomputer with similar hardware in cases where the effects of noise are small.\n'] , [""  Prediction of dynamic environmental variables in unmonitored sites remains a\nlong-standing challenge for water resources science. The majority of the\nworld's freshwater resources have inadequate monitoring of critical\nenvironmental variables needed for management. Yet, the need to have widespread\npredictions of hydrological variables such as river flow and water quality has\nbecome increasingly urgent due to climate and land use change over the past\ndecades, and their associated impacts on water resources. Modern machine\nlearning methods increasingly outperform their process-based and empirical\nmodel counterparts for hydrologic time series prediction with their ability to\nextract information from large, diverse data sets. We review relevant\nstate-of-the art applications of machine learning for streamflow, water\nquality, and other water resources prediction and discuss opportunities to\nimprove the use of machine learning with emerging methods for incorporating\nwatershed characteristics into deep learning models, transfer learning, and\nincorporating process knowledge into machine learning models. The analysis here\nsuggests most prior efforts have been focused on deep learning learning\nframeworks built on many sites for predictions at daily time scales in the\nUnited States, but that comparisons between different classes of machine\nlearning methods are few and inadequate. We identify several open questions for\ntime series predictions in unmonitored sites that include incorporating dynamic\ninputs and site characteristics, mechanistic understanding and spatial context,\nand explainable AI techniques in modern machine learning frameworks.\n"", '  In recent years, climate extremes such as floods have created significant\nenvironmental and economic hazards for Australia, causing damage to the\nenvironment and economy and losses of human and animal lives. An efficient\nmethod of forecasting floods is crucial to limit this damage. Techniques for\nflood prediction are currently based on hydrological, and hydrodynamic\n(physically-based) numerical models. Machine learning methods that include deep\nlearning offer certain advantages over conventional physically based\napproaches, including flexibility and accuracy. Deep learning methods have been\npromising for predicting small to medium-sized climate extreme events over a\nshort time horizon; however, large flooding events present a critical\nchallenge. We present an ensemble-based machine learning approach that\naddresses large-scale extreme flooding challenges using a switching mechanism\nmotivated by extreme-value theory for long-short-term-memory (LSTM) deep\nlearning models. We use a multivariate and multi-step time-series prediction\napproach to predict streamflow for multiple days ahead in the major catchments\nof Australia. The ensemble framework also employs static information to enrich\nthe time-series information, allowing for regional modelling across catchments.\nOur results demonstrate enhanced prediction of streamflow extremes, with\nnotable efficacy for large flooding scenarios in the selected Australian\ncatchments. Through comparative analysis, our methodology underscores the\npotential for deep learning models to revolutionise flood forecasting across\ndiverse regions.\n', '  The application of process-based and data-driven hydrological models is\ncrucial in modern hydrological research, especially for predicting key water\ncycle variables such as runoff, evapotranspiration (ET), and soil moisture.\nThese models provide a scientific basis for water resource management, flood\nforecasting, and ecological protection. Process-based models simulate the\nphysical mechanisms of watershed hydrological processes, while data-driven\nmodels leverage large datasets and advanced machine learning algorithms. This\npaper reviewed and compared methods for assessing and enhancing the\nextrapolability of both model types, discussing their prospects and\nlimitations. Key strategies include the use of leave-one-out cross-validation\nand similarity-based methods to evaluate model performance in ungauged regions.\nDeep learning, transfer learning, and domain adaptation techniques are also\npromising in their potential to improve model predictions in data-sparse and\nextreme conditions. Interdisciplinary collaboration and continuous algorithmic\nadvancements are also important to strengthen the global applicability and\nreliability of hydrological models.\n']",Advanced Computing Methods for Environmental Prediction and Modeling,Reservoir Computing and Chaotic Time Series Prediction
187,"Cancer Prognosis Prediction using AI and Imaging , AI in Medical Diagnosis and Predictive Analytics","['prognosis', 'lung', 'cancer', 'ai', 'cnn', 'predict', 'radiology', 'oncology', 'prediction', 'melanoma'] , ['biomarkers', 'ai', 'diagnoses', 'diagnosis', 'diagnostic', 'predicting', 'classification', 'predictive', 'interpretability', 'prediction']","[""  Accurately predicting the survival rate of cancer patients is crucial for\naiding clinicians in planning appropriate treatment, reducing cancer-related\nmedical expenses, and significantly enhancing patients' quality of life.\nMultimodal prediction of cancer patient survival offers a more comprehensive\nand precise approach. However, existing methods still grapple with challenges\nrelated to missing multimodal data and information interaction within\nmodalities. This paper introduces SELECTOR, a heterogeneous graph-aware network\nbased on convolutional mask encoders for robust multimodal prediction of cancer\npatient survival. SELECTOR comprises feature edge reconstruction, convolutional\nmask encoder, feature cross-fusion, and multimodal survival prediction modules.\nInitially, we construct a multimodal heterogeneous graph and employ the\nmeta-path method for feature edge reconstruction, ensuring comprehensive\nincorporation of feature information from graph edges and effective embedding\nof nodes. To mitigate the impact of missing features within the modality on\nprediction accuracy, we devised a convolutional masked autoencoder (CMAE) to\nprocess the heterogeneous graph post-feature reconstruction. Subsequently, the\nfeature cross-fusion module facilitates communication between modalities,\nensuring that output features encompass all features of the modality and\nrelevant information from other modalities. Extensive experiments and analysis\non six cancer datasets from TCGA demonstrate that our method significantly\noutperforms state-of-the-art methods in both modality-missing and\nintra-modality information-confirmed cases. Our codes are made available at\nhttps://github.com/panliangrui/Selector.\n"", ""  The COVID-19 pandemic has strained global public health, necessitating\naccurate diagnosis and intervention to control disease spread and reduce\nmortality rates. This paper introduces an interpretable deep survival\nprediction model designed specifically for improved understanding and trust in\nCOVID-19 prognosis using chest X-ray (CXR) images. By integrating a large-scale\npretrained image encoder, Risk-specific Grad-CAM, and anatomical region\ndetection techniques, our approach produces regional interpretable outcomes\nthat effectively capture essential disease features while focusing on rare but\ncritical abnormal regions. Our model's predictive results provide enhanced\nclarity and transparency through risk area localization, enabling clinicians to\nmake informed decisions regarding COVID-19 diagnosis with better understanding\nof prognostic insights. We evaluate the proposed method on a multi-center\nsurvival dataset and demonstrate its effectiveness via quantitative and\nqualitative assessments, achieving superior C-indexes (0.764 and 0.727) and\ntime-dependent AUCs (0.799 and 0.691). These results suggest that our\nexplainable deep survival prediction model surpasses traditional survival\nanalysis methods in risk prediction, improving interpretability for clinical\ndecision making and enhancing AI system trustworthiness.\n"", ""  At present, the incidence and fatality rate of lung cancer in China rank\nfirst among all malignant tumors. Despite the continuous development and\nimprovement of China's medical level, the overall 5-year survival rate of lung\ncancer patients is still lower than 20% and is staged. A number of studies have\nconfirmed that early diagnosis and treatment of early stage lung cancer is of\ngreat significance to improve the prognosis of patients. In recent years,\nartificial intelligence technology has gradually begun to be applied in\noncology. ai is used in cancer screening, clinical diagnosis, radiation therapy\n(image acquisition, at-risk organ segmentation, image calibration and delivery)\nand other aspects of rapid development. However, whether medical ai can be\nsocialized depends on the public's attitude and acceptance to a certain extent.\nHowever, at present, there are few studies on the diagnosis of early lung\ncancer by AI technology combined with SCT scanning. In view of this, this study\napplied the combined method in early lung cancer screening, aiming to find a\nsafe and efficient screening mode and provide a reference for clinical\ndiagnosis and treatment.\n""] , ['  Although data-driven artificial intelligence (AI) in medical image diagnosis\nhas shown impressive performance in silico, the lack of interpretability makes\nit difficult to incorporate the ""black box"" into clinicians\' workflows. To make\nthe diagnostic patterns learned from data understandable by clinicians, we\ndevelop an interpretable model, knowledge-guided diagnosis model (KGDM), that\nprovides a visualized reasoning process containing AI-based biomarkers and\nretrieved cases that with the same diagnostic patterns. It embraces clinicians\'\nprompts into the interpreted reasoning through human-AI interaction, leading to\npotentially enhanced safety and more accurate predictions. This study\ninvestigates the performance, interpretability, and clinical utility of KGDM in\nthe diagnosis of infectious keratitis (IK), which is the leading cause of\ncorneal blindness. The classification performance of KGDM is evaluated on a\nprospective validation dataset, an external testing dataset, and an publicly\navailable testing dataset. The diagnostic odds ratios (DOR) of the interpreted\nAI-based biomarkers are effective, ranging from 3.011 to 35.233 and exhibit\nconsistent diagnostic patterns with clinic experience. Moreover, a human-AI\ncollaborative diagnosis test is conducted and the participants with\ncollaboration achieved a performance exceeding that of both humans and AI. By\nsynergistically integrating interpretability and interaction, this study\nfacilitates the convergence of clinicians\' expertise and data-driven\nintelligence. The promotion of inexperienced ophthalmologists with the aid of\nAI-based biomarkers, as well as increased AI prediction by intervention from\nexperienced ones, demonstrate a promising diagnostic paradigm for infectious\nkeratitis using KGDM, which holds the potential for extension to other diseases\nwhere experienced medical practitioners are limited and the safety of AI is\nconcerned.\n', ""  Thyroid cancer, the most prevalent endocrine cancer, has gained significant\nglobal attention due to its impact on public health. Extensive research efforts\nhave been dedicated to leveraging artificial intelligence (AI) methods for the\nearly detection of this disease, aiming to reduce its morbidity rates. However,\na comprehensive understanding of the structured organization of research\napplications in this particular field remains elusive. To address this\nknowledge gap, we conducted a systematic review and developed a comprehensive\ntaxonomy of machine learning-based applications in thyroid cancer pathogenesis,\ndiagnosis, and prognosis. Our primary objective was to facilitate the research\ncommunity's ability to stay abreast of technological advancements and\npotentially lead the emerging trends in this field. This survey presents a\ncoherent literature review framework for interpreting the advanced techniques\nused in thyroid cancer research. A total of 758 related studies were identified\nand scrutinized. To the best of our knowledge, this is the first review that\nprovides an in-depth analysis of the various aspects of AI applications\nemployed in the context of thyroid cancer. Furthermore, we highlight key\nchallenges encountered in this domain and propose future research opportunities\nfor those interested in studying the latest trends or exploring\nless-investigated aspects of thyroid cancer research. By presenting this\ncomprehensive review and taxonomy, we contribute to the existing knowledge in\nthe field, while providing valuable insights for researchers, clinicians, and\nstakeholders in advancing the understanding and management of this disease.\n"", '  Thyroid cancer is an increasing global health concern that requires advanced\ndiagnostic methods. The application of AI and radiomics to thyroid cancer\ndiagnosis is examined in this review. A review of multiple databases was\nconducted in compliance with PRISMA guidelines until October 2023. A\ncombination of keywords led to the discovery of an English academic publication\non thyroid cancer and related subjects. 267 papers were returned from the\noriginal search after 109 duplicates were removed. Relevant studies were\nselected according to predetermined criteria after 124 articles were eliminated\nbased on an examination of their abstract and title. After the comprehensive\nanalysis, an additional six studies were excluded. Among the 28 included\nstudies, radiomics analysis, which incorporates ultrasound (US) images,\ndemonstrated its effectiveness in diagnosing thyroid cancer. Various results\nwere noted, some of the studies presenting new strategies that outperformed the\nstatus quo. The literature has emphasized various challenges faced by AI\nmodels, including interpretability issues, dataset constraints, and operator\ndependence. The synthesized findings of the 28 included studies mentioned the\nneed for standardization efforts and prospective multicenter studies to address\nthese concerns. Furthermore, approaches to overcome these obstacles were\nidentified, such as advances in explainable AI technology and personalized\nmedicine techniques. The review focuses on how AI and radiomics could transform\nthe diagnosis and treatment of thyroid cancer. Despite challenges, future\nresearch on multidisciplinary cooperation, clinical applicability validation,\nand algorithm improvement holds the potential to improve patient outcomes and\ndiagnostic precision in the treatment of thyroid cancer.\n']",Artificial Intelligence in Medical Diagnosis and Prognosis,Cancer Prognosis Prediction using AI and Imaging
188,"Single-Cell RNA Sequencing Analysis and Modeling , Multi-Omics in Cancer Research and Genomics , Cancer Treatment Prediction using Genomic Data","['transcriptomics', 'transcriptomic', 'transcriptome', 'rna', 'bioinformatics', 'cell', 'cells', 'gene', 'genes', 'cellular'] , ['multiomics', 'transcriptomics', 'genomics', 'biomarker', 'genome', 'biomarkers', 'genomic', 'oncology', 'cancer', 'prognosis'] , ['predicting', 'biomarkers', 'prediction', 'personalized', 'learning', 'lstm', 'biomedical', 'cancer', 'genomic', 'genome']","['  Inferring gene regulatory networks (GRNs) from single-cell RNA sequencing\n(scRNA-seq) data is a complex challenge that requires capturing the intricate\nrelationships between genes and their regulatory interactions. In this study,\nwe tackle this challenge by leveraging the single-cell BERT-based pre-trained\ntransformer model (scBERT), trained on extensive unlabeled scRNA-seq data, to\naugment structured biological knowledge from existing GRNs. We introduce a\nnovel joint graph learning approach that combines the rich contextual\nrepresentations learned by pre-trained single-cell language models with the\nstructured knowledge encoded in GRNs using graph neural networks (GNNs). By\nintegrating these two modalities, our approach effectively reasons over boththe\ngene expression level constraints provided by the scRNA-seq data and the\nstructured biological knowledge inherent in GRNs. We evaluate our method on\nhuman cell benchmark datasets from the BEELINE study with cell type-specific\nground truth networks. The results demonstrate superior performance over\ncurrent state-of-the-art baselines, offering a deeper understanding of cellular\nregulatory mechanisms.\n', '  The transformers have achieved significant accomplishments in the natural\nlanguage processing as its outstanding parallel processing capabilities and\nhighly flexible attention mechanism. In addition, increasing studies based on\ntransformers have been proposed to model single-cell data. In this review, we\nattempt to systematically summarize the single-cell language models and\napplications based on transformers. First, we provide a detailed introduction\nabout the structure and principles of transformers. Then, we review the\nsingle-cell language models and large language models for single-cell data\nanalysis. Moreover, we explore the datasets and applications of single-cell\nlanguage models in downstream tasks such as batch correction, cell clustering,\ncell type annotation, gene regulatory network inference and perturbation\nresponse. Further, we discuss the challenges of single-cell language models and\nprovide promising research directions. We hope this review will serve as an\nup-to-date reference for researchers interested in the direction of single-cell\nlanguage models.\n', '  Single-cell RNA sequencing (scRNA-seq) data are important for studying the\nlaws of life at single-cell level. However, it is still challenging to obtain\nenough high-quality scRNA-seq data. To mitigate the limited availability of\ndata, generative models have been proposed to computationally generate\nsynthetic scRNA-seq data. Nevertheless, the data generated with current models\nare not very realistic yet, especially when we need to generate data with\ncontrolled conditions. In the meantime, the Diffusion models have shown their\npower in generating data at high fidelity, providing a new opportunity for\nscRNA-seq generation.\n  In this study, we developed scDiffusion, a generative model combining\ndiffusion model and foundation model to generate high-quality scRNA-seq data\nwith controlled conditions. We designed multiple classifiers to guide the\ndiffusion process simultaneously, enabling scDiffusion to generate data under\nmultiple condition combinations. We also proposed a new control strategy called\nGradient Interpolation. This strategy allows the model to generate continuous\ntrajectories of cell development from a given cell state.\n  Experiments showed that scDiffusion can generate single-cell gene expression\ndata closely resembling real scRNA-seq data. Also, scDiffusion can\nconditionally produce data on specific cell types including rare cell types.\nFurthermore, we could use the multiple-condition generation of scDiffusion to\ngenerate cell type that was out of the training data. Leveraging the Gradient\nInterpolation strategy, we generated a continuous developmental trajectory of\nmouse embryonic cells. These experiments demonstrate that scDiffusion is a\npowerful tool for augmenting the real scRNA-seq data and can provide insights\ninto cell fate research.\n'] , ['  The application of machine learning to transcriptomics data has led to\nsignificant advances in cancer research. However, the high dimensionality and\ncomplexity of RNA sequencing (RNA-seq) data pose significant challenges in\npan-cancer studies. This study hypothesizes that gene sets derived from\nsingle-cell RNA sequencing (scRNA-seq) data will outperform those selected\nusing bulk RNA-seq in pan-cancer downstream tasks. We analyzed scRNA-seq data\nfrom 181 tumor biopsies across 13 cancer types. High-dimensional weighted gene\nco-expression network analysis (hdWGCNA) was performed to identify relevant\ngene sets, which were further refined using XGBoost for feature selection.\nThese gene sets were applied to downstream tasks using TCGA pan-cancer RNA-seq\ndata and compared to six reference gene sets and oncogenes from OncoKB\nevaluated with deep learning models, including multilayer perceptrons (MLPs)\nand graph neural networks (GNNs). The XGBoost-refined hdWGCNA gene set\ndemonstrated higher performance in most tasks, including tumor mutation burden\nassessment, microsatellite instability classification, mutation prediction,\ncancer subtyping, and grading. In particular, genes such as DPM1, BAD, and\nFKBP4 emerged as important pan-cancer biomarkers, with DPM1 consistently\nsignificant across tasks. This study presents a robust approach for feature\nselection in cancer genomics by integrating scRNA-seq data and advanced\nanalysis techniques, offering a promising avenue for improving predictive\naccuracy in cancer research.\n', ""  Multi-omics research has enhanced our understanding of cancer heterogeneity\nand progression. Investigating molecular data through multi-omics approaches is\ncrucial for unraveling the complex biological mechanisms underlying cancer,\nthereby enabling effective diagnosis, treatment, and prevention strategies.\nHowever, predicting patient outcomes through integration of all available\nmulti-omics data is an under-study research direction. Here, we present SeNMo\n(Self-normalizing Network for Multi-omics), a deep neural network trained on\nmulti-omics data across 33 cancer types. SeNMo is efficient in handling\nmulti-omics data characterized by high-width (many features) and low-length\n(fewer samples) attributes. We trained SeNMo for the task of overall survival\nusing pan-cancer data involving 33 cancer sites from Genomics Data Commons\n(GDC). The training data includes gene expression, DNA methylation, miRNA\nexpression, DNA mutations, protein expression modalities, and clinical data. We\nevaluated the model's performance in predicting overall survival using\nconcordance index (C-Index). SeNMo performed consistently well in training\nregime, with the validation C-Index of 0.76 on GDC's public data. In the\ntesting regime, SeNMo performed with a C-Index of 0.758 on a held-out test set.\nThe model showed an average accuracy of 99.8% on the task of classifying the\nprimary cancer type on the pan-cancer test cohort. SeNMo proved to be a\nmini-foundation model for multi-omics oncology data because it demonstrated\nrobust performance, and adaptability not only across molecular data types but\nalso on the classification task of predicting the primary cancer type of\npatients. SeNMo can be further scaled to any cancer site and molecular data\ntype. We believe SeNMo and similar models are poised to transform the oncology\nlandscape, offering hope for more effective, efficient, and patient-centric\ncancer care.\n"", '  The recent development of high-throughput sequencing creates a large\ncollection of multi-omics data, which enables researchers to better investigate\ncancer molecular profiles and cancer taxonomy based on molecular subtypes.\nIntegrating multi-omics data has been proven to be effective for building more\nprecise classification models. Current multi-omics integrative models mainly\nuse early fusion by concatenation or late fusion based on deep neural networks.\nDue to the nature of biological systems, graphs are a better representation of\nbio-medical data. Although few graph neural network (GNN) based multi-omics\nintegrative methods have been proposed, they suffer from three common\ndisadvantages. One is most of them use only one type of connection, either\ninter-omics or intra-omic connection; second, they only consider one kind of\nGNN layer, either graph convolution network (GCN) or graph attention network\n(GAT); and third, most of these methods lack testing on a more complex cancer\nclassification task. We propose a novel end-to-end multi-omics GNN framework\nfor accurate and robust cancer subtype classification. The proposed model\nutilizes multi-omics data in the form of heterogeneous multi-layer graphs that\ncombines both inter-omics and intra-omic connections from established\nbiological knowledge. The proposed model incorporates learned graph features\nand global genome features for accurate classification. We test the proposed\nmodel on TCGA Pan-cancer dataset and TCGA breast cancer dataset for molecular\nsubtype and cancer subtype classification, respectively. The proposed model\noutperforms four current state-of-the-art baseline models in multiple\nevaluation metrics. The comparative analysis of GAT-based models and GCN-based\nmodels reveals that GAT-based models are preferred for smaller graphs with less\ninformation and GCN-based models are preferred for larger graphs with extra\ninformation.\n'] , ['  The development of single-cell sequencing technology had promoted the\ngeneration of a large amount of single-cell transcriptional profiles, providing\nvaluable opportunities to explore drug-resistant cell subpopulations in a\ntumor. However, the drug sensitivity data in single-cell level is still scarce\nto date, pressing an urgent and highly challenging task for computational\nprediction of the drug sensitivity to individual cells. This paper proposed\nscAdaDrug, a multi-source adaptive weighting model to predict single-cell drug\nsensitivity. We used an autoencoder to extract domain-invariant features\nrelated to drug sensitivity from multiple source domains by exploiting\nadversarial domain adaptation. Especially, we introduced an adaptive weight\ngenerator to produce importance-aware and mutual independent weights, which\ncould adaptively modulate the embedding of each sample in dimension-level for\nboth source and target domains. Extensive experimental results showed that our\nmodel achieved state-of-the-art performance in predicting drug sensitivity on\nsinle-cell datasets, as well as on cell line and patient datasets.\n', ""  Cancer, a leading cause of death globally, occurs due to genomic changes and\nmanifests heterogeneously across patients. To advance research on personalized\ntreatment strategies, the effectiveness of various drugs on cells derived from\ncancers (`cell lines') is experimentally determined in laboratory settings.\nNevertheless, variations in the distribution of genomic data and drug responses\nbetween cell lines and humans arise due to biological and environmental\ndifferences. Moreover, while genomic profiles of many cancer patients are\nreadily available, the scarcity of corresponding drug response data limits the\nability to train machine learning models that can predict drug response in\npatients effectively. Recent cancer drug response prediction methods have\nlargely followed the paradigm of unsupervised domain-invariant representation\nlearning followed by a downstream drug response classification step.\nIntroducing supervision in both stages is challenging due to heterogeneous\npatient response to drugs and limited drug response data. This paper addresses\nthese challenges through a novel representation learning method in the first\nphase and weak supervision in the second. Experimental results on real patient\ndata demonstrate the efficacy of our method (WISER) over state-of-the-art\nalternatives on predicting personalized drug response.\n"", '  AI-driven precision oncology has the transformative potential to reshape\ncancer treatment by leveraging the power of AI models to analyze the\ninteraction between complex patient characteristics and their corresponding\ntreatment outcomes. New technological platforms have facilitated the timely\nacquisition of multimodal data on tumor biology at an unprecedented resolution,\nsuch as single-cell multi-omics data, making this quality and quantity of data\navailable for data-driven improved clinical decision-making. In this work, we\npropose a modular machine learning framework designed for personalized\ncounterfactual cancer treatment suggestions based on an ensemble of machine\nlearning experts trained on diverse multi-omics technologies. These specialized\ncounterfactual experts per technology are consistently aggregated into a more\npowerful expert with superior performance and can provide both confidence and\nan explanation of its decision. The framework is tailored to address critical\nchallenges inherent in data-driven cancer research, including the\nhigh-dimensional nature of the data, and the presence of treatment assignment\nbias in the retrospective observational data. The framework is showcased\nthrough comprehensive demonstrations using data from in-vitro and in-vivo\ntreatment responses from a cohort of patients with ovarian cancer. Our method\naims to empower clinicians with a reality-centric decision-support tool\nincluding probabilistic treatment suggestions with calibrated confidence and\npersonalized explanations for tailoring treatment strategies to multi-omics\ncharacteristics of individual cancer patients.\n']",Computational Methods for Cancer Genomics and Transcriptomics,Multi-Omics in Cancer Research and Genomics
189,"Retrosynthesis Planning in Organic Chemistry , ""Chemical Reaction Prediction and Synthesis""","['retrosynthesis', 'retrosynthetic', 'synthesisability', 'synthesis', 'synthesize', 'retrosig', 'retrograph', 'retrowise', 'retrogfn', 'synthons'] , ['chemical', 'chemistry', 'chemists', 'chemist', 'molecule', 'molecular', 'catalysts', 'chemicals', 'synthesis', 'molecules']","['  Retrosynthesis is a fundamental but challenging task in organic chemistry,\nwith broad applications in fields such as drug design and synthesis. Given a\ntarget molecule, the goal of retrosynthesis is to find out a series of\nreactions which could be assembled into a synthetic route which starts from\npurchasable molecules and ends at the target molecule. The uncertainty of\nreactions used in retrosynthetic planning, which is caused by hallucinations of\nbackward models, has recently been noticed. In this paper we propose a succinct\nprobabilistic model to describe such uncertainty. Based on the model, we\npropose a new retrosynthesis planning algorithm called retro-prob to maximize\nthe successful synthesis probability of target molecules, which acquires high\nefficiency by utilizing the chain rule of derivatives. Experiments on the\nParoutes benchmark show that retro-prob outperforms previous algorithms, retro*\nand retro-fallback, both in speed and in the quality of synthesis plans.\n', '  Retrosynthesis planning is a fundamental challenge in chemistry which aims at\ndesigning reaction pathways from commercially available starting materials to a\ntarget molecule. Each step in multi-step retrosynthesis planning requires\naccurate prediction of possible precursor molecules given the target molecule\nand confidence estimates to guide heuristic search algorithms. We model\nsingle-step retrosynthesis planning as a distribution learning problem in a\ndiscrete state space. First, we introduce the Markov Bridge Model, a generative\nframework aimed to approximate the dependency between two intractable discrete\ndistributions accessible via a finite sample of coupled data points. Our\nframework is based on the concept of a Markov bridge, a Markov process pinned\nat its endpoints. Unlike diffusion-based methods, our Markov Bridge Model does\nnot need a tractable noise distribution as a sampling proxy and directly\noperates on the input product molecules as samples from the intractable prior\ndistribution. We then address the retrosynthesis planning problem with our\nnovel framework and introduce RetroBridge, a template-free retrosynthesis\nmodeling approach that achieves state-of-the-art results on standard evaluation\nbenchmarks.\n', '  Retrosynthesis consists of breaking down a chemical compound recursively\nstep-by-step into molecular precursors until a set of commercially available\nmolecules is found with the goal to provide a synthesis route. Its two primary\nresearch directions, single-step retrosynthesis prediction, which models the\nchemical reaction logic, and multi-step synthesis planning, which tries to find\nthe correct sequence of reactions, are inherently intertwined. Still, this\nconnection is not reflected in contemporary research. In this work, we combine\nthese two major research directions by applying multiple single-step\nretrosynthesis models within multi-step synthesis planning and analyzing their\nimpact using public and proprietary reaction data. We find a disconnection\nbetween high single-step performance and potential route-finding success,\nsuggesting that single-step models must be evaluated within synthesis planning\nin the future. Furthermore, we show that the commonly used single-step\nretrosynthesis benchmark dataset USPTO-50k is insufficient as this evaluation\ntask does not represent model performance and scalability on larger and more\ndiverse datasets. For multi-step synthesis planning, we show that the choice of\nthe single-step model can improve the overall success rate of synthesis\nplanning by up to +28% compared to the commonly used baseline model. Finally,\nwe show that each single-step model finds unique synthesis routes, and differs\nin aspects such as route-finding success, the number of found synthesis routes,\nand chemical validity, making the combination of single-step retrosynthesis\nprediction and multi-step synthesis planning a crucial aspect when developing\nfuture methods.\n'] , [""  The task of chemical reaction predictions (CRPs) plays a pivotal role in\nadvancing drug discovery and material science. However, its effectiveness is\nconstrained by the vast and uncertain chemical reaction space and challenges in\ncapturing reaction selectivity, particularly due to existing methods'\nlimitations in exploiting the data's inherent knowledge. To address these\nchallenges, we introduce a data-curated self-feedback knowledge elicitation\napproach. This method starts from iterative optimization of molecular\nrepresentations and facilitates the extraction of knowledge on chemical\nreaction types (RTs). Then, we employ adaptive prompt learning to infuse the\nprior knowledge into the large language model (LLM). As a result, we achieve\nsignificant enhancements: a 14.2% increase in retrosynthesis prediction\naccuracy, a 74.2% rise in reagent prediction accuracy, and an expansion in the\nmodel's capability for handling multi-task chemical reactions. This research\noffers a novel paradigm for knowledge elicitation in scientific research and\nshowcases the untapped potential of LLMs in CRPs.\n"", '  Chemical reactions are the fundamental building blocks of drug design and\norganic chemistry research. In recent years, there has been a growing need for\na large-scale deep-learning framework that can efficiently capture the basic\nrules of chemical reactions. In this paper, we have proposed a unified\nframework that addresses both the reaction representation learning and molecule\ngeneration tasks, which allows for a more holistic approach. Inspired by the\norganic chemistry mechanism, we develop a novel pretraining framework that\nenables us to incorporate inductive biases into the model. Our framework\nachieves state-of-the-art results on challenging downstream tasks. By\npossessing chemical knowledge, our generative framework overcome the\nlimitations of current molecule generation models that rely on a small number\nof reaction templates. In the extensive experiments, our model generates\nsynthesizable drug-like structures of high quality. Overall, our work presents\na significant step toward a large-scale deep-learning framework for a variety\nof reaction-based applications.\n', '  Large language models (LLMs) have gained widespread interest due to their\nability to process human language and perform tasks on which they have not been\nexplicitly trained. This is relevant for the chemical sciences, which face the\nproblem of small and diverse datasets that are frequently in the form of text.\nLLMs have shown promise in addressing these issues and are increasingly being\nharnessed to predict chemical properties, optimize reactions, and even design\nand conduct experiments autonomously. However, we still have only a very\nlimited systematic understanding of the chemical reasoning capabilities of\nLLMs, which would be required to improve models and mitigate potential harms.\nHere, we introduce ""ChemBench,"" an automated framework designed to rigorously\nevaluate the chemical knowledge and reasoning abilities of state-of-the-art\nLLMs against the expertise of human chemists. We curated more than 7,000\nquestion-answer pairs for a wide array of subfields of the chemical sciences,\nevaluated leading open and closed-source LLMs, and found that the best models\noutperformed the best human chemists in our study on average. The models,\nhowever, struggle with some chemical reasoning tasks that are easy for human\nexperts and provide overconfident, misleading predictions, such as about\nchemicals\' safety profiles. These findings underscore the dual reality that,\nalthough LLMs demonstrate remarkable proficiency in chemical tasks, further\nresearch is critical to enhancing their safety and utility in chemical\nsciences. Our findings also indicate a need for adaptations to chemistry\ncurricula and highlight the importance of continuing to develop evaluation\nframeworks to improve safe and useful LLMs.\n']",Artificial Intelligence in Organic Chemistry Synthesis,"""Chemical Reaction Prediction and Synthesis"""
190,"Molecular Representation Learning for Drug Discovery , ""Crystal Structure Prediction and Materials Discovery"" , Molecular Structure Modeling and Prediction , Bioinformatics and Molecular Biology","['molecular', 'molecule', 'molecules', 'ligands', 'proteins', 'ligand', 'protein', 'receptor', 'modeling', 'models'] , ['crystals', 'crystal', 'crystalline', 'molecular', 'lattice', 'atoms', 'atom', 'alloys', 'predicting', 'molecules'] , ['molecular', 'molecule', 'molecules', 'networks', 'neural', 'graphs', 'representations', 'ligand', 'graph', 'protein'] , ['bioinformatics', 'proteins', 'knowledgebase', 'molecular', 'protein', 'bio', 'molecule', 'biomedical', 'gene', 'enzyme']","[""  Molecular representation learning is pivotal for various molecular property\nprediction tasks related to drug discovery. Robust and accurate benchmarks are\nessential for refining and validating current methods. Existing molecular\nproperty benchmarks derived from wet experiments, however, face limitations\nsuch as data volume constraints, unbalanced label distribution, and noisy\nlabels. To address these issues, we construct a large-scale and precise\nmolecular representation dataset of approximately 140,000 small molecules,\nmeticulously designed to capture an extensive array of chemical, physical, and\nbiological properties, derived through a robust computational ligand-target\nbinding analysis pipeline. We conduct extensive experiments on various deep\nlearning models, demonstrating that our dataset offers significant\nphysicochemical interpretability to guide model development and design.\nNotably, the dataset's properties are linked to binding affinity metrics,\nproviding additional insights into model performance in drug-target interaction\ntasks. We believe this dataset will serve as a more accurate and reliable\nbenchmark for molecular representation learning, thereby expediting progress in\nthe field of artificial intelligence-driven drug discovery.\n"", ""  The rise of cost involved with drug discovery and current speed of which they\nare discover, underscore the need for more efficient structure-based drug\ndesign (SBDD) methods. We employ Generative Flow Networks (GFlowNets), to\neffectively explore the vast combinatorial space of drug-like molecules, which\ntraditional virtual screening methods fail to cover. We introduce a novel\nmodification to the GFlowNet framework by incorporating trigonometrically\nconsistent embeddings, previously utilized in tasks involving protein\nconformation and protein-ligand interactions, to enhance the model's ability to\ngenerate molecules tailored to specific protein pockets. We have modified the\nexisting protein conditioning used by GFlowNets, blending geometric information\nfrom both protein and ligand embeddings to achieve more geometrically\nconsistent embeddings. Experiments conducted using CrossDocked2020 demonstrated\nan improvement in the binding affinity between generated molecules and protein\npockets for both single and multi-objective tasks, compared to previous work.\nAdditionally, we propose future work aimed at further increasing the geometric\ninformation captured in protein-ligand interactions.\n"", ""  Diffusion models have emerged as powerful tools for molecular generation,\nparticularly in the context of 3D molecular structures. Inspired by\nnon-equilibrium statistical physics, these models can generate 3D molecular\nstructures with specific properties or requirements crucial to drug discovery.\nDiffusion models were particularly successful at learning 3D molecular\ngeometries' complex probability distributions and their corresponding chemical\nand physical properties through forward and reverse diffusion processes. This\nreview focuses on the technical implementation of diffusion models tailored for\n3D molecular generation. It compares the performance, evaluation methods, and\nimplementation details of various diffusion models used for molecular\ngeneration tasks. We cover strategies for atom and bond representation,\narchitectures of reverse diffusion denoising networks, and challenges\nassociated with generating stable 3D molecular structures. This review also\nexplores the applications of diffusion models in $\\textit{de novo}$ drug design\nand related areas of computational chemistry, such as structure-based drug\ndesign, including target-specific molecular generation, molecular docking, and\nmolecular dynamics of protein-ligand complexes. We also cover conditional\ngeneration on physical properties, conformation generation, and fragment-based\ndrug design. By summarizing the state-of-the-art diffusion models for 3D\nmolecular generation, this review sheds light on their role in advancing drug\ndiscovery as well as their current limitations.\n""] , ['  Recent advances in deep learning have enabled the generation of realistic\ndata by training generative models on large datasets of text, images, and\naudio. While these models have demonstrated exceptional performance in\ngenerating novel and plausible data, it remains an open question whether they\ncan effectively accelerate scientific discovery through the data generation and\ndrive significant advancements across various scientific fields. In particular,\nthe discovery of new inorganic materials with promising properties poses a\ncritical challenge, both scientifically and for industrial applications.\nHowever, unlike textual or image data, materials, or more specifically crystal\nstructures, consist of multiple types of variables - including lattice vectors,\natom positions, and atomic species. This complexity in data give rise to a\nvariety of approaches for representing and generating such data. Consequently,\nthe design choices of generative models for crystal structures remain an open\nquestion. In this study, we explore a new type of diffusion model for the\ngenerative inverse design of crystal structures, with a backbone based on a\nTransformer architecture. We demonstrate our models are superior to previous\nmethods in their versatility for generating crystal structures with desired\nproperties. Furthermore, our empirical results suggest that the optimal\nconditioning methods vary depending on the dataset.\n', '  The calculation of electron density distribution using density functional\ntheory (DFT) in materials and molecules is central to the study of their\nquantum and macro-scale properties, yet accurate and efficient calculation\nremains a long-standing challenge. We introduce ChargE3Net, an E(3)-equivariant\ngraph neural network for predicting electron density in atomic systems.\nChargE3Net enables the learning of higher-order equivariant feature to achieve\nhigh predictive accuracy and model expressivity. We show that ChargE3Net\nexceeds the performance of prior work on diverse sets of molecules and\nmaterials. When trained on the massive dataset of over 100K materials in the\nMaterials Project database, our model is able to capture the complexity and\nvariability in the data, leading to a significant 26.7% reduction in\nself-consistent iterations when used to initialize DFT calculations on unseen\nmaterials. Furthermore, we show that non-self-consistent DFT calculations using\nour predicted charge densities yield near-DFT performance on electronic and\nthermodynamic property prediction at a fraction of the computational cost.\nFurther analysis attributes the greater predictive accuracy to improved\nmodeling of systems with high angular variations. These results illuminate a\npathway towards a machine learning-accelerated ab initio calculations for\nmaterials discovery.\n', ""  Computational prediction of stable crystal structures has a profound impact\non the large-scale discovery of novel functional materials. However, predicting\nthe crystal structure solely from a material's composition or formula is a\npromising yet challenging task, as traditional ab initio crystal structure\nprediction (CSP) methods rely on time-consuming global searches and\nfirst-principles free energy calculations. Inspired by the recent success of\ndeep learning approaches in protein structure prediction, which utilize\npairwise amino acid interactions to describe 3D structures, we present\nAlphaCrystal-II, a novel knowledge-based solution that exploits the abundant\ninter-atomic interaction patterns found in existing known crystal structures.\nAlphaCrystal-II predicts the atomic distance matrix of a target crystal\nmaterial and employs this matrix to reconstruct its 3D crystal structure. By\nleveraging the wealth of inter-atomic relationships of known crystal\nstructures, our approach demonstrates remarkable effectiveness and reliability\nin structure prediction through comprehensive experiments. This work highlights\nthe potential of data-driven methods in accelerating the discovery and design\nof new materials with tailored properties.\n""] , [""  The integration of molecule and language has garnered increasing attention in\nmolecular science. Recent advancements in Language Models (LMs) have\ndemonstrated potential for the comprehensive modeling of molecule and language.\nHowever, existing works exhibit notable limitations. Most existing works\noverlook the modeling of 3D information, which is crucial for understanding\nmolecular structures and also functions. While some attempts have been made to\nleverage external structure encoding modules to inject the 3D molecular\ninformation into LMs, there exist obvious difficulties that hinder the\nintegration of molecular structure and language text, such as modality\nalignment and separate tuning. To bridge this gap, we propose 3D-MolT5, a\nunified framework designed to model both 1D molecular sequence and 3D molecular\nstructure. The key innovation lies in our methodology for mapping fine-grained\n3D substructure representations (based on 3D molecular fingerprints) to a\nspecialized 3D token vocabulary for 3D-MolT5. This 3D structure token\nvocabulary enables the seamless combination of 1D sequence and 3D structure\nrepresentations in a tokenized format, allowing 3D-MolT5 to encode molecular\nsequence (SELFIES), molecular structure, and text sequences within a unified\narchitecture. Alongside, we further introduce 1D and 3D joint pre-training to\nenhance the model's comprehension of these diverse modalities in a joint\nrepresentation space and better generalize to various tasks for our foundation\nmodel. Through instruction tuning on multiple downstream datasets, our proposed\n3D-MolT5 shows superior performance than existing methods in molecular property\nprediction, molecule captioning, and text-based molecule generation tasks. Our\ncode will be available on GitHub soon.\n"", ""  Molecular property prediction is a key component of AI-driven drug discovery\nand molecular characterization learning. Despite recent advances, existing\nmethods still face challenges such as limited ability to generalize, and\ninadequate representation of learning from unlabeled data, especially for tasks\nspecific to molecular structures. To address these limitations, we introduce\nDIG-Mol, a novel self-supervised graph neural network framework for molecular\nproperty prediction. This architecture leverages the power of contrast learning\nwith dual interaction mechanisms and unique molecular graph enhancement\nstrategies. DIG-Mol integrates a momentum distillation network with two\ninterconnected networks to efficiently improve molecular characterization. The\nframework's ability to extract key information about molecular structure and\nhigher-order semantics is supported by minimizing loss of contrast. We have\nestablished DIG-Mol's state-of-the-art performance through extensive\nexperimental evaluation in a variety of molecular property prediction tasks. In\naddition to demonstrating superior transferability in a small number of\nlearning scenarios, our visualizations highlight DIG-Mol's enhanced\ninterpretability and representation capabilities. These findings confirm the\neffectiveness of our approach in overcoming challenges faced by traditional\nmethods and mark a significant advance in molecular property prediction.\n"", '  Diffusion generative models have emerged as a powerful framework for\naddressing problems in structural biology and structure-based drug design.\nThese models operate directly on 3D molecular structures. Due to the\nunfavorable scaling of graph neural networks (GNNs) with graph size as well as\nthe relatively slow inference speeds inherent to diffusion models, many\nexisting molecular diffusion models rely on coarse-grained representations of\nprotein structure to make training and inference feasible. However, such\ncoarse-grained representations discard essential information for modeling\nmolecular interactions and impair the quality of generated structures. In this\nwork, we present a novel GNN-based architecture for learning latent\nrepresentations of molecular structure. When trained end-to-end with a\ndiffusion model for de novo ligand design, our model achieves comparable\nperformance to one with an all-atom protein representation while exhibiting a\n3-fold reduction in inference time.\n'] , [""  In this study, we generate and maintain a database of 10 million virtual\nlipids through METiS's in-house de novo lipid generation algorithms and lipid\nvirtual screening techniques. These virtual lipids serve as a corpus for\npre-training, lipid representation learning, and downstream task knowledge\ntransfer, culminating in state-of-the-art LNP property prediction performance.\nWe propose LipidBERT, a BERT-like model pre-trained with the Masked Language\nModel (MLM) and various secondary tasks. Additionally, we compare the\nperformance of embeddings generated by LipidBERT and PhatGPT, our GPT-like\nlipid generation model, on downstream tasks. The proposed bilingual LipidBERT\nmodel operates in two languages: the language of ionizable lipid pre-training,\nusing in-house dry-lab lipid structures, and the language of LNP fine-tuning,\nutilizing in-house LNP wet-lab data. This dual capability positions LipidBERT\nas a key AI-based filter for future screening tasks, including new versions of\nMETiS de novo lipid libraries and, more importantly, candidates for in vivo\ntesting for orgran-targeting LNPs. To the best of our knowledge, this is the\nfirst successful demonstration of the capability of a pre-trained language\nmodel on virtual lipids and its effectiveness in downstream tasks using web-lab\ndata. This work showcases the clever utilization of METiS's in-house de novo\nlipid library as well as the power of dry-wet lab integration.\n"", '  Recent research trends in computational biology have increasingly focused on\nintegrating text and bio-entity modeling, especially in the context of\nmolecules and proteins. However, previous efforts like BioT5 faced challenges\nin generalizing across diverse tasks and lacked a nuanced understanding of\nmolecular structures, particularly in their textual representations (e.g.,\nIUPAC). This paper introduces BioT5+, an extension of the BioT5 framework,\ntailored to enhance biological research and drug discovery. BioT5+ incorporates\nseveral novel features: integration of IUPAC names for molecular understanding,\ninclusion of extensive bio-text and molecule data from sources like bioRxiv and\nPubChem, the multi-task instruction tuning for generality across tasks, and a\nnumerical tokenization technique for improved processing of numerical data.\nThese enhancements allow BioT5+ to bridge the gap between molecular\nrepresentations and their textual descriptions, providing a more holistic\nunderstanding of biological entities, and largely improving the grounded\nreasoning of bio-text and bio-sequences. The model is pre-trained and\nfine-tuned with a large number of experiments, including \\emph{3 types of\nproblems (classification, regression, generation), 15 kinds of tasks, and 21\ntotal benchmark datasets}, demonstrating the remarkable performance and\nstate-of-the-art results in most cases. BioT5+ stands out for its ability to\ncapture intricate relationships in biological data, thereby contributing\nsignificantly to bioinformatics and computational biology. Our code is\navailable at \\url{https://github.com/QizhiPei/BioT5}.\n', '  Expert curation is essential to capture knowledge of enzyme functions from\nthe scientific literature in FAIR open knowledgebases but cannot keep pace with\nthe rate of new discoveries and new publications. In this work we present\nEnzChemRED, for Enzyme Chemistry Relation Extraction Dataset, a new training\nand benchmarking dataset to support the development of Natural Language\nProcessing (NLP) methods such as (large) language models that can assist enzyme\ncuration. EnzChemRED consists of 1,210 expert curated PubMed abstracts in which\nenzymes and the chemical reactions they catalyze are annotated using\nidentifiers from the UniProt Knowledgebase (UniProtKB) and the ontology of\nChemical Entities of Biological Interest (ChEBI). We show that fine-tuning\npre-trained language models with EnzChemRED can significantly boost their\nability to identify mentions of proteins and chemicals in text (Named Entity\nRecognition, or NER) and to extract the chemical conversions in which they\nparticipate (Relation Extraction, or RE), with average F1 score of 86.30% for\nNER, 86.66% for RE for chemical conversion pairs, and 83.79% for RE for\nchemical conversion pairs and linked enzymes. We combine the best performing\nmethods after fine-tuning using EnzChemRED to create an end-to-end pipeline for\nknowledge extraction from text and apply this to abstracts at PubMed scale to\ncreate a draft map of enzyme functions in literature to guide curation efforts\nin UniProtKB and the reaction knowledgebase Rhea. The EnzChemRED corpus is\nfreely available at https://ftp.expasy.org/databases/rhea/nlp/.\n']",Computational Methods for Molecular and Materials Science,Molecular Structure Modeling and Prediction
191,Boltzmann Generators for Molecular Systems,"['boltzmann', 'molecule', 'molecules', 'molecular', 'simulations', 'sampling', 'kinetic', 'atomnet', 'particles', 'flow']","['  The generation of equilibrium samples of molecular systems has been a\nlong-standing problem in statistical physics. Boltzmann Generators are a\ngenerative machine learning method that addresses this issue by learning a\ntransformation via a normalizing flow from a simple prior distribution to the\ntarget Boltzmann distribution of interest. Recently, flow matching has been\nemployed to train Boltzmann Generators for small molecular systems in Cartesian\ncoordinates. We extend this work and propose a first framework for Boltzmann\nGenerators that are transferable across chemical space, such that they predict\nzero-shot Boltzmann distributions for test molecules without being retrained\nfor these systems. These transferable Boltzmann Generators allow approximate\nsampling from the target distribution of unseen systems, as well as efficient\nreweighting to the target Boltzmann distribution. The transferability of the\nproposed framework is evaluated on dipeptides, where we show that it\ngeneralizes efficiently to unseen systems. Furthermore, we demonstrate that our\nproposed architecture enhances the efficiency of Boltzmann Generators trained\non single molecular systems.\n', ""  Sampling all possible transition paths between two 3D states of a molecular\nsystem has various applications ranging from catalyst design to drug discovery.\nCurrent approaches to sample transition paths use Markov chain Monte Carlo and\nrely on time-intensive molecular dynamics simulations to find new paths. Our\napproach operates in the latent space of a normalizing flow that maps from the\nmolecule's Boltzmann distribution to a Gaussian, where we propose new paths\nwithout requiring molecular simulations. Using alanine dipeptide, we explore\nMetropolis-Hastings acceptance criteria in the latent space for exact sampling\nand investigate different latent proposal mechanisms.\n"", '  Efficient sampling of the Boltzmann distribution of molecular systems is a\nlong-standing challenge. Recently, instead of generating long molecular\ndynamics simulations, generative machine learning methods such as normalizing\nflows have been used to learn the Boltzmann distribution directly, without\nsamples. However, this approach is susceptible to mode collapse and thus often\ndoes not explore the full configurational space. In this work, we address this\nchallenge by separating the problem into two levels, the fine-grained and\ncoarse-grained degrees of freedom. A normalizing flow conditioned on the\ncoarse-grained space yields a probabilistic connection between the two levels.\nTo explore the configurational space, we employ coarse-grained simulations with\nactive learning which allows us to update the flow and make all-atom potential\nenergy evaluations only when necessary. Using alanine dipeptide as an example,\nwe show that our methods obtain a speedup to molecular dynamics simulations of\napproximately 15.9 to 216.2 compared to the speedup of 4.5 of the current\nstate-of-the-art machine learning approach.\n']",Machine Learning for Molecular Simulation and Sampling,Boltzmann Generators for Molecular Systems
192,"""Social Media Analysis for Depression Detection"" , ""Depression Detection using Multimodal Data and AI"" , ""Depression Detection using Social Media Data"" , Multimodal Depression Detection","['depressive', 'depression', 'tweets', 'twitter', 'cnn', 'nlp', 'depressed', 'facebook', 'suicide', 'lstm'] , ['depressive', 'depression', 'depressed', 'speech', 'audio', 'distress', 'suicide', 'recordings', 'features', 'psychiatric'] , ['depression', 'depressive', 'health', 'wellbeing', 'sentiment', 'data', 'mental', 'anxiety', 'psychological', 'monitoring'] , ['multimodal', 'attention', 'taskbot', 'modality', 'multidialog', 'depression', 'modal', 'features', 'conversational', 'visual']","[""  Depression is one of the most common mental disorders affecting an\nindividual's personal and professional life. In this work, we investigated the\npossibility of utilizing social media posts to identify depression in\nindividuals. To achieve this goal, we conducted a preliminary study where we\nextracted and analyzed the top Reddit posts made in 2022 from\ndepression-related forums. The collected data were labeled as depressive and\nnon-depressive using UMLS Metathesaurus. Further, the pre-processed data were\nfed to classical machine learning models, where we achieved an accuracy of\n92.28\\% in predicting the depressive and non-depressive posts.\n"", '  Depression is a widespread mental health issue, affecting an estimated 3.8%\nof the global population. It is also one of the main contributors to disability\nworldwide. Recently it is becoming popular for individuals to use social media\nplatforms (e.g., Reddit) to express their difficulties and health issues (e.g.,\ndepression) and seek support from other users in online communities. It opens\ngreat opportunities to automatically identify social media users with\ndepression by parsing millions of posts for potential interventions. Deep\nlearning methods have begun to dominate in the field of machine learning and\nnatural language processing (NLP) because of their ease of use, efficient\nprocessing, and state-of-the-art results on many NLP tasks. In this work, we\npropose a hybrid deep learning model which combines a pretrained sentence BERT\n(SBERT) and convolutional neural network (CNN) to detect individuals with\ndepression with their Reddit posts. The sentence BERT is used to learn the\nmeaningful representation of semantic information in each post. CNN enables the\nfurther transformation of those embeddings and the temporal identification of\nbehavioral patterns of users. We trained and evaluated the model performance to\nidentify Reddit users with depression by utilizing the Self-reported Mental\nHealth Diagnoses (SMHD) data. The hybrid deep learning model achieved an\naccuracy of 0.86 and an F1 score of 0.86 and outperformed the state-of-the-art\ndocumented result (F1 score of 0.79) by other machine learning models in the\nliterature. The results show the feasibility of the hybrid model to identify\nindividuals with depression. Although the hybrid model is validated to detect\ndepression with Reddit posts, it can be easily tuned and applied to other text\nclassification tasks and different clinical applications.\n', ""  The COVID-19 pandemic has escalated mental health crises worldwide, with\nsocial isolation and economic instability contributing to a rise in suicidal\nbehavior. Suicide can result from social factors such as shame, abuse,\nabandonment, and mental health conditions like depression, Post-Traumatic\nStress Disorder (PTSD), Attention-Deficit/Hyperactivity Disorder (ADHD),\nanxiety disorders, and bipolar disorders. As these conditions develop, signs of\nsuicidal ideation may manifest in social media interactions. Analyzing social\nmedia data using artificial intelligence (AI) techniques can help identify\npatterns of suicidal behavior, providing invaluable insights for suicide\nprevention agencies, professionals, and broader community awareness\ninitiatives. Machine learning algorithms for this purpose require large volumes\nof accurately labeled data. Previous research has not fully explored the\npotential of incorporating explanations in analyzing and labeling longitudinal\nsocial media data. In this study, we employed a model explanation method, Layer\nIntegrated Gradients, on top of a fine-tuned state-of-the-art language model,\nto assign each token from Reddit users' posts an attribution score for\npredicting suicidal ideation. By extracting and analyzing attributions of\ntokens from the data, we propose a methodology for preliminary screening of\nsocial media posts for suicidal ideation without using large language models\nduring inference.\n""] , [""  Depression can significantly impact many aspects of an individual's life,\nincluding their personal and social functioning, academic and work performance,\nand overall quality of life. Many researchers within the field of affective\ncomputing are adopting deep learning technology to explore potential patterns\nrelated to the detection of depression. However, because of subjects' privacy\nprotection concerns, that data in this area is still scarce, presenting a\nchallenge for the deep discriminative models used in detecting depression. To\nnavigate these obstacles, a large-scale multimodal vlog dataset (LMVD), for\ndepression recognition in the wild is built. In LMVD, which has 1823 samples\nwith 214 hours of the 1475 participants captured from four multimedia platforms\n(Sina Weibo, Bilibili, Tiktok, and YouTube). A novel architecture termed\nMDDformer to learn the non-verbal behaviors of individuals is proposed.\nExtensive validations are performed on the LMVD dataset, demonstrating superior\nperformance for depression detection. We anticipate that the LMVD will\ncontribute a valuable function to the depression detection community. The data\nand code will released at the link: https://github.com/helang818/LMVD/.\n"", '  Depression is a critical concern in global mental health, prompting extensive\nresearch into AI-based detection methods. Among various AI technologies, Large\nLanguage Models (LLMs) stand out for their versatility in mental healthcare\napplications. However, their primary limitation arises from their exclusive\ndependence on textual input, which constrains their overall capabilities.\nFurthermore, the utilization of LLMs in identifying and analyzing depressive\nstates is still relatively untapped. In this paper, we present an innovative\napproach to integrating acoustic speech information into the LLMs framework for\nmultimodal depression detection. We investigate an efficient method for\ndepression detection by integrating speech signals into LLMs utilizing Acoustic\nLandmarks. By incorporating acoustic landmarks, which are specific to the\npronunciation of spoken words, our method adds critical dimensions to text\ntranscripts. This integration also provides insights into the unique speech\npatterns of individuals, revealing the potential mental states of individuals.\nEvaluations of the proposed approach on the DAIC-WOZ dataset reveal\nstate-of-the-art results when compared with existing Audio-Text baselines. In\naddition, this approach is not only valuable for the detection of depression\nbut also represents a new perspective in enhancing the ability of LLMs to\ncomprehend and process speech signals.\n', '  Current automatic depression detection systems provide predictions directly\nwithout relying on the individual symptoms/items of depression as denoted in\nthe clinical depression rating scales. In contrast, clinicians assess each item\nin the depression rating scale in a clinical setting, thus implicitly providing\na more detailed rationale for a depression diagnosis. In this work, we make a\nfirst step towards using the acoustic features of speech to predict individual\nitems of the depression rating scale before obtaining the final depression\nprediction. For this, we use convolutional (CNN) and recurrent (long short-term\nmemory (LSTM)) neural networks. We consider different approaches to learning\nthe temporal context of speech. Further, we analyze two variants of voting\nschemes for individual item prediction and depression detection. We also\ninclude an animated visualization that shows an example of item prediction over\ntime as the speech progresses.\n'] , [""  Depression is a common disease worldwide. It is difficult to diagnose and\ncontinues to be underdiagnosed. Because depressed patients constantly share\ntheir symptoms, major life events, and treatments on social media, researchers\nare turning to user-generated digital traces on social media for depression\ndetection. Such methods have distinct advantages in combating depression\nbecause they can facilitate innovative approaches to fight depression and\nalleviate its social and economic burden. However, most existing studies lack\neffective means to incorporate established medical domain knowledge in\ndepression detection or suffer from feature extraction difficulties that impede\ngreater performance. Following the design science research paradigm, we propose\na Deep Knowledge-aware Depression Detection (DKDD) framework to accurately\ndetect social media users at risk of depression and explain the critical\nfactors that contribute to such detection. Extensive empirical studies with\nreal-world data demonstrate that, by incorporating domain knowledge, our method\noutperforms existing state-of-the-art methods. Our work has significant\nimplications for IS research in knowledge-aware machine learning, digital\ntraces utilization, and NLP research in IS. Practically, by providing early\ndetection and explaining the critical factors, DKDD can supplement clinical\ndepression screening and enable large-scale evaluations of a population's\nmental health status.\n"", '  This work explores the utilization of Romanized Sinhala social media data to\nidentify individuals at risk of depression. A machine learning-based framework\nis presented for the automatic screening of depression symptoms by analyzing\nlanguage patterns, sentiment, and behavioural cues within a comprehensive\ndataset of social media posts. The research has been carried out to compare the\nsuitability of Neural Networks over the classical machine learning techniques.\nThe proposed Neural Network with an attention layer which is capable of\nhandling long sequence data, attains a remarkable accuracy of 93.25% in\ndetecting depression symptoms, surpassing current state-of-the-art methods.\nThese findings underscore the efficacy of this approach in pinpointing\nindividuals in need of proactive interventions and support. Mental health\nprofessionals, policymakers, and social media companies can gain valuable\ninsights through the proposed model. Leveraging natural language processing\ntechniques and machine learning algorithms, this work offers a promising\npathway for mental health screening in the digital era. By harnessing the\npotential of social media data, the framework introduces a proactive method for\nrecognizing and assisting individuals at risk of depression. In conclusion,\nthis research contributes to the advancement of proactive interventions and\nsupport systems for mental health, thereby influencing both research and\npractical applications in the field.\n', '  We introduce a multi-layer perceptron (MLP) called the COVID-19 Depression\nand Anxiety Predictor (CoDAP) to predict mental health trends, particularly\nanxiety and depression, during the COVID-19 pandemic. Our method utilizes a\ncomprehensive dataset, which tracked mental health symptoms weekly over ten\nweeks during the initial COVID-19 wave (April to June 2020) in a diverse cohort\nof U.S. adults. This period, characterized by a surge in mental health symptoms\nand conditions, offers a critical context for our analysis. Our focus was to\nextract and analyze patterns of anxiety and depression through a unique lens of\nqualitative individual attributes using CoDAP. This model not only predicts\npatterns of anxiety and depression during the pandemic but also unveils key\ninsights into the interplay of demographic factors, behavioral changes, and\nsocial determinants of mental health. These findings contribute to a more\nnuanced understanding of the complexity of mental health issues in times of\nglobal health crises, potentially guiding future early interventions.\n'] , ['  Multimodal depression detection is an important research topic that aims to\npredict human mental states using multimodal data. Previous methods treat\ndifferent modalities equally and fuse each modality by na\\""ive mathematical\noperations without measuring the relative importance between them, which cannot\nobtain well-performed multimodal representations for downstream depression\ntasks. In order to tackle the aforementioned concern, we present a Cross-modal\nAttention Network with Adaptive Multi-modal Recurrent Fusion (CANAMRF) for\nmultimodal depression detection. CANAMRF is constructed by a multimodal feature\nextractor, an Adaptive Multimodal Recurrent Fusion module, and a Hybrid\nAttention Module. Through experimentation on two benchmark datasets, CANAMRF\ndemonstrates state-of-the-art performance, underscoring the effectiveness of\nour proposed approach.\n', '  Early detection plays a crucial role in the treatment of depression.\nTherefore, numerous studies have focused on social media platforms, where\nindividuals express their emotions, aiming to achieve early detection of\ndepression. However, the majority of existing approaches often rely on specific\nfeatures, leading to limited scalability across different types of social media\ndatasets, such as text, images, or videos. To overcome this limitation, we\nintroduce a Multimodal Object-Oriented Graph Attention Model (MOGAM), which can\nbe applied to diverse types of data, offering a more scalable and versatile\nsolution. Furthermore, to ensure that our model can capture authentic symptoms\nof depression, we only include vlogs from users with a clinical diagnosis. To\nleverage the diverse features of vlogs, we adopt a multimodal approach and\ncollect additional metadata such as the title, description, and duration of the\nvlogs. To effectively aggregate these multimodal features, we employed a\ncross-attention mechanism. MOGAM achieved an accuracy of 0.871 and an F1-score\nof 0.888. Moreover, to validate the scalability of MOGAM, we evaluated its\nperformance with a benchmark dataset and achieved comparable results with prior\nstudies (0.61 F1-score). In conclusion, we believe that the proposed model,\nMOGAM, is an effective solution for detecting depression in social media,\noffering potential benefits in the early detection and treatment of this mental\nhealth condition.\n', ""  Depression, a prevalent and serious mental health issue, affects\napproximately 3.8\\% of the global population. Despite the existence of\neffective treatments, over 75\\% of individuals in low- and middle-income\ncountries remain untreated, partly due to the challenge in accurately\ndiagnosing depression in its early stages. This paper introduces a novel method\nfor detecting depression based on multi-modal feature fusion utilizing\ncross-attention. By employing MacBERT as a pre-training model to extract\nlexical features from text and incorporating an additional Transformer module\nto refine task-specific contextual understanding, the model's adaptability to\nthe targeted task is enhanced. Diverging from previous practices of simply\nconcatenating multimodal features, this approach leverages cross-attention for\nfeature integration, significantly improving the accuracy in depression\ndetection and enabling a more comprehensive and precise analysis of user\nemotions and behaviors. Furthermore, a Multi-Modal Feature Fusion Network based\non Cross-Attention (MFFNC) is constructed, demonstrating exceptional\nperformance in the task of depression identification. The experimental results\nindicate that our method achieves an accuracy of 0.9495 on the test dataset,\nmarking a substantial improvement over existing approaches. Moreover, it\noutlines a promising methodology for other social media platforms and tasks\ninvolving multi-modal processing. Timely identification and intervention for\nindividuals with depression are crucial for saving lives, highlighting the\nimmense potential of technology in facilitating early intervention for mental\nhealth issues.\n""]",Depression Detection using AI and Multimodal Data,"""Depression Detection using Social Media Data"""
193,"""Missing Data Imputation Methods and Techniques"" , ""Electronic Health Records Imputation and Prediction"" , Time Series Imputation and Analysis , Data Imputation Methods","['imputations', 'imputation', 'missingness', 'imputing', 'imputed', 'completion', 'missforestpredict', 'datasets', 'imputes', 'impute'] , ['imputations', 'imputation', 'missingness', 'data', 'personalized', 'ehrs', 'ehr', 'health', 'prediction', 'healthcare'] , ['imputations', 'imputation', '_imputation', 'missingness', 'imputed', 'imputing', 'forecasting', 'forecasts', 'impute', 'incomplete'] , ['imputation', 'missingness', 'data', 'datasets', 'imputing', 'impute', 'missing', 'records', 'comprehensive', 'rows']","['  Missing data is a common problem in practical settings. Various imputation\nmethods have been developed to deal with missing data. However, even though the\nlabel is usually available in the training data, the common practice of\nimputation usually only relies on the input and ignores the label. In this\nwork, we illustrate how stacking the label into the input can significantly\nimprove the imputation of the input. In addition, we propose a classification\nstrategy that initializes the predicted test label with missing values and\nstacks the label with the input for imputation. This allows imputing the label\nand the input at the same time. Also, the technique is capable of handling data\ntraining with missing labels without any prior imputation and is applicable to\ncontinuous, categorical, or mixed-type data. Experiments show promising results\nin terms of accuracy.\n', '  Many datasets suffer from missing values due to various reasons,which not\nonly increases the processing difficulty of related tasks but also reduces the\naccuracy of classification. To address this problem, the mainstream approach is\nto use missing value imputation to complete the dataset. Existing imputation\nmethods estimate the missing parts based on the observed values in the original\nfeature space, and they treat all features as equally important during data\ncompletion, while in fact different features have different importance.\nTherefore, we have designed an imputation method that considers feature\nimportance. This algorithm iteratively performs matrix completion and feature\nimportance learning, and specifically, matrix completion is based on a filling\nloss that incorporates feature importance. Our experimental analysis involves\nthree types of datasets: synthetic datasets with different noisy features and\nmissing values, real-world datasets with artificially generated missing values,\nand real-world datasets originally containing missing values. The results on\nthese datasets consistently show that the proposed method outperforms the\nexisting five imputation algorithms.To the best of our knowledge, this is the\nfirst work that considers feature importance in the imputation model.\n', '  Missing values or data is one popular characteristic of real-world datasets,\nespecially healthcare data. This could be frustrating when using machine\nlearning algorithms on such datasets, simply because most machine learning\nmodels perform poorly in the presence of missing values. The aim of this study\nis to compare the performance of seven imputation techniques, namely Mean\nimputation, Median Imputation, Last Observation carried Forward (LOCF)\nimputation, K-Nearest Neighbor (KNN) imputation, Interpolation imputation,\nMissforest imputation, and Multiple imputation by Chained Equations (MICE), on\nthree healthcare datasets. Some percentage of missing values - 10\\%, 15\\%, 20\\%\nand 25\\% - were introduced into the dataset, and the imputation techniques were\nemployed to impute these missing values. The comparison of their performance\nwas evaluated by using root mean squared error (RMSE) and mean absolute error\n(MAE). The results show that Missforest imputation performs the best followed\nby MICE imputation. Additionally, we try to determine whether it is better to\nperform feature selection before imputation or vice versa by using the\nfollowing metrics - the recall, precision, f1-score and accuracy. Due to the\nfact that there are few literature on this and some debate on the subject among\nresearchers, we hope that the results from this experiment will encourage data\nscientists and researchers to perform imputation first before feature selection\nwhen dealing with data containing missing values.\n'] , ['  Electronic health record (EHR) data has emerged as a valuable resource for\nanalyzing patient health status. However, the prevalence of missing data in EHR\nposes significant challenges to existing methods, leading to spurious\ncorrelations and suboptimal predictions. While various imputation techniques\nhave been developed to address this issue, they often obsess unnecessary\ndetails and may introduce additional noise when making clinical predictions. To\ntackle this problem, we propose SMART, a Self-Supervised Missing-Aware\nRepresenTation Learning approach for patient health status prediction, which\nencodes missing information via elaborated attentions and learns to impute\nmissing values through a novel self-supervised pre-training approach that\nreconstructs missing data representations in the latent space. By adopting\nmissing-aware attentions and focusing on learning higher-order representations,\nSMART promotes better generalization and robustness to missing data. We\nvalidate the effectiveness of SMART through extensive experiments on six EHR\ntasks, demonstrating its superiority over state-of-the-art methods.\n', ""  Anemia is a prevalent medical condition that typically requires invasive\nblood tests for diagnosis and monitoring. Electronic health records (EHRs) have\nemerged as valuable data sources for numerous medical studies. EHR-based\nhemoglobin level/anemia degree prediction is non-invasive and rapid but still\nfaces some challenges due to the fact that EHR data is typically an irregular\nmultivariate time series containing a significant number of missing values and\nirregular time intervals. To address these issues, we introduce HgbNet, a\nmachine learning-based prediction model that emulates clinicians'\ndecision-making processes for hemoglobin level/anemia degree prediction. The\nmodel incorporates a NanDense layer with a missing indicator to handle missing\nvalues and employs attention mechanisms to account for both local irregularity\nand global irregularity. We evaluate the proposed method using two real-world\ndatasets across two use cases. In our first use case, we predict hemoglobin\nlevel/anemia degree at moment T+1 by utilizing records from moments prior to\nT+1. In our second use case, we integrate all historical records with\nadditional selected test results at moment T+1 to predict hemoglobin\nlevel/anemia degree at the same moment, T+1. HgbNet outperforms the best\nbaseline results across all datasets and use cases. These findings demonstrate\nthe feasibility of estimating hemoglobin levels and anemia degree from EHR\ndata, positioning HgbNet as an effective non-invasive anemia diagnosis solution\nthat could potentially enhance the quality of life for millions of affected\nindividuals worldwide. To our knowledge, HgbNet is the first machine learning\nmodel leveraging EHR data for hemoglobin level/anemia degree prediction.\n"", ""  Electronic Health Records present a valuable modality for driving\npersonalized medicine, where treatment is tailored to fit individual-level\ndifferences. For this purpose, many data-driven machine learning and\nstatistical models rely on the wealth of longitudinal EHRs to study patients'\nphysiological and treatment effects. However, longitudinal EHRs tend to be\nsparse and highly missing, where missingness could also be informative and\nreflect the underlying patient's health status. Therefore, the success of\ndata-driven models for personalized medicine highly depends on how the EHR data\nis represented from physiological data, treatments, and the missing values in\nthe data. To this end, we propose a novel deep-learning model that learns the\nunderlying patient dynamics over time across multivariate data to generate\npersonalized realistic values conditioning on an individual's demographic\ncharacteristics and treatments. Our proposed model, IGNITE (Individualized\nGeNeration of Imputations in Time-series Electronic health records), utilises a\nconditional dual-variational autoencoder augmented with dual-stage attention to\ngenerate missing values for an individual. In IGNITE, we further propose a\nnovel individualized missingness mask (IMM), which helps our model generate\nvalues based on the individual's observed data and missingness patterns. We\nfurther extend the use of IGNITE from imputing missingness to a personalized\ndata synthesizer, where it generates missing EHRs that were never observed\nprior or even generates new patients for various applications. We validate our\nmodel on three large publicly available datasets and show that IGNITE\noutperforms state-of-the-art approaches in missing data reconstruction and task\nprediction.\n""] , ['  Time series imputation is one of the most fundamental tasks for time series.\nReal-world time series datasets are frequently incomplete (or irregular with\nmissing observations), in which case imputation is strongly required. Many\ndifferent time series imputation methods have been proposed. Recent\nself-attention-based methods show the state-of-the-art imputation performance.\nHowever, it has been overlooked for a long time to design an imputation method\nbased on continuous-time recurrent neural networks (RNNs), i.e., neural\ncontrolled differential equations (NCDEs). To this end, we redesign time series\n(variational) autoencoders based on NCDEs. Our method, called continuous-time\nautoencoder (CTA), encodes an input time series sample into a continuous hidden\npath (rather than a hidden vector) and decodes it to reconstruct and impute the\ninput. In our experiments with 4 datasets and 19 baselines, our method shows\nthe best imputation performance in almost all cases.\n', ""  Time series imputation plays a crucial role in various real-world systems and\nhas been extensively explored. Models for time series imputation often require\nspecialization, necessitating distinct designs for different domains and\nmissing patterns. In this study, we introduce NuwaTS, a framework to repurpose\nPre-trained Language Model (PLM) for general time series imputation. Once\ntrained, this model can be applied to imputation tasks on incomplete time\nseries from any domain with any missing patterns. We begin by devising specific\nembeddings for each sub-series patch of the incomplete time series. These\nembeddings encapsulate information about the patch itself, the missing data\npatterns within the patch, and the patch's statistical characteristics. To\nenhance the model's adaptability to different missing patterns, we propose a\ncontrastive learning approach to make representations of the same patch more\nsimilar across different missing patterns. By combining this contrastive loss\nwith the missing data imputation task, we train PLMs to obtain a one-for-all\nimputation model. Furthermore, we utilize a plug-and-play layer-wise\nfine-tuning approach to train domain-specific models. Experimental results\ndemonstrate that leveraging a dataset of over seventeen million time series\nfrom diverse domains, we obtain a one-for-all imputation model which\noutperforms existing domain-specific models across various datasets and missing\npatterns. Additionally, we find that NuwaTS can be generalized to other time\nseries tasks such as forecasting. Our codes are available at\nhttps://github.com/Chengyui/NuwaTS.\n"", '  Time series classification with missing data is a prevalent issue in time\nseries analysis, as temporal data often contain missing values in practical\napplications. The traditional two-stage approach, which handles imputation and\nclassification separately, can result in sub-optimal performance as label\ninformation is not utilized in the imputation process. On the other hand, a\none-stage approach can learn features under missing information, but feature\nrepresentation is limited as imputed errors are propagated in the\nclassification process. To overcome these challenges, this study proposes an\nend-to-end neural network that unifies data imputation and representation\nlearning within a single framework, allowing the imputation process to take\nadvantage of label information. Differing from previous methods, our approach\nplaces less emphasis on the accuracy of imputation data and instead prioritizes\nclassification performance. A specifically designed multi-scale feature\nlearning module is implemented to extract useful information from the\nnoise-imputation data. The proposed model is evaluated on 68 univariate time\nseries datasets from the UCR archive, as well as a multivariate time series\ndataset with various missing data ratios and 4 real-world datasets with missing\ninformation. The results indicate that the proposed model outperforms\nstate-of-the-art approaches for incomplete time series classification,\nparticularly in scenarios with high levels of missing data.\n'] , ['  The ubiquity of missing data has sparked considerable attention and focus on\ntabular data imputation methods. Diffusion models, recognized as the\ncutting-edge technique for data generation, demonstrate significant potential\nin tabular data imputation tasks. However, in pursuit of diversity, vanilla\ndiffusion models often exhibit sensitivity to initialized noises, which hinders\nthe models from generating stable and accurate imputation results.\nAdditionally, the sparsity inherent in tabular data poses challenges for\ndiffusion models in accurately modeling the data manifold, impacting the\nrobustness of these models for data imputation. To tackle these challenges,\nthis paper introduces an advanced diffusion model named Self-supervised\nimputation Diffusion Model (SimpDM for brevity), specifically tailored for\ntabular data imputation tasks. To mitigate sensitivity to noise, we introduce a\nself-supervised alignment mechanism that aims to regularize the model, ensuring\nconsistent and stable imputation predictions. Furthermore, we introduce a\ncarefully devised state-dependent data augmentation strategy within SimpDM,\nenhancing the robustness of the diffusion model when dealing with limited data.\nExtensive experiments demonstrate that SimpDM matches or outperforms\nstate-of-the-art imputation methods across various scenarios.\n', '  We introduce a novel classification framework for time-series imputation\nusing deep learning, with a particular focus on clinical data. By identifying\nconceptual gaps in the literature and existing reviews, we devise a taxonomy\ngrounded on the inductive bias of neural imputation frameworks, resulting in a\nclassification of existing deep imputation strategies based on their\nsuitability for specific imputation scenarios and data-specific properties. Our\nreview further examines the existing methodologies employed to benchmark deep\nimputation models, evaluating their effectiveness in capturing the missingness\nscenarios found in clinical data and emphasising the importance of reconciling\nmathematical abstraction with clinical insights. Our classification aims to\nserve as a guide for researchers to facilitate the selection of appropriate\ndeep learning imputation techniques tailored to their specific clinical data.\nOur novel perspective also highlights the significance of bridging the gap\nbetween computational methodologies and medical insights to achieve clinically\nsound imputation models.\n', '  Objective: The proper handling of missing values is critical to delivering\nreliable estimates and decisions, especially in high-stakes fields such as\nclinical research. The increasing diversity and complexity of data have led\nmany researchers to develop deep learning (DL)-based imputation techniques. We\nconducted a systematic review to evaluate the use of these techniques, with a\nparticular focus on data types, aiming to assist healthcare researchers from\nvarious disciplines in dealing with missing values.\n  Methods: We searched five databases (MEDLINE, Web of Science, Embase, CINAHL,\nand Scopus) for articles published prior to August 2021 that applied DL-based\nmodels to imputation. We assessed selected publications from four perspectives:\nhealth data types, model backbone (i.e., main architecture), imputation\nstrategies, and comparison with non-DL-based methods. Based on data types, we\ncreated an evidence map to illustrate the adoption of DL models.\n  Results: We included 64 articles, of which tabular static (26.6%, 17/64) and\ntemporal data (37.5%, 24/64) were the most frequently investigated. We found\nthat model backbone(s) differed among data types as well as the imputation\nstrategy. The ""integrated"" strategy, that is, the imputation task being solved\nconcurrently with downstream tasks, was popular for tabular temporal (50%,\n12/24) and multi-modal data (71.4%, 5/7), but limited for other data types.\nMoreover, DL-based imputation methods yielded better imputation accuracy in\nmost studies, compared with non-DL-based methods.\n  Conclusion: DL-based imputation models can be customized based on data type,\naddressing the corresponding missing patterns, and its associated ""integrated""\nstrategy can enhance the efficacy of imputation, especially in scenarios where\ndata is complex. Future research may focus on the portability and fairness of\nDL-based models for healthcare data imputation.\n']",Data Imputation and Missing Value Analysis,Data Imputation Methods
194,"Autonomous Driving and Traffic Safety , ""LLM Safety Evaluation Benchmarks"" , Autonomous Systems and AI Safety Assessment","['highway', 'driving', 'autonomous', 'traffic', 'road', 'vehicles', 'lane', 'planning', 'vehicle', 'cars'] , ['safetybench', 'safety', 'unsafe', 'safeguards', 'risks', 'vulnerabilities', 'ai', 'risk', 'language', 'testing'] , ['ai', 'autonomous', 'autonomy', 'safety', 'automated', 'driving', 'prediction', 'vehicles', 'vehicle', 'assessment']","['  Reinforcement learning has been demonstrated to outperform even the best\nhumans in complex domains like video games. However, running reinforcement\nlearning experiments on the required scale for autonomous driving is extremely\ndifficult. Building a large scale reinforcement learning system and\ndistributing it across many GPUs is challenging. Gathering experience during\ntraining on real world vehicles is prohibitive from a safety and scalability\nperspective. Therefore, an efficient and realistic driving simulator is\nrequired that uses a large amount of data from real-world driving. We bring\nthese capabilities together and conduct large-scale reinforcement learning\nexperiments for autonomous driving. We demonstrate that our policy performance\nimproves with increasing scale. Our best performing policy reduces the failure\nrate by 64% while improving the rate of driving progress by 25% compared to the\npolicies produced by state-of-the-art machine learning for autonomous driving.\n', '  Autonomous driving technology can improve traffic safety and reduce traffic\naccidents. In addition, it improves traffic flow, reduces congestion, saves\nenergy and increases travel efficiency. In the relatively mature automatic\ndriving technology, the automatic driving function is divided into several\nmodules: perception, decision-making, planning and control, and a reasonable\ndivision of labor can improve the stability of the system. Therefore,\nautonomous vehicles need to have the ability to predict the trajectory of\nsurrounding vehicles in order to make reasonable decision planning and safety\nmeasures to improve driving safety. By using deep learning method, a\nsafety-sensitive deep learning model based on short term memory (LSTM) network\nis proposed. This model can alleviate the shortcomings of current automatic\ndriving trajectory planning, and the output trajectory not only ensures high\naccuracy but also improves safety. The cell state simulation algorithm\nsimulates the trackability of the trajectory generated by this model. The\nresearch results show that compared with the traditional model-based method,\nthe trajectory prediction method based on LSTM network has obvious advantages\nin predicting the trajectory in the long time domain. The intention recognition\nmodule considering interactive information has higher prediction and accuracy,\nand the algorithm results show that the trajectory is very smooth based on the\npremise of safe prediction and efficient lane change. And autonomous vehicles\ncan efficiently and safely complete lane changes.\n', ""  Pedestrians' safety is a crucial factor in assessing autonomous driving\nscenarios. However, pedestrian safety evaluation is rarely considered by\nexisting autonomous driving simulation platforms. This paper proposes a\npedestrian safety evaluation method for autonomous driving, in which not only\nthe collision events but also the conflict events together with the\ncharacteristics of pedestrians are fully considered. Moreover, to apply the\npedestrian safety evaluation system, we construct a high-fidelity simulation\nframework embedded with pedestrian safety-critical characteristics. We\ndemonstrate our simulation framework and pedestrian safety evaluation with a\ncomparative experiment with two kinds of autonomous driving perception\nalgorithms -- single-vehicle perception and vehicle-to-infrastructure (V2I)\ncooperative perception. The results show that our framework can evaluate\ndifferent autonomous driving algorithms with detailed and quantitative\npedestrian safety indexes. To this end, the proposed simulation method and\nframework can be used to access different autonomous driving algorithms and\nevaluate pedestrians' safety performance in future autonomous driving\nsimulations, which can inspire more pedestrian-friendly autonomous driving\nalgorithms.\n""] , ['  Large language models (LLMs) have exhibited great potential in autonomously\ncompleting tasks across real-world applications. Despite this, these LLM agents\nintroduce unexpected safety risks when operating in interactive environments.\nInstead of centering on LLM-generated content safety in most prior studies,\nthis work addresses the imperative need for benchmarking the behavioral safety\nof LLM agents within diverse environments. We introduce R-Judge, a benchmark\ncrafted to evaluate the proficiency of LLMs in judging and identifying safety\nrisks given agent interaction records. R-Judge comprises 162 records of\nmulti-turn agent interaction, encompassing 27 key risk scenarios among 7\napplication categories and 10 risk types. It incorporates human consensus on\nsafety with annotated safety labels and high-quality risk descriptions.\nEvaluation of 9 LLMs on R-Judge shows considerable room for enhancing the risk\nawareness of LLMs: The best-performing model, GPT-4, achieves 72.52% in\ncontrast to the human score of 89.07%, while all other models score less than\nthe random. Moreover, further experiments demonstrate that leveraging risk\ndescriptions as environment feedback achieves substantial performance gains.\nWith case studies, we reveal that correlated to parameter amount, risk\nawareness in open agent scenarios is a multi-dimensional capability involving\nknowledge and reasoning, thus challenging for current LLMs. R-Judge is publicly\navailable at https://github.com/Lordog/R-Judge.\n', ""  With the profound development of large language models(LLMs), their safety\nconcerns have garnered increasing attention. However, there is a scarcity of\nChinese safety benchmarks for LLMs, and the existing safety taxonomies are\ninadequate, lacking comprehensive safety detection capabilities in authentic\nChinese scenarios. In this work, we introduce CHiSafetyBench, a dedicated\nsafety benchmark for evaluating LLMs' capabilities in identifying risky content\nand refusing answering risky questions in Chinese contexts. CHiSafetyBench\nincorporates a dataset that covers a hierarchical Chinese safety taxonomy\nconsisting of 5 risk areas and 31 categories. This dataset comprises two types\nof tasks: multiple-choice questions and question-answering, evaluating LLMs\nfrom the perspectives of risk content identification and the ability to refuse\nanswering risky questions respectively. Utilizing this benchmark, we validate\nthe feasibility of automatic evaluation as a substitute for human evaluation\nand conduct comprehensive automatic safety assessments on mainstream Chinese\nLLMs. Our experiments reveal the varying performance of different models across\nvarious safety domains, indicating that all models possess considerable\npotential for improvement in Chinese safety capabilities. Our dataset is\npublicly available at\nhttps://github.com/UnicomAI/DataSet/tree/main/TestData/Safety.\n"", ""  Large Language Models have gained considerable attention for their\nrevolutionary capabilities. However, there is also growing concern on their\nsafety implications, making a comprehensive safety evaluation for LLMs urgently\nneeded before model deployment. In this work, we propose S-Eval, a new\ncomprehensive, multi-dimensional and open-ended safety evaluation benchmark. At\nthe core of S-Eval is a novel LLM-based automatic test prompt generation and\nselection framework, which trains an expert testing LLM Mt combined with a\nrange of test selection strategies to automatically construct a high-quality\ntest suite for the safety evaluation. The key to the automation of this process\nis a novel expert safety-critique LLM Mc able to quantify the riskiness score\nof an LLM's response, and additionally produce risk tags and explanations.\nBesides, the generation process is also guided by a carefully designed risk\ntaxonomy with four different levels, covering comprehensive and\nmulti-dimensional safety risks of concern. Based on these, we systematically\nconstruct a new and large-scale safety evaluation benchmark for LLMs consisting\nof 220,000 evaluation prompts, including 20,000 base risk prompts (10,000 in\nChinese and 10,000 in English) and 200,000 corresponding attack prompts derived\nfrom 10 popular adversarial instruction attacks against LLMs. Moreover,\nconsidering the rapid evolution of LLMs and accompanied safety threats, S-Eval\ncan be flexibly configured and adapted to include new risks, attacks and\nmodels. S-Eval is extensively evaluated on 20 popular and representative LLMs.\nThe results confirm that S-Eval can better reflect and inform the safety risks\nof LLMs compared to existing benchmarks. We also explore the impacts of\nparameter scales, language environments, and decoding parameters on the\nevaluation, providing a systematic methodology for evaluating the safety of\nLLMs.\n""] , ['  Contemporary artificial intelligence systems are pivotal in enhancing human\nefficiency and safety across various domains. One such domain is autonomous\nsystems, especially in automotive and defense use cases. Artificial\nintelligence brings learning and enhanced decision-making to autonomy system\ngoal-oriented behaviors and human independence. However, the lack of clear\nunderstanding of autonomy system capabilities hampers human-machine or\nmachine-machine interaction and interdiction. This necessitates varying degrees\nof human involvement for safety, accountability, and explainability purposes.\nYet, measuring the level autonomous capability in an autonomous system presents\na challenge. Two scales of measurement exist, yet measuring autonomy\npresupposes a variety of elements not available in the wild. This is why\nexisting measures for level of autonomy are operationalized only during design\nor test and evaluation phases. No measure for level of autonomy based on\nobserved system behavior exists at this time. To address this, we outline a\npotential measure for predicting level of autonomy using observable actions. We\nalso present an algorithm incorporating the proposed measure. The measure and\nalgorithm have significance to researchers and practitioners interested in a\nmethod to blind compare autonomous systems at runtime. Defense-based\nimplementations are likewise possible because counter-autonomy depends on\nrobust identification of autonomous systems.\n', '  This paper explores the role and challenges of Artificial Intelligence (AI)\nalgorithms, specifically AI-based software elements, in autonomous driving\nsystems. These AI systems are fundamental in executing real-time critical\nfunctions in complex and high-dimensional environments. They handle vital tasks\nlike multi-modal perception, cognition, and decision-making tasks such as\nmotion planning, lane keeping, and emergency braking. A primary concern relates\nto the ability (and necessity) of AI models to generalize beyond their initial\ntraining data. This generalization issue becomes evident in real-time\nscenarios, where models frequently encounter inputs not represented in their\ntraining or validation data. In such cases, AI systems must still function\neffectively despite facing distributional or domain shifts. This paper\ninvestigates the risk associated with overconfident AI models in\nsafety-critical applications like autonomous driving. To mitigate these risks,\nmethods for training AI models that help maintain performance without\noverconfidence are proposed. This involves implementing certainty reporting\narchitectures and ensuring diverse training data. While various\ndistribution-based methods exist to provide safety mechanisms for AI models,\nthere is a noted lack of systematic assessment of these methods, especially in\nthe context of safety-critical automotive applications. Many methods in the\nliterature do not adapt well to the quick response times required in\nsafety-critical edge applications. This paper reviews these methods, discusses\ntheir suitability for safety-critical applications, and highlights their\nstrengths and limitations. The paper also proposes potential improvements to\nenhance the safety and reliability of AI algorithms in autonomous vehicles in\nthe context of rapid and accurate decision-making processes.\n', '  In learning-enabled autonomous systems, safety monitoring of learned\ncomponents is crucial to ensure their outputs do not lead to system safety\nviolations, given the operational context of the system. However, developing a\nsafety monitor for practical deployment in real-world applications is\nchallenging. This is due to limited access to internal workings and training\ndata of the learned component. Furthermore, safety monitors should predict\nsafety violations with low latency, while consuming a reasonable amount of\ncomputation.\n  To address the challenges, we propose a safety monitoring method based on\nprobabilistic time series forecasting. Given the learned component outputs and\nan operational context, we empirically investigate different Deep Learning\n(DL)-based probabilistic forecasting to predict the objective measure capturing\nthe satisfaction or violation of a safety requirement (safety metric). We\nempirically evaluate safety metric and violation prediction accuracy, and\ninference latency and resource usage of four state-of-the-art models, with\nvarying horizons, using an autonomous aviation case study. Our results suggest\nthat probabilistic forecasting of safety metrics, given learned component\noutputs and scenarios, is effective for safety monitoring. Furthermore, for the\nautonomous aviation case study, Temporal Fusion Transformer (TFT) was the most\naccurate model for predicting imminent safety violations, with acceptable\nlatency and resource consumption.\n']",Autonomous Systems and Safety Assessment,Autonomous Systems and AI Safety Assessment
195,"Air Pollution Analysis and Prediction , Traffic Accident Analysis and Prediction","['pollution', 'emissions', 'meteorological', 'environmental', 'predicting', 'forecasting', 'climate', 'aerosol', 'pollutants', 'pollutant'] , ['traffic', 'accidentgpt', 'driving', 'accidents', 'roads', 'lanes', 'crash', 'vehicles', 'predicting', 'prediction']","['  Policymakers frequently analyze air quality and climate change in isolation,\ndisregarding their interactions. This study explores the influence of specific\nclimate factors on air quality by contrasting a regression model with K-Means\nClustering, Hierarchical Clustering, and Random Forest techniques. We employ\nPhysics-based Deep Learning (PBDL) and Long Short-Term Memory (LSTM) to examine\nthe air pollution predictions. Our analysis utilizes ten years (2009-2018) of\ndaily traffic, weather, and air pollution data from three major cities in\nNorway. Findings from feature selection reveal a correlation between rising\nheating degree days and heightened air pollution levels, suggesting increased\nheating activities in Norway are a contributing factor to worsening air\nquality. PBDL demonstrates superior accuracy in air pollution predictions\ncompared to LSTM. This paper contributes to the growing literature on PBDL\nmethods for more accurate air pollution predictions using environmental\nvariables, aiding policymakers in formulating effective data-driven climate\npolicies.\n', ""  Ambient air pollution remains a critical issue in the United Kingdom, where\ndata on air pollution concentrations form the foundation for interventions\naimed at improving air quality. However, the current air pollution monitoring\nstation network in the UK is characterized by spatial sparsity, heterogeneous\nplacement, and frequent temporal data gaps, often due to issues such as power\noutages. We introduce a scalable data-driven supervised machine learning model\nframework designed to address temporal and spatial data gaps by filling missing\nmeasurements. This approach provides a comprehensive dataset for England\nthroughout 2018 at a 1kmx1km hourly resolution. Leveraging machine learning\ntechniques and real-world data from the sparsely distributed monitoring\nstations, we generate 355,827 synthetic monitoring stations across the study\narea, yielding data valued at approximately \\pounds70 billion. Validation was\nconducted to assess the model's performance in forecasting, estimating missing\nlocations, and capturing peak concentrations. The resulting dataset is of\nparticular interest to a diverse range of stakeholders engaged in downstream\nassessments supported by outdoor air pollution concentration data for NO2, O3,\nPM10, PM2.5, and SO2. This resource empowers stakeholders to conduct studies at\na higher resolution than was previously possible.\n"", '  Ambient air pollution is a pervasive issue with wide-ranging effects on human\nhealth, ecosystem vitality, and economic structures. Utilizing data on ambient\nair pollution concentrations, researchers can perform comprehensive analyses to\nuncover the multifaceted impacts of air pollution across society. To this end,\nwe introduce Environmental Insights, an open-source Python package designed to\ndemocratize access to air pollution concentration data. This tool enables users\nto easily retrieve historical air pollution data and employ a Machine Learning\nmodel for forecasting potential future conditions. Moreover, Environmental\nInsights includes a suite of tools aimed at facilitating the dissemination of\nanalytical findings and enhancing user engagement through dynamic\nvisualizations. This comprehensive approach ensures that the package caters to\nthe diverse needs of individuals looking to explore and understand air\npollution trends and their implications.\n'] , ['  Traffic accidents, being a significant contributor to both human casualties\nand property damage, have long been a focal point of research for many scholars\nin the field of traffic safety. However, previous studies, whether focusing on\nstatic environmental assessments or dynamic driving analyses, as well as\npre-accident predictions or post-accident rule analyses, have typically been\nconducted in isolation. There has been a lack of an effective framework for\ndeveloping a comprehensive understanding and application of traffic safety. To\naddress this gap, this paper introduces AccidentGPT, a comprehensive accident\nanalysis and prevention multi-modal large model. AccidentGPT establishes a\nmulti-modal information interaction framework grounded in multi-sensor\nperception, thereby enabling a holistic approach to accident analysis and\nprevention in the field of traffic safety. Specifically, our capabilities can\nbe categorized as follows: for autonomous driving vehicles, we provide\ncomprehensive environmental perception and understanding to control the vehicle\nand avoid collisions. For human-driven vehicles, we offer proactive long-range\nsafety warnings and blind-spot alerts while also providing safety driving\nrecommendations and behavioral norms through human-machine dialogue and\ninteraction. Additionally, for traffic police and management agencies, our\nframework supports intelligent and real-time analysis of traffic safety,\nencompassing pedestrian, vehicles, roads, and the environment through\ncollaborative perception from multiple vehicles and road testing devices. The\nsystem is also capable of providing a thorough analysis of accident causes and\nliability after vehicle collisions. Our framework stands as the first large\nmodel to integrate comprehensive scene understanding into traffic safety\nstudies. Project page: https://accidentgpt.github.io\n', '  We consider the problem of traffic accident analysis on a road network based\non road network connections and traffic volume. Previous works have designed\nvarious deep-learning methods using historical records to predict traffic\naccident occurrences. However, there is a lack of consensus on how accurate\nexisting methods are, and a fundamental issue is the lack of public accident\ndatasets for comprehensive evaluations. This paper constructs a large-scale,\nunified dataset of traffic accident records from official reports of various\nstates in the US, totaling 9 million records, accompanied by road networks and\ntraffic volume reports. Using this new dataset, we evaluate existing\ndeep-learning methods for predicting the occurrence of accidents on road\nnetworks. Our main finding is that graph neural networks such as GraphSAGE can\naccurately predict the number of accidents on roads with less than 22% mean\nabsolute error (relative to the actual count) and whether an accident will\noccur or not with over 87% AUROC, averaged over states. We achieve these\nresults by using multitask learning to account for cross-state variabilities\n(e.g., availability of accident labels) and transfer learning to combine\ntraffic volume with accident prediction. Ablation studies highlight the\nimportance of road graph-structural features, amongst other features. Lastly,\nwe discuss the implications of the analysis and develop a package for easily\nusing our new dataset.\n', '  The precise prediction of multi-scale traffic is a ubiquitous challenge in\nthe urbanization process for car owners, road administrators, and governments.\nIn the case of complex road networks, current and past traffic information from\nboth upstream and downstream roads are crucial since various road networks have\ndifferent semantic information about traffic. Rationalizing the utilization of\nsemantic information can realize short-term, long-term, and unseen road traffic\nprediction. As the demands of multi-scale traffic analysis increase, on-demand\ninteractions and visualizations are expected to be available for transportation\nparticipants. We have designed a multi-scale traffic generation system, namely\nTrafficGPT, using three AI agents to process multi-scale traffic data, conduct\nmulti-scale traffic analysis, and present multi-scale visualization results.\nTrafficGPT consists of three essential AI agents: 1) a text-to-demand agent\nthat is employed with Question & Answer AI to interact with users and extract\nprediction tasks through texts; 2) a traffic prediction agent that leverages\nmulti-scale traffic data to generate temporal features and similarity, and fuse\nthem with limited spatial features and similarity, to achieve accurate\nprediction of three tasks; and 3) a suggestion and visualization agent that\nuses the prediction results to generate suggestions and visualizations,\nproviding users with a comprehensive understanding of traffic conditions. Our\nTrafficGPT system focuses on addressing concerns about traffic prediction from\ntransportation participants, and conducted extensive experiments on five\nreal-world road datasets to demonstrate its superior predictive and interactive\nperformance\n']",Environmental and Transportation Predictive Analytics,Air Pollution Analysis and Prediction
196,"Survival Analysis and Prediction Models , Battery Health Prediction and Degradation Analysis","['survival', 'predicting', 'prediction', 'predict', 'coxtime', 'predictive', 'cox', 'hazards', 'outcomes', 'survmixclust'] , ['batteryml', 'batteries', 'battery', 'lithium', 'predicting', 'prediction', 'prognostics', 'estimating', 'lifespan', 'degradation']","['  Kernel survival analysis models estimate individual survival distributions\nwith the help of a kernel function, which measures the similarity between any\ntwo data points. Such a kernel function can be learned using deep kernel\nsurvival models. In this paper, we present a new deep kernel survival model\ncalled a survival kernet, which scales to large datasets in a manner that is\namenable to model interpretation and also theoretical analysis. Specifically,\nthe training data are partitioned into clusters based on a recently developed\ntraining set compression scheme for classification and regression called kernel\nnetting that we extend to the survival analysis setting. At test time, each\ndata point is represented as a weighted combination of these clusters, and each\nsuch cluster can be visualized. For a special case of survival kernets, we\nestablish a finite-sample error bound on predicted survival distributions that\nis, up to a log factor, optimal. Whereas scalability at test time is achieved\nusing the aforementioned kernel netting compression strategy, scalability\nduring training is achieved by a warm-start procedure based on tree ensembles\nsuch as XGBoost and a heuristic approach to accelerating neural architecture\nsearch. On four standard survival analysis datasets of varying sizes (up to\nroughly 3 million data points), we show that survival kernets are highly\ncompetitive compared to various baselines tested in terms of time-dependent\nconcordance index. Our code is available at:\nhttps://github.com/georgehc/survival-kernets\n', ""  Scoring systems are highly interpretable and widely used to evaluate\ntime-to-event outcomes in healthcare research. However, existing time-to-event\nscores are predominantly created ad-hoc using a few manually selected variables\nbased on clinician's knowledge, suggesting an unmet need for a robust and\nefficient generic score-generating method.\n  AutoScore was previously developed as an interpretable machine learning score\ngenerator, integrated both machine learning and point-based scores in the\nstrong discriminability and accessibility. We have further extended it to\ntime-to-event data and developed AutoScore-Survival, for automatically\ngenerating time-to-event scores with right-censored survival data. Random\nsurvival forest provides an efficient solution for selecting variables, and Cox\nregression was used for score weighting. We illustrated our method in a\nreal-life study of 90-day mortality of patients in intensive care units and\ncompared its performance with survival models (i.e., Cox) and the random\nsurvival forest.\n  The AutoScore-Survival-derived scoring model was more parsimonious than\nsurvival models built using traditional variable selection methods (e.g.,\npenalized likelihood approach and stepwise variable selection), and its\nperformance was comparable to survival models using the same set of variables.\nAlthough AutoScore-Survival achieved a comparable integrated area under the\ncurve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores\ngenerated are favorable in clinical applications because they are easier to\ncompute and interpret.\n  Our proposed AutoScore-Survival provides an automated, robust and easy-to-use\nmachine learning-based clinical score generator to studies of time-to-event\noutcomes. It provides a systematic guideline to facilitate the future\ndevelopment of time-to-event scores for clinical applications.\n"", '  Survival analysis stands as a pivotal process in cancer treatment research,\ncrucial for predicting patient survival rates accurately. Recent advancements\nin data collection techniques have paved the way for enhancing survival\npredictions by integrating information from multiple modalities. However,\nreal-world scenarios often present challenges with incomplete data,\nparticularly when dealing with censored survival labels. Prior works have\naddressed missing modalities but have overlooked incomplete labels, which can\nintroduce bias and limit model efficacy. To bridge this gap, we introduce a\nnovel framework that simultaneously handles incomplete data across modalities\nand censored survival labels. Our approach employs advanced foundation models\nto encode individual modalities and align them into a universal representation\nspace for seamless fusion. By generating pseudo labels and incorporating\nuncertainty, we significantly enhance predictive accuracy. The proposed method\ndemonstrates outstanding prediction accuracy in two survival analysis tasks on\nboth employed datasets. This innovative approach overcomes limitations\nassociated with disparate modalities and improves the feasibility of\ncomprehensive survival analysis using multiple large foundation models.\n'] , ['  Batteries are dynamic systems with complicated nonlinear aging, highly\ndependent on cell design, chemistry, manufacturing, and operational conditions.\nPrediction of battery cycle life and estimation of aging states is important to\naccelerate battery R&D, testing, and to further the understanding of how\nbatteries degrade. Beyond testing, battery management systems rely on real-time\nmodels and onboard diagnostics and prognostics for safe operation. Estimating\nthe state of health and remaining useful life of a battery is important to\noptimize performance and use resources optimally.\n  This tutorial begins with an overview of first-principles, machine learning,\nand hybrid battery models. Then, a typical pipeline for the development of\ninterpretable machine learning models is explained and showcased for cycle life\nprediction from laboratory testing data. We highlight the challenges of machine\nlearning models, motivating the incorporation of physics in hybrid modeling\napproaches, which are needed to decipher the aging trajectory of batteries but\nrequire more data and further work on the physics of battery degradation. The\ntutorial closes with a discussion on generalization and further research\ndirections.\n', '  Battery life estimation is critical for optimizing battery performance and\nguaranteeing minimal degradation for better efficiency and reliability of\nbattery-powered systems. The existing methods to predict the Remaining Useful\nLife(RUL) of Lithium-ion Batteries (LiBs) neglect the relational dependencies\nof the battery parameters to model the nonlinear degradation trajectories. We\npresent the Battery GraphNets framework that jointly learns to incorporate a\ndiscrete dependency graph structure between battery parameters to capture the\ncomplex interactions and the graph-learning algorithm to model the intrinsic\nbattery degradation for RUL prognosis. The proposed method outperforms several\npopular methods by a significant margin on publicly available battery datasets\nand achieves SOTA performance. We report the ablation studies to support the\nefficacy of our approach.\n', '  Lithium-ion batteries are pivotal to technological advancements in\ntransportation, electronics, and clean energy storage. The optimal operation\nand safety of these batteries require proper and reliable estimation of battery\ncapacities to monitor the state of health. Current methods for estimating the\ncapacities fail to adequately account for long-term temporal dependencies of\nkey variables (e.g., voltage, current, and temperature) associated with battery\naging and degradation. In this study, we explore the usage of transformer\nnetworks to enhance the estimation of battery capacity. We develop a\ntransformer-based battery capacity prediction model that accounts for both\nlong-term and short-term patterns in battery data. Further, to tackle the data\nscarcity issue, data augmentation is used to increase the data size, which\nhelps to improve the performance of the model. Our proposed method is validated\nwith benchmark datasets. Simulation results show the effectiveness of data\naugmentation and the transformer network in improving the accuracy and\nrobustness of battery capacity prediction.\n']",Predictive Modeling for Time-to-Event Outcomes and Battery Health,Battery Health Prediction and Degradation Analysis
197,"Toxicity Detection in Multilingual Text and Speech , Text Detoxification Methods","['toxicity', 'frenchtoxicityprompts', 'toxic', 'corpus', 'annotated', 'harmful', 'language', 'languages', 'harm', 'polyglotoxicityprompts'] , ['detoxify', 'detoxifying', 'detoxified', 'detoxifier', 'detoxifies', 'detoxification', 'detox', 'detoxifiable', 'detoxifiability', 'corpus']","[""  Large language models (LLMs) are increasingly popular but are also prone to\ngenerating bias, toxic or harmful language, which can have detrimental effects\non individuals and communities. Although most efforts is put to assess and\nmitigate toxicity in generated content, it is primarily concentrated on\nEnglish, while it's essential to consider other languages as well. For\naddressing this issue, we create and release FrenchToxicityPrompts, a dataset\nof 50K naturally occurring French prompts and their continuations, annotated\nwith toxicity scores from a widely used toxicity classifier. We evaluate 14\ndifferent models from four prevalent open-sourced families of LLMs against our\ndataset to assess their potential toxicity across various dimensions. We hope\nthat our contribution will foster future research on toxicity detection and\nmitigation beyond Englis\n"", ""  In the pursuit of developing Large Language Models (LLMs) that adhere to\nsocietal standards, it is imperative to detect the toxicity in the generated\ntext. The majority of existing toxicity metrics rely on encoder models trained\non specific toxicity datasets, which are susceptible to out-of-distribution\n(OOD) problems and depend on the dataset's definition of toxicity. In this\npaper, we introduce a robust metric grounded on LLMs to flexibly measure\ntoxicity according to the given definition. We first analyze the toxicity\nfactors, followed by an examination of the intrinsic toxic attributes of LLMs\nto ascertain their suitability as evaluators. Finally, we evaluate the\nperformance of our metric with detailed analysis. Our empirical results\ndemonstrate outstanding performance in measuring toxicity within verified\nfactors, improving on conventional metrics by 12 points in the F1 score. Our\nfindings also indicate that upstream toxicity significantly influences\ndownstream metrics, suggesting that LLMs are unsuitable for toxicity\nevaluations within unverified factors.\n"", '  Toxicity classification for voice heavily relies on the semantic content of\nspeech. We propose a novel framework that utilizes cross-modal learning to\nintegrate the semantic embedding of text into a multilabel speech toxicity\nclassifier during training. This enables us to incorporate textual information\nduring training while still requiring only audio during inference. We evaluate\nthis classifier on large-scale datasets with real-world characteristics to\nvalidate the effectiveness of this framework. Through ablation studies, we\ndemonstrate that general-purpose semantic text embeddings are rich and aligned\nwith speech for toxicity classification purposes. Conducting experiments across\nmultiple languages at scale, we show improvements in voice toxicity\nclassification across five languages and different toxicity categories.\n'] , ['  Prior works on detoxification are scattered in the sense that they do not\ncover all aspects of detoxification needed in a real-world scenario. Notably,\nprior works restrict the task of developing detoxification models to only a\nseen subset of platforms, leaving the question of how the models would perform\non unseen platforms unexplored. Additionally, these works do not address\nnon-detoxifiability, a phenomenon whereby the toxic text cannot be detoxified\nwithout altering the meaning. We propose GreenLLaMA, the first comprehensive\nend-to-end detoxification framework, which attempts to alleviate the\naforementioned limitations. We first introduce a cross-platform pseudo-parallel\ncorpus applying multi-step data processing and generation strategies leveraging\nChatGPT. We then train a suite of detoxification models with our cross-platform\ncorpus. We show that our detoxification models outperform the SoTA model\ntrained with human-annotated parallel corpus. We further introduce explanation\nto promote transparency and trustworthiness. GreenLLaMA additionally offers a\nunique paraphrase detector especially dedicated for the detoxification task to\ntackle the non-detoxifiable cases. Through experimental analysis, we\ndemonstrate the effectiveness of our cross-platform corpus and the robustness\nof GreenLLaMA against adversarial toxicity.\n', '  Toxicity mitigation consists in rephrasing text in order to remove offensive\nor harmful meaning. Neural natural language processing (NLP) models have been\nwidely used to target and mitigate textual toxicity. However, existing methods\nfail to detoxify text while preserving the initial non-toxic meaning at the\nsame time. In this work, we propose to apply counterfactual generation methods\nfrom the eXplainable AI (XAI) field to target and mitigate textual toxicity. In\nparticular, we perform text detoxification by applying local feature importance\nand counterfactual generation methods to a toxicity classifier distinguishing\nbetween toxic and non-toxic texts. We carry out text detoxification through\ncounterfactual generation on three datasets and compare our approach to three\ncompetitors. Automatic and human evaluations show that recently developed NLP\ncounterfactual generators can mitigate toxicity accurately while better\npreserving the meaning of the initial text as compared to classical\ndetoxification methods. Finally, we take a step back from using automated\ndetoxification tools, and discuss how to manage the polysemous nature of\ntoxicity and the risk of malicious use of detoxification tools. This work is\nthe first to bridge the gap between counterfactual generation and text\ndetoxification and paves the way towards more practical application of XAI\nmethods.\n', '  Text detoxification aims to minimize the risk of language models producing\ntoxic content. Existing detoxification methods of directly constraining the\nmodel output or further training the model on the non-toxic corpus fail to\nachieve a decent balance between detoxification effectiveness and generation\nquality. This issue stems from the neglect of constrain imposed by the context\nsince language models are designed to generate output that closely matches the\ncontext while detoxification methods endeavor to ensure the safety of the\noutput even if it semantically deviates from the context. In view of this, we\nintroduce a Context-aware Model self-Detoxification~(CMD) framework that pays\nattention to both the context and the detoxification process, i.e., first\ndetoxifying the context and then making the language model generate along the\nsafe context. Specifically, CMD framework involves two phases: utilizing\nlanguage models to synthesize data and applying these data for training. We\nalso introduce a toxic contrastive loss that encourages the model generation\naway from the negative toxic samples. Experiments on various LLMs have verified\nthe effectiveness of our MSD framework, which can yield the best performance\ncompared to baselines.\n']",Toxicity Detection and Mitigation in Natural Language Processing,Toxicity Detection in Multilingual Text and Speech
198,"""Predicting Drug Interactions and Synergies"" , Opioid Use Disorder Detection using NLP","['drug', 'pharmaceutical', 'drugclip', 'pharmacology', 'drugs', 'graphs', 'predicting', 'graph', 'proteins', 'molecular'] , ['opioids', 'opioid', 'nlp', 'annotated', 'tweets', 'drug', 'classification', 'text', 'addiction', 'posts']","[""  Drug development is a lengthy process with a high failure rate. Increasingly,\nmachine learning is utilized to facilitate the drug development processes.\nThese models aim to enhance our understanding of drug characteristics,\nincluding their activity in biological contexts. However, a major challenge in\ndrug response (DR) prediction is model interpretability as it aids in the\nvalidation of findings. This is important in biomedicine, where models need to\nbe understandable in comparison with established knowledge of drug interactions\nwith proteins. drGAT, a graph deep learning model, leverages a heterogeneous\ngraph composed of relationships between proteins, cell lines, and drugs. drGAT\nis designed with two objectives: DR prediction as a binary sensitivity\nprediction and elucidation of drug mechanism from attention coefficients. drGAT\nhas demonstrated superior performance over existing models, achieving 78\\%\naccuracy (and precision), and 76\\% F1 score for 269 DNA-damaging compounds of\nthe NCI60 drug response dataset. To assess the model's interpretability, we\nconducted a review of drug-gene co-occurrences in Pubmed abstracts in\ncomparison to the top 5 genes with the highest attention coefficients for each\ndrug. We also examined whether known relationships were retained in the model\nby inspecting the neighborhoods of topoisomerase-related drugs. For example,\nour model retained TOP1 as a highly weighted predictive feature for irinotecan\nand topotecan, in addition to other genes that could potentially be regulators\nof the drugs. Our method can be used to accurately predict sensitivity to drugs\nand may be useful in the identification of biomarkers relating to the treatment\nof cancer patients.\n"", '  Drug-drug interaction prediction is a crucial issue in molecular biology.\nTraditional methods of observing drug-drug interactions through medical\nexperiments require significant resources and labor. This paper presents a\nmedical knowledge graph question answering model, dubbed MedKGQA, that predicts\ndrug-drug interaction by employing machine reading comprehension from\nclosed-domain literature and constructing a knowledge graph of drug-protein\ntriplets from open-domain documents. The model vectorizes the drug-protein\ntarget attributes in the graph using entity embeddings and establishes directed\nconnections between drug and protein entities based on the metabolic\ninteraction pathways of protein targets in the human body. This aligns multiple\nexternal knowledge and applies it to learn the graph neural network. Without\nbells and whistles, the proposed model achieved a 4.5% improvement in terms of\ndrug-drug interaction prediction accuracy compared to previous state-of-the-art\nmodels on the Qangaroo MedHop dataset. Experimental results demonstrate the\nefficiency and effectiveness of the model and verify the feasibility of\nintegrating external knowledge in machine reading comprehension tasks.\n', '  Drug synergy arises when the combined impact of two drugs exceeds the sum of\ntheir individual effects. While single-drug effects on cell lines are\nwell-documented, the scarcity of data on drug synergy, considering the vast\narray of potential drug combinations, prompts a growing interest in\ncomputational approaches for predicting synergies in untested drug pairs. We\nintroduce a Graph Neural Network (\\textit{GNN}) based model for drug synergy\nprediction, which utilizes drug chemical structures and cell line gene\nexpression data. We extract data from the largest available drug combination\ndatabase (DrugComb) and generate multiple synergy scores (commonly used in the\nliterature) to create seven datasets that serve as a reliable benchmark with\nhigh confidence. In contrast to conventional models relying on pre-computed\nchemical features, our GNN-based approach learns task-specific drug\nrepresentations directly from the graph structure of the drugs, providing\nsuperior performance in predicting drug synergies. Our work suggests that\nlearning task-specific drug representations and leveraging a diverse dataset is\na promising approach to advancing our understanding of drug-drug interaction\nand synergy.\n'] , ['  Background: Electronic health records (EHRs) are a data source for opioid\nresearch. Opioid use disorder is known to be under-coded as a diagnosis, yet\nproblematic opioid use can be documented in clinical notes.\n  Objectives: Our goals were 1) to identify problematic opioid use from a full\nrange of clinical notes; and 2) to compare the characteristics of patients\nidentified as having problematic opioid use, exclusively documented in clinical\nnotes, to those having documented ICD opioid use disorder diagnostic codes.\n  Materials and Methods: We developed and applied a natural language processing\n(NLP) tool to the clinical notes of a patient cohort (n=222,371) from two\nVeteran Affairs service regions to identify patients with problematic opioid\nuse. We also used a set of ICD diagnostic codes to identify patients with\nopioid use disorder from the same cohort. We compared the demographic and\nclinical characteristics of patients identified only through NLP, to those of\npatients identified through ICD codes.\n  Results: NLP exclusively identified 57,331 patients; 6,997 patients had\npositive ICD code identifications. Patients exclusively identified through NLP\nwere more likely to be women. Those identified through ICD codes were more\nlikely to be male, younger, have concurrent benzodiazepine prescriptions, more\ncomorbidities, more care encounters, and less likely to be married. Patients in\nthe NLP and ICD groups had substantially elevated comorbidity levels compared\nto patients not documented as experiencing problematic opioid use.\n  Conclusions: NLP is a feasible approach for identifying problematic opioid\nuse not otherwise recorded by ICD codes. Clinicians may be reluctant to code\nfor opioid use disorder. It is therefore incumbent on the healthcare team to\nsearch for documentation of opioid concerns within clinical notes.\n', '  The opioid epidemic, referring to the growing hospitalizations and deaths\nbecause of overdose of opioid usage and addiction, has become a severe health\nproblem in the United States. Many strategies have been developed by the\nfederal and local governments and health communities to combat this crisis.\nAmong them, improving our understanding of the epidemic through better health\nsurveillance is one of the top priorities. In addition to direct testing,\nmachine learning approaches may also allow us to detect opioid users by\nanalyzing data from social media because many opioid users may choose not to do\nthe tests but may share their experiences on social media anonymously. In this\npaper, we take advantage of recent advances in machine learning, collect and\nanalyze user posts from a popular social network Reddit with the goal to\nidentify opioid users. Posts from more than 1,000 users who have posted on\nthree sub-reddits over a period of one month have been collected. In addition\nto the ones that contain keywords such as opioid, opiate, or heroin, we have\nalso collected posts that contain slang words of opioid such as black or\nchocolate. We apply an attention-based bidirectional long short memory model to\nidentify opioid users. Experimental results show that the approaches\nsignificantly outperform competitive algorithms in terms of F1-score.\nFurthermore, the model allows us to extract most informative words, such as\nopiate, opioid, and black, from posts via the attention layer, which provides\nmore insights on how the machine learning algorithm works in distinguishing\ndrug users from non-drug users.\n', '  In the last decade, the United States has lost more than 500,000 people from\nan overdose involving prescription and illicit opioids making it a national\npublic health emergency (USDHHS, 2017). Medical practitioners require robust\nand timely tools that can effectively identify at-risk patients.\nCommunity-based social media platforms such as Reddit allow self-disclosure for\nusers to discuss otherwise sensitive drug-related behaviors. We present a\nmoderate size corpus of 2500 opioid-related posts from various subreddits\nlabeled with six different phases of opioid use: Medical Use, Misuse,\nAddiction, Recovery, Relapse, Not Using. For every post, we annotate span-level\nextractive explanations and crucially study their role both in annotation\nquality and model development. We evaluate several state-of-the-art models in a\nsupervised, few-shot, or zero-shot setting. Experimental results and error\nanalysis show that identifying the phases of opioid use disorder is highly\ncontextual and challenging. However, we find that using explanations during\nmodeling leads to a significant boost in classification accuracy demonstrating\ntheir beneficial role in a high-stakes domain such as studying the opioid use\ndisorder continuum.\n']",Machine Learning for Pharmaceutical and Healthcare Applications,"""Predicting Drug Interactions and Synergies"""
199,"Sepsis Mortality Prediction in ICU Patients , Reinforcement Learning for Sepsis Treatment","['predicting', 'sepsis', 'mortality', 'hospital', 'predict', 'prediction', 'cohorts', 'icu', 'biomarkers', 'healthcare'] , ['reinforcement', 'sepsis', 'interventions', 'adaptive', 'reward', 'medication', 'cpr', 'medications', 'policies', 'outcomes']","[""  Quantifying a patient's health status provides clinicians with insight into\npatient risk, and the ability to better triage and manage resources. Early\nWarning Scores (EWS) are widely deployed to measure overall health status, and\nrisk of adverse outcomes, in hospital patients. However, current EWS are\nlimited both by their lack of personalisation and use of static observations.\nWe propose a pipeline that groups intensive care unit patients by the\ntrajectories of observations data throughout their stay as a basis for the\ndevelopment of personalised risk predictions. Feature importance is considered\nto provide model explainability. Using the MIMIC-IV dataset, six clusters were\nidentified, capturing differences in disease codes, observations, lengths of\nadmissions and outcomes. Applying the pipeline to data from just the first four\nhours of each ICU stay assigns the majority of patients to the same cluster as\nwhen the entire stay duration is considered. In-hospital mortality prediction\nmodels trained on individual clusters had higher F1 score performance in five\nof the six clusters when compared against the unclustered patient cohort. The\npipeline could form the basis of a clinical decision support tool, working to\nimprove the clinical characterisation of risk groups and the early detection of\npatient deterioration.\n"", ""  Sepsis poses a major global health threat, accounting for millions of deaths\nannually and significant economic costs. Accurate predictions of mortality risk\nin sepsis patients facilitate the efficient allocation of medical resources,\nthereby enhancing patient survival and quality of life. Through precise risk\nassessments, healthcare facilities can effectively distribute intensive care\nbeds, medical equipment, and staff, ensuring high-risk patients receive timely\nand appropriate care. Early identification and intervention significantly\ndecrease mortality rates and improve patient outcomes. Current methods\ntypically utilize only one type of data--either constant, temporal, or ICD\ncodes. This study introduces the Time-Constant KAN Integrated Network(TCKIN),\nan innovative model that enhances the accuracy of sepsis mortality risk\npredictions by integrating both temporal and constant data from electronic\nhealth records and ICD codes. Validated against the MIMIC-III and MIMIC-IV\ndatasets, TCKIN surpasses existing machine learning and deep learning methods\nin accuracy, sensitivity, and specificity. Notably, TCKIN achieved AUCs of\n87.76% and 88.07%, demonstrating superior capability in identifying high-risk\npatients. Additionally, TCKIN effectively combats the prevalent issue of data\nimbalance in clinical settings, improving the detection of patients at elevated\nrisk of mortality and facilitating timely interventions. These results confirm\nthe model's effectiveness and its potential to transform patient management and\ntreatment optimization in clinical practice. With this advanced risk assessment\ntool, healthcare providers can devise more tailored treatment plans, optimize\nresource utilization, and ultimately enhance survival rates and quality of life\nfor sepsis patients.\n"", ""  Background: Sepsis is a severe condition responsible for many deaths\nworldwide. Accurate prediction of sepsis outcomes is crucial for timely and\neffective treatment. Although previous studies have used ML to forecast\noutcomes, they faced limitations in feature selection and model\ncomprehensibility, resulting in less effective predictions. Thus, this research\naims to develop an interpretable and accurate ML model to help clinical\nprofessionals predict in-hospital mortality.\n  Methods: We analyzed ICU patient records from the MIMIC-III database based on\nspecific criteria and extracted relevant data. Our feature selection process\nincluded a literature review, clinical input refinement, and using Random\nForest to select the top 35 features. We performed data preprocessing,\nincluding cleaning, imputation, standardization, and applied SMOTE for\noversampling to address imbalance, resulting in 4,683 patients, with admission\ncounts of 17,429. We compared the performance of Random Forest, Gradient\nBoosting, Logistic Regression, SVM, and KNN models.\n  Results: The Random Forest model was the most effective in predicting\nsepsis-related in-hospital mortality. It outperformed other models, achieving\nan accuracy of 0.90 and an AUROC of 0.97, significantly better than the\nexisting literature. Our meticulous feature selection contributed to the\nmodel's precision and identified critical determinants of sepsis mortality.\nThese results underscore the pivotal role of data-driven ML in healthcare,\nespecially for predicting in-hospital mortality due to sepsis.\n  Conclusion: This study represents a significant advancement in predicting\nin-hospital sepsis mortality, highlighting the potential of ML in healthcare.\nThe implications are profound, offering a data-driven approach that enhances\ndecision-making in patient care and reduces in-hospital mortality.\n""] , ['  Reinforcement learning (RL) has garnered increasing recognition for its\npotential to optimise dynamic treatment regimes (DTRs) in personalised\nmedicine, particularly for drug dosage prescriptions and medication\nrecommendations. However, a significant challenge persists: the absence of a\nunified framework for simulating diverse healthcare scenarios and a\ncomprehensive analysis to benchmark the effectiveness of RL algorithms within\nthese contexts. To address this gap, we introduce \\textit{DTR-Bench}, a\nbenchmarking platform comprising four distinct simulation environments tailored\nto common DTR applications, including cancer chemotherapy, radiotherapy,\nglucose management in diabetes, and sepsis treatment. We evaluate various\nstate-of-the-art RL algorithms across these settings, particularly highlighting\ntheir performance amidst real-world challenges such as\npharmacokinetic/pharmacodynamic (PK/PD) variability, noise, and missing data.\nOur experiments reveal varying degrees of performance degradation among RL\nalgorithms in the presence of noise and patient variability, with some\nalgorithms failing to converge. Additionally, we observe that using temporal\nobservation representations does not consistently lead to improved performance\nin DTR settings. Our findings underscore the necessity of developing robust,\nadaptive RL algorithms capable of effectively managing these complexities to\nenhance patient-specific healthcare. We have open-sourced our benchmark and\ncode at https://github.com/GilesLuo/DTR-Bench.\n', ""  Offline reinforcement learning has shown promise for solving tasks in\nsafety-critical settings, such as clinical decision support. Its application,\nhowever, has been limited by the lack of interpretability and interactivity for\nclinicians. To address these challenges, we propose the medical decision\ntransformer (MeDT), a novel and versatile framework based on the\ngoal-conditioned reinforcement learning paradigm for sepsis treatment\nrecommendation. MeDT uses the decision transformer architecture to learn a\npolicy for drug dosage recommendation. During offline training, MeDT utilizes\ncollected treatment trajectories to predict administered treatments for each\ntime step, incorporating known treatment outcomes, target acuity scores, past\ntreatment decisions, and current and past medical states. This analysis enables\nMeDT to capture complex dependencies among a patient's medical history,\ntreatment decisions, outcomes, and short-term effects on stability. Our\nproposed conditioning uses acuity scores to address sparse reward issues and to\nfacilitate clinician-model interactions, enhancing decision-making. Following\ntraining, MeDT can generate tailored treatment recommendations by conditioning\non the desired positive outcome (survival) and user-specified short-term\nstability improvements. We carry out rigorous experiments on data from the\nMIMIC-III dataset and use off-policy evaluation to demonstrate that MeDT\nrecommends interventions that outperform or are competitive with existing\noffline reinforcement learning methods while enabling a more interpretable,\npersonalized and clinician-directed approach.\n"", '  We present ICU-Sepsis, an environment that can be used in benchmarks for\nevaluating reinforcement learning (RL) algorithms. Sepsis management is a\ncomplex task that has been an important topic in applied RL research in recent\nyears. Therefore, MDPs that model sepsis management can serve as part of a\nbenchmark to evaluate RL algorithms on a challenging real-world problem.\nHowever, creating usable MDPs that simulate sepsis care in the ICU remains a\nchallenge due to the complexities involved in acquiring and processing patient\ndata. ICU-Sepsis is a lightweight environment that models personalized care of\nsepsis patients in the ICU. The environment is a tabular MDP that is widely\ncompatible and is challenging even for state-of-the-art RL algorithms, making\nit a valuable tool for benchmarking their performance. However, we emphasize\nthat while ICU-Sepsis provides a standardized environment for evaluating RL\nalgorithms, it should not be used to draw conclusions that guide medical\npractice.\n']",Sepsis Management and Treatment Optimization,Reinforcement Learning for Sepsis Treatment
200,"""Disaster Response using Social Media Text Analysis"" , ""Automated Incident Management and Root Cause Analysis"" , ""Wildfires and Floods: AI-Driven Disaster Management""","['tweets', 'crisistransformers', 'twitter', 'microblogs', 'disasters', 'crises', 'tweet', 'disaster', 'crisisfacts', 'textual'] , ['incidents', 'automation', 'automated', 'monitoring', 'faultprofit', 'incident', 'alerts', 'maintenance', 'microservices', 'faults'] , ['wildfires', 'wildfire', 'disasters', 'flood', 'fires', 'floodwater', 'disaster', 'rescue', 'emergency', 'emergencies']","['  Online social media platforms, such as Twitter, provide valuable information\nduring disaster events. Existing tweet disaster summarization approaches\nprovide a summary of these events to aid government agencies, humanitarian\norganizations, etc., to ensure effective disaster response. In the literature,\nthere are two types of approaches for disaster summarization, namely,\nsupervised and unsupervised approaches. Although supervised approaches are\ntypically more effective, they necessitate a sizable number of disaster event\nsummaries for testing and training. However, there is a lack of good number of\ndisaster summary datasets for training and evaluation. This motivates us to add\nmore datasets to make supervised learning approaches more efficient. In this\npaper, we present ADSumm, which adds annotated ground-truth summaries for eight\ndisaster events which consist of both natural and man-made disaster events\nbelonging to seven different countries. Our experimental analysis shows that\nthe newly added datasets improve the performance of the supervised\nsummarization approaches by 8-28% in terms of ROUGE-N F1-score. Moreover, in\nnewly annotated dataset, we have added a category label for each input tweet\nwhich helps to ensure good coverage from different categories in summary.\nAdditionally, we have added two other features relevance label and key-phrase,\nwhich provide information about the quality of a tweet and explanation about\nthe inclusion of the tweet into summary, respectively. For ground-truth summary\ncreation, we provide the annotation procedure adapted in detail, which has not\nbeen described in existing literature. Experimental analysis shows the quality\nof ground-truth summary is very good with Coverage, Relevance and Diversity.\n', '  In the field of crisis/disaster informatics, social media is increasingly\nbeing used for improving situational awareness to inform response and relief\nefforts. Efficient and accurate text classification tools have been a focal\narea of investigation in crisis informatics. However, current methods mostly\nrely on single-label text classification models, which fails to capture\ndifferent insights embedded in dynamic and multifaceted disaster-related social\nmedia data. This study introduces a novel approach to disaster text\nclassification by enhancing a pre-trained Large Language Model (LLM) through\ninstruction fine-tuning targeted for multi-label classification of\ndisaster-related tweets. Our methodology involves creating a comprehensive\ninstruction dataset from disaster-related tweets, which is then used to\nfine-tune an open-source LLM, thereby embedding it with disaster-specific\nknowledge. This fine-tuned model can classify multiple aspects of\ndisaster-related information simultaneously, such as the type of event,\ninformativeness, and involvement of human aid, significantly improving the\nutility of social media data for situational awareness in disasters. The\nresults demonstrate that this approach enhances the categorization of critical\ninformation from social media posts, thereby facilitating a more effective\ndeployment for situational awareness during emergencies. This research paves\nthe way for more advanced, adaptable, and robust disaster management tools,\nleveraging the capabilities of LLMs to improve real-time situational awareness\nand response strategies in disaster scenarios.\n', '  Social media platforms play an essential role in crisis communication, but\nanalyzing crisis-related social media texts is challenging due to their\ninformal nature. Transformer-based pre-trained models like BERT and RoBERTa\nhave shown success in various NLP tasks, but they are not tailored for\ncrisis-related texts. Furthermore, general-purpose sentence encoders are used\nto generate sentence embeddings, regardless of the textual complexities in\ncrisis-related texts. Advances in applications like text classification,\nsemantic search, and clustering contribute to the effective processing of\ncrisis-related texts, which is essential for emergency responders to gain a\ncomprehensive view of a crisis event, whether historical or real-time. To\naddress these gaps in crisis informatics literature, this study introduces\nCrisisTransformers, an ensemble of pre-trained language models and sentence\nencoders trained on an extensive corpus of over 15 billion word tokens from\ntweets associated with more than 30 crisis events, including disease outbreaks,\nnatural disasters, conflicts, and other critical incidents. We evaluate\nexisting models and CrisisTransformers on 18 crisis-specific public datasets.\nOur pre-trained models outperform strong baselines across all datasets in\nclassification tasks, and our best-performing sentence encoder improves the\nstate-of-the-art by 17.43% in sentence encoding tasks. Additionally, we\ninvestigate the impact of model initialization on convergence and evaluate the\nsignificance of domain-specific models in generating semantically meaningful\nsentence embeddings. The models are publicly available at:\nhttps://huggingface.co/crisistransformers\n'] , [""  Despite significant reliability efforts, large-scale cloud services\ninevitably experience production incidents that can significantly impact\nservice availability and customer's satisfaction. Worse, in many cases one\nincident can lead to multiple downstream failures due to cascading effects that\ncreates several related incidents across different dependent services. Often\ntime On-call Engineers (OCEs) examine these incidents in silos that lead to\nsignificant amount of manual toil and increase the overall time-to-mitigate\nincidents. Therefore, developing efficient incident linking models is of\nparamount importance for grouping related incidents into clusters so as to\nquickly resolve major outages and reduce on-call fatigue. Existing incident\nlinking methods mostly leverages textual and contextual information of\nincidents (e.g., title, description, severity, impacted components), thus\nfailing to leverage the inter-dependencies between services. In this paper, we\npropose the dependency-aware incident linking (DiLink) framework which\nleverages both textual and service dependency graph information to improve the\naccuracy and coverage of incident links not only coming from same service, but\nalso from different services and workloads. Furthermore, we propose a novel\nmethod to align the embeddings of multi-modal (i.e., textual and graphical)\ndata using Orthogonal Procrustes. Extensive experimental results on real-world\nincidents from 5 workloads of Microsoft demonstrate that our alignment method\nhas an F1-score of 0.96 (14% gain over current state-of-the-art methods). We\nare also in the process of deploying this solution across 610 services from\nthese 5 workloads for continuously supporting OCEs improving incident\nmanagement and reducing manual toil.\n"", ""  Root Cause Analysis (RCA) plays a pivotal role in the incident diagnosis\nprocess for cloud services, requiring on-call engineers to identify the primary\nissues and implement corrective actions to prevent future recurrences.\nImproving the incident RCA process is vital for minimizing service downtime,\ncustomer impact and manual toil. Recent advances in artificial intelligence\nhave introduced state-of-the-art Large Language Models (LLMs) like GPT-4, which\nhave proven effective in tackling various AIOps problems, ranging from code\nauthoring to incident management. Nonetheless, the GPT-4 model's immense size\npresents challenges when trying to fine-tune it on user data because of the\nsignificant GPU resource demand and the necessity for continuous model\nfine-tuning with the emergence of new data. To address the high cost of\nfine-tuning LLM, we propose an in-context learning approach for automated root\ncausing, which eliminates the need for fine-tuning. We conduct extensive study\nover 100,000 production incidents, comparing several large language models\nusing multiple metrics. The results reveal that our in-context learning\napproach outperforms the previous fine-tuned large language models such as\nGPT-3 by an average of 24.8\\% across all metrics, with an impressive 49.7\\%\nimprovement over the zero-shot model. Moreover, human evaluation involving\nactual incident owners demonstrates its superiority over the fine-tuned model,\nachieving a 43.5\\% improvement in correctness and an 8.7\\% enhancement in\nreadability. The impressive results demonstrate the viability of utilizing a\nvanilla GPT model for the RCA task, thereby avoiding the high computational and\nmaintenance costs associated with a fine-tuned model.\n"", ""  The growing complexity of cloud based software systems has resulted in\nincident management becoming an integral part of the software development\nlifecycle. Root cause analysis (RCA), a critical part of the incident\nmanagement process, is a demanding task for on-call engineers, requiring deep\ndomain knowledge and extensive experience with a team's specific services.\nAutomation of RCA can result in significant savings of time, and ease the\nburden of incident management on on-call engineers. Recently, researchers have\nutilized Large Language Models (LLMs) to perform RCA, and have demonstrated\npromising results. However, these approaches are not able to dynamically\ncollect additional diagnostic information such as incident related logs,\nmetrics or databases, severely restricting their ability to diagnose root\ncauses. In this work, we explore the use of LLM based agents for RCA to address\nthis limitation. We present a thorough empirical evaluation of a ReAct agent\nequipped with retrieval tools, on an out-of-distribution dataset of production\nincidents collected at Microsoft. Results show that ReAct performs\ncompetitively with strong retrieval and reasoning baselines, but with highly\nincreased factual accuracy. We then extend this evaluation by incorporating\ndiscussions associated with incident reports as additional inputs for the\nmodels, which surprisingly does not yield significant performance improvements.\nLastly, we conduct a case study with a team at Microsoft to equip the ReAct\nagent with tools that give it access to external diagnostic services that are\nused by the team for manual RCA. Our results show how agents can overcome the\nlimitations of prior work, and practical considerations for implementing such a\nsystem in practice.\n""] , ['  Over the past few years, wildfires have become a worldwide environmental\nemergency, resulting in substantial harm to natural habitats and playing a part\nin the acceleration of climate change. Wildfire management methods involve\nprevention, response, and recovery efforts. Despite improvements in detection\ntechniques, the rising occurrence of wildfires demands creative solutions for\nprompt identification and effective control. This research investigates\nproactive methods for detecting and handling wildfires in the United States,\nutilizing Artificial Intelligence (AI), Machine Learning (ML), and 5G\ntechnology. The specific objective of this research covers proactive detection\nand prevention of wildfires using advanced technology; Active monitoring and\nmapping with remote sensing and signaling leveraging on 5G technology; and\nAdvanced response mechanisms to wildfire using drones and IOT devices. This\nstudy was based on secondary data collected from government databases and\nanalyzed using descriptive statistics. In addition, past publications were\nreviewed through content analysis, and narrative synthesis was used to present\nthe observations from various studies. The results showed that developing new\ntechnology presents an opportunity to detect and manage wildfires proactively.\nUtilizing advanced technology could save lives and prevent significant economic\nlosses caused by wildfires. Various methods, such as AI-enabled remote sensing\nand 5G-based active monitoring, can enhance proactive wildfire detection and\nmanagement. In addition, super intelligent drones and IOT devices can be used\nfor safer responses to wildfires. This forms the core of the recommendation to\nthe fire Management Agencies and the government.\n', ""  Real-time flood forecasting plays a crucial role in enabling timely and\neffective emergency responses. However, a significant challenge lies in\nbridging the gap between complex numerical flood models and practical\ndecision-making. Decision-makers often rely on experts to interpret these\nmodels for optimizing flood mitigation strategies. And the public requires\ncomplex techniques to inquiry and understand socio-cultural and institutional\nfactors, often hinders the public's understanding of flood risks. To overcome\nthese challenges, our study introduces an innovative solution: a customized AI\nAssistant powered by the GPT-4 Large Language Model. This AI Assistant is\ndesigned to facilitate effective communication between decision-makers, the\ngeneral public, and flood forecasters, without the requirement of specialized\nknowledge. The new framework utilizes GPT-4's advanced natural language\nunderstanding and function calling capabilities to provide immediate flood\nalerts and respond to various flood-related inquiries. Our developed prototype\nintegrates real-time flood warnings with flood maps and social vulnerability\ndata. It also effectively translates complex flood zone information into\nactionable risk management advice. To assess its performance, we evaluated the\nprototype using six criteria within three main categories: relevance, error\nresilience, and understanding of context. Our research marks a significant step\ntowards a more accessible and user-friendly approach in flood risk management.\nThis study highlights the potential of advanced AI tools like GPT-4 in\ndemocratizing information and enhancing public engagement in critical social\nand environmental issues.\n"", '  Wildfires have emerged as one of the most destructive natural disasters\nworldwide, causing catastrophic losses in both human lives and forest wildlife.\nRecently, the use of Artificial Intelligence (AI) in wildfires, propelled by\nthe integration of Unmanned Aerial Vehicles (UAVs) and deep learning models,\nhas created an unprecedented momentum to implement and develop more effective\nwildfire management. Although some of the existing survey papers have explored\nvarious learning-based approaches, a comprehensive review emphasizing the\napplication of AI-enabled UAV systems and their subsequent impact on\nmulti-stage wildfire management is notably lacking. This survey aims to bridge\nthese gaps by offering a systematic review of the recent state-of-the-art\ntechnologies, highlighting the advancements of UAV systems and AI models from\npre-fire, through the active-fire stage, to post-fire management. To this aim,\nwe provide an extensive analysis of the existing remote sensing systems with a\nparticular focus on the UAV advancements, device specifications, and sensor\ntechnologies relevant to wildfire management. We also examine the pre-fire and\npost-fire management approaches, including fuel monitoring, prevention\nstrategies, as well as evacuation planning, damage assessment, and operation\nstrategies. Additionally, we review and summarize a wide range of computer\nvision techniques in active-fire management, with an emphasis on Machine\nLearning (ML), Reinforcement Learning (RL), and Deep Learning (DL) algorithms\nfor wildfire classification, segmentation, detection, and monitoring tasks.\nUltimately, we underscore the substantial advancement in wildfire modeling\nthrough the integration of cutting-edge AI techniques and UAV-based data,\nproviding novel insights and enhanced predictive capabilities to understand\ndynamic wildfire behavior.\n']",Disaster Management and Response using AI and Data Analytics,"""Disaster Response using Social Media Text Analysis"""
201,"Epidemic Modeling and Analysis , COVID-19 Vaccine Sentiment on Twitter","['epidemics', 'epidemic', 'outbreak', 'pandemics', 'pandemic', 'outbreaks', 'influenza', 'covid', 'infectious', 'pandemicllm'] , ['tweets', 'twitter', 'pandemic', 'covid', 'sentiment', 'vaccine', 'vaccination', 'coronavirus', 'vaccines', 'retweets']","['  In the context of natural disasters, human responses inevitably intertwine\nwith natural factors. The COVID-19 pandemic, as a significant stress factor,\nhas brought to light profound variations among different countries in terms of\ntheir adaptive dynamics in addressing the spread of infection outbreaks across\ndifferent regions. This emphasizes the crucial role of cultural characteristics\nin natural disaster analysis. The theoretical understanding of large-scale\nepidemics primarily relies on mean-field kinetic models. However, conventional\nSIR-like models failed to fully explain the observed phenomena at the onset of\nthe COVID-19 outbreak. These phenomena included the unexpected cessation of\nexponential growth, the reaching of plateaus, and the occurrence of multi-wave\ndynamics. In situations where an outbreak of a highly virulent and unfamiliar\ninfection arises, it becomes crucial to respond swiftly at a non-medical level\nto mitigate the negative socio-economic impact. Here we present a theoretical\nexamination of the first wave of the epidemic based on a simple SIRSS model\n(SIR with Social Stress). We conduct an analysis of the socio-cultural features\nof na\\""ive population behaviors across various countries worldwide. The unique\ncharacteristics of each country/territory are encapsulated in only a few\nconstants within our model, derived from the fitted COVID-19 statistics. These\nconstants also reflect the societal response dynamics to the external stress\nfactor, underscoring the importance of studying the mutual behavior of humanity\nand natural factors during global social disasters. Based on these distinctive\ncharacteristics of specific regions, local authorities can optimize their\nstrategies to effectively combat epidemics until vaccines are developed.\n', '  Since the onset of the COVID-19 pandemic, there has been a growing interest\nin studying epidemiological models. Traditional mechanistic models\nmathematically describe the transmission mechanisms of infectious diseases.\nHowever, they often suffer from limitations of oversimplified or fixed\nassumptions, which could cause sub-optimal predictive power and inefficiency in\ncapturing complex relation information. Consequently, Graph Neural Networks\n(GNNs) have emerged as a progressively popular tool in epidemic research. In\nthis paper, we endeavor to furnish a comprehensive review of GNNs in epidemic\ntasks and highlight potential future directions. To accomplish this objective,\nwe introduce hierarchical taxonomies for both epidemic tasks and methodologies,\noffering a trajectory of development within this domain. For epidemic tasks, we\nestablish a taxonomy akin to those typically employed within the epidemic\ndomain. For methodology, we categorize existing work into Neural Models and\nHybrid Models. Following this, we perform an exhaustive and systematic\nexamination of the methodologies, encompassing both the tasks and their\ntechnical details. Furthermore, we discuss the limitations of existing methods\nfrom diverse perspectives and systematically propose future research\ndirections. This survey aims to bridge literature gaps and promote the\nprogression of this promising field, with a list of relevant papers at\nhttps://github.com/Emory-Melody/awesome-epidemic-modelingpapers. We hope that\nit will facilitate synergies between the communities of GNNs and epidemiology,\nand contribute to their collective progress.\n', '  In this paper, we propose a mathematical framework that governs the evolution\nof epidemic dynamics, encompassing both intra-population dynamics and\ninter-population mobility within a metapopulation network. By linearizing this\ndynamical system, we can identify the spatial starting point(s), namely the\nsource(s) (A) and the initiation time (B) of any epidemic, which we refer to as\nthe ""Big Bang"" of the epidemic. Furthermore, we introduce a novel concept of\neffective distance to track disease spread within the network. Our analysis\nreveals that the contagion geometry can be represented as a line with a\nuniversal slope, independent of disease type (R0) or mobility network\nconfiguration. The mathematical derivations presented in this framework are\ncorroborated by empirical data, including observations from the COVID-19\npandemic in Iran and the US, as well as the H1N1 outbreak worldwide. Within\nthis framework, in order to detect the Big Bang of an epidemic we require two\ntypes of data: A) A snapshot of the active infected cases in each subpopulation\nduring the linear phase. B) A coarse-grained representation of inter-population\nmobility. Also even with access to only type A data, we can still demonstrate\nthe universal contagion geometric pattern. Additionally, we can estimate errors\nand assess the precision of the estimations. This comprehensive approach\nenhances our understanding of when and where epidemics began and how they\nspread, and equips us with valuable insights for developing effective public\nhealth policies and mitigating the impact of infectious diseases on populations\nworldwide.\n'] , [""  The Covid-19 pandemic had an enormous effect on our lives, especially on\npeople's interactions. By introducing Covid-19 vaccines, both positive and\nnegative opinions were raised over the subject of taking vaccines or not. In\nthis paper, using data gathered from Twitter, including tweets and user\nprofiles, we offer a comprehensive analysis of public opinion in Iran about the\nCoronavirus vaccines. For this purpose, we applied a search query technique\ncombined with a topic modeling approach to extract vaccine-related tweets. We\nutilized transformer-based models to classify the content of the tweets and\nextract themes revolving around vaccination. We also conducted an emotion\nanalysis to evaluate the public happiness and anger around this topic. Our\nresults demonstrate that Covid-19 vaccination has attracted considerable\nattention from different angles, such as governmental issues, safety or\nhesitancy, and side effects. Moreover, Coronavirus-relevant phenomena like\npublic vaccination and the rate of infection deeply impacted public emotional\nstatus and users' interactions.\n"", '  Misinformation has emerged as a major societal threat in recent years in\ngeneral; specifically in the context of the COVID-19 pandemic, it has wrecked\nhavoc, for instance, by fuelling vaccine hesitancy. Cost-effective, scalable\nsolutions for combating misinformation are the need of the hour. This work\nexplored how existing information obtained from social media and augmented with\nmore curated fact checked data repositories can be harnessed to facilitate\nautomated rebuttal of misinformation at scale. While the ideas herein can be\ngeneralized and reapplied in the broader context of misinformation mitigation\nusing a multitude of information sources and catering to the spectrum of social\nmedia platforms, this work serves as a proof of concept, and as such, it is\nconfined in its scope to only rebuttal of tweets, and in the specific context\nof misinformation regarding COVID-19. It leverages two publicly available\ndatasets, viz. FaCov (fact-checked articles) and misleading (social media\nTwitter) data on COVID-19 Vaccination.\n', ""  A drastic rise in potentially life-threatening misinformation has been a\nby-product of the COVID-19 pandemic. Computational support to identify false\ninformation within the massive body of data on the topic is crucial to prevent\nharm. Researchers proposed many methods for flagging online misinformation\nrelated to COVID-19. However, these methods predominantly target specific\ncontent types (e.g., news) or platforms (e.g., Twitter). The methods'\ncapabilities to generalize were largely unclear so far. We evaluate fifteen\nTransformer-based models on five COVID-19 misinformation datasets that include\nsocial media posts, news articles, and scientific papers to fill this gap. We\nshow tokenizers and models tailored to COVID-19 data do not provide a\nsignificant advantage over general-purpose ones. Our study provides a realistic\nassessment of models for detecting COVID-19 misinformation. We expect that\nevaluating a broad spectrum of datasets and models will benefit future research\nin developing misinformation detection systems.\n""]",COVID-19 Research and Public Perception,COVID-19 Vaccine Sentiment on Twitter
202,"Gaussian Process Models and Kernels , Gaussian Mixture Models Estimation","['gaussian', 'gps', 'kernels', 'ensemble', 'gpr', 'kernel', 'gp', 'svgp', 'bayesian', 'lfgp'] , ['mixtures', 'mixture', 'clustering', 'likelihood', 'clusterability', 'gaussian', 'estimation', 'estimating', 'mixing', 'gmms']","['  Recently, there has been a growing interest for mixed-categorical meta-models\nbased on Gaussian process (GP) surrogates. In this setting, several existing\napproaches use different strategies either by using continuous kernels (e.g.,\ncontinuous relaxation and Gower distance based GP) or by using a direct\nestimation of the correlation matrix. In this paper, we present a kernel-based\napproach that extends continuous exponential kernels to handle\nmixed-categorical variables. The proposed kernel leads to a new GP surrogate\nthat generalizes both the continuous relaxation and the Gower distance based GP\nmodels. We demonstrate, on both analytical and engineering problems, that our\nproposed GP model gives a higher likelihood and a smaller residual error than\nthe other kernel-based state-of-the-art models. Our method is available in the\nopen-source software SMT.\n', '  Gaussian process (GP) models have received increasingly attentions in recent\nyears due to their superb prediction accuracy and modeling flexibility. To\naddress the computational burdens of GP models for large-scale datasets,\ndistributed learning for GPs are often adopted. Current aggregation models for\ndistributed GPs are not time-efficient when incorporating correlations between\nGP experts. In this work, we propose a novel approach for aggregated prediction\nin distributed GPs. The technique is suitable for both the exact and sparse\nvariational GPs. The proposed method incorporates correlations among experts,\nleading to better prediction accuracy with manageable computational\nrequirements. As demonstrated by empirical studies, the proposed approach\nresults in more stable predictions in less time than state-of-the-art\nconsistent aggregation models.\n', '  We present a new strategy for learning the functional relation between a pair\nof variables, while addressing inhomogeneities in the correlation structure of\nthe available data, by modelling the sought function as a sample function of a\nnon-stationary Gaussian Process (GP), that nests within itself multiple other\nGPs, each of which we prove can be stationary, thereby establishing sufficiency\nof two GP layers. In fact, a non-stationary kernel is envisaged, with each\nhyperparameter set as dependent on the sample function drawn from the outer\nnon-stationary GP, such that a new sample function is drawn at every pair of\ninput values at which the kernel is computed. However, such a model cannot be\nimplemented, and we substitute this by recalling that the average effect of\ndrawing different sample functions from a given GP is equivalent to that of\ndrawing a sample function from each of a set of GPs that are rendered\ndifferent, as updated during the equilibrium stage of the undertaken inference\n(via MCMC). The kernel is fully non-parametric, and it suffices to learn one\nhyperparameter per layer of GP, for each dimension of the input variable. We\nillustrate this new learning strategy on a real dataset.\n'] , ['  The purpose of this paper is twofold. First, we propose a novel algorithm for\nestimating parameters in one-dimensional Gaussian mixture models (GMMs). The\nalgorithm takes advantage of the Hankel structure inherent in the Fourier data\nobtained from independent and identically distributed (i.i.d) samples of the\nmixture. For GMMs with a unified variance, a singular value ratio functional\nusing the Fourier data is introduced and used to resolve the variance and\ncomponent number simultaneously. The consistency of the estimator is derived.\nCompared to classic algorithms such as the method of moments and the maximum\nlikelihood method, the proposed algorithm does not require prior knowledge of\nthe number of Gaussian components or good initial guesses. Numerical\nexperiments demonstrate its superior performance in estimation accuracy and\ncomputational cost. Second, we reveal that there exists a fundamental limit to\nthe problem of estimating the number of Gaussian components or model order in\nthe mixture model if the number of i.i.d samples is finite. For the case of a\nsingle variance, we show that the model order can be successfully estimated\nonly if the minimum separation distance between the component means exceeds a\ncertain threshold value and can fail if below. We derive a lower bound for this\nthreshold value, referred to as the computational resolution limit, in terms of\nthe number of i.i.d samples, the variance, and the number of Gaussian\ncomponents. Numerical experiments confirm this phase transition phenomenon in\nestimating the model order. Moreover, we demonstrate that our algorithm\nachieves better scores in likelihood, AIC, and BIC when compared to the EM\nalgorithm.\n', '  We investigate the landscape of the negative log-likelihood function of\nGaussian Mixture Models (GMMs) with a general number of components in the\npopulation limit. As the objective function is non-convex, there can be\nmultiple local minima that are not globally optimal, even for well-separated\nmixture models. Our study reveals that all local minima share a common\nstructure that partially identifies the cluster centers (i.e., means of the\nGaussian components) of the true location mixture. Specifically, each local\nminimum can be represented as a non-overlapping combination of two types of\nsub-configurations: fitting a single mean estimate to multiple Gaussian\ncomponents or fitting multiple estimates to a single true component. These\nresults apply to settings where the true mixture components satisfy a certain\nseparation condition, and are valid even when the number of components is over-\nor under-specified. We also present a more fine-grained analysis for the\nsetting of one-dimensional GMMs with three components, which provide sharper\napproximation error bounds with improved dependence on the separation.\n', '  We consider the parameter estimation problem in the deviated Gaussian mixture\nof experts in which the data are generated from $(1 - \\lambda^{\\ast}) g_0(Y|\nX)+ \\lambda^{\\ast} \\sum_{i = 1}^{k_{\\ast}} p_{i}^{\\ast}\nf(Y|(a_{i}^{\\ast})^{\\top}X+b_i^{\\ast},\\sigma_{i}^{\\ast})$, where $X, Y$ are\nrespectively a covariate vector and a response variable, $g_{0}(Y|X)$ is a\nknown function, $\\lambda^{\\ast} \\in [0, 1]$ is true but unknown mixing\nproportion, and $(p_{i}^{\\ast}, a_{i}^{\\ast}, b_{i}^{\\ast}, \\sigma_{i}^{\\ast})$\nfor $1 \\leq i \\leq k^{\\ast}$ are unknown parameters of the Gaussian mixture of\nexperts. This problem arises from the goodness-of-fit test when we would like\nto test whether the data are generated from $g_{0}(Y|X)$ (null hypothesis) or\nthey are generated from the whole mixture (alternative hypothesis). Based on\nthe algebraic structure of the expert functions and the distinguishability\nbetween $g_0$ and the mixture part, we construct novel Voronoi-based loss\nfunctions to capture the convergence rates of maximum likelihood estimation\n(MLE) for our models. We further demonstrate that our proposed loss functions\ncharacterize the local convergence rates of parameter estimation more\naccurately than the generalized Wasserstein, a loss function being commonly\nused for estimating parameters in the Gaussian mixture of experts.\n']",Gaussian Process and Mixture Models,Gaussian Process Models and Kernels
203,"Kernel Ridge Regression Analysis , Kernel Methods for Nonparametric Testing , Kernel Methods for Regression and Approximation","['kernels', 'regularization', 'kernel', 'ridge', 'regularized', 'ridgeless', 'generalization', 'generalized', 'optimal', 'overfitting'] , ['kernels', 'nonparametric', 'kernel', 'stein', 'compression', 'distributions', 'unnormalised', 'gaussian', 'statistical', 'ksd'] , ['kernels', 'kernel', 'approximations', 'rkhs', 'gaussian', 'hessian', 'approximation', 'neural', 'spectral', 'dimensional']","['  Motivated by the studies of neural networks (e.g.,the neural tangent kernel\ntheory), we perform a study on the large-dimensional behavior of kernel ridge\nregression (KRR) where the sample size $n \\asymp d^{\\gamma}$ for some $\\gamma >\n0$. Given an RKHS $\\mathcal{H}$ associated with an inner product kernel defined\non the sphere $\\mathbb{S}^{d}$, we suppose that the true function $f_{\\rho}^{*}\n\\in [\\mathcal{H}]^{s}$, the interpolation space of $\\mathcal{H}$ with source\ncondition $s>0$. We first determined the exact order (both upper and lower\nbound) of the generalization error of kernel ridge regression for the optimally\nchosen regularization parameter $\\lambda$. We then further showed that when\n$0<s\\le1$, KRR is minimax optimal; and when $s>1$, KRR is not minimax optimal\n(a.k.a. he saturation effect). Our results illustrate that the curves of rate\nvarying along $\\gamma$ exhibit the periodic plateau behavior and the multiple\ndescent behavior and show how the curves evolve with $s>0$. Interestingly, our\nwork provides a unified viewpoint of several recent works on kernel regression\nin the large-dimensional setting, which correspond to $s=0$ and $s=1$\nrespectively.\n', ""  Kernel ridge regression (KRR) is a popular class of machine learning models\nthat has become an important tool for understanding deep learning. Much of the\nfocus has been on studying the proportional asymptotic regime, $n \\asymp d$,\nwhere $n$ is the number of training samples and $d$ is the dimension of the\ndataset. In this regime, under certain conditions on the data distribution, the\nkernel random matrix involved in KRR exhibits behavior akin to that of a linear\nkernel. In this work, we extend the study of kernel regression to the quadratic\nasymptotic regime, where $n \\asymp d^2$. In this regime, we demonstrate that a\nbroad class of inner-product kernels exhibit behavior similar to a quadratic\nkernel. Specifically, we establish an operator norm approximation bound for the\ndifference between the original kernel random matrix and a quadratic kernel\nrandom matrix with additional correction terms compared to the Taylor expansion\nof the kernel functions. The approximation works for general data distributions\nunder a Gaussian-moment-matching assumption with a covariance structure. This\nnew approximation is utilized to obtain a limiting spectral distribution of the\noriginal kernel matrix and characterize the precise asymptotic training and\ngeneralization errors for KRR in the quadratic regime when $n/d^2$ converges to\na non-zero constant. The generalization errors are obtained for both\ndeterministic and random teacher models. Our proof techniques combine moment\nmethods, Wick's formula, orthogonal polynomials, and resolvent analysis of\nrandom matrices with correlated entries.\n"", '  Kernel ridge regression, KRR, is a generalization of linear ridge regression\nthat is non-linear in the data, but linear in the parameters. Here, we\nintroduce an equivalent formulation of the objective function of KRR, opening\nup both for using penalties other than the ridge penalty and for studying\nkernel ridge regression from the perspective of gradient descent. Using a\ncontinuous-time perspective, we derive a closed-form solution for solving\nkernel regression with gradient descent, something we refer to as kernel\ngradient flow, KGF, and theoretically bound the differences between KRR and\nKGF, where, for the latter, regularization is obtained through early stopping.\nWe also generalize KRR by replacing the ridge penalty with the $\\ell_1$ and\n$\\ell_\\infty$ penalties, respectively, and use the fact that analogous to the\nsimilarities between KGF and KRR, $\\ell_1$ regularization and forward stagewise\nregression (also known as coordinate descent), and $\\ell_\\infty$ regularization\nand sign gradient descent, follow similar solution paths. We can thus alleviate\nthe need for computationally heavy algorithms based on proximal gradient\ndescent. We show theoretically and empirically how the $\\ell_1$ and\n$\\ell_\\infty$ penalties, and the corresponding gradient-based optimization\nalgorithms, produce sparse and robust kernel regression solutions,\nrespectively.\n'] , ['  Over the last decade, an approach that has gained a lot of popularity to\ntackle nonparametric testing problems on general (i.e., non-Euclidean) domains\nis based on the notion of reproducing kernel Hilbert space (RKHS) embedding of\nprobability distributions. The main goal of our work is to understand the\noptimality of two-sample tests constructed based on this approach. First, we\nshow the popular MMD (maximum mean discrepancy) two-sample test to be not\noptimal in terms of the separation boundary measured in Hellinger distance.\nSecond, we propose a modification to the MMD test based on spectral\nregularization by taking into account the covariance information (which is not\ncaptured by the MMD test) and prove the proposed test to be minimax optimal\nwith a smaller separation boundary than that achieved by the MMD test. Third,\nwe propose an adaptive version of the above test which involves a data-driven\nstrategy to choose the regularization parameter and show the adaptive test to\nbe almost minimax optimal up to a logarithmic factor. Moreover, our results\nhold for the permutation variant of the test where the test threshold is chosen\nelegantly through the permutation of the samples. Through numerical experiments\non synthetic and real data, we demonstrate the superior performance of the\nproposed test in comparison to the MMD test and other popular tests in the\nliterature.\n', '  Modern compression methods can summarize a target distribution $\\mathbb{P}$\nmore succinctly than i.i.d. sampling but require access to a low-bias input\nsequence like a Markov chain converging quickly to $\\mathbb{P}$. We introduce a\nnew suite of compression methods suitable for compression with biased input\nsequences. Given $n$ points targeting the wrong distribution and quadratic\ntime, Stein kernel thinning (SKT) returns $\\sqrt{n}$ equal-weighted points with\n$\\widetilde{O}(n^{-1/2})$ maximum mean discrepancy (MMD) to $\\mathbb{P}$. For\nlarger-scale compression tasks, low-rank SKT achieves the same feat in\nsub-quadratic time using an adaptive low-rank debiasing procedure that may be\nof independent interest. For downstream tasks that support simplex or\nconstant-preserving weights, Stein recombination and Stein Cholesky achieve\neven greater parsimony, matching the guarantees of SKT with as few as\n$\\text{poly-log}(n)$ weighted points. Underlying these advances are new\nguarantees for the quality of simplex-weighted coresets, the spectral decay of\nkernel matrices, and the covering numbers of Stein kernel Hilbert spaces. In\nour experiments, our techniques provide succinct and accurate posterior\nsummaries while overcoming biases due to burn-in, approximate Markov chain\nMonte Carlo, and tempering.\n', '  Kernel methods underpin many of the most successful approaches in data\nscience and statistics, and they allow representing probability measures as\nelements of a reproducing kernel Hilbert space without loss of information.\nRecently, the kernel Stein discrepancy (KSD), which combines Stein\'s method\nwith kernel techniques, gained considerable attention. Through the Stein\noperator, KSD allows the construction of powerful goodness-of-fit tests where\nit is sufficient to know the target distribution up to a multiplicative\nconstant. However, the typical U- and V-statistic-based KSD estimators suffer\nfrom a quadratic runtime complexity, which hinders their application in\nlarge-scale settings. In this work, we propose a Nystr\\""om-based KSD\nacceleration -- with runtime $\\mathcal O\\!\\left(mn+m^3\\right)$ for $n$ samples\nand $m\\ll n$ Nystr\\""om points -- , show its $\\sqrt{n}$-consistency under the\nnull with a classical sub-Gaussian assumption, and demonstrate its\napplicability for goodness-of-fit testing on a suite of benchmarks.\n'] , ['  The generalization error curve of certain kernel regression method aims at\ndetermining the exact order of generalization error with various source\ncondition, noise level and choice of the regularization parameter rather than\nthe minimax rate. In this work, under mild assumptions, we rigorously provide a\nfull characterization of the generalization error curves of the kernel gradient\ndescent method (and a large class of analytic spectral algorithms) in kernel\nregression. Consequently, we could sharpen the near inconsistency of kernel\ninterpolation and clarify the saturation effects of kernel regression\nalgorithms with higher qualification, etc. Thanks to the neural tangent kernel\ntheory, these results greatly improve our understanding of the generalization\nbehavior of training the wide neural networks. A novel technical contribution,\nthe analytic functional argument, might be of independent interest.\n', '  Various methods in statistical learning build on kernels considered in\nreproducing kernel Hilbert spaces. In applications, the kernel is often\nselected based on characteristics of the problem and the data. This kernel is\nthen employed to infer response variables at points, where no explanatory data\nwere observed. The data considered here are located in compact sets in higher\ndimensions and the paper addresses approximations of the kernel itself. The new\napproach considers Taylor series approximations of radial kernel functions. For\nthe Gauss kernel on the unit cube, the paper establishes an upper bound of the\nassociated eigenfunctions, which grows only polynomially with respect to the\nindex. The novel approach substantiates smaller regularization parameters than\nconsidered in the literature, overall leading to better approximations. This\nimprovement confirms low rank approximation methods such as the Nystr\\""om\nmethod.\n', '  For the past 30 years or so, machine learning has stimulated a great deal of\nresearch in the study of approximation capabilities (expressive power) of a\nmultitude of processes, such as approximation by shallow or deep neural\nnetworks, radial basis function networks, and a variety of kernel based\nmethods. Motivated by applications such as invariant learning, transfer\nlearning, and synthetic aperture radar imaging, we initiate in this paper a\ngeneral approach to study the approximation capabilities of kernel based\nnetworks using non-symmetric kernels. While singular value decomposition is a\nnatural instinct to study such kernels, we consider a more general approach to\ninclude the use of a family of kernels, such as generalized translation\nnetworks (which include neural networks and translation invariant kernels as\nspecial cases) and rotated zonal function kernels. Naturally, unlike\ntraditional kernel based approximation, we cannot require the kernels to be\npositive definite. In particular, we obtain estimates on the accuracy of\nuniform approximation of functions in a ($L^2$)-Sobolev class by ReLU$^r$\nnetworks when $r$ is not necessarily an integer. Our general results apply to\nthe approximation of functions with small smoothness compared to the dimension\nof the input space.\n']",Kernel Methods for Machine Learning and Statistical Analysis,Kernel Methods for Regression and Approximation
204,"Robust Sparse Regression Methods , Robust Regression Estimation","['lasso', 'lassoglm', 'regularization', 'sparse', 'penalized', 'glmnet', 'predictors', 'robust', 'shrinkage', 'regression'] , ['estimation', 'estimators', 'optimal', 'estimator', 'lasso', 'robust', 'outliers', 'outlier', 'unbiased', 'empirical']","['  This paper introduces a new regularized version of the robust\n$\\tau$-regression estimator for analyzing high-dimensional datasets subject to\ngross contamination in the response variables and covariates (explanatory\nvariables). The resulting estimator, termed adaptive $\\tau$-Lasso, is robust to\noutliers and high-leverage points. It also incorporates an adaptive\n$\\ell_1$-norm penalty term, which enables the selection of relevant variables\nand reduces the bias associated with large true regression coefficients. More\nspecifically, this adaptive $\\ell_1$-norm penalty term assigns a weight to each\nregression coefficient. For a fixed number of predictors $p$, we show that the\nadaptive $\\tau$-Lasso has the oracle property, ensuring both variable-selection\nconsistency and asymptotic normality. Asymptotic normality applies only to the\nentries of the regression vector corresponding to the true support, assuming\nknowledge of the true regression vector support. We characterize its robustness\nby establishing the finite-sample breakdown point and the influence function.\nWe carry out extensive simulations and observe that the class of $\\tau$-Lasso\nestimators exhibits robustness and reliable performance in both contaminated\nand uncontaminated data settings. We also validate our theoretical findings on\nrobustness properties through simulations. In the face of outliers and\nhigh-leverage points, the adaptive $\\tau$-Lasso and $\\tau$-Lasso estimators\nachieve the best performance or close-to-best performance in terms of\nprediction and variable selection accuracy compared to other competing\nregularized estimators for all scenarios considered in this study. Therefore,\nthe adaptive $\\tau$-Lasso and $\\tau$-Lasso estimators provide attractive tools\nfor a variety of sparse linear regression problems, particularly in\nhigh-dimensional settings and when the data is contaminated by outliers and\nhigh-leverage points.\n', '  This paper presents a comprehensive exploration of the theoretical properties\ninherent in the Adaptive Lasso and the Transfer Lasso. The Adaptive Lasso, a\nwell-established method, employs regularization divided by initial estimators\nand is characterized by asymptotic normality and variable selection\nconsistency. In contrast, the recently proposed Transfer Lasso employs\nregularization subtracted by initial estimators with the demonstrated capacity\nto curtail non-asymptotic estimation errors. A pivotal question thus emerges:\nGiven the distinct ways the Adaptive Lasso and the Transfer Lasso employ\ninitial estimators, what benefits or drawbacks does this disparity confer upon\neach method? This paper conducts a theoretical examination of the asymptotic\nproperties of the Transfer Lasso, thereby elucidating its differentiation from\nthe Adaptive Lasso. Informed by the findings of this analysis, we introduce a\nnovel method, one that amalgamates the strengths and compensates for the\nweaknesses of both methods. The paper concludes with validations of our theory\nand comparisons of the methods via simulation experiments.\n', '  The sparse-group lasso performs both variable and group selection, making\nsimultaneous use of the strengths of the lasso and group lasso. It has found\nwidespread use in genetics, a field that regularly involves the analysis of\nhigh-dimensional data, due to its sparse-group penalty, which allows it to\nutilize grouping information. However, the sparse-group lasso can be\ncomputationally more expensive than both the lasso and group lasso, due to the\nadded shrinkage complexity, and its additional hyper-parameter that needs\ntuning. In this paper a novel dual feature reduction method, Dual Feature\nReduction (DFR), is presented that uses strong screening rules for the\nsparse-group lasso and the adaptive sparse-group lasso to reduce their input\nspace before optimization. DFR applies two layers of screening and is based on\nthe dual norms of the sparse-group lasso and adaptive sparse-group lasso.\nThrough synthetic and real numerical studies, it is shown that the proposed\nfeature reduction approach is able to drastically reduce the computational cost\nin many different scenarios.\n'] , ['  We consider the problem of linear regression with self-selection bias in the\nunknown-index setting, as introduced in recent work by Cherapanamjeri,\nDaskalakis, Ilyas, and Zampetakis [STOC 2023]. In this model, one observes $m$\ni.i.d. samples $(\\mathbf{x}_{\\ell},z_{\\ell})_{\\ell=1}^m$ where\n$z_{\\ell}=\\max_{i\\in [k]}\\{\\mathbf{x}_{\\ell}^T\\mathbf{w}_i+\\eta_{i,\\ell}\\}$,\nbut the maximizing index $i_{\\ell}$ is unobserved. Here, the\n$\\mathbf{x}_{\\ell}$ are assumed to be $\\mathcal{N}(0,I_n)$ and the noise\ndistribution $\\mathbf{\\eta}_{\\ell}\\sim \\mathcal{D}$ is centered and independent\nof $\\mathbf{x}_{\\ell}$. We provide a novel and near optimally sample-efficient\n(in terms of $k$) algorithm to recover $\\mathbf{w}_1,\\ldots,\\mathbf{w}_k\\in\n\\mathbb{R}^n$ up to additive $\\ell_2$-error $\\varepsilon$ with polynomial\nsample complexity $\\tilde{O}(n)\\cdot \\mathsf{poly}(k,1/\\varepsilon)$ and\nsignificantly improved time complexity\n$\\mathsf{poly}(n,k,1/\\varepsilon)+O(\\log(k)/\\varepsilon)^{O(k)}$. When\n$k=O(1)$, our algorithm runs in $\\mathsf{poly}(n,1/\\varepsilon)$ time,\ngeneralizing the polynomial guarantee of an explicit moment matching algorithm\nof Cherapanamjeri, et al. for $k=2$ and when it is known that\n$\\mathcal{D}=\\mathcal{N}(0,I_k)$. Our algorithm succeeds under significantly\nrelaxed noise assumptions, and therefore also succeeds in the related setting\nof max-linear regression where the added noise is taken outside the maximum.\nFor this problem, our algorithm is efficient in a much larger range of $k$ than\nthe state-of-the-art due to Ghosh, Pananjady, Guntuboyina, and Ramchandran\n[IEEE Trans. Inf. Theory 2022] for not too small $\\varepsilon$, and leads to\nimproved algorithms for any $\\varepsilon$ by providing a warm start for\nexisting local convergence methods.\n', ""  We study the problem of robust multivariate polynomial regression: let\n$p\\colon\\mathbb{R}^n\\to\\mathbb{R}$ be an unknown $n$-variate polynomial of\ndegree at most $d$ in each variable. We are given as input a set of random\nsamples $(\\mathbf{x}_i,y_i) \\in [-1,1]^n \\times \\mathbb{R}$ that are noisy\nversions of $(\\mathbf{x}_i,p(\\mathbf{x}_i))$. More precisely, each\n$\\mathbf{x}_i$ is sampled independently from some distribution $\\chi$ on\n$[-1,1]^n$, and for each $i$ independently, $y_i$ is arbitrary (i.e., an\noutlier) with probability at most $\\rho < 1/2$, and otherwise satisfies\n$|y_i-p(\\mathbf{x}_i)|\\leq\\sigma$. The goal is to output a polynomial\n$\\hat{p}$, of degree at most $d$ in each variable, within an\n$\\ell_\\infty$-distance of at most $O(\\sigma)$ from $p$.\n  Kane, Karmalkar, and Price [FOCS'17] solved this problem for $n=1$. We\ngeneralize their results to the $n$-variate setting, showing an algorithm that\nachieves a sample complexity of $O_n(d^n\\log d)$, where the hidden constant\ndepends on $n$, if $\\chi$ is the $n$-dimensional Chebyshev distribution. The\nsample complexity is $O_n(d^{2n}\\log d)$, if the samples are drawn from the\nuniform distribution instead. The approximation error is guaranteed to be at\nmost $O(\\sigma)$, and the run-time depends on $\\log(1/\\sigma)$. In the setting\nwhere each $\\mathbf{x}_i$ and $y_i$ are known up to $N$ bits of precision, the\nrun-time's dependence on $N$ is linear. We also show that our sample\ncomplexities are optimal in terms of $d^n$. Furthermore, we show that it is\npossible to have the run-time be independent of $1/\\sigma$, at the cost of a\nhigher sample complexity.\n"", '  We consider the multivariate max-linear regression problem where the model\nparameters\n$\\boldsymbol{\\beta}_{1},\\dotsc,\\boldsymbol{\\beta}_{k}\\in\\mathbb{R}^{p}$ need to\nbe estimated from $n$ independent samples of the (noisy) observations $y =\n\\max_{1\\leq j \\leq k} \\boldsymbol{\\beta}_{j}^{\\mathsf{T}} \\boldsymbol{x} +\n\\mathrm{noise}$. The max-linear model vastly generalizes the conventional\nlinear model, and it can approximate any convex function to an arbitrary\naccuracy when the number of linear models $k$ is large enough. However, the\ninherent nonlinearity of the max-linear model renders the estimation of the\nregression parameters computationally challenging. Particularly, no estimator\nbased on convex programming is known in the literature. We formulate and\nanalyze a scalable convex program given by anchored regression (AR) as the\nestimator for the max-linear regression problem. Under the standard Gaussian\nobservation setting, we present a non-asymptotic performance guarantee showing\nthat the convex program recovers the parameters with high probability. When the\n$k$ linear components are equally likely to achieve the maximum, our result\nshows a sufficient number of noise-free observations for exact recovery scales\nas {$k^{4}p$} up to a logarithmic factor. { This sample complexity coincides\nwith that by alternating minimization (Ghosh et al., {2021}). Moreover, the\nsame sample complexity applies when the observations are corrupted with\narbitrary deterministic noise. We provide empirical results that show that our\nmethod performs as our theoretical result predicts, and is competitive with the\nalternating minimization algorithm particularly in presence of multiplicative\nBernoulli noise. Furthermore, we also show empirically that a recursive\napplication of AR can significantly improve the estimation accuracy.}\n']",Robust Regression Methods,Robust Sparse Regression Methods
205,"Tabular Data Prediction , Symbolic Regression Methods , Prediction-Powered Statistical Inference","['datasets', 'dataset', 'tables', 'tabular', 'table', 'learning', 'tabpfn', 'neural', 'features', 'data'] , ['solvers', 'symbolic', 'regression', 'expressions', 'formulas', 'formulagpt', 'variables', 'neural', 'discovering', 'mathematical'] , ['prediction', 'inferences', 'predict', 'predictions', 'inference', 'predicted', 'estimates', 'statistical', 'labeled', 'confidence']","['  In the domain of data science, the predictive tasks of classification,\nregression, and imputation of missing values are commonly encountered\nchallenges associated with tabular data. This research endeavors to apply Large\nLanguage Models (LLMs) towards addressing these predictive tasks. Despite their\nproficiency in comprehending natural language, LLMs fall short in dealing with\nstructured tabular data. This limitation stems from their lacking exposure to\nthe intricacies of tabular data during their foundational training. Our\nresearch aims to mitigate this gap by compiling a comprehensive corpus of\ntables annotated with instructions and executing large-scale training of\nLlama-2 on this enriched dataset. Furthermore, we investigate the practical\napplication of applying the trained model to zero-shot prediction, few-shot\nprediction, and in-context learning scenarios. Through extensive experiments,\nour methodology has shown significant improvements over existing benchmarks.\nThese advancements highlight the efficacy of tailoring LLM training to solve\ntable-related problems in data science, thereby establishing a new benchmark in\nthe utilization of LLMs for enhancing tabular intelligence.\n', '  Tabular data from different tables exhibit significant diversity due to\nvaried definitions and types of features, as well as complex inter-feature and\nfeature-target relationships. Cross-dataset pretraining, which learns reusable\npatterns from upstream data to support downstream tasks, have shown notable\nsuccess in various fields. Yet, when applied to tabular data prediction, this\nparadigm faces challenges due to the limited reusable patterns among diverse\ntabular datasets (tables) and the general scarcity of tabular data available\nfor fine-tuning. In this study, we fill this gap by introducing a cross-table\npretrained Transformer, XTFormer, for versatile downstream tabular prediction\ntasks. Our methodology insight is pretraining XTFormer to establish a\n""meta-function"" space that encompasses all potential feature-target mappings.\nIn pre-training, a variety of potential mappings are extracted from\npre-training tabular datasets and are embedded into the ""meta-function"" space,\nand suited mappings are extracted from the ""meta-function"" space for downstream\ntasks by a specified coordinate positioning approach. Experiments show that, in\n190 downstream tabular prediction tasks, our cross-table pretrained XTFormer\nwins both XGBoost and Catboost on 137 (72%) tasks, and surpasses representative\ndeep learning models FT-Transformer and the tabular pre-training approach XTab\non 144 (76%) and 162 (85%) tasks.\n', ""  Tabular data is prevalent across various domains in machine learning.\nAlthough Deep Neural Network (DNN)-based methods have shown promising\nperformance comparable to tree-based ones, in-depth evaluation of these methods\nis challenging due to varying performance ranks across diverse datasets. In\nthis paper, we propose a comprehensive benchmark comprising 300 tabular\ndatasets, covering a wide range of task types, size distributions, and domains.\nWe perform an extensive comparison between state-of-the-art deep tabular\nmethods and tree-based methods, revealing the average rank of all methods and\nhighlighting the key factors that influence the success of deep tabular\nmethods. Next, we analyze deep tabular methods based on their training\ndynamics, including changes in validation metrics and other statistics. For\neach dataset-method pair, we learn a mapping from both the meta-features of\ndatasets and the first part of the validation curve to the final validation set\nperformance and even the evolution of validation curves. This mapping extracts\nessential meta-features that influence prediction accuracy, helping the\nanalysis of tabular methods from novel aspects. Based on the performance of all\nmethods on this large benchmark, we identify two subsets of 45 datasets each.\nThe first subset contains datasets that favor either tree-based methods or\nDNN-based methods, serving as effective analysis tools to evaluate strategies\n(e.g., attribute encoding strategies) for improving deep tabular models. The\nsecond subset contains datasets where the ranks of methods are consistent with\nthe overall benchmark, acting as a probe for tabular analysis. These ``tiny\ntabular benchmarks'' will facilitate further studies on tabular data.\n""] , ['  Formulas are the language of communication between humans and nature. It is\nan important research topic of artificial intelligence to find expressions from\nobserved data to reflect the relationship between each variable in the data,\nwhich is called a symbolic regression problem. The existing symbolic regression\nmethods directly generate expressions according to the given observation data,\nand we cannot require the algorithm to generate expressions that meet specific\nrequirements according to the known prior knowledge. For example, the\nexpression needs to contain $\\sin$ or be symmetric, and so on. Even if it can,\nit often requires very complex operations, which is very inconvenient. In this\npaper, based on multi-modal large language models, we propose MLLM-SR, a\nconversational symbolic regression method that can generate expressions that\nmeet the requirements simply by describing the requirements with natural\nlanguage instructions. By experimenting on the Nguyen dataset, we can\ndemonstrate that MLLM-SR leads the state-of-the-art baselines in fitting\nperformance. More notably, we experimentally demonstrate that MLLM-SR can well\nunderstand the prior knowledge we add to the natural language instructions.\nMoreover, the addition of prior knowledge can effectively guide MLLM-SR to\ngenerate correct expressions.\n', '  Symbolic regression (SR) is a powerful technique for discovering the\nanalytical mathematical expression from data, finding various applications in\nnatural sciences due to its good interpretability of results. However, existing\nmethods face scalability issues when dealing with complex equations involving\nmultiple variables. To address this challenge, we propose ScaleSR, a scalable\nsymbolic regression model that leverages control variables to enhance both\naccuracy and scalability. The core idea is to decompose multi-variable symbolic\nregression into a set of single-variable SR problems, which are then combined\nin a bottom-up manner. The proposed method involves a four-step process. First,\nwe learn a data generator from observed data using deep neural networks (DNNs).\nSecond, the data generator is used to generate samples for a certain variable\nby controlling the input variables. Thirdly, single-variable symbolic\nregression is applied to estimate the corresponding mathematical expression.\nLastly, we repeat steps 2 and 3 by gradually adding variables one by one until\ncompletion. We evaluate the performance of our method on multiple benchmark\ndatasets. Experimental results demonstrate that the proposed ScaleSR\nsignificantly outperforms state-of-the-art baselines in discovering\nmathematical expressions with multiple variables. Moreover, it can\nsubstantially reduce the search space for symbolic regression. The source code\nwill be made publicly available upon publication.\n', ""  Mathematical equations have been unreasonably effective in describing complex\nnatural phenomena across various scientific disciplines. However, discovering\nsuch insightful equations from data presents significant challenges due to the\nnecessity of navigating extremely high-dimensional combinatorial and nonlinear\nhypothesis spaces. Traditional methods of equation discovery, commonly known as\nsymbolic regression, largely focus on extracting equations from data alone,\noften neglecting the rich domain-specific prior knowledge that scientists\ntypically depend on. To bridge this gap, we introduce LLM-SR, a novel approach\nthat leverages the extensive scientific knowledge and robust code generation\ncapabilities of Large Language Models (LLMs) to discover scientific equations\nfrom data in an efficient manner. Specifically, LLM-SR treats equations as\nprograms with mathematical operators and combines LLMs' scientific priors with\nevolutionary search over equation programs. The LLM iteratively proposes new\nequation skeleton hypotheses, drawing from its physical understanding, which\nare then optimized against data to estimate skeleton parameters. We demonstrate\nLLM-SR's effectiveness across three diverse scientific domains, where it\ndiscovers physically accurate equations that provide significantly better fits\nto in-domain and out-of-domain data compared to the well-established symbolic\nregression baselines. Incorporating scientific prior knowledge also enables\nLLM-SR to search the equation space more efficiently than baselines. Code is\navailable at: https://github.com/deep-symbolic-mathematics/LLM-SR\n""] , ['  We present PPI++: a computationally lightweight methodology for estimation\nand inference based on a small labeled dataset and a typically much larger\ndataset of machine-learning predictions. The methods automatically adapt to the\nquality of available predictions, yielding easy-to-compute confidence sets --\nfor parameters of any dimensionality -- that always improve on classical\nintervals using only the labeled data. PPI++ builds on prediction-powered\ninference (PPI), which targets the same problem setting, improving its\ncomputational and statistical efficiency. Real and synthetic experiments\ndemonstrate the benefits of the proposed adaptations.\n', ""  Prediction-powered inference (PPI) is a method that improves statistical\nestimates based on limited human-labeled data. Specifically, PPI methods\nprovide tighter confidence intervals by combining small amounts of\nhuman-labeled data with larger amounts of data labeled by a reasonably\naccurate, but potentially biased, automatic system. We propose a framework for\nPPI based on Bayesian inference that allows researchers to develop new\ntask-appropriate PPI methods easily. Exploiting the ease with which we can\ndesign new metrics, we propose improved PPI methods for several importantcases,\nsuch as autoraters that give discrete responses (e.g., prompted LLM ``judges'')\nand autoraters with scores that have a non-linear relationship to human scores.\n"", '  Prediction-powered inference (PPI) is a method that improves statistical\nestimates based on limited human-labeled data. PPI achieves this by combining\nsmall amounts of human-labeled data with larger amounts of data labeled by a\nreasonably accurate -- but potentially biased -- automatic system, in a way\nthat results in tighter confidence intervals for certain parameters of interest\n(e.g., the mean performance of a language model). In this paper, we propose a\nmethod called Stratified Prediction-Powered Inference (StratPPI), in which we\nshow that the basic PPI estimates can be considerably improved by employing\nsimple data stratification strategies. Without making any assumptions on the\nunderlying automatic labeling system or data distribution, we derive an\nalgorithm for computing provably valid confidence intervals for population\nparameters (such as averages) that is based on stratified sampling. In\nparticular, we show both theoretically and empirically that, with appropriate\nchoices of stratification and sample allocation, our approach can provide\nsubstantially tighter confidence intervals than unstratified approaches.\nSpecifically, StratPPI is expected to improve in cases where the performance of\nthe autorater varies across different conditional distributions of the target\ndata.\n']",Machine Learning for Data Analysis and Modeling,Prediction-Powered Statistical Inference
206,"Mixup Data Augmentation Techniques , Data Augmentation Techniques","['augmentation', 'mixup', 'mix', 'learning', 'mixing', 'trained', 'mixda', 'classifier', 'augmix', 'generalization'] , ['augmentation', 'augmenting', 'augmentations', 'auggpt', 'aug', 'improving', 'augmented', 'improve', 'text', 'autoaugment']","['  Mixup and its variants form a popular class of data augmentation\ntechniques.Using a random sample pair, it generates a new sample by linear\ninterpolation of the inputs and labels. However, generating only one single\ninterpolation may limit its augmentation ability. In this paper, we propose a\nsimple yet effective extension called multi-mix, which generates multiple\ninterpolations from a sample pair. With an ordered sequence of generated\nsamples, multi-mix can better guide the training process than standard mixup.\nMoreover, theoretically, this can also reduce the stochastic gradient variance.\nExtensive experiments on a number of synthetic and large-scale data sets\ndemonstrate that multi-mix outperforms various mixup variants and\nnon-mixup-based baselines in terms of generalization, robustness, and\ncalibration.\n', ""  Mixup is a data augmentation strategy that employs convex combinations of\ntraining instances and their respective labels to augment the robustness and\ncalibration of deep neural networks. Despite its widespread adoption, the\nnuanced mechanisms that underpin its success are not entirely understood. The\nobserved phenomenon of Neural Collapse, where the last-layer activations and\nclassifier of deep networks converge to a simplex equiangular tight frame\n(ETF), provides a compelling motivation to explore whether mixup induces\nalternative geometric configurations and whether those could explain its\nsuccess. In this study, we delve into the last-layer activations of training\ndata for deep networks subjected to mixup, aiming to uncover insights into its\noperational efficacy. Our investigation, spanning various architectures and\ndataset pairs, reveals that mixup's last-layer activations predominantly\nconverge to a distinctive configuration different than one might expect. In\nthis configuration, activations from mixed-up examples of identical classes\nalign with the classifier, while those from different classes delineate\nchannels along the decision boundary. Moreover, activations in earlier layers\nexhibit patterns, as if trained with manifold mixup. These findings are\nunexpected, as mixed-up features are not simple convex combinations of feature\nclass means (as one might get, for example, by training mixup with the mean\nsquared error loss). By analyzing this distinctive geometric configuration, we\nelucidate the mechanisms by which mixup enhances model calibration. To further\nvalidate our empirical observations, we conduct a theoretical analysis under\nthe assumption of an unconstrained features model, utilizing the mixup loss.\nThrough this, we characterize and derive the optimal last-layer features under\nthe assumption that the classifier forms a simplex ETF.\n"", '  We study the problem of robust data augmentation for regression tasks in the\npresence of noisy data. Data augmentation is essential for generalizing deep\nlearning models, but most of the techniques like the popular Mixup are\nprimarily designed for classification tasks on image data. Recently, there are\nalso Mixup techniques that are specialized to regression tasks like C-Mixup. In\ncomparison to Mixup, which takes linear interpolations of pairs of samples,\nC-Mixup is more selective in which samples to mix based on their label\ndistances for better regression performance. However, C-Mixup does not\ndistinguish noisy versus clean samples, which can be problematic when mixing\nand lead to suboptimal model performance. At the same time, robust training has\nbeen heavily studied where the goal is to train accurate models against noisy\ndata through multiple rounds of model training. We thus propose our data\naugmentation strategy RC-Mixup, which tightly integrates C-Mixup with\nmulti-round robust training methods for a synergistic effect. In particular,\nC-Mixup improves robust training in identifying clean data, while robust\ntraining provides cleaner data to C-Mixup for it to perform better. A key\nadvantage of RC-Mixup is that it is data-centric where the robust model\ntraining algorithm itself does not need to be modified, but can simply benefit\nfrom data mixing. We show in our experiments that RC-Mixup significantly\noutperforms C-Mixup and robust training baselines on noisy data benchmarks and\ncan be integrated with various robust training methods.\n'] , ['  Data augmentation is one of the regularization strategies for the training of\ndeep learning models, which enhances generalizability and prevents overfitting,\nleading to performance improvement. Although researchers have proposed various\ndata augmentation techniques, they often lack consideration for the difficulty\nof augmented data. Recently, another line of research suggests incorporating\nthe concept of curriculum learning with data augmentation in the field of\nnatural language processing. In this study, we adopt curriculum data\naugmentation for image data augmentation and propose colorful cutout, which\ngradually increases the noise and difficulty introduced in the augmented image.\nOur experimental results highlight the possibility of curriculum data\naugmentation for image data. We publicly released our source code to improve\nthe reproducibility of our study.\n', '  Large models, encompassing large language and diffusion models, have shown\nexceptional promise in approximating human-level intelligence, garnering\nsignificant interest from both academic and industrial spheres. However, the\ntraining of these large models necessitates vast quantities of high-quality\ndata, and with continuous updates to these models, the existing reservoir of\nhigh-quality data may soon be depleted. This challenge has catalyzed a surge in\nresearch focused on data augmentation methods. Leveraging large models, these\ndata augmentation techniques have outperformed traditional approaches. This\npaper offers an exhaustive review of large model-driven data augmentation\nmethods, adopting a comprehensive perspective. We begin by establishing a\nclassification of relevant studies into three main categories: image\naugmentation, text augmentation, and paired data augmentation. Following this,\nwe delve into various data post-processing techniques pertinent to large\nmodel-based data augmentation. Our discussion then expands to encompass the\narray of applications for these data augmentation methods within natural\nlanguage processing, computer vision, and audio signal processing. We proceed\nto evaluate the successes and limitations of large model-based data\naugmentation across different scenarios. Concluding our review, we highlight\nprospective challenges and avenues for future exploration in the field of data\naugmentation. Our objective is to furnish researchers with critical insights,\nultimately contributing to the advancement of more sophisticated large models.\nWe consistently maintain the related open-source materials at:\nhttps://github.com/MLGroup-JLU/LLM-data-aug-survey.\n', '  Augmentation is an effective alternative to utilize the small amount of\nlabeled protein data. However, most of the existing work focuses on design-ing\nnew architectures or pre-training tasks, and relatively little work has studied\ndata augmentation for proteins. This paper extends data augmentation techniques\npreviously used for images and texts to proteins and then benchmarks these\ntechniques on a variety of protein-related tasks, providing the first\ncomprehensive evaluation of protein augmentation. Furthermore, we propose two\nnovel semantic-level protein augmentation methods, namely Integrated Gradients\nSubstitution and Back Translation Substitution, which enable protein\nsemantic-aware augmentation through saliency detection and biological\nknowledge. Finally, we integrate extended and proposed augmentations into an\naugmentation pool and propose a simple but effective framework, namely\nAutomated Protein Augmentation (APA), which can adaptively select the most\nsuitable augmentation combinations for different tasks. Extensive experiments\nhave shown that APA enhances the performance of five protein related tasks by\nan average of 10.55% across three architectures compared to vanilla\nimplementations without augmentation, highlighting its potential to make a\ngreat impact on the field.\n']",Data Augmentation Techniques for Deep Learning,Data Augmentation Techniques
207,"Low-Rank Adaptation for Efficient Fine-Tuning , ""Unsupervised Domain Adaptation Methods"" , Test-Time Adaptation (TTA) Methods","['loras', 'lora', 'tuning', 'rank', 'adapting', 'adaptation', 'lorahub', 'tuned', 'sparse', 'trained'] , ['adversarial', 'adaptation', 'supervised', 'domainnet', 'learn', 'features', 'feature', 'classification', 'training', 'regularization'] , ['adapting', 'adaptation', 'tta', 'test', 'batches', 'batch', 'cta', 'memory', 'drift', 'ctta']","['  The recent trend in scaling language models has led to a growing demand for\nparameter-efficient tuning (PEFT) methods such as LoRA (Low-Rank Adaptation).\nLoRA consistently matches or surpasses the full fine-tuning baseline with fewer\nparameters. However, handling numerous task-specific or user-specific LoRA\nmodules on top of a base model still presents significant storage challenges.\nTo address this, we introduce LoRA-XS (Low-Rank Adaptation with eXtremely Small\nnumber of parameters), a novel approach leveraging Singular Value Decomposition\n(SVD) for parameter-efficient fine-tuning. LoRA-XS introduces a small r x r\nweight matrix between frozen LoRA matrices, which are constructed by SVD of the\noriginal weight matrix. Training only r x r weight matrices ensures\nindependence from model dimensions, enabling more parameter-efficient\nfine-tuning, especially for larger models. LoRA-XS achieves a remarkable\nreduction of trainable parameters by over 100x in 7B models compared to LoRA.\nOur benchmarking across various scales, including GLUE, GSM8k, and MATH\nbenchmarks, shows that our approach outperforms LoRA and recent\nstate-of-the-art approaches like VeRA in terms of parameter efficiency while\nmaintaining competitive performance.\n', '  Low-Rank Adaptation, also known as LoRA, has emerged as a prominent method\nfor parameter-efficient fine-tuning foundation models by re-parameterizing the\noriginal matrix into the product of two low-rank matrices. Despite its\nefficiency, LoRA often yields inferior performance compared to full\nfine-tuning. In this paper, we propose LoRA-Pro to bridge this performance gap.\nFirstly, we delve into the optimization processes in LoRA and full fine-tuning.\nWe reveal that while LoRA employs low-rank approximation, it neglects to\napproximate the optimization process of full fine-tuning. To address this, we\nintroduce a novel concept called the ""equivalent gradient."" This virtual\ngradient makes the optimization process on the re-parameterized matrix\nequivalent to LoRA, which can be used to quantify the differences between LoRA\nand full fine-tuning. The equivalent gradient is derived from the gradients of\nmatrices $A$ and $B$. To narrow the performance gap, our approach minimizes the\ndifferences between the equivalent gradient and the gradient obtained from full\nfine-tuning during the optimization process. By solving this objective, we\nderive optimal closed-form solutions for updating matrices $A$ and $B$. Our\nmethod constrains the optimization process, shrinking the performance gap\nbetween LoRA and full fine-tuning. Extensive experiments on natural language\nprocessing tasks validate the effectiveness of our method.\n', '  Low-Rank Adaptation (LoRA) is currently the most commonly used\nParameter-efficient fine-tuning (PEFT) method, it introduces auxiliary\nparameters for each layer to fine-tune the pre-trained model under limited\ncomputing resources. However, it still faces resource consumption challenges\nduring training when scaling up to larger models. Most previous studies have\ntackled this issue by using pruning techniques, which involve removing LoRA\nparameters deemed unimportant. Nonetheless, these efforts only analyze LoRA\nparameter features to evaluate their importance, such as parameter count, size,\nand gradient. In fact, the output of LoRA (product of LoRA parameter and hidden\nstate), directly impacts the final results. Preliminary experiments indicate\nthat a fraction of LoRA elements possesses significantly high output values,\nsubstantially influencing the layer output. Motivated by the observation, we\npropose LoRA-drop. Concretely, LoRA-drop evaluates the importance of LoRA based\non the LoRA output. Then we retain LoRA for important layers and the other\nlayers share the same LoRA. We conduct abundant experiments with models of\ndifferent scales on NLU and NLG tasks. Results demonstrate that LoRA-drop can\nachieve performance comparable to full fine-tuning and LoRA, while retaining\n50\\% of the LoRA parameters on average.\n'] , ['  Prior Unsupervised Domain Adaptation (UDA) methods often aim to train a\ndomain-invariant feature extractor, which may hinder the model from learning\nsufficiently discriminative features. To tackle this, a line of works based on\nprompt learning leverages the power of large-scale pre-trained vision-language\nmodels to learn both domain-invariant and specific features through a set of\ndomain-agnostic and domain-specific learnable prompts. Those studies typically\nenforce invariant constraints on representation, output, or prompt space to\nlearn such prompts. Differently, we cast UDA as a multiple-objective\noptimization problem in which each objective is represented by a domain loss.\nUnder this new framework, we propose aligning per-objective gradients to foster\nconsensus between them. Additionally, to prevent potential overfitting when\nfine-tuning this deep learning architecture, we penalize the norm of these\ngradients. To achieve these goals, we devise a practical gradient update\nprocedure that can work under both single-source and multi-source UDA.\nEmpirically, our method consistently surpasses other prompt-based baselines by\na large margin on different UDA benchmarks\n', '  Unsupervised domain adaptation (UDA) aims to transfer knowledge from a\nlabeled source domain to an unlabeled target domain. The most recent UDA\nmethods always resort to adversarial training to yield state-of-the-art results\nand a dominant number of existing UDA methods employ convolutional neural\nnetworks (CNNs) as feature extractors to learn domain invariant features.\nVision transformer (ViT) has attracted tremendous attention since its emergence\nand has been widely used in various computer vision tasks, such as image\nclassification, object detection, and semantic segmentation, yet its potential\nin adversarial domain adaptation has never been investigated. In this paper, we\nfill this gap by employing the ViT as the feature extractor in adversarial\ndomain adaptation. Moreover, we empirically demonstrate that ViT can be a\nplug-and-play component in adversarial domain adaptation, which means directly\nreplacing the CNN-based feature extractor in existing UDA methods with the\nViT-based feature extractor can easily obtain performance improvement. The code\nis available at https://github.com/LluckyYH/VT-ADA.\n', ""  Unsupervised domain adaptation (UDA) has achieved remarkable success in fault\ndiagnosis, bringing significant benefits to diverse industrial applications.\nWhile most UDA methods focus on cross-working condition scenarios where the\nsource and target domains are notably similar, real-world applications often\ngrapple with severe domain shifts. We coin the term `distant domain adaptation\nproblem' to describe the challenge of adapting from a labeled source domain to\na significantly disparate unlabeled target domain. This problem exhibits the\nrisk of negative transfer, where extraneous knowledge from the source domain\nadversely affects the target domain performance. Unfortunately, conventional\nUDA methods often falter in mitigating this negative transfer, leading to\nsuboptimal performance. In response to this challenge, we propose a novel\nOnline Selective Adversarial Alignment (OSAA) approach. Central to OSAA is its\nability to dynamically identify and exclude distant source samples via an\nonline gradient masking approach, focusing primarily on source samples that\nclosely resemble the target samples. Furthermore, recognizing the inherent\ncomplexities in bridging the source and target domains, we construct an\nintermediate domain to act as a transitional domain and ease the adaptation\nprocess. Lastly, we develop a class-conditional adversarial adaptation to\naddress the label distribution disparities while learning domain invariant\nrepresentation to account for potential label distribution disparities between\nthe domains. Through detailed experiments and ablation studies on two\nreal-world datasets, we validate the superior performance of the OSAA method\nover state-of-the-art methods, underscoring its significant utility in\npractical scenarios with severe domain shifts.\n""] , ['  Given a model trained on source data, Test-Time Adaptation (TTA) enables\nadaptation and inference in test data streams with domain shifts from the\nsource. Current methods predominantly optimize the model for each incoming test\ndata batch using self-training loss. While these methods yield commendable\nresults in ideal test data streams, where batches are independently and\nidentically sampled from the target distribution, they falter under more\npractical test data streams that are not independent and identically\ndistributed (non-i.i.d.). The data batches in a non-i.i.d. stream display\nprominent label shifts relative to each other. It leads to conflicting\noptimization objectives among batches during the TTA process. Given the\ninherent risks of adapting the source model to unpredictable test-time\ndistributions, we reverse the adaptation process and propose a novel\nDistribution Alignment loss for TTA. This loss guides the distributions of\ntest-time features back towards the source distributions, which ensures\ncompatibility with the well-trained source model and eliminates the pitfalls\nassociated with conflicting optimization objectives. Moreover, we devise a\ndomain shift detection mechanism to extend the success of our proposed TTA\nmethod in the continual domain shift scenarios. Our extensive experiments\nvalidate the logic and efficacy of our method. On six benchmark datasets, we\nsurpass existing methods in non-i.i.d. scenarios and maintain competitive\nperformance under the ideal i.i.d. assumption.\n', '  Test Time Adaptation (TTA) addresses the problem of distribution shift by\nenabling pretrained models to learn new features on an unseen domain at test\ntime. However, it poses a significant challenge to maintain a balance between\nlearning new features and retaining useful pretrained features. In this paper,\nwe propose Layerwise EArly STopping (LEAST) for TTA to address this problem.\nThe key idea is to stop adapting individual layers during TTA if the features\nbeing learned do not appear beneficial for the new domain. For that purpose, we\npropose using a novel gradient-based metric to measure the relevance of the\ncurrent learnt features to the new domain without the need for supervised\nlabels. More specifically, we propose to use this metric to determine\ndynamically when to stop updating each layer during TTA. This enables a more\nbalanced adaptation, restricted to layers benefiting from it, and only for a\ncertain number of steps. Such an approach also has the added effect of limiting\nthe forgetting of pretrained features useful for dealing with new domains.\nThrough extensive experiments, we demonstrate that Layerwise Early Stopping\nimproves the performance of existing TTA approaches across multiple datasets,\ndomain shifts, model architectures, and TTA losses.\n', ""  This paper proposes a novel online evaluation protocol for Test Time\nAdaptation (TTA) methods, which penalizes slower methods by providing them with\nfewer samples for adaptation. TTA methods leverage unlabeled data at test time\nto adapt to distribution shifts. Although many effective methods have been\nproposed, their impressive performance usually comes at the cost of\nsignificantly increased computation budgets. Current evaluation protocols\noverlook the effect of this extra computation cost, affecting their real-world\napplicability. To address this issue, we propose a more realistic evaluation\nprotocol for TTA methods, where data is received in an online fashion from a\nconstant-speed data stream, thereby accounting for the method's adaptation\nspeed. We apply our proposed protocol to benchmark several TTA methods on\nmultiple datasets and scenarios. Extensive experiments show that, when\naccounting for inference speed, simple and fast approaches can outperform more\nsophisticated but slower methods. For example, SHOT from 2020, outperforms the\nstate-of-the-art method SAR from 2023 in this setting. Our results reveal the\nimportance of developing practical TTA methods that are both accurate and\nefficient.\n""]",Efficient Model Adaptation Techniques,Low-Rank Adaptation for Efficient Fine-Tuning
208,"Tensor Recovery and Regression Methods , Optimized Sparse Tensor Compilers for Deep Learning , Tensor Decomposition and Operations","['tensors', 'tensor', 'kronecker', 'factorization', 'tucker', 'multilinear', 'multidimensional', 'decompositions', 'decomposition', 'dimensional'] , ['tensors', 'sparse', 'parallelization', 'tensor', 'gpus', 'gpu', 'cores', 'optimized', 'compilers', 'pytorch'] , ['tensorization', 'tensors', 'tensorflow', 'tensor', 'tensorial', 'tensorkrowch', 'sparse', 'pytorch', 'decomposition', 'decompositions']","['  Recently, numerous tensor singular value decomposition (t-SVD)-based tensor\nrecovery methods have shown promise in processing visual data, such as color\nimages and videos. However, these methods often suffer from severe performance\ndegradation when confronted with tensor data exhibiting non-smooth changes. It\nhas been commonly observed in real-world scenarios but ignored by the\ntraditional t-SVD-based methods. In this work, we introduce a novel tensor\nrecovery model with a learnable tensor nuclear norm to address such a\nchallenge. We develop a new optimization algorithm named the Alternating\nProximal Multiplier Method (APMM) to iteratively solve the proposed tensor\ncompletion model. Theoretical analysis demonstrates the convergence of the\nproposed APMM to the Karush-Kuhn-Tucker (KKT) point of the optimization\nproblem. In addition, we propose a multi-objective tensor recovery framework\nbased on APMM to efficiently explore the correlations of tensor data across its\nvarious dimensions, providing a new perspective on extending the t-SVD-based\nmethod to higher-order tensor cases. Numerical experiments demonstrated the\neffectiveness of the proposed method in tensor completion.\n', '  We proposed the tensor-input tree (TT) method for scalar-on-tensor and\ntensor-on-tensor regression problems. We first address scalar-on-tensor problem\nby proposing scalar-output regression tree models whose input variable are\ntensors (i.e., multi-way arrays). We devised and implemented fast randomized\nand deterministic algorithms for efficient fitting of scalar-on-tensor trees,\nmaking TT competitive against tensor-input GP models. Based on scalar-on-tensor\ntree models, we extend our method to tensor-on-tensor problems using additive\ntree ensemble approaches. Theoretical justification and extensive experiments\non real and synthetic datasets are provided to illustrate the performance of\nTT.\n', ""  This paper studies the prediction task of tensor-on-tensor regression in\nwhich both covariates and responses are multi-dimensional arrays (a.k.a.,\ntensors) across time with arbitrary tensor order and data dimension. Existing\nmethods either focused on linear models without accounting for possibly\nnonlinear relationships between covariates and responses, or directly employed\nblack-box deep learning algorithms that failed to utilize the inherent tensor\nstructure. In this work, we propose a Factor Augmented Tensor-on-Tensor Neural\nNetwork (FATTNN) that integrates tensor factor models into deep neural\nnetworks. We begin with summarizing and extracting useful predictive\ninformation (represented by the ``factor tensor'') from the complex structured\ntensor covariates, and then proceed with the prediction task using the\nestimated factor tensor as input of a temporal convolutional neural network.\nThe proposed methods effectively handle nonlinearity between complex data\nstructures, and improve over traditional statistical models and conventional\ndeep learning approaches in both prediction accuracy and computational cost. By\nleveraging tensor factor models, our proposed methods exploit the underlying\nlatent factor structure to enhance the prediction, and in the meantime,\ndrastically reduce the data dimensionality that speeds up the computation. The\nempirical performances of our proposed methods are demonstrated via simulation\nstudies and real-world applications to three public datasets. Numerical results\nshow that our proposed algorithms achieve substantial increases in prediction\naccuracy and significant reductions in computational time compared to benchmark\nmethods.\n""] , ['  The ongoing trend of hardware specialization has led to a growing use of\ncustom data formats when processing sparse workloads, which are typically\nmemory-bound. These formats facilitate optimized software/hardware\nimplementations by utilizing sparsity pattern- or target-aware data structures\nand layouts to enhance memory access latency and bandwidth utilization.\nHowever, existing sparse tensor programming models and compilers offer little\nor no support for productively customizing the sparse formats. Additionally,\nbecause these frameworks represent formats using a limited set of per-dimension\nattributes, they lack the flexibility to accommodate numerous new variations of\ncustom sparse data structures and layouts. To overcome this deficiency, we\npropose UniSparse, an intermediate language that provides a unified abstraction\nfor representing and customizing sparse formats. Unlike the existing\nattribute-based frameworks, UniSparse decouples the logical representation of\nthe sparse tensor (i.e., the data structure) from its low-level memory layout,\nenabling the customization of both. As a result, a rich set of format\ncustomizations can be succinctly expressed in a small set of well-defined\nquery, mutation, and layout primitives. We also develop a compiler leveraging\nthe MLIR infrastructure, which supports adaptive customization of formats, and\nautomatic code generation of format conversion and compute operations for\nheterogeneous architectures. We demonstrate the efficacy of our approach\nthrough experiments running commonly-used sparse linear algebra operations with\nspecialized formats on multiple different hardware targets, including an Intel\nCPU, an NVIDIA GPU, an AMD Xilinx FPGA, and a simulated processing-in-memory\n(PIM) device.\n', ""  The rapid growth in the size of deep learning models strains the capabilities\nof traditional dense computation paradigms. Leveraging sparse computation has\nbecome increasingly popular for training and deploying large-scale models, but\nexisting deep learning frameworks lack extensive support for sparse operations.\nTo bridge this gap, we introduce Scorch, a library that seamlessly integrates\nefficient sparse tensor computation into the PyTorch ecosystem, with an initial\nfocus on inference workloads on CPUs. Scorch provides a flexible and intuitive\ninterface for sparse tensors, supporting diverse sparse data structures. Scorch\nintroduces a compiler stack that automates key optimizations, including\nautomatic loop ordering, tiling, and format inference. Combined with a runtime\nthat adapts its execution to both dense and sparse data, Scorch delivers\nsubstantial speedups over hand-written PyTorch Sparse (torch.sparse) operations\nwithout sacrificing usability. More importantly, Scorch enables efficient\ncomputation of complex sparse operations that lack hand-optimized PyTorch\nimplementations. This flexibility is crucial for exploring novel sparse\narchitectures. We demonstrate Scorch's ease of use and performance gains on\ndiverse deep learning models across multiple domains. With only minimal code\nchanges, Scorch achieves 1.05-5.78x speedups over PyTorch Sparse on end-to-end\ntasks. Scorch's seamless integration and performance gains make it a valuable\naddition to the PyTorch ecosystem. We believe Scorch will enable wider\nexploration of sparsity as a tool for scaling deep learning and inform the\ndevelopment of other sparse libraries.\n"", '  With the rapid development of deep learning models and hardware support for\ndense computing, the deep learning workload characteristics changed\nsignificantly from a few hot spots on compute-intensive operations to a broad\nrange of operations scattered across the models. Accelerating a few\ncompute-intensive operations using the expert-tuned implementation of\nprimitives does not fully exploit the performance potential of AI hardware.\nVarious efforts have been made to compile a full deep neural network (DNN)\ngraph. One of the biggest challenges is to achieve high-performance tensor\ncompilation by generating expert level performance code for the dense\ncompute-intensive operations and applying compilation optimization at the scope\nof DNN computation graph across multiple compute-intensive operations.\n  We present oneDNN Graph Compiler, a tensor compiler that employs a hybrid\napproach of using techniques from both compiler optimization and expert-tuned\nkernels for high performance code generation of the deep neural network graph.\noneDNN Graph Compiler addresses unique optimization challenges in the deep\nlearning domain, such as low-precision computation, aggressive fusion of graph\noperations, optimization for static tensor shapes and memory layout, constant\nweight optimization, and memory buffer reuse. Experimental results demonstrate\nsignificant performance gains over existing tensor compiler and primitives\nlibrary for performance-critical DNN computation graphs and end-to-end models\non Intel Xeon Scalable Processors.\n'] , ['  Tensor decompositions are invaluable tools in analyzing multimodal datasets.\nIn many real-world scenarios, such datasets are far from being static, to the\ncontrary they tend to grow over time. For instance, in an online social network\nsetting, as we observe new interactions over time, our dataset gets updated in\nits ""time"" mode. How can we maintain a valid and accurate tensor decomposition\nof such a dynamically evolving multimodal dataset, without having to re-compute\nthe entire decomposition after every single update? In this paper we introduce\nSaMbaTen, a Sampling-based Batch Incremental Tensor Decomposition algorithm,\nwhich incrementally maintains the decomposition given new updates to the tensor\ndataset. SaMbaTen is able to scale to datasets that the state-of-the-art in\nincremental tensor decomposition is unable to operate on, due to its ability to\neffectively summarize the existing tensor and the incoming updates, and perform\nall computations in the reduced summary space. We extensively evaluate SaMbaTen\nusing synthetic and real datasets. Indicatively, SaMbaTen achieves comparable\naccuracy to state-of-the-art incremental and non-incremental techniques, while\nbeing 25-30 times faster. Furthermore, SaMbaTen scales to very large sparse and\ndense dynamically evolving tensors of dimensions up to 100K x 100K x 100K where\nstate-of-the-art incremental approaches were not able to operate.\n', ""  Graphical tensor notation is a simple way of denoting linear operations on\ntensors, originating from physics. Modern deep learning consists almost\nentirely of operations on or between tensors, so easily understanding tensor\noperations is quite important for understanding these systems. This is\nespecially true when attempting to reverse-engineer the algorithms learned by a\nneural network in order to understand its behavior: a field known as\nmechanistic interpretability. It's often easy to get confused about which\noperations are happening between tensors and lose sight of the overall\nstructure, but graphical tensor notation makes it easier to parse things at a\nglance and see interesting equivalences. The first half of this document\nintroduces the notation and applies it to some decompositions (SVD, CP, Tucker,\nand tensor network decompositions), while the second half applies it to some\nexisting some foundational approaches for mechanistically understanding\nlanguage models, loosely following ``A Mathematical Framework for Transformer\nCircuits'', then constructing an example ``induction head'' circuit in\ngraphical tensor notation.\n"", '  Sparse tensor operations are gaining attention in emerging applications such\nas social networks, deep learning, diagnosis, crime, and review analysis.\nHowever, a major obstacle for research in sparse tensor operations is the\ndeficiency of a broad-scale sparse tensor dataset. Another challenge in sparse\ntensor operations is examining the sparse tensor features, which are not only\nimportant for revealing its nonzero pattern but also have a significant impact\non determining the best-suited storage format, the decomposition algorithm, and\nthe reordering methods. However, due to the large sizes of real tensors, even\nextracting these features becomes costly without caution. To address these gaps\nin the literature, we have developed a smart sparse tensor generator that\nmimics the substantial features of real sparse tensors. Moreover, we propose\nvarious methods for efficiently extracting an extensive set of features for\nsparse tensors. The effectiveness of our generator is validated through the\nquality of features and the performance of decomposition in the generated\ntensors. Both the sparse tensor feature extractor and the tensor generator are\nopen source with all the artifacts available at\nhttps://github.com/sparcityeu/feaTen and https://github.com/sparcityeu/genTen,\nrespectively.\n']",Tensor Methods and Applications,Tensor Decomposition and Operations
209,"Knowledge Distillation for Model Compression , ""Dataset Distillation Methods""","['models', 'knowledge', 'trained', 'training', 'teacher', 'students', 'distillation', 'teachers', 'distilling', 'model'] , ['distillation', 'distilling', 'datasets', 'dataset', 'distill', 'distilled', 'imagenet', 'data', 'training', 'samples']","[""  Knowledge Distillation (KD) has proven effective for compressing large\nteacher models into smaller student models. While it is well known that student\nmodels can achieve similar accuracies as the teachers, it has also been shown\nthat they nonetheless often do not learn the same function. It is, however,\noften highly desirable that the student's and teacher's functions share similar\nproperties such as basing the prediction on the same input features, as this\nensures that students learn the 'right features' from the teachers. In this\nwork, we explore whether this can be achieved by not only optimizing the\nclassic KD loss but also the similarity of the explanations generated by the\nteacher and the student. Despite the idea being simple and intuitive, we find\nthat our proposed 'explanation-enhanced' KD (e$^2$KD) (1) consistently provides\nlarge gains in terms of accuracy and student-teacher agreement, (2) ensures\nthat the student learns from the teacher to be right for the right reasons and\nto give similar explanations, and (3) is robust with respect to the model\narchitectures, the amount of training data, and even works with 'approximate',\npre-computed explanations.\n"", ""  Knowledge distillation (KD) involves transferring the knowledge from one\nneural network to another, often from a larger, well-trained model (teacher) to\na smaller, more efficient model (student). Traditional KD methods minimize the\nKullback-Leibler (KL) divergence between the probabilistic outputs of the\nteacher and student networks. However, this approach often overlooks crucial\nstructural knowledge embedded within the teacher's network. In this paper, we\nintroduce Invariant Consistency Distillation (ICD), a novel methodology\ndesigned to enhance KD by ensuring that the student model's representations are\nconsistent with those of the teacher. Our approach combines contrastive\nlearning with an explicit invariance penalty, capturing significantly more\ninformation from the teacher's representation of the data. Our results on\nCIFAR-100 demonstrate that ICD outperforms traditional KD techniques and\nsurpasses 13 state-of-the-art methods. In some cases, the student model even\nexceeds the teacher model in terms of accuracy. Furthermore, we successfully\ntransfer our method to other datasets, including Tiny ImageNet and STL-10. The\ncode will be made public soon.\n"", '  Deep cascaded architectures for magnetic resonance imaging (MRI) acceleration\nhave shown remarkable success in providing high-quality reconstruction.\nHowever, as the number of cascades increases, the improvements in\nreconstruction tend to become marginal, indicating possible excess model\ncapacity. Knowledge distillation (KD) is an emerging technique to compress\nthese models, in which a trained deep teacher network is used to distill\nknowledge to a smaller student network such that the student learns to mimic\nthe behavior of the teacher. Most KD methods focus on effectively training the\nstudent with a pre-trained teacher unaware of the student model. We propose\nSFT-KD-Recon, a student-friendly teacher training approach along with the\nstudent as a prior step to KD to make the teacher aware of the structure and\ncapacity of the student and enable aligning the representations of the teacher\nwith the student. In SFT, the teacher is jointly trained with the unfolded\nbranch configurations of the student blocks using three loss terms -\nteacher-reconstruction loss, student-reconstruction loss, and teacher-student\nimitation loss, followed by KD of the student. We perform extensive experiments\nfor MRI acceleration in 4x and 5x under-sampling on the brain and cardiac\ndatasets on five KD methods using the proposed approach as a prior step. We\nconsider the DC-CNN architecture and setup teacher as D5C5 (141765 parameters),\nand student as D3C5 (49285 parameters), denoting a compression of 2.87:1.\nResults show that (i) our approach consistently improves the KD methods with\nimproved reconstruction performance and image quality, and (ii) the student\ndistilled using our approach is competitive with the teacher, with the\nperformance gap reduced from 0.53 dB to 0.03 dB.\n'] , ['  Data $\\textit{quality}$ is a crucial factor in the performance of machine\nlearning models, a principle that dataset distillation methods exploit by\ncompressing training datasets into much smaller counterparts that maintain\nsimilar downstream performance. Understanding how and why data distillation\nmethods work is vital not only for improving these methods but also for\nrevealing fundamental characteristics of ""good"" training data. However, a major\nchallenge in achieving this goal is the observation that distillation\napproaches, which rely on sophisticated but mostly disparate methods to\ngenerate synthetic data, have little in common with each other. In this work,\nwe highlight a largely overlooked aspect common to most of these methods: the\nuse of soft (probabilistic) labels. Through a series of ablation experiments,\nwe study the role of soft labels in depth. Our results reveal that the main\nfactor explaining the performance of state-of-the-art distillation methods is\nnot the specific techniques used to generate synthetic data but rather the use\nof soft labels. Furthermore, we demonstrate that not all soft labels are\ncreated equal; they must contain $\\textit{structured information}$ to be\nbeneficial. We also provide empirical scaling laws that characterize the\neffectiveness of soft labels as a function of images-per-class in the distilled\ndataset and establish an empirical Pareto frontier for data-efficient learning.\nCombined, our findings challenge conventional wisdom in dataset distillation,\nunderscore the importance of soft labels in learning, and suggest new\ndirections for improving distillation methods. Code for all experiments is\navailable at https://github.com/sunnytqin/no-distillation.\n', '  Dataset distillation aims at synthesizing a dataset by a small number of\nartificially generated data items, which, when used as training data, reproduce\nor approximate a machine learning (ML) model as if it were trained on the\nentire original dataset. Consequently, data distillation methods are usually\ntied to a specific ML algorithm. While recent literature deals mainly with\ndistillation of large collections of images in the context of neural network\nmodels, tabular data distillation is much less represented and mainly focused\non a theoretical perspective. The current paper explores the potential of a\nsimple distillation technique previously proposed in the context of\nLess-than-one shot learning. The main goal is to push further the performance\nof prototype-based soft-labels distillation in terms of classification\naccuracy, by integrating optimization steps in the distillation process. The\nanalysis is performed on real-world data sets with various degrees of\nimbalance. Experimental studies trace the capability of the method to distill\nthe data, but also the opportunity to act as an augmentation method, i.e. to\ngenerate new data that is able to increase model accuracy when used in\nconjunction with - as opposed to instead of - the original data.\n', '  Data-efficient learning has garnered significant attention, especially given\nthe current trend of large multi-modal models. Recently, dataset distillation\nhas become an effective approach by synthesizing data samples that are\nessential for network training. However, it remains to be explored which\nsamples are essential for the dataset distillation process itself. In this\nwork, we study the data efficiency and selection for the dataset distillation\ntask. By re-formulating the dynamics of distillation, we provide insight into\nthe inherent redundancy in the real dataset, both theoretically and\nempirically. We propose to use the empirical loss value as a static data\npruning criterion. To further compensate for the variation of the data value in\ntraining, we find the most contributing samples based on their causal effects\non the distillation. The proposed selection strategy can efficiently exploit\nthe training dataset, outperform the previous SOTA distillation algorithms, and\nconsistently enhance the distillation algorithms, even on much larger-scale and\nmore heterogeneous datasets, e.g., full ImageNet-1K and Kinetics-400. We\nbelieve this paradigm will open up new avenues in the dynamics of distillation\nand pave the way for efficient dataset distillation. Our code is available on\nhttps://github.com/silicx/GoldFromOres-BiLP.\n']",Model and Data Distillation Techniques,Knowledge Distillation for Model Compression
210,"Mixture of Experts (MoE) Models and Training , Mixture-of-Experts (MoE) Optimization for Efficient GPU Inference , Mixture of Experts Modeling and Gating Functions","['sparse', 'expert', 'experts', 'moe', 'models', 'specialization', 'sparsely', 'moerging', 'training', 'moes'] , ['gpu', 'memory', 'gpus', 'sparse', 'throughput', 'cpu', 'optimizer', 'hardware', 'speedup', 'efficient'] , ['softmax', 'gaussian', 'experts', 'overparameterization', 'generalization', 'minimax', 'expert', 'overparameterized', 'neural', 'regularization']","['  Mixture-of-Expert (MoE) based large language models (LLMs), such as the\nrecent Mixtral and DeepSeek-MoE, have shown great promise in scaling model size\nwithout suffering from the quadratic growth of training cost of dense\ntransformers. Like dense models, training MoEs requires answering the same\nquestion: given a training budget, what is the optimal allocation on the model\nsize and number of tokens? We study the scaling law of MoE-based LLMs regarding\nthe relations between the model performance, model size, dataset size, and the\nexpert degree. Echoing previous research studying MoE in different contexts, we\nobserve the diminishing return of increasing the number of experts, but this\nseems to suggest we should scale the number of experts until saturation, as the\ntraining cost would remain constant, which is problematic during inference\ntime. We propose to amend the scaling law of MoE by introducing inference\nefficiency as another metric besides the validation loss. We find that MoEs\nwith a few (4/8) experts are the most serving efficient solution under the same\nperformance, but costs 2.5-3.5x more in training. On the other hand, training a\n(16/32) expert MoE much smaller (70-85%) than the loss-optimal solution, but\nwith a larger training dataset is a promising setup under a training budget.\n', ""  Mixture-of-Experts (MoE) represents an ensemble methodology that amalgamates\npredictions from several specialized sub-models (referred to as experts). This\nfusion is accomplished through a router mechanism, dynamically assigning\nweights to each expert's contribution based on the input data. Conventional MoE\nmechanisms select all available experts, incurring substantial computational\ncosts. In contrast, Sparse Mixture-of-Experts (Sparse MoE) selectively engages\nonly a limited number, or even just one expert, significantly reducing\ncomputation overhead while empirically preserving, and sometimes even\nenhancing, performance. Despite its wide-ranging applications and these\nadvantageous characteristics, MoE's theoretical underpinnings have remained\nelusive. In this paper, we embark on an exploration of Sparse MoE's\ngeneralization error concerning various critical factors. Specifically, we\ninvestigate the impact of the number of data samples, the total number of\nexperts, the sparsity in expert selection, the complexity of the routing\nmechanism, and the complexity of individual experts. Our analysis sheds light\non \\textit{how \\textbf{sparsity} contributes to the MoE's generalization},\noffering insights from the perspective of classical learning theory.\n"", '  Mixture-of-experts (MoE) is gaining increasing attention due to its unique\nproperties and remarkable performance, especially for language tasks. By\nsparsely activating a subset of parameters for each token, MoE architecture\ncould increase the model size without sacrificing computational efficiency,\nachieving a better trade-off between performance and training costs. However,\nthe underlying mechanism of MoE still lacks further exploration, and its\nmodularization degree remains questionable. In this paper, we make an initial\nattempt to understand the inner workings of MoE-based large language models.\nConcretely, we comprehensively study the parametric and behavioral features of\nthree recent MoE-based models and reveal some intriguing observations,\nincluding (1) Neurons act like fine-grained experts. (2) The router of MoE\nusually selects experts with larger output norms. (3) The expert diversity\nincreases as the layer increases, while the last layer is an outlier. Based on\nthe observations, we also provide suggestions for a broad spectrum of MoE\npractitioners, such as router design and expert allocation. We hope this work\ncould shed light on future research on the MoE framework and other modular\narchitectures. Code is available at\nhttps://github.com/kamanphoebe/Look-into-MoEs.\n'] , [""  Mixture-of-Experts (MoE) has emerged as a favorable architecture in the era\nof large models due to its inherent advantage, i.e., enlarging model capacity\nwithout incurring notable computational overhead. Yet, the realization of such\nbenefits often results in ineffective GPU memory utilization, as large portions\nof the model parameters remain dormant during inference. Moreover, the memory\ndemands of large models consistently outpace the memory capacity of\ncontemporary GPUs. Addressing this, we introduce SiDA-MoE\n($\\textbf{S}$parsity-$\\textbf{i}$nspired $\\textbf{D}$ata-$\\textbf{A}$ware), an\nefficient inference approach tailored for large MoE models. SiDA-MoE\njudiciously exploits both the system's main memory, which is now abundant and\nreadily scalable, and GPU memory by capitalizing on the inherent sparsity on\nexpert activation in MoE models. By adopting a data-aware perspective, SiDA-MoE\nachieves enhanced model efficiency with a neglectable performance drop.\nSpecifically, SiDA-MoE attains a remarkable speedup in MoE inference with up to\n$3.93\\times$ throughput increasing, up to $72\\%$ latency reduction, and up to\n$80\\%$ GPU memory saving with down to $1\\%$ performance drop. This work paves\nthe way for scalable and efficient deployment of large MoE models, even with\nconstrained resources. Code is available at:\nhttps://github.com/timlee0212/SiDA-MoE.\n"", ""  Large language models (LLMs) based on transformers have made significant\nstrides in recent years, the success of which is driven by scaling up their\nmodel size. Despite their high algorithmic performance, the computational and\nmemory requirements of LLMs present unprecedented challenges. To tackle the\nhigh compute requirements of LLMs, the Mixture-of-Experts (MoE) architecture\nwas introduced which is able to scale its model size without proportionally\nscaling up its computational requirements. Unfortunately, MoE's high memory\ndemands and dynamic activation of sparse experts restrict its applicability to\nreal-world problems. Previous solutions that offload MoE's memory-hungry expert\nparameters to CPU memory fall short because the latency to migrate activated\nexperts from CPU to GPU incurs high performance overhead. Our proposed\nPre-gated MoE system effectively tackles the compute and memory challenges of\nconventional MoE architectures using our algorithm-system co-design. Pre-gated\nMoE employs our novel pre-gating function which alleviates the dynamic nature\nof sparse expert activation, allowing our proposed system to address the large\nmemory footprint of MoEs while also achieving high performance. We demonstrate\nthat Pre-gated MoE is able to improve performance, reduce GPU memory\nconsumption, while also maintaining the same level of model quality. These\nfeatures allow our Pre-gated MoE system to cost-effectively deploy large-scale\nLLMs using just a single GPU with high performance.\n"", ""  This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity\n""] , ['  Top-K sparse softmax gating mixture of experts has been widely used for\nscaling up massive deep-learning architectures without increasing the\ncomputational cost. Despite its popularity in real-world applications, the\ntheoretical understanding of that gating function has remained an open problem.\nThe main challenge comes from the structure of the top-K sparse softmax gating\nfunction, which partitions the input space into multiple regions with distinct\nbehaviors. By focusing on a Gaussian mixture of experts, we establish\ntheoretical results on the effects of the top-K sparse softmax gating function\non both density and parameter estimations. Our results hinge upon defining\nnovel loss functions among parameters to capture different behaviors of the\ninput regions. When the true number of experts $k_{\\ast}$ is known, we\ndemonstrate that the convergence rates of density and parameter estimations are\nboth parametric on the sample size. However, when $k_{\\ast}$ becomes unknown\nand the true model is over-specified by a Gaussian mixture of $k$ experts where\n$k > k_{\\ast}$, our findings suggest that the number of experts selected from\nthe top-K sparse softmax gating function must exceed the total cardinality of a\ncertain number of Voronoi cells associated with the true parameters to\nguarantee the convergence of the density estimation. Moreover, while the\ndensity estimation rate remains parametric under this setting, the parameter\nestimation rates become substantially slow due to an intrinsic interaction\nbetween the softmax gating and expert functions.\n', '  The softmax gating function is arguably the most popular choice in mixture of\nexperts modeling. Despite its widespread use in practice, softmax gating may\nlead to unnecessary competition among experts, potentially causing the\nundesirable phenomenon of representation collapse due to its inherent\nstructure. In response, the sigmoid gating function has been recently proposed\nas an alternative and has been demonstrated empirically to achieve superior\nperformance. However, a rigorous examination of the sigmoid gating function is\nlacking in current literature. In this paper, we verify theoretically that\nsigmoid gating, in fact, enjoys a higher sample efficiency than softmax gating\nfor the statistical task of expert estimation. Towards that goal, we consider a\nregression framework in which the unknown regression function is modeled as a\nmixture of experts, and study the rates of convergence of the least squares\nestimator in the over-specified case in which the number of experts fitted is\nlarger than the true value. We show that two gating regimes naturally arise\nand, in each of them, we formulate identifiability conditions for the expert\nfunctions and derive the corresponding convergence rates. In both cases, we\nfind that experts formulated as feed-forward networks with commonly used\nactivation such as $\\mathrm{ReLU}$ and $\\mathrm{GELU}$ enjoy faster convergence\nrates under sigmoid gating than softmax gating. Furthermore, given the same\nchoice of experts, we demonstrate that the sigmoid gating function requires a\nsmaller sample size than its softmax counterpart to attain the same error of\nexpert estimation and, therefore, is more sample efficient.\n', '  Mixture-of-experts (MoE) model incorporates the power of multiple submodels\nvia gating functions to achieve greater performance in numerous regression and\nclassification applications. From a theoretical perspective, while there have\nbeen previous attempts to comprehend the behavior of that model under the\nregression settings through the convergence analysis of maximum likelihood\nestimation in the Gaussian MoE model, such analysis under the setting of a\nclassification problem has remained missing in the literature. We close this\ngap by establishing the convergence rates of density estimation and parameter\nestimation in the softmax gating multinomial logistic MoE model. Notably, when\npart of the expert parameters vanish, these rates are shown to be slower than\npolynomial rates owing to an inherent interaction between the softmax gating\nand expert functions via partial differential equations. To address this issue,\nwe propose using a novel class of modified softmax gating functions which\ntransform the input before delivering them to the gating functions. As a\nresult, the previous interaction disappears and the parameter estimation rates\nare significantly improved.\n']",Mixture of Experts (MoE) Models and Their Optimizations,Mixture of Experts (MoE) Models and Training
211,"Face Recognition and Biometrics , Optical Character Recognition of Handwritten Text , Open-Set Recognition and Classification","['faces', 'facial', 'face', 'recognition', 'classifiers', 'biometrics', 'biometric', 'generative', 'siamese', 'classifier'] , ['ocr', 'handwriting', 'handwritten', 'recognition', 'text', 'inscriptions', 'locr', 'manuscripts', 'marksheet', 'documents'] , ['classifiers', 'classification', 'classifier', 'classifying', 'misclassify', 'classes', 'recognition', 'softmax', 'openness', 'wiseopen']","['  Computer vision systems have been deployed in various applications involving\nbiometrics like human faces. These systems can identify social media users,\nsearch for missing persons, and verify identity of individuals. While computer\nvision models are often evaluated for accuracy on available benchmarks, more\nannotated data is necessary to learn about their robustness and fairness\nagainst semantic distributional shifts in input data, especially in face data.\nAmong annotated data, counterfactual examples grant strong explainability\ncharacteristics. Because collecting natural face data is prohibitively\nexpensive, we put forth a generative AI-based framework to construct targeted,\ncounterfactual, high-quality synthetic face data. Our synthetic data pipeline\nhas many use cases, including face recognition systems sensitivity evaluations\nand image understanding system probes. The pipeline is validated with multiple\nuser studies. We showcase the efficacy of our face generation pipeline on a\nleading commercial vision model. We identify facial attributes that cause\nvision systems to fail.\n', '  In this work we focus on learning facial representations that can be adapted\nto train effective face recognition models, particularly in the absence of\nlabels. Firstly, compared with existing labelled face datasets, a vastly larger\nmagnitude of unlabeled faces exists in the real world. We explore the learning\nstrategy of these unlabeled facial images through self-supervised pretraining\nto transfer generalized face recognition performance. Moreover, motivated by\none recent finding, that is, the face saliency area is critical for face\nrecognition, in contrast to utilizing random cropped blocks of images for\nconstructing augmentations in pretraining, we utilize patches localized by\nextracted facial landmarks. This enables our method - namely LAndmark-based\nFacial Self-supervised learning LAFS), to learn key representation that is more\ncritical for face recognition. We also incorporate two landmark-specific\naugmentations which introduce more diversity of landmark information to further\nregularize the learning. With learned landmark-based facial representations, we\nfurther adapt the representation for face recognition with regularization\nmitigating variations in landmark positions. Our method achieves significant\nimprovement over the state-of-the-art on multiple face recognition benchmarks,\nespecially on more challenging few-shot scenarios.\n', '  As a significant step for human face modeling, editing, and generation, face\nlandmarking aims at extracting facial keypoints from images. A generalizable\nface landmarker is required in practice because real-world facial images, e.g.,\nthe avatars in animations and games, are often stylized in various ways.\nHowever, achieving generalizable face landmarking is challenging due to the\ndiversity of facial styles and the scarcity of labeled stylized faces. In this\nstudy, we propose a simple but effective paradigm to learn a generalizable face\nlandmarker based on labeled real human faces and unlabeled stylized faces. Our\nmethod learns the face landmarker as the key module of a conditional face\nwarper. Given a pair of real and stylized facial images, the conditional face\nwarper predicts a warping field from the real face to the stylized one, in\nwhich the face landmarker predicts the ending points of the warping field and\nprovides us with high-quality pseudo landmarks for the corresponding stylized\nfacial images. Applying an alternating optimization strategy, we learn the face\nlandmarker to minimize $i)$ the discrepancy between the stylized faces and the\nwarped real ones and $ii)$ the prediction errors of both real and pseudo\nlandmarks. Experiments on various datasets show that our method outperforms\nexisting state-of-the-art domain adaptation methods in face landmarking tasks,\nleading to a face landmarker with better generalizability. Code is available at\nhttps://plustwo0.github.io/project-face-landmarker.\n'] , [""  This project undertakes the training and analysis of optical character\nrecognition OCR methods applied to 10th century ancient Tamil inscriptions\ndiscovered on the walls of the Brihadeeswarar Temple.The chosen OCR methods\ninclude Tesseract,a widely used OCR engine,using modern ICR techniques to pre\nprocess the raw data and a box editing software to finetune our model.The\nanalysis with Tesseract aims to evaluate their effectiveness in accurately\ndeciphering the nuances of the ancient Tamil characters.The performance of our\nmodel for the dataset are determined by their accuracy rate where the evaluated\ndataset divided into training set and testing set.By addressing the unique\nchallenges posed by the script's historical context,this study seeks to\ncontribute valuable insights to the broader field of OCR,facilitating improved\npreservation and interpretation of ancient inscriptions\n"", '  Teaching Computer Science (CS) by having students write programs by hand on\npaper has key pedagogical advantages: It allows focused learning and requires\ncareful thinking compared to the use of Integrated Development Environments\n(IDEs) with intelligent support tools or ""just trying things out"". The familiar\nenvironment of pens and paper also lessens the cognitive load of students with\nno prior experience with computers, for whom the mere basic usage of computers\ncan be intimidating. Finally, this teaching approach opens learning\nopportunities to students with limited access to computers.\n  However, a key obstacle is the current lack of teaching methods and support\nsoftware for working with and running handwritten programs. Optical character\nrecognition (OCR) of handwritten code is challenging: Minor OCR errors, perhaps\ndue to varied handwriting styles, easily make code not run, and recognizing\nindentation is crucial for languages like Python but is difficult to do due to\ninconsistent horizontal spacing in handwriting. Our approach integrates two\ninnovative methods. The first combines OCR with an indentation recognition\nmodule and a language model designed for post-OCR error correction without\nintroducing hallucinations. This method, to our knowledge, surpasses all\nexisting systems in handwritten code recognition. It reduces error from 30\\% in\nthe state of the art to 5\\% with minimal hallucination of logical fixes to\nstudent programs. The second method leverages a multimodal language model to\nrecognize handwritten programs in an end-to-end fashion. We hope this\ncontribution can stimulate further pedagogical research and contribute to the\ngoal of making CS education universally accessible. We release a dataset of\nhandwritten programs and code to support future research at\nhttps://github.com/mdoumbouya/codeocr\n', '  The adoption of tablets with touchscreens and styluses is increasing, and a\nkey feature is converting handwriting to text, enabling search, indexing, and\nAI assistance. Meanwhile, vision-language models (VLMs) are now the go-to\nsolution for image understanding, thanks to both their state-of-the-art\nperformance across a variety of tasks and the simplicity of a unified approach\nto training, fine-tuning, and inference. While VLMs obtain high performance on\nimage-based tasks, they perform poorly on handwriting recognition when applied\nnaively, i.e., by rendering handwriting as an image and performing optical\ncharacter recognition (OCR). In this paper, we study online handwriting\nrecognition with VLMs, going beyond naive OCR. We propose a novel tokenized\nrepresentation of digital ink (online handwriting) that includes both a\ntime-ordered sequence of strokes as text, and as image. We show that this\nrepresentation yields results comparable to or better than state-of-the-art\nonline handwriting recognizers. Wide applicability is shown through results\nwith two different VLM families, on multiple public datasets. Our approach can\nbe applied to off-the-shelf VLMs, does not require any changes in their\narchitecture, and can be used in both fine-tuning and parameter-efficient\ntuning. We perform a detailed ablation study to identify the key elements of\nthe proposed representation.\n'] , ['  Classifying patterns of known classes and rejecting ambiguous and novel (also\ncalled as out-of-distribution (OOD)) inputs are involved in open world pattern\nrecognition. Deep neural network models usually excel in closed-set\nclassification while performs poorly in rejecting OOD inputs. To tackle this\nproblem, numerous methods have been designed to perform open set recognition\n(OSR) or OOD rejection/detection tasks. Previous methods mostly take\npost-training score transformation or hybrid models to ensure low scores on OOD\ninputs while separating known classes. In this paper, we attempt to build a\nunified framework for building open set classifiers for both classification and\nOOD rejection. We formulate the open set recognition of $ K $-known-class as a\n$ (K+1) $-class classification problem with model trained on known-class\nsamples only. By decomposing the $ K $-class problem into $ K $ one-versus-all\n(OVA) binary classification tasks and binding some parameters, we show that\ncombining the scores of OVA classifiers can give $ (K+1) $-class posterior\nprobabilities, which enables classification and OOD rejection in a unified\nframework. To maintain the closed-set classification accuracy of the OVA\ntrained classifier, we propose a hybrid training strategy combining OVA loss\nand multi-class cross-entropy loss. We implement the OVA framework and hybrid\ntraining strategy on the recently proposed convolutional prototype network and\nprototype classifier on vision transformer (ViT) backbone. Experiments on\npopular OSR and OOD detection datasets demonstrate that the proposed framework,\nusing a single multi-class classifier, yields competitive performance in\nclosed-set classification, OOD detection, and misclassification detection.\n', ""  Open-set Semi-supervised Learning (OSSL) holds a realistic setting that\nunlabeled data may come from classes unseen in the labeled set, i.e.,\nout-of-distribution (OOD) data, which could cause performance degradation in\nconventional SSL models. To handle this issue, except for the traditional\nin-distribution (ID) classifier, some existing OSSL approaches employ an extra\nOOD detection module to avoid the potential negative impact of the OOD data.\nNevertheless, these approaches typically employ the entire set of open-set data\nduring their training process, which may contain data unfriendly to the OSSL\ntask that can negatively influence the model performance. This inspires us to\ndevelop a robust open-set data selection strategy for OSSL. Through a\ntheoretical understanding from the perspective of learning theory, we propose\nWise Open-set Semi-supervised Learning (WiseOpen), a generic OSSL framework\nthat selectively leverages the open-set data for training the model. By\napplying a gradient-variance-based selection mechanism, WiseOpen exploits a\nfriendly subset instead of the whole open-set dataset to enhance the model's\ncapability of ID classification. Moreover, to reduce the computational expense,\nwe also propose two practical variants of WiseOpen by adopting low-frequency\nupdate and loss-based selection respectively. Extensive experiments demonstrate\nthe effectiveness of WiseOpen in comparison with the state-of-the-art.\n"", ""  In open-set recognition, existing methods generally learn statically fixed\ndecision boundaries using known classes to reject unknown classes. Though they\nhave achieved promising results, such decision boundaries are evidently\ninsufficient for universal unknown classes in dynamic and open scenarios as\nthey can potentially appear at any position in the feature space. Moreover,\nthese methods just simply reject unknown class samples during testing without\nany effective utilization for them. In fact, such samples completely can\nconstitute the true instantiated representation of the unknown classes to\nfurther enhance the model's performance. To address these issues, this paper\nproposes a novel dynamic against dynamic idea, i.e., dynamic method against\ndynamic changing open-set world, where an open-set self-learning (OSSL)\nframework is correspondingly developed. OSSL starts with a good closed-set\nclassifier trained by known classes and utilizes available test samples for\nmodel adaptation during testing, thus gaining the adaptability to changing data\ndistributions. In particular, a novel self-matching module is designed for\nOSSL, which can achieve the adaptation in automatically identifying known class\nsamples while rejecting unknown class samples which are further utilized to\nenhance the discriminability of the model as the instantiated representation of\nunknown classes. Our method establishes new performance milestones respectively\nin almost all standard and cross-data benchmarks.\n""]",Computer Vision and Pattern Recognition,Open-Set Recognition and Classification
212,"""Visible-Infrared Person Re-identification""","['reidentification', 'matching', 'discriminative', 'pedestrians', 'pedestrian', 'persons', 'feature', 'identification', 'infrared', 'cameras']","['  Unsupervised visible-infrared person re-identification (USL-VI-ReID) aims to\nmatch pedestrian images of the same identity from different modalities without\nannotations. Existing works mainly focus on alleviating the modality gap by\naligning instance-level features of the unlabeled samples. However, the\nrelationships between cross-modality clusters are not well explored. To this\nend, we propose a novel bilateral cluster matching-based learning framework to\nreduce the modality gap by matching cross-modality clusters. Specifically, we\ndesign a Many-to-many Bilateral Cross-Modality Cluster Matching (MBCCM)\nalgorithm through optimizing the maximum matching problem in a bipartite graph.\nThen, the matched pairwise clusters utilize shared visible and infrared\npseudo-labels during the model training. Under such a supervisory signal, a\nModality-Specific and Modality-Agnostic (MSMA) contrastive learning framework\nis proposed to align features jointly at a cluster-level. Meanwhile, the\ncross-modality Consistency Constraint (CC) is proposed to explicitly reduce the\nlarge modality discrepancy. Extensive experiments on the public SYSU-MM01 and\nRegDB datasets demonstrate the effectiveness of the proposed method, surpassing\nstate-of-the-art approaches by a large margin of 8.76% mAP on average.\n', '  The Visible-Infrared Person Re-identification (VI ReID) aims to match visible\nand infrared images of the same pedestrians across non-overlapped camera views.\nThese two input modalities contain both invariant information, such as shape,\nand modality-specific details, such as color. An ideal model should utilize\nvaluable information from both modalities during training for enhanced\nrepresentational capability. However, the gap caused by modality-specific\ninformation poses substantial challenges for the VI ReID model to handle\ndistinct modality inputs simultaneously. To address this, we introduce the\nModality-aware and Instance-aware Visual Prompts (MIP) network in our work,\ndesigned to effectively utilize both invariant and specific information for\nidentification. Specifically, our MIP model is built on the transformer\narchitecture. In this model, we have designed a series of modality-specific\nprompts, which could enable our model to adapt to and make use of the specific\ninformation inherent in different modality inputs, thereby reducing the\ninterference caused by the modality gap and achieving better identification.\nBesides, we also employ each pedestrian feature to construct a group of\ninstance-specific prompts. These customized prompts are responsible for guiding\nour model to adapt to each pedestrian instance dynamically, thereby capturing\nidentity-level discriminative clues for identification. Through extensive\nexperiments on SYSU-MM01 and RegDB datasets, the effectiveness of both our\ndesigned modules is evaluated. Additionally, our proposed MIP performs better\nthan most state-of-the-art methods.\n', '  Unsupervised learning visible-infrared person re-identification (USL-VI-ReID)\naims at learning modality-invariant features from unlabeled cross-modality\ndataset, which is crucial for practical applications in video surveillance\nsystems. The key to essentially address the USL-VI-ReID task is to solve the\ncross-modality data association problem for further heterogeneous joint\nlearning. To address this issue, we propose a Dual Optimal Transport Label\nAssignment (DOTLA) framework to simultaneously assign the generated labels from\none modality to its counterpart modality. The proposed DOTLA mechanism\nformulates a mutual reinforcement and efficient solution to cross-modality data\nassociation, which could effectively reduce the side-effects of some\ninsufficient and noisy label associations. Besides, we further propose a\ncross-modality neighbor consistency guided label refinement and regularization\nmodule, to eliminate the negative effects brought by the inaccurate supervised\nsignals, under the assumption that the prediction or label distribution of each\nexample should be similar to its nearest neighbors. Extensive experimental\nresults on the public SYSU-MM01 and RegDB datasets demonstrate the\neffectiveness of the proposed method, surpassing existing state-of-the-art\napproach by a large margin of 7.76% mAP on average, which even surpasses some\nsupervised VI-ReID methods.\n']",Visible-Infrared Person Re-identification,"""Visible-Infrared Person Re-identification"""
213,"PAC-Bayes Bounds for Learning , Kolmogorov-Arnold Networks (KANs) Analysis , Long-tailed Classification Methods","['bound', 'generalization', 'bounds', 'learning', 'optimal', 'learnability', 'learnable', 'bayes', 'complexity', 'compression'] , ['perceptrons', 'neural', 'networks', 'imagenet1k', 'neurons', 'perceptron', 'kolmogorov', 'learnable', 'convolutional', 'fcnns'] , ['classifier', 'classification', 'imagenet', 'supervised', 'learning', 'classes', 'datasets', 'imbalance', 'dataset', 'imbalanced']","['  We introduce a new PAC-Bayes oracle bound for unbounded losses. This result\ncan be understood as a PAC-Bayesian version of the Cram\\\'er-Chernoff bound. The\nproof technique relies on controlling the tails of certain random variables\ninvolving the Cram\\\'er transform of the loss. We highlight several applications\nof the main theorem. First, we show that our result naturally allows exact\noptimization of the free parameter on many PAC-Bayes bounds. Second, we recover\nand generalize previous results. Finally, we show that our approach allows\nworking with richer assumptions that result in more informative and potentially\ntighter bounds. In this direction, we provide a general bound under a new\n``model-dependent bounded CGF"" assumption from which we obtain bounds based on\nparameter norms and log-Sobolev inequalities. All these bounds can be minimized\nto obtain novel posteriors.\n', '  This paper studies the truncation method from Alquier [1] to derive\nhigh-probability PAC-Bayes bounds for unbounded losses with heavy tails.\nAssuming that the $p$-th moment is bounded, the resulting bounds interpolate\nbetween a slow rate $1 / \\sqrt{n}$ when $p=2$, and a fast rate $1 / n$ when $p\n\\to \\infty$ and the loss is essentially bounded. Moreover, the paper derives a\nhigh-probability PAC-Bayes bound for losses with a bounded variance. This bound\nhas an exponentially better dependence on the confidence parameter and the\ndependency measure than previous bounds in the literature. Finally, the paper\nextends all results to guarantees in expectation and single-draw PAC-Bayes. In\norder to so, it obtains analogues of the PAC-Bayes fast rate bound for bounded\nlosses from [2] in these settings.\n', ""  We propose data-dependent uniform generalization bounds by approaching the\nproblem from a PAC-Bayesian perspective. We first apply the PAC-Bayesian\nframework on `random sets' in a rigorous way, where the training algorithm is\nassumed to output a data-dependent hypothesis set after observing the training\ndata. This approach allows us to prove data-dependent bounds, which can be\napplicable in numerous contexts. To highlight the power of our approach, we\nconsider two main applications. First, we propose a PAC-Bayesian formulation of\nthe recently developed fractal-dimension-based generalization bounds. The\nderived results are shown to be tighter and they unify the existing results\naround one simple proof technique. Second, we prove uniform bounds over the\ntrajectories of continuous Langevin dynamics and stochastic gradient Langevin\ndynamics. These results provide novel information about the generalization\nproperties of noisy algorithms.\n""] , ['  Kolmogorov-Arnold Networks (KANs) offer an efficient and interpretable\nalternative to traditional multi-layer perceptron (MLP) architectures due to\ntheir finite network topology. However, according to the results of Kolmogorov\nand Vitushkin, the representation of generic smooth functions by KAN\nimplementations using analytic functions constrained to a finite number of\ncutoff points cannot be exact. Hence, the convergence of KAN throughout the\ntraining process may be limited. This paper explores the relevance of\nsmoothness in KANs, proposing that smooth, structurally informed KANs can\nachieve equivalence to MLPs in specific function classes. By leveraging\ninherent structural knowledge, KANs may reduce the data required for training\nand mitigate the risk of generating hallucinated predictions, thereby enhancing\nmodel reliability and performance in computational biomedicine.\n', '  Kolmogorov-Arnold Networks (KANs) introduce a paradigm of neural modeling\nthat implements learnable functions on the edges of the networks, diverging\nfrom the traditional node-centric activations in neural networks. This work\nassesses the applicability and efficacy of KANs in visual modeling, focusing on\nthe image recognition task. We mainly analyze the performance and efficiency of\ndifferent network architectures built using KAN concepts along with\nconventional building blocks of convolutional and linear layers, enabling a\ncomparative analysis with the conventional models. Our findings are aimed at\ncontributing to understanding the potential of KANs in computer vision,\nhighlighting both their strengths and areas for further research. Our\nevaluation shows that whereas KAN-based architectures perform in-line with the\noriginal claims of KAN paper for performance and model-complexity in the case\nof simpler vision datasets like MNIST, the advantages seem to diminish even for\nslightly more complex datasets like CIFAR-10.\n', '  Recently, Kolmogorov-Arnold Networks (KANs) have been proposed as an\nalternative to multilayer perceptrons, suggesting advantages in performance and\ninterpretability. We study a typical binary event classification task in\nhigh-energy physics including high-level features and comment on the\nperformance and interpretability of KANs in this context. We find that the\nlearned activation functions of a one-layer KAN resemble the log-likelihood\nratio of the input features. In deeper KANs, the activations in the first KAN\nlayer differ from those in the one-layer KAN, which indicates that the deeper\nKANs learn more complex representations of the data. We study KANs with\ndifferent depths and widths and we compare them to multilayer perceptrons in\nterms of performance and number of trainable parameters. For the chosen\nclassification task, we do not find that KANs are more parameter efficient.\nHowever, small KANs may offer advantages in terms of interpretability that come\nat the cost of only a moderate loss in performance.\n'] , ['  Long-tailed (LT) classification is an unavoidable and challenging problem in\nthe real world. Most existing long-tailed classification methods focus only on\nsolving the class-wise imbalance while ignoring the attribute-wise imbalance.\nThe deviation of a classification model is caused by both class-wise and\nattribute-wise imbalance. Due to the fact that attributes are implicit in most\ndatasets and the combination of attributes is complex, attribute-wise imbalance\nis more difficult to handle. For this purpose, we proposed a novel long-tailed\nclassification framework, aiming to build a multi-granularity classification\nmodel by means of invariant feature learning. This method first unsupervisedly\nconstructs Coarse-Grained forest (CLF) to better characterize the distribution\nof attributes within a class. Depending on the distribution of attributes, one\ncan customize suitable sampling strategies to construct different imbalanced\ndatasets. We then introduce multi-center loss (MCL) that aims to gradually\neliminate confusing attributes during feature learning process. The proposed\nframework does not necessarily couple to a specific LT classification model\nstructure and can be integrated with any existing LT method as an independent\ncomponent. Extensive experiments show that our approach achieves\nstate-of-the-art performance on both existing benchmarks ImageNet-GLT and\nMSCOCO-GLT and can improve the performance of existing LT methods. Our codes\nare available on GitHub: \\url{https://github.com/jinyery/cognisance}\n', '  It is not uncommon that real-world data are distributed with a long tail. For\nsuch data, the learning of deep neural networks becomes challenging because it\nis hard to classify tail classes correctly. In the literature, several existing\nmethods have addressed this problem by reducing classifier bias, provided that\nthe features obtained with long-tailed data are representative enough. However,\nwe find that training directly on long-tailed data leads to uneven embedding\nspace. That is, the embedding space of head classes severely compresses that of\ntail classes, which is not conducive to subsequent classifier learning. This\npaper therefore studies the problem of long-tailed visual recognition from the\nperspective of feature level. We introduce feature augmentation to balance the\nembedding distribution. The features of different classes are perturbed with\nvarying amplitudes in Gaussian form. Based on these perturbed features, two\nnovel logit adjustment methods are proposed to improve model performance at a\nmodest computational overhead. Subsequently, the distorted embedding spaces of\nall classes can be calibrated. In such balanced-distributed embedding spaces,\nthe biased classifier can be eliminated by simply retraining the classifier\nwith class-balanced sampling data. Extensive experiments conducted on benchmark\ndatasets demonstrate the superior performance of the proposed method over the\nstate-of-the-art ones. Source code is available at\nhttps://github.com/Keke921/GCLLoss.\n', '  Long-tailed data is a special type of multi-class imbalanced data with a very\nlarge amount of minority/tail classes that have a very significant combined\ninfluence. Long-tailed learning aims to build high-performance models on\ndatasets with long-tailed distributions, which can identify all the classes\nwith high accuracy, in particular the minority/tail classes. It is a\ncutting-edge research direction that has attracted a remarkable amount of\nresearch effort in the past few years. In this paper, we present a\ncomprehensive survey of latest advances in long-tailed visual learning. We\nfirst propose a new taxonomy for long-tailed learning, which consists of eight\ndifferent dimensions, including data balancing, neural architecture, feature\nenrichment, logits adjustment, loss function, bells and whistles, network\noptimization, and post hoc processing techniques. Based on our proposed\ntaxonomy, we present a systematic review of long-tailed learning methods,\ndiscussing their commonalities and alignable differences. We also analyze the\ndifferences between imbalance learning and long-tailed learning approaches.\nFinally, we discuss prospects and future directions in this field.\n']",Machine Learning Theory and Methods,PAC-Bayes Bounds for Learning
214,"Hyperspectral Image Analysis and Classification , Genomic Analysis and Biodiversity Classification","['hyperspectral', 'multispectral', 'spectral', 'supervised', 'imagery', 'sensing', 'classification', 'denoising', 'hyperview', 'images'] , ['embeddings', 'biodiversity', 'embedding', 'supervised', 'genomes', 'genome', 'taxonomic', 'classification', 'datasets', 'species']","['  Land cover analysis using hyperspectral images (HSI) remains an open problem\ndue to their low spatial resolution and complex spectral information. Recent\nstudies are primarily dedicated to designing Transformer-based architectures\nfor spatial-spectral long-range dependencies modeling, which is computationally\nexpensive with quadratic complexity. Selective structured state space model\n(Mamba), which is efficient for modeling long-range dependencies with linear\ncomplexity, has recently shown promising progress. However, its potential in\nhyperspectral image processing that requires handling numerous spectral bands\nhas not yet been explored. In this paper, we innovatively propose S$^2$Mamba, a\nspatial-spectral state space model for hyperspectral image classification, to\nexcavate spatial-spectral contextual features, resulting in more efficient and\naccurate land cover analysis. In S$^2$Mamba, two selective structured state\nspace models through different dimensions are designed for feature extraction,\none for spatial, and the other for spectral, along with a spatial-spectral\nmixture gate for optimal fusion. More specifically, S$^2$Mamba first captures\nspatial contextual relations by interacting each pixel with its adjacent\nthrough a Patch Cross Scanning module and then explores semantic information\nfrom continuous spectral bands through a Bi-directional Spectral Scanning\nmodule. Considering the distinct expertise of the two attributes in homogenous\nand complicated texture scenes, we realize the Spatial-spectral Mixture Gate by\na group of learnable matrices, allowing for the adaptive incorporation of\nrepresentations learned across different dimensions. Extensive experiments\nconducted on HSI classification benchmarks demonstrate the superiority and\nprospect of S$^2$Mamba. The code will be made available at:\nhttps://github.com/PURE-melo/S2Mamba.\n', '  Hyperspectral images (HSIs) contain rich spectral and spatial information.\nMotivated by the success of transformers in the field of natural language\nprocessing and computer vision where they have shown the ability to learn long\nrange dependencies within input data, recent research has focused on using\ntransformers for HSIs. However, current state-of-the-art hyperspectral\ntransformers only tokenize the input HSI sample along the spectral dimension,\nresulting in the under-utilization of spatial information. Moreover,\ntransformers are known to be data-hungry and their performance relies heavily\non large-scale pretraining, which is challenging due to limited annotated\nhyperspectral data. Therefore, the full potential of HSI transformers has not\nbeen fully realized. To overcome these limitations, we propose a novel\nfactorized spectral-spatial transformer that incorporates factorized\nself-supervised pretraining procedures, leading to significant improvements in\nperformance. The factorization of the inputs allows the spectral and spatial\ntransformers to better capture the interactions within the hyperspectral data\ncubes. Inspired by masked image modeling pretraining, we also devise efficient\nmasking strategies for pretraining each of the spectral and spatial\ntransformers. We conduct experiments on six publicly available datasets for HSI\nclassification task and demonstrate that our model achieves state-of-the-art\nperformance in all the datasets. The code for our model will be made available\nat https://github.com/csiro-robotics/factoformer.\n', '  Contrastive learning has demonstrated great effectiveness in representation\nlearning especially for image classification tasks. However, there is still a\nshortage in the studies targeting regression tasks, and more specifically\napplications on hyperspectral data. In this paper, we propose a contrastive\nlearning framework for the regression tasks for hyperspectral data. To this\nend, we provide a collection of transformations relevant for augmenting\nhyperspectral data, and investigate contrastive learning for regression.\nExperiments on synthetic and real hyperspectral datasets show that the proposed\nframework and transformations significantly improve the performance of\nregression models, achieving better scores than other state-of-the-art\ntransformations.\n'] , ['  This study proposes CGRclust, a novel combination of unsupervised twin\ncontrastive clustering of Chaos Game Representations (CGR) of DNA sequences,\nwith convolutional neural networks (CNNs). To the best of our knowledge,\nCGRclust is the first method to use unsupervised learning for image\nclassification (herein applied to two-dimensional CGR images) for clustering\ndatasets of DNA sequences. CGRclust overcomes the limitations of traditional\nsequence classification methods by leveraging unsupervised twin contrastive\nlearning to detect distinctive sequence patterns, without requiring DNA\nsequence alignment or biological/taxonomic labels. CGRclust accurately\nclustered twenty-five diverse datasets, with sequence lengths ranging from 664\nbp to 100 kbp, including mitochondrial genomes of fish, fungi, and protists, as\nwell as viral whole genome assemblies and synthetic DNA sequences. Compared\nwith three recent clustering methods for DNA sequences (DeLUCS, iDeLUCS, and\nMeShClust v3.0.), CGRclust is the only method that surpasses 81.70% accuracy\nacross all four taxonomic levels tested for mitochondrial DNA genomes of fish.\nMoreover, CGRclust also consistently demonstrates superior performance across\nall the viral genomic datasets. The high clustering accuracy of CGRclust on\nthese twenty-five datasets, which vary significantly in terms of sequence\nlength, number of genomes, number of clusters, and level of taxonomy,\ndemonstrates its robustness, scalability, and versatility.\n', ""  Effective DNA embedding remains crucial in genomic analysis, particularly in\nscenarios lacking labeled data for model fine-tuning, despite the significant\nadvancements in genome foundation models. A prime example is metagenomics\nbinning, a critical process in microbiome research that aims to group DNA\nsequences by their species from a complex mixture of DNA sequences derived from\npotentially thousands of distinct, often uncharacterized species. To fill the\nlack of effective DNA embedding models, we introduce DNABERT-S, a genome\nfoundation model that specializes in creating species-aware DNA embeddings. To\nencourage effective embeddings to error-prone long-read DNA sequences, we\nintroduce Manifold Instance Mixup (MI-Mix), a contrastive objective that mixes\nthe hidden representations of DNA sequences at randomly selected layers and\ntrains the model to recognize and differentiate these mixed proportions at the\noutput layer. We further enhance it with the proposed Curriculum Contrastive\nLearning (C$^2$LR) strategy. Empirical results on 18 diverse datasets showed\nDNABERT-S's remarkable performance. It outperforms the top baseline's\nperformance in 10-shot species classification with just a 2-shot training while\ndoubling the Adjusted Rand Index (ARI) in species clustering and substantially\nincreasing the number of correctly identified species in metagenomics binning.\nThe code, data, and pre-trained model are publicly available at\nhttps://github.com/Zhihan1996/DNABERT_S.\n"", '  Measuring biodiversity is crucial for understanding ecosystem health. While\nprior works have developed machine learning models for the taxonomic\nclassification of photographic images and DNA separately, in this work, we\nintroduce a multimodal approach combining both, using CLIP-style contrastive\nlearning to align images, DNA barcodes, and textual data in a unified embedding\nspace. This allows for accurate classification of both known and unknown insect\nspecies without task-specific fine-tuning, leveraging contrastive learning for\nthe first time to fuse DNA and image data. Our method surpasses previous\nsingle-modality approaches in accuracy by over 11% on zero-shot learning tasks,\nshowcasing its effectiveness in biodiversity studies.\n']",Multimodal Data Analysis for Environmental and Biological Applications,Genomic Analysis and Biodiversity Classification
215,"Approximate Nearest Neighbor Search Algorithms , Support Vector Machines (SVM) Optimization and Variants","['retrieval', 'nearest', 'search', 'locality', 'indexing', 'similarity', 'embeddings', 'hashing', 'algorithms', 'indexes'] , ['svm', 'svms', 'classification', 'kernelized', 'svr', 'vector', 'rademacher', 'outliers', 'norm', 'optimization']","[""  Vector search systems, pivotal in AI applications, often rely on the\nHierarchical Navigable Small Worlds (HNSW) algorithm. However, the behaviour of\nHNSW under real-world scenarios using vectors generated with deep learning\nmodels remains under-explored. Existing Approximate Nearest Neighbours (ANN)\nbenchmarks and research typically has an over-reliance on simplistic datasets\nlike MNIST or SIFT1M and fail to reflect the complexity of current use-cases.\nOur investigation focuses on HNSW's efficacy across a spectrum of datasets,\nincluding synthetic vectors tailored to mimic specific intrinsic\ndimensionalities, widely-used retrieval benchmarks with popular embedding\nmodels, and proprietary e-commerce image data with CLIP models. We survey the\nmost popular HNSW vector databases and collate their default parameters to\nprovide a realistic fixed parameterisation for the duration of the paper.\n  We discover that the recall of approximate HNSW search, in comparison to\nexact K Nearest Neighbours (KNN) search, is linked to the vector space's\nintrinsic dimensionality and significantly influenced by the data insertion\nsequence. Our methodology highlights how insertion order, informed by\nmeasurable properties such as the pointwise Local Intrinsic Dimensionality\n(LID) or known categories, can shift recall by up to 12 percentage points. We\nalso observe that running popular benchmark datasets with HNSW instead of KNN\ncan shift rankings by up to three positions for some models. This work\nunderscores the need for more nuanced benchmarks and design considerations in\ndeveloping robust vector search systems using approximate vector search\nalgorithms. This study presents a number of scenarios with varying real world\napplicability which aim to better increase understanding and future development\nof ANN algorithms and embedding\n"", '  A critical piece of the modern information retrieval puzzle is approximate\nnearest neighbor search. Its objective is to return a set of $k$ data points\nthat are closest to a query point, with its accuracy measured by the proportion\nof exact nearest neighbors captured in the returned set. One popular approach\nto this question is clustering: The indexing algorithm partitions data points\ninto non-overlapping subsets and represents each partition by a point such as\nits centroid. The query processing algorithm first identifies the nearest\nclusters -- a process known as routing -- then performs a nearest neighbor\nsearch over those clusters only. In this work, we make a simple observation:\nThe routing function solves a ranking problem. Its quality can therefore be\nassessed with a ranking metric, making the function amenable to\nlearning-to-rank. Interestingly, ground-truth is often freely available: Given\na query distribution in a top-$k$ configuration, the ground-truth is the set of\nclusters that contain the exact top-$k$ vectors. We develop this insight and\napply it to Maximum Inner Product Search (MIPS). As we demonstrate empirically\non various datasets, learning a simple linear function consistently improves\nthe accuracy of clustering-based MIPS.\n', '  We define and investigate the problem of $\\textit{c-approximate window\nsearch}$: approximate nearest neighbor search where each point in the dataset\nhas a numeric label, and the goal is to find nearest neighbors to queries\nwithin arbitrary label ranges. Many semantic search problems, such as image and\ndocument search with timestamp filters, or product search with cost filters,\nare natural examples of this problem. We propose and theoretically analyze a\nmodular tree-based framework for transforming an index that solves the\ntraditional c-approximate nearest neighbor problem into a data structure that\nsolves window search. On standard nearest neighbor benchmark datasets equipped\nwith random label values, adversarially constructed embeddings, and image\nsearch embeddings with real timestamps, we obtain up to a $75\\times$ speedup\nover existing solutions at the same level of recall.\n'] , ['  Support vector machine (SVM) has achieved many successes in machine learning,\nespecially for a small sample problem. As a famous extension of the traditional\nSVM, the $\\nu$ support vector machine ($\\nu$-SVM) has shown outstanding\nperformance due to its great model interpretability. However, it still faces\nchallenges in training overhead for large-scale problems. To address this\nissue, we propose a safe screening rule with bi-level optimization for\n$\\nu$-SVM (SRBO-$\\nu$-SVM) which can screen out inactive samples before\ntraining and reduce the computational cost without sacrificing the prediction\naccuracy. Our SRBO-$\\nu$-SVM is strictly deduced by integrating the\nKarush-Kuhn-Tucker (KKT) conditions, the variational inequalities of convex\nproblems and the $\\nu$-property. Furthermore, we develop an efficient dual\ncoordinate descent method (DCDM) to further improve computational speed.\nFinally, a unified framework for SRBO is proposed to accelerate many SVM-type\nmodels, and it is successfully applied to one-class SVM. Experimental results\non 6 artificial data sets and 30 benchmark data sets have verified the\neffectiveness and safety of our proposed methods in supervised and unsupervised\ntasks.\n', '  The previous support vector machine(SVM) including $0/1$ loss SVM, hinge loss\nSVM, ramp loss SVM, truncated pinball loss SVM, and others, overlooked the\ndegree of penalty for the correctly classified samples within the margin. This\noversight affects the generalization ability of the SVM classifier to some\nextent. To address this limitation, from the perspective of confidence margin,\nwe propose a novel Slide loss function ($\\ell_s$) to construct the support\nvector machine classifier($\\ell_s$-SVM). By introducing the concept of proximal\nstationary point, and utilizing the property of Lipschitz continuity, we derive\nthe first-order optimality conditions for $\\ell_s$-SVM. Based on this, we\ndefine the $\\ell_s$ support vectors and working set of $\\ell_s$-SVM. To\nefficiently handle $\\ell_s$-SVM, we devise a fast alternating direction method\nof multipliers with the working set ($\\ell_s$-ADMM), and provide the\nconvergence analysis. The numerical experiments on real world datasets confirm\nthe robustness and effectiveness of the proposed method.\n', '  Loss function plays a vital role in supervised learning frameworks. The\nselection of the appropriate loss function holds the potential to have a\nsubstantial impact on the proficiency attained by the acquired model. The\ntraining of supervised learning algorithms inherently adheres to predetermined\nloss functions during the optimization process. In this paper, we present a\nnovel contribution to the realm of supervised machine learning: an asymmetric\nloss function named wave loss. It exhibits robustness against outliers,\ninsensitivity to noise, boundedness, and a crucial smoothness property.\nTheoretically, we establish that the proposed wave loss function manifests the\nessential characteristic of being classification-calibrated. Leveraging this\nbreakthrough, we incorporate the proposed wave loss function into the least\nsquares setting of support vector machines (SVM) and twin support vector\nmachines (TSVM), resulting in two robust and smooth models termed Wave-SVM and\nWave-TSVM, respectively. To address the optimization problem inherent in\nWave-SVM, we utilize the adaptive moment estimation (Adam) algorithm. It is\nnoteworthy that this paper marks the first instance of the Adam algorithm\napplication to solve an SVM model. Further, we devise an iterative algorithm to\nsolve the optimization problems of Wave-TSVM. To empirically showcase the\neffectiveness of the proposed Wave-SVM and Wave-TSVM, we evaluate them on\nbenchmark UCI and KEEL datasets (with and without feature noise) from diverse\ndomains. Moreover, to exemplify the applicability of Wave-SVM in the biomedical\ndomain, we evaluate it on the Alzheimer Disease Neuroimaging Initiative (ADNI)\ndataset. The experimental outcomes unequivocally reveal the prowess of Wave-SVM\nand Wave-TSVM in achieving superior prediction accuracy against the baseline\nmodels.\n']",Efficient Search and Classification Algorithms,Support Vector Machines (SVM) Optimization and Variants
216,Fuzzy Rough Sets and Quantification Models,"['fuzzy', 'sets', 'rough', 'soft', 'vague', 'granular', 'approximations', 'granularly', 'quantification', 'quantifier']","[""  One of the weaknesses of classical (fuzzy) rough sets is their sensitivity to\nnoise, which is particularly undesirable for machine learning applications. One\napproach to solve this issue is by making use of fuzzy quantifiers, as done by\nthe vaguely quantified fuzzy rough set (VQFRS) model. While this idea is\nintuitive, the VQFRS model suffers from both theoretical flaws as well as from\nsuboptimal performance in applications. In this paper, we improve on VQFRS by\nintroducing fuzzy quantifier-based fuzzy rough sets (FQFRS), an intuitive\ngeneralization of fuzzy rough sets that makes use of general unary and binary\nquantification models. We show how several existing models fit in this\ngeneralization as well as how it inspires novel ones. Several binary\nquantification models are proposed to be used with FQFRS. We conduct a\ntheoretical study of their properties, and investigate their potential by\napplying them to classification problems. In particular, we highlight Yager's\nWeighted Implication-based (YWI) binary quantification model, which induces a\nfuzzy rough set model that is both a significant improvement on VQFRS, as well\nas a worthy competitor to the popular ordered weighted averaging based fuzzy\nrough set (OWAFRS) model.\n"", '  Fuzzy rough set theory can be used as a tool for dealing with inconsistent\ndata when there is a gradual notion of indiscernibility between objects. It\ndoes this by providing lower and upper approximations of concepts. In classical\nfuzzy rough sets, the lower and upper approximations are determined using the\nminimum and maximum operators, respectively. This is undesirable for machine\nlearning applications, since it makes these approximations sensitive to\noutlying samples. To mitigate this problem, ordered weighted average (OWA)\nbased fuzzy rough sets were introduced. In this paper, we show how the\nOWA-based approach can be interpreted intuitively in terms of vague\nquantification, and then generalize it to Choquet-based fuzzy rough sets\n(CFRS). This generalization maintains desirable theoretical properties, such as\nduality and monotonicity. Furthermore, it provides more flexibility for machine\nlearning applications. In particular, we show that it enables the seamless\nintegration of outlier detection algorithms, to enhance the robustness of\nmachine learning algorithms based on fuzzy rough sets.\n', '  Rough set theory is a well-known mathematical framework that can deal with\ninconsistent data by providing lower and upper approximations of concepts. A\nprominent property of these approximations is their granular representation:\nthat is, they can be written as unions of simple sets, called granules. The\nlatter can be identified with ""if. . . , then. . . "" rules, which form the\nbackbone of rough set rule induction. It has been shown previously that this\nproperty can be maintained for various fuzzy rough set models, including those\nbased on ordered weighted average (OWA) operators. In this paper, we will focus\non some instances of the general class of fuzzy quantifier-based fuzzy rough\nsets (FQFRS). In these models, the lower and upper approximations are evaluated\nusing binary and unary fuzzy quantifiers, respectively. One of the main targets\nof this study is to examine the granular representation of different models of\nFQFRS. The main findings reveal that Choquet-based fuzzy rough sets can be\nrepresented granularly under the same conditions as OWA-based fuzzy rough sets,\nwhereas Sugeno-based FRS can always be represented granularly. This observation\nhighlights the potential of these models for resolving data inconsistencies and\nmanaging noise.\n']",Fuzzy Rough Sets and Quantification Models for Handling Inconsistent Data,Fuzzy Rough Sets and Quantification Models
217,"Knowledge Graph Completion Methods , Low-Rank Matrix Factorization and Completion","['semantic', 'knowledge', 'relational', 'entities', 'embeddings', 'subgraph', 'entity', 'completion', 'relations', 'kgexplainer'] , ['regularization', 'regularized', 'sparse', 'factorization', 'matrix', 'completion', 'matrices', 'overparameterization', 'minimization', 'rank']","['  Knowledge graph completion (KGC) is a widely used method to tackle\nincompleteness in knowledge graphs (KGs) by making predictions for missing\nlinks. Description-based KGC leverages pre-trained language models to learn\nentity and relation representations with their names or descriptions, which\nshows promising results. However, the performance of description-based KGC is\nstill limited by the quality of text and the incomplete structure, as it lacks\nsufficient entity descriptions and relies solely on relation names, leading to\nsub-optimal results. To address this issue, we propose MPIKGC, a general\nframework to compensate for the deficiency of contextualized knowledge and\nimprove KGC by querying large language models (LLMs) from various perspectives,\nwhich involves leveraging the reasoning, explanation, and summarization\ncapabilities of LLMs to expand entity descriptions, understand relations, and\nextract structures, respectively. We conducted extensive evaluation of the\neffectiveness and improvement of our framework based on four description-based\nKGC models and four datasets, for both link prediction and triplet\nclassification tasks.\n', '  Knowledge graphs (KGs) consist of links that describe relationships between\nentities. Due to the difficulty of manually enumerating all relationships\nbetween entities, automatically completing them is essential for KGs. Knowledge\nGraph Completion (KGC) is a task that infers unseen relationships between\nentities in a KG. Traditional embedding-based KGC methods, such as RESCAL,\nTransE, DistMult, ComplEx, RotatE, HAKE, HousE, etc., infer missing links using\nonly the knowledge from training data. In contrast, the recent Pre-trained\nLanguage Model (PLM)-based KGC utilizes knowledge obtained during pre-training.\nTherefore, PLM-based KGC can estimate missing links between entities by reusing\nmemorized knowledge from pre-training without inference. This approach is\nproblematic because building KGC models aims to infer unseen links between\nentities. However, conventional evaluations in KGC do not consider inference\nand memorization abilities separately. Thus, a PLM-based KGC method, which\nachieves high performance in current KGC evaluations, may be ineffective in\npractical applications. To address this issue, we analyze whether PLM-based KGC\nmethods make inferences or merely access memorized knowledge. For this purpose,\nwe propose a method for constructing synthetic datasets specified in this\nanalysis and conclude that PLMs acquire the inference abilities required for\nKGC through pre-training, even though the performance improvements mostly come\nfrom textual information of entities and relations.\n', '  Inductive knowledge graph completion (KGC) aims to infer the missing relation\nfor a set of newly-coming entities that never appeared in the training set.\nSuch a setting is more in line with reality, as real-world KGs are constantly\nevolving and introducing new knowledge. Recent studies have shown promising\nresults using message passing over subgraphs to embed newly-coming entities for\ninductive KGC. However, the inductive capability of these methods is usually\nlimited by two key issues. (i) KGC always suffers from data sparsity, and the\nsituation is even exacerbated in inductive KGC where new entities often have\nfew or no connections to the original KG. (ii) Cold-start problem. It is over\ncoarse-grained for accurate KG reasoning to generate representations for new\nentities by gathering the local information from few neighbors. To this end, we\npropose a novel iNfOmax RelAtion Network, namely NORAN, for inductive KG\ncompletion. It aims to mine latent relation patterns for inductive KG\ncompletion. Specifically, by centering on relations, NORAN provides a hyper\nview towards KG modeling, where the correlations between relations can be\nnaturally captured as entity-independent logical evidence to conduct inductive\nKGC. Extensive experiment results on five benchmarks show that our framework\nsubstantially outperforms the state-of-the-art KGC methods.\n'] , ['  When applying nonnegative matrix factorization (NMF), generally the rank\nparameter is unknown. Such rank in NMF, called the nonnegative rank, is usually\nestimated heuristically since computing the exact value of it is NP-hard. In\nthis work, we propose an approximation method to estimate such rank while\nsolving NMF on-the-fly. We use sum-of-norm (SON), a group-lasso structure that\nencourages pairwise similarity, to reduce the rank of a factor matrix where the\nrank is overestimated at the beginning. On various datasets, SON-NMF is able to\nreveal the correct nonnegative rank of the data without any prior knowledge nor\ntuning.\n  SON-NMF is a nonconvx nonsmmoth non-separable non-proximable problem, solving\nit is nontrivial. First, as rank estimation in NMF is NP-hard, the proposed\napproach does not enjoy a lower computational complexity. Using a\ngraph-theoretic argument, we prove that the complexity of the SON-NMF is almost\nirreducible. Second, the per-iteration cost of any algorithm solving SON-NMF is\npossibly high, which motivated us to propose a first-order BCD algorithm to\napproximately solve SON-NMF with a low per-iteration cost, in which we do so by\nthe proximal average operator. Lastly, we propose a simple greedy method for\npost-processing.\n  SON-NMF exhibits favourable features for applications. Beside the ability to\nautomatically estimate the rank from data, SON-NMF can deal with rank-deficient\ndata matrix, can detect weak component with small energy. Furthermore, on the\napplication of hyperspectral imaging, SON-NMF handle the issue of spectral\nvariability naturally.\n', '  This paper considers the problem of estimating a low-rank matrix from the\nobservation of all or a subset of its entries in the presence of Poisson noise.\nWhen we observe all entries, this is a problem of matrix denoising; when we\nobserve only a subset of the entries, this is a problem of matrix completion.\nIn both cases, we exploit an assumption that the underlying matrix is low-rank.\nSpecifically, we analyze several estimators, including a constrained\nnuclear-norm minimization program, nuclear-norm regularized least squares, and\na nonconvex constrained low-rank optimization problem. We show that for all\nthree estimators, with high probability, we have an upper error bound (in the\nFrobenius norm error metric) that depends on the matrix rank, the fraction of\nthe elements observed, and maximal row and column sums of the true matrix. We\nfurthermore show that the above results are minimax optimal (within a universal\nconstant) in classes of matrices with low rank and bounded row and column sums.\nWe also extend these results to handle the case of matrix multinomial denoising\nand completion.\n', '  In this paper, we develop a relative error bound for nuclear norm regularized\nmatrix completion, with the focus on the completion of full-rank matrices.\nUnder the assumption that the top eigenspaces of the target matrix are\nincoherent, we derive a relative upper bound for recovering the best low-rank\napproximation of the unknown matrix. Although multiple works have been devoted\nto analyzing the recovery error of full-rank matrix completion, their error\nbounds are usually additive, making it impossible to obtain the perfect\nrecovery case and more generally difficult to leverage the skewed distribution\nof eigenvalues. Our analysis is built upon the optimality condition of the\nregularized formulation and existing guarantees for low-rank matrix completion.\nTo the best of our knowledge, this is the first relative bound that has been\nproved for the regularized formulation of matrix completion.\n']",Knowledge Representation and Matrix Completion Methods,Low-Rank Matrix Factorization and Completion
218,"t-SNE Visualization and Dimensionality Reduction , Dimensionality Reduction Techniques , Principal Component Analysis Algorithms , ""Unsupervised Feature Selection and Dimensionality Reduction""","['embeddings', 'embedding', 'visualizing', 'visualizations', 'visualization', 'visualize', 'visualisation', 'visualise', 'dimensionality', 'sne'] , ['dimensionality', 'dimensional', 'multidimensional', 'dimension', 'manifolds', 'manifold', 'clustering', 'eigenmaps', 'embedding', 'embeddings'] , ['pca', 'eigenvector', 'eigenspace', 'eigen', 'cpca', 'sparse', 'algorithms', 'spectral', 'matrix', 'eigenvalue'] , ['factorization', 'dimensionality', 'supervised', 'autoencoders', 'regularization', 'features', 'discriminative', 'dimensional', 'matrix', 'feature']","['  This paper presents a new insight into improving the performance of\nStochastic Neighbour Embedding (t-SNE) by using Isolation kernel instead of\nGaussian kernel. Isolation kernel outperforms Gaussian kernel in two aspects.\nFirst, the use of Isolation kernel in t-SNE overcomes the drawback of\nmisrepresenting some structures in the data, which often occurs when Gaussian\nkernel is applied in t-SNE. This is because Gaussian kernel determines each\nlocal bandwidth based on one local point only, while Isolation kernel is\nderived directly from the data based on space partitioning. Second, the use of\nIsolation kernel yields a more efficient similarity computation because\ndata-dependent Isolation kernel has only one parameter that needs to be tuned.\nIn contrast, the use of data-independent Gaussian kernel increases the\ncomputational cost by determining n bandwidths for a dataset of n points. As\nthe root cause of these deficiencies in t-SNE is Gaussian kernel, we show that\nsimply replacing Gaussian kernel with Isolation kernel in t-SNE significantly\nimproves the quality of the final visualisation output (without creating\nmisrepresented structures) and removes one key obstacle that prevents t-SNE\nfrom processing large datasets. Moreover, Isolation kernel enables t-SNE to\ndeal with large-scale datasets in less runtime without trading off accuracy,\nunlike existing methods in speeding up t-SNE.\n', '  We present S+t-SNE, an adaptation of the t-SNE algorithm designed to handle\ninfinite data streams. The core idea behind S+t-SNE is to update the t-SNE\nembedding incrementally as new data arrives, ensuring scalability and\nadaptability to handle streaming scenarios. By selecting the most important\npoints at each step, the algorithm ensures scalability while keeping\ninformative visualisations. Employing a blind method for drift management\nadjusts the embedding space, facilitating continuous visualisation of evolving\ndata dynamics. Our experimental evaluations demonstrate the effectiveness and\nefficiency of S+t-SNE. The results highlight its ability to capture patterns in\na streaming scenario. We hope our approach offers researchers and practitioners\na real-time tool for understanding and interpreting high-dimensional data.\n', ""  t-Distributed Stochastic Neighbor Embedding (t-SNE) for the visualization of\nmultidimensional data has proven to be a popular approach, with successful\napplications in a wide range of domains. Despite their usefulness, t-SNE\nprojections can be hard to interpret or even misleading, which hurts the\ntrustworthiness of the results. Understanding the details of t-SNE itself and\nthe reasons behind specific patterns in its output may be a daunting task,\nespecially for non-experts in dimensionality reduction. In this work, we\npresent t-viSNE, an interactive tool for the visual exploration of t-SNE\nprojections that enables analysts to inspect different aspects of their\naccuracy and meaning, such as the effects of hyper-parameters, distance and\nneighborhood preservation, densities and costs of specific neighborhoods, and\nthe correlations between dimensions and visual patterns. We propose a coherent,\naccessible, and well-integrated collection of different views for the\nvisualization of t-SNE projections. The applicability and usability of t-viSNE\nare demonstrated through hypothetical usage scenarios with real data sets.\nFinally, we present the results of a user study where the tool's effectiveness\nwas evaluated. By bringing to light information that would normally be lost\nafter running t-SNE, we hope to support analysts in using t-SNE and making its\nresults better understandable.\n""] , ['  Dimensionality reduction techniques map values from a high dimensional space\nto one with a lower dimension. The result is a space which requires less\nphysical memory and has a faster distance calculation. These techniques are\nwidely used where required properties of the reduced-dimension space give an\nacceptable accuracy with respect to the original space. Many such transforms\nhave been described. They have been classified in two main groups: linear and\ntopological. Linear methods such as Principal Component Analysis (PCA) and\nRandom Projection (RP) define matrix-based transforms into a lower dimension of\nEuclidean space. Topological methods such as Multidimensional Scaling (MDS)\nattempt to preserve higher-level aspects such as the nearest-neighbour\nrelation, and some may be applied to non-Euclidean spaces. Here, we introduce\nnSimplex Zen, a novel topological method of reducing dimensionality. Like MDS,\nit relies only upon pairwise distances measured in the original space. The use\nof distances, rather than coordinates, allows the technique to be applied to\nboth Euclidean and other Hilbert spaces, including those governed by Cosine,\nJensen-Shannon and Quadratic Form distances. We show that in almost all cases,\ndue to geometric properties of high-dimensional spaces, our new technique gives\nbetter properties than others, especially with reduction to very low\ndimensions.\n', '  The characteristics of data like distribution and heterogeneity, become more\ncomplex and counterintuitive as the dimensionality increases. This phenomenon\nis known as curse of dimensionality, where common patterns and relationships\n(e.g., internal and boundary pattern) that hold in low-dimensional space may be\ninvalid in higher-dimensional space. It leads to a decreasing performance for\nthe regression, classification or clustering models or algorithms. Curse of\ndimensionality can be attributed to many causes. In this paper, we first\nsummarize five challenges associated with manipulating high-dimensional data,\nand explains the potential causes for the failure of regression, classification\nor clustering tasks. Subsequently, we delve into two major causes of the curse\nof dimensionality, distance concentration and manifold effect, by performing\ntheoretical and empirical analyses. The results demonstrate that nearest\nneighbor search (NNS) using three typical distance measurements, Minkowski\ndistance, Chebyshev distance, and cosine distance, becomes meaningless as the\ndimensionality increases. Meanwhile, the data incorporates more redundant\nfeatures, and the variance contribution of principal component analysis (PCA)\nis skewed towards a few dimensions. By interpreting the causes of the curse of\ndimensionality, we can better understand the limitations of current models and\nalgorithms, and drive to improve the performance of data analysis and machine\nlearning tasks in high-dimensional space.\n', ""  Dimensionality reduction methods are employed to decrease data\ndimensionality, either to enhance machine learning performance or to facilitate\ndata visualization in two or three-dimensional spaces. These methods typically\nfall into two categories: feature selection and feature transformation. Feature\nselection retains significant features, while feature transformation projects\ndata into a lower-dimensional space, with linear and nonlinear methods. While\nnonlinear methods excel in preserving local structures and capturing nonlinear\nrelationships, they may struggle with interpreting global structures and can be\ncomputationally intensive. Recent algorithms, such as the t-SNE, UMAP, TriMap,\nand PaCMAP prioritize preserving local structures, often at the expense of\naccurately representing global structures, leading to clusters being spread out\nmore in lower-dimensional spaces. Moreover, these methods heavily rely on\nhyperparameters, making their results sensitive to parameter settings. To\naddress these limitations, this study introduces a clustering-based approach,\nnamely CBMAP (Clustering-Based Manifold Approximation and Projection), for\ndimensionality reduction. CBMAP aims to preserve both global and local\nstructures, ensuring that clusters in lower-dimensional spaces closely resemble\nthose in high-dimensional spaces. Experimental evaluations on benchmark\ndatasets demonstrate CBMAP's efficacy, offering speed, scalability, and minimal\nreliance on hyperparameters. Importantly, CBMAP enables low-dimensional\nprojection of test data, addressing a critical need in machine learning\napplications. CBMAP is made freely available at\nhttps://github.com/doganlab/cbmap and can be installed from the Python Package\nDirectory (PyPI) software repository with the command pip install cbmap.\n""] , [""  In this paper we analyze the behavior of the Oja's algorithm for\nonline/streaming principal component subspace estimation. It is proved that\nwith high probability it performs an efficient, gap-free, global convergence\nrate to approximate an principal component subspace for any sub-Gaussian\ndistribution. Moreover, it is the first time to show that the convergence rate,\nnamely the upper bound of the approximation, exactly matches the lower bound of\nan approximation obtained by the offline/classical PCA up to a constant factor.\n"", '  The $k$-principal component analysis ($k$-PCA) problem is a fundamental\nalgorithmic primitive that is widely-used in data analysis and dimensionality\nreduction applications. In statistical settings, the goal of $k$-PCA is to\nidentify a top eigenspace of the covariance matrix of a distribution, which we\nonly have black-box access to via samples. Motivated by these settings, we\nanalyze black-box deflation methods as a framework for designing $k$-PCA\nalgorithms, where we model access to the unknown target matrix via a black-box\n$1$-PCA oracle which returns an approximate top eigenvector, under two popular\nnotions of approximation. Despite being arguably the most natural\nreduction-based approach to $k$-PCA algorithm design, such black-box methods,\nwhich recursively call a $1$-PCA oracle $k$ times, were previously\npoorly-understood.\n  Our main contribution is significantly sharper bounds on the approximation\nparameter degradation of deflation methods for $k$-PCA. For a quadratic form\nnotion of approximation we term ePCA (energy PCA), we show deflation methods\nsuffer no parameter loss. For an alternative well-studied approximation notion\nwe term cPCA (correlation PCA), we tightly characterize the parameter regimes\nwhere deflation methods are feasible. Moreover, we show that in all feasible\nregimes, $k$-cPCA deflation algorithms suffer no asymptotic parameter loss for\nany constant $k$. We apply our framework to obtain state-of-the-art $k$-PCA\nalgorithms robust to dataset contamination, improving prior work in sample\ncomplexity by a $\\mathsf{poly}(k)$ factor.\n', ""  Oja's algorithm for streaming Principal Component Analysis (PCA) for $n$\ndatapoints in a $d$ dimensional space achieves the same sin-squared error\n$O(r_\\mathsf{eff}/n)$ as the offline algorithm in $O(d)$ space and $O(nd)$ time\nand a single pass through the datapoints. Here $r_\\mathsf{eff}$ is the\neffective rank (ratio of the trace and the principal eigenvalue of the\npopulation covariance matrix $\\Sigma$). Under this computational budget, we\nconsider the problem of sparse PCA, where the principal eigenvector of $\\Sigma$\nis $s$-sparse, and $r_\\mathsf{eff}$ can be large. In this setting, to our\nknowledge, \\textit{there are no known single-pass algorithms} that achieve the\nminimax error bound in $O(d)$ space and $O(nd)$ time without either requiring\nstrong initialization conditions or assuming further structure (e.g., spiked)\nof the covariance matrix. We show that a simple single-pass procedure that\nthresholds the output of Oja's algorithm (the Oja vector) can achieve the\nminimax error bound under some regularity conditions in $O(d)$ space and\n$O(nd)$ time as long as $r_\\mathsf{eff}=O(n/\\log n)$. We present a nontrivial\nand novel analysis of the entries of the unnormalized Oja vector, which\ninvolves the projection of a product of independent random matrices on a random\ninitial vector. This is completely different from previous analyses of Oja's\nalgorithm and matrix products, which have been done when the $r_\\mathsf{eff}$\nis bounded.\n""] , ['  By removing irrelevant and redundant features, feature selection aims to find\na good representation of the original features. With the prevalence of\nunlabeled data, unsupervised feature selection has been proven effective in\nalleviating the so-called curse of dimensionality. Most existing matrix\nfactorization-based unsupervised feature selection methods are built upon\nsubspace learning, but they have limitations in capturing nonlinear structural\ninformation among features. It is well-known that kernel techniques can capture\nnonlinear structural information. In this paper, we construct a model by\nintegrating kernel functions and kernel alignment, which can be equivalently\ncharacterized as a matrix factorization problem. However, such an extension\nraises another issue: the algorithm performance heavily depends on the choice\nof kernel, which is often unknown a priori. Therefore, we further propose a\nmultiple kernel-based learning method. By doing so, our model can learn both\nlinear and nonlinear similarity information and automatically generate the most\nappropriate kernel. Experimental analysis on real-world data demonstrates that\nthe two proposed methods outperform other classic and state-of-the-art\nunsupervised feature selection methods in terms of clustering results and\nredundancy reduction in almost all datasets tested.\n', '  Unlike typical visual scene recognition domains, in which massive datasets\nare accessible to deep neural networks, medical image interpretations are often\nobstructed by the paucity of data. In this paper, we investigate the\neffectiveness of data-based few-shot learning in medical imaging by exploring\ndifferent data attribute representations in a low-dimensional space. We\nintroduce different types of non-negative matrix factorization (NMF) in\nfew-shot learning, addressing the data scarcity issue in medical image\nclassification. Extensive empirical studies are conducted in terms of\nvalidating the effectiveness of NMF, especially its supervised variants (e.g.,\ndiscriminative NMF, and supervised and constrained NMF with sparseness), and\nthe comparison with principal component analysis (PCA), i.e., the collaborative\nrepresentation-based dimensionality reduction technique derived from\neigenvectors. With 14 different datasets covering 11 distinct illness\ncategories, thorough experimental results and comparison with related\ntechniques demonstrate that NMF is a competitive alternative to PCA for\nfew-shot learning in medical imaging, and the supervised NMF algorithms are\nmore discriminative in the subspace with greater effectiveness. Furthermore, we\nshow that the part-based representation of NMF, especially its supervised\nvariants, is dramatically impactful in detecting lesion areas in medical\nimaging with limited samples.\n', '  Dimensionality Reduction plays a pivotal role in improving feature learning\naccuracy and reducing training time by eliminating redundant features, noise,\nand irrelevant data. Nonnegative Matrix Factorization (NMF) has emerged as a\npopular and powerful method for dimensionality reduction. Despite its extensive\nuse, there remains a need for a comprehensive analysis of NMF in the context of\ndimensionality reduction. To address this gap, this paper presents a\ncomprehensive survey of NMF, focusing on its applications in both feature\nextraction and feature selection. We introduce a classification of\ndimensionality reduction, enhancing understanding of the underlying concepts.\nSubsequently, we delve into a thorough summary of diverse NMF approaches used\nfor feature extraction and selection. Furthermore, we discuss the latest\nresearch trends and potential future directions of NMF in dimensionality\nreduction, aiming to highlight areas that need further exploration and\ndevelopment.\n']",Dimensionality Reduction and Data Visualization Techniques,Dimensionality Reduction Techniques
219,"Chart Understanding and Reasoning , ""Data Visualization Tools and Techniques""","['chartmimic', 'charts', 'visualizations', 'visual', 'chartformer', 'chart', 'chartcheck', 'visualization', 'chartqa', 'flowcharts'] , ['visualizations', 'visualization', 'charts', 'visual', 'interactive', 'colormaps', 'analytics', 'colormap', 'tools', 'bioinformatics']","[""  Recent studies customizing Multimodal Large Language Models (MLLMs) for\ndomain-specific tasks have yielded promising results, especially in the field\nof scientific chart comprehension. These studies generally utilize visual\ninstruction tuning with specialized datasets to enhance question and answer\n(QA) accuracy within the chart domain. However, they often neglect the\nfundamental discrepancy between natural image-caption pre-training data and\ndigital chart image-QA data, particularly in the models' capacity to extract\nunderlying numeric values from charts. This paper tackles this oversight by\nexploring the training processes necessary to improve MLLMs' comprehension of\ncharts. We present three key findings: (1) Incorporating raw data values in\nalignment pre-training markedly improves comprehension of chart data. (2)\nReplacing images with their textual representation randomly during end-to-end\nfine-tuning transfer the language reasoning capability to chart interpretation\nskills. (3) Requiring the model to first extract the underlying chart data and\nthen answer the question in the fine-tuning can further improve the accuracy.\nConsequently, we introduce CHOPINLLM, an MLLM tailored for in-depth chart\ncomprehension. CHOPINLLM effectively interprets various types of charts,\nincluding unannotated ones, while maintaining robust reasoning abilities.\nFurthermore, we establish a new benchmark to evaluate MLLMs' understanding of\ndifferent chart types across various comprehension levels. Experimental results\nshow that CHOPINLLM exhibits strong performance in understanding both annotated\nand unannotated charts across a wide range of types.\n"", '  Data visualization in the form of charts plays a pivotal role in data\nanalysis, offering critical insights and aiding in informed decision-making.\nAutomatic chart understanding has witnessed significant advancements with the\nrise of large foundation models in recent years. Foundation models, such as\nlarge language models, have revolutionized various natural language processing\ntasks and are increasingly being applied to chart understanding tasks. This\nsurvey paper provides a comprehensive overview of the recent developments,\nchallenges, and future directions in chart understanding within the context of\nthese foundation models. We review fundamental building blocks crucial for\nstudying chart understanding tasks. Additionally, we explore various tasks and\ntheir evaluation metrics and sources of both charts and textual inputs. Various\nmodeling strategies are then examined, encompassing both classification-based\nand generation-based approaches, along with tool augmentation techniques that\nenhance chart understanding performance. Furthermore, we discuss the\nstate-of-the-art performance of each task and discuss how we can improve the\nperformance. Challenges and future directions are addressed, highlighting the\nimportance of several topics, such as domain-specific charts, lack of efforts\nin developing evaluation metrics, and agent-oriented settings. This survey\npaper serves as a comprehensive resource for researchers and practitioners in\nthe fields of natural language processing, computer vision, and data analysis,\nproviding valuable insights and directions for future research in chart\nunderstanding leveraging large foundation models. The studies mentioned in this\npaper, along with emerging new research, will be continually updated at:\nhttps://github.com/khuangaf/Awesome-Chart-Understanding.\n', ""  Natural language is a powerful complementary modality of communication for\ndata visualizations, such as bar and line charts. To facilitate chart-based\nreasoning using natural language, various downstream tasks have been introduced\nrecently such as chart question answering, chart summarization, and\nfact-checking with charts. These tasks pose a unique challenge, demanding both\nvision-language reasoning and a nuanced understanding of chart data tables,\nvisual encodings, and natural language prompts. Despite the recent success of\nLarge Language Models (LLMs) across diverse NLP tasks, their abilities and\nlimitations in the realm of data visualization remain under-explored, possibly\ndue to their lack of multi-modal capabilities. To bridge the gap, this paper\npresents the first comprehensive evaluation of the recently developed large\nvision language models (LVLMs) for chart understanding and reasoning tasks. Our\nevaluation includes a comprehensive assessment of LVLMs, including GPT-4V and\nGemini, across four major chart reasoning tasks. Furthermore, we perform a\nqualitative evaluation of LVLMs' performance on a diverse range of charts,\naiming to provide a thorough analysis of their strengths and weaknesses. Our\nfindings reveal that LVLMs demonstrate impressive abilities in generating\nfluent texts covering high-level data insights while also encountering common\nproblems like hallucinations, factual errors, and data bias. We highlight the\nkey strengths and limitations of chart comprehension tasks, offering insights\nfor future research.\n""] , [""  Computational notebooks, such as Jupyter Notebook, have become data\nscientists' de facto programming environments. Many visualization researchers\nand practitioners have developed interactive visualization tools that support\nnotebooks, yet little is known about the appropriate design of these tools. To\naddress this critical research gap, we investigate the design strategies in\nthis space by analyzing 163 notebook visualization tools. Our analysis\nencompasses 64 systems from academic papers and 105 systems sourced from a pool\nof 55k notebooks containing interactive visualizations that we obtain via\nscraping 8.6 million notebooks on GitHub. Through this study, we identify key\ndesign implications and trade-offs, such as leveraging multimodal data in\nnotebooks as well as balancing the degree of visualization-notebook\nintegration. Furthermore, we provide empirical evidence that tools compatible\nwith more notebook platforms have a greater impact. Finally, we develop\nSuperNOVA, an open-source interactive browser to help researchers explore\nexisting notebook visualization tools. SuperNOVA is publicly accessible at:\nhttps://poloclub.github.io/supernova/.\n"", '  Machine learning (ML) models are nowadays used in complex applications in\nvarious domains, such as medicine, bioinformatics, and other sciences. Due to\ntheir black box nature, however, it may sometimes be hard to understand and\ntrust the results they provide. This has increased the demand for reliable\nvisualization tools related to enhancing trust in ML models, which has become a\nprominent topic of research in the visualization community over the past\ndecades. To provide an overview and present the frontiers of current research\non the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in\nML models with the use of interactive visualization. We define and describe the\nbackground of the topic, introduce a categorization for visualization\ntechniques that aim to accomplish this goal, and discuss insights and\nopportunities for future research directions. Among our contributions is a\ncategorization of trust against different facets of interactive ML, expanded\nand improved from previous research. Our results are investigated from\ndifferent analytical perspectives: (a) providing a statistical overview, (b)\nsummarizing key findings, (c) performing topic analyses, and (d) exploring the\ndata sets used in the individual papers, all with the support of an interactive\nweb-based survey browser. We intend this survey to be beneficial for\nvisualization researchers whose interests involve making ML models more\ntrustworthy, as well as researchers and practitioners from other disciplines in\ntheir search for effective visualization techniques suitable for solving their\ntasks with confidence and conveying meaning to their data.\n', '  The growing importance of data visualization in business intelligence and\ndata science emphasizes the need for tools that can efficiently generate\nmeaningful visualizations from large datasets. Existing tools fall into two\nmain categories: human-powered tools (e.g., Tableau and PowerBI), which require\nintensive expert involvement, and AI-powered automated tools (e.g., Draco and\nTable2Charts), which often fall short of guessing specific user needs. In this\npaper, we aim to achieve the best of both worlds. Our key idea is to initially\nauto-generate a set of high-quality visualizations to minimize manual effort,\nthen refine this process iteratively with user feedback to more closely align\nwith their needs. To this end, we present HAIChart, a reinforcement\nlearning-based framework designed to iteratively recommend good visualizations\nfor a given dataset by incorporating user feedback. Specifically, we propose a\nMonte Carlo Graph Search-based visualization generation algorithm paired with a\ncomposite reward function to efficiently explore the visualization space and\nautomatically generate good visualizations. We devise a visualization hints\nmechanism to actively incorporate user feedback, thus progressively refining\nthe visualization generation module. We further prove that the top-k\nvisualization hints selection problem is NP-hard and design an efficient\nalgorithm. We conduct both quantitative evaluations and user studies, showing\nthat HAIChart significantly outperforms state-of-the-art human-powered tools\n(21% better at Recall and 1.8 times faster) and AI-powered automatic tools\n(25.1% and 14.9% better in terms of Hit@3 and R10@30, respectively).\n']",Data Visualization and Chart Understanding,Chart Understanding and Reasoning
220,"Multi-View Clustering Methods , ""Multi-View Classification and Ensemble Methods""","['views', 'view', 'clustering', 'cluster', 'clusters', 'embedding', 'mvcan', 'representations', 'feature', 'learns'] , ['classifiers', 'classifier', 'classification', 'ensemble', 'ensembles', 'boosting', 'datasets', 'outliers', 'outlier', 'unifier']","['  Multi-view clustering has attracted growing attention owing to its\ncapabilities of aggregating information from various sources and its promising\nhorizons in public affairs. Up till now, many advanced approaches have been\nproposed in recent literature. However, there are several ongoing difficulties\nto be tackled. One common dilemma occurs while attempting to align the features\nof different views. {Moreover, due to the fact that many existing multi-view\nclustering algorithms stem from spectral clustering, this results to cubic time\ncomplexity w.r.t. the number of dataset. However, we propose Anchor-based\nMulti-view Subspace Clustering with Hierarchical Feature Descent(MVSC-HFD) to\ntackle the discrepancy among views through hierarchical feature descent and\nproject to a common subspace( STAGE 1), which reveals dependency of different\nviews. We further reduce the computational complexity to linear time cost\nthrough a unified sampling strategy in the common subspace( STAGE 2), followed\nby anchor-based subspace clustering to learn the bipartite graph collectively(\nSTAGE 3). }Extensive experimental results on public benchmark datasets\ndemonstrate that our proposed model consistently outperforms the\nstate-of-the-art techniques.\n', '  Multi-view clustering has become a significant area of research, with\nnumerous methods proposed over the past decades to enhance clustering accuracy.\nHowever, in many real-world applications, it is crucial to demonstrate a clear\ndecision-making process-specifically, explaining why samples are assigned to\nparticular clusters. Consequently, there remains a notable gap in developing\ninterpretable methods for clustering multi-view data. To fill this crucial gap,\nwe make the first attempt towards this direction by introducing an\ninterpretable multi-view clustering framework. Our method begins by extracting\nembedded features from each view and generates pseudo-labels to guide the\ninitial construction of the decision tree. Subsequently, it iteratively\noptimizes the feature representation for each view along with refining the\ninterpretable decision tree. Experimental results on real datasets demonstrate\nthat our method not only provides a transparent clustering process for\nmulti-view data but also delivers performance comparable to state-of-the-art\nmulti-view clustering methods. To the best of our knowledge, this is the first\neffort to design an interpretable clustering framework specifically for\nmulti-view data, opening a new avenue in this field.\n', '  In this paper, we propose a novel multi-view clustering model, named\nDual-space Co-training Large-scale Multi-view Clustering (DSCMC). The main\nobjective of our approach is to enhance the clustering performance by\nleveraging co-training in two distinct spaces. In the original space, we learn\na projection matrix to obtain latent consistent anchor graphs from different\nviews. This process involves capturing the inherent relationships and\nstructures between data points within each view. Concurrently, we employ a\nfeature transformation matrix to map samples from various views to a shared\nlatent space. This transformation facilitates the alignment of information from\nmultiple views, enabling a comprehensive understanding of the underlying data\ndistribution. We jointly optimize the construction of the latent consistent\nanchor graph and the feature transformation to generate a discriminative anchor\ngraph. This anchor graph effectively captures the essential characteristics of\nthe multi-view data and serves as a reliable basis for subsequent clustering\nanalysis. Moreover, the element-wise method is proposed to avoid the impact of\ndiverse information between different views. Our algorithm has an approximate\nlinear computational complexity, which guarantees its successful application on\nlarge-scale datasets. Through experimental validation, we demonstrate that our\nmethod significantly reduces computational complexity while yielding superior\nclustering performance compared to existing approaches.\n'] , ['  Multi-view datasets offer diverse forms of data that can enhance prediction\nmodels by providing complementary information. However, the use of multi-view\ndata leads to an increase in high-dimensional data, which poses significant\nchallenges for the prediction models that can lead to poor generalization.\nTherefore, relevant feature selection from multi-view datasets is important as\nit not only addresses the poor generalization but also enhances the\ninterpretability of the models. Despite the success of traditional feature\nselection methods, they have limitations in leveraging intrinsic information\nacross modalities, lacking generalizability, and being tailored to specific\nclassification tasks. We propose a novel genetic algorithm strategy to overcome\nthese limitations of traditional feature selection methods for multi-view data.\nOur proposed approach, called the multi-view multi-objective feature selection\ngenetic algorithm (MMFS-GA), simultaneously selects the optimal subset of\nfeatures within a view and between views under a unified framework. The MMFS-GA\nframework demonstrates superior performance and interpretability for feature\nselection on multi-view datasets in both binary and multiclass classification\ntasks. The results of our evaluations on three benchmark datasets, including\nsynthetic and real data, show improvement over the best baseline methods. This\nwork provides a promising solution for multi-view feature selection and opens\nup new possibilities for further research in multi-view datasets.\n', ""  Multi-class ensemble classification remains a popular focus of investigation\nwithin the research community. The popularization of cloud services has sped up\ntheir adoption due to the ease of deploying large-scale machine-learning\nmodels. It has also drawn the attention of the industrial sector because of its\nability to identify common problems in production. However, there are\nchallenges to conform an ensemble classifier, namely a proper selection and\neffective training of the pool of classifiers, the definition of a proper\narchitecture for multi-class classification, and uncertainty quantification of\nthe ensemble classifier. The robustness and effectiveness of the ensemble\nclassifier lie in the selection of the pool of classifiers, as well as in the\nlearning process. Hence, the selection and the training procedure of the pool\nof classifiers play a crucial role. An (ensemble) classifier learns to detect\nthe classes that were used during the supervised training. However, when\ninjecting data with unknown conditions, the trained classifier will intend to\npredict the classes learned during the training. To this end, the uncertainty\nof the individual and ensemble classifier could be used to assess the learning\ncapability. We present a novel approach for novel detection using ensemble\nclassification and evidence theory. A pool selection strategy is presented to\nbuild a solid ensemble classifier. We present an architecture for multi-class\nensemble classification and an approach to quantify the uncertainty of the\nindividual classifiers and the ensemble classifier. We use uncertainty for the\nanomaly detection approach. Finally, we use the benchmark Tennessee Eastman to\nperform experiments to test the ensemble classifier's prediction and anomaly\ndetection capabilities.\n"", '  Out-of-distribution (OOD) detection methods have been developed to identify\nobjects that a model has not seen during training. The Outlier Exposure (OE)\nmethods use auxiliary datasets to train OOD detectors directly. However, the\ncollection and learning of representative OOD samples may pose challenges. To\ntackle these issues, we propose the Outlier Aware Metric Learning (OAML)\nframework. The main idea of our method is to use the k-NN algorithm and Stable\nDiffusion model to generate outliers for training at the feature level without\nmaking any distributional assumptions. To increase feature discrepancies in the\nsemantic space, we develop a mutual information-based contrastive learning\napproach for learning from OOD data effectively. Both theoretical and empirical\nresults confirm the effectiveness of this contrastive learning technique.\nFurthermore, we incorporate knowledge distillation into our learning framework\nto prevent degradation of in-distribution classification accuracy. The\ncombination of contrastive learning and knowledge distillation algorithms\nsignificantly enhances the performance of OOD detection. Experimental results\nacross various datasets show that our method significantly outperforms previous\nOE methods.\n']",Multi-View Learning and Ensemble Methods,"""Multi-View Classification and Ensemble Methods"""
221,"Clustering Algorithms and Techniques , ""Clustering and Visualization with Self-Organizing Maps""","['clusterings', 'clustering', 'cluster', 'clusters', 'clustered', 'dbscan', 'algorithms', 'algorithm', 'hpclust', 'unsupervised'] , ['clustering', 'cluster', 'clusters', 'organizing', 'dataset', 'map', 'maps', 'visualization', 'soms', 'unsupervised']","[""  Clustering algorithms aim to organize data into groups or clusters based on\nthe inherent patterns and similarities within the data. They play an important\nrole in today's life, such as in marketing and e-commerce, healthcare, data\norganization and analysis, and social media. Numerous clustering algorithms\nexist, with ongoing developments introducing new ones. Each algorithm possesses\nits own set of strengths and weaknesses, and as of now, there is no universally\napplicable algorithm for all tasks. In this work, we analyzed existing\nclustering algorithms and classify mainstream algorithms across five different\ndimensions: underlying principles and characteristics, data point assignment to\nclusters, dataset capacity, predefined cluster numbers and application area.\nThis classification facilitates researchers in understanding clustering\nalgorithms from various perspectives and helps them identify algorithms\nsuitable for solving specific tasks. Finally, we discussed the current trends\nand potential future directions in clustering algorithms. We also identified\nand discussed open challenges and unresolved issues in the field.\n"", '  Data clustering involves identifying latent similarities within a dataset and\norganizing them into clusters or groups. The outcomes of various clustering\nalgorithms differ as they are susceptible to the intrinsic characteristics of\nthe original dataset, including noise and dimensionality. The effectiveness of\nsuch clustering procedures directly impacts the homogeneity of clusters,\nunderscoring the significance of evaluating algorithmic outcomes. Consequently,\nthe assessment of clustering quality presents a significant and complex\nendeavor. A pivotal aspect affecting clustering validation is the cluster\nvalidity metric, which aids in determining the optimal number of clusters. The\nmain goal of this study is to comprehensively review and explain the\nmathematical operation of internal and external cluster validity indices, but\nnot all, to categorize these indices and to brainstorm suggestions for future\nadvancement of clustering validation research. In addition, we review and\nevaluate the performance of internal and external clustering validation indices\non the most common clustering algorithms, such as the evolutionary clustering\nalgorithm star (ECA*). Finally, we suggest a classification framework for\nexamining the functionality of both internal and external clustering validation\nmeasures regarding their ideal values, user-friendliness, responsiveness to\ninput data, and appropriateness across various fields. This classification aids\nresearchers in selecting the appropriate clustering validation measure to suit\ntheir specific requirements.\n', '  This paper focuses on density-based clustering, particularly the Density Peak\n(DP) algorithm and the one based on density-connectivity DBSCAN; and proposes a\nnew method which takes advantage of the individual strengths of these two\nmethods to yield a density-based hierarchical clustering algorithm. Our\ninvestigation begins with formally defining the types of clusters DP and DBSCAN\nare designed to detect; and then identifies the kinds of distributions that DP\nand DBSCAN individually fail to detect all clusters in a dataset. These\nidentified weaknesses inspire us to formally define a new kind of clusters and\npropose a new method called DC-HDP to overcome these weaknesses to identify\nclusters with arbitrary shapes and varied densities. In addition, the new\nmethod produces a richer clustering result in terms of hierarchy or dendrogram\nfor better cluster structures understanding. Our empirical evaluation results\nshow that DC-HDP produces the best clustering results on 14 datasets in\ncomparison with 7 state-of-the-art clustering algorithms.\n'] , ['  Streaming data clustering is a popular research topic in the fields of data\nmining and machine learning. Compared to static data, streaming data, which is\nusually analyzed in data chunks, is more susceptible to encountering the\ndynamic cluster imbalanced issue. That is, the imbalanced degree of clusters\nvaries in different streaming data chunks, leading to corruption in either the\naccuracy or the efficiency of streaming data analysis based on existing\nclustering methods. Therefore, we propose an efficient approach called Learning\nSelf-Refined Organizing Map (LSROM) to handle the imbalanced streaming data\nclustering problem, where we propose an advanced SOM for representing the\nglobal data distribution. The constructed SOM is first refined for guiding the\npartition of the dataset to form many micro-clusters to avoid the missing small\nclusters in imbalanced data. Then an efficient merging of the micro-clusters is\nconducted through quick retrieval based on the SOM, which can automatically\nyield a true number of imbalanced clusters. In comparison to existing\nimbalanced data clustering approaches, LSROM is with a lower time complexity\n$O(n\\log n)$, while achieving very competitive clustering accuracy. Moreover,\nLSROM is interpretable and insensitive to hyper-parameters. Extensive\nexperiments have verified its efficacy.\n', '  High-dimensional single-cell data poses significant challenges in identifying\nunderlying biological patterns due to the complexity and heterogeneity of\ncellular states. We propose a comprehensive gene-cell dependency visualization\nvia unsupervised clustering, Growing Hierarchical Self-Organizing Map (GHSOM),\nspecifically designed for analyzing high-dimensional single-cell data like\nsingle-cell sequencing and CRISPR screens. GHSOM is applied to cluster samples\nin a hierarchical structure such that the self-growth structure of clusters\nsatisfies the required variations between and within. We propose a novel\nSignificant Attributes Identification Algorithm to identify features that\ndistinguish clusters. This algorithm pinpoints attributes with minimal\nvariation within a cluster but substantial variation between clusters. These\nkey attributes can then be used for targeted data retrieval and downstream\nanalysis. Furthermore, we present two innovative visualization tools: Cluster\nFeature Map and Cluster Distribution Map. The Cluster Feature Map highlights\nthe distribution of specific features across the hierarchical structure of\nGHSOM clusters. This allows for rapid visual assessment of cluster uniqueness\nbased on chosen features. The Cluster Distribution Map depicts leaf clusters as\ncircles on the GHSOM grid, with circle size reflecting cluster data size and\ncolor customizable to visualize features like cell type or other attributes. We\napply our analysis to three single-cell datasets and one CRISPR dataset\n(cell-gene database) and evaluate clustering methods with internal and external\nCH and ARI scores. GHSOM performs well, being the best performer in internal\nevaluation (CH=4.2). In external evaluation, GHSOM has the third-best\nperformance of all methods.\n', '  Motivation: Unraveling the connection between genes and traits is crucial for\nsolving many biological puzzles. Genes provide instructions for building\ncellular machinery, directing the processes that sustain life. RNA molecules\nand proteins, derived from these genetic instructions, play crucial roles in\nshaping cell structures, influencing reactions, and guiding behavior. This\nfundamental biological principle links genetic makeup to observable traits, but\nintegrating and extracting meaningful relationships from this complex,\nmultimodal data presents a significant challenge. Results: We introduce\nevolSOM, a novel R package that utilizes Self-Organizing Maps (SOMs) to explore\nand visualize the conservation of biological variables, easing the integration\nof phenotypic and genotypic attributes. By constructing species-specific or\ncondition-specific SOMs that capture non-redundant patterns, evolSOM allows the\nanalysis of displacement of biological variables between species or conditions.\nVariables displaced together suggest membership in the same regulatory network,\nand the nature of the displacement may hold biological significance. The\npackage automatically calculates and graphically presents these displacements,\nenabling efficient comparison and revealing conserved and displaced variables.\nThe package facilitates the integration of diverse phenotypic data types,\nenabling the exploration of potential gene drivers underlying observed\nphenotypic changes. Its user-friendly interface and visualization capabilities\nenhance the accessibility of complex network analyses. Illustratively, we\nemployed evolSOM to study the displacement of genes and phenotypic traits,\nsuccessfully identifying potential drivers of phenotypic differentiation in\ngrass leaves. Availability: The package is open-source and is available at\nhttps://github.com/sanprochetto/evolSOM.\n']",Clustering Techniques and Algorithms,"""Clustering and Visualization with Self-Organizing Maps"""
222,"Meta-Learning and Few-Shot Learning , Few-Shot Learning Methods , ""Few-shot and Zero-shot Learning in NLP""","['learning', 'learned', 'training', 'learn', 'learners', 'overfitting', 'regularization', 'learner', 'meta', 'multitask'] , ['learning', 'shot', 'classification', 'dataset', 'overfitting', 'trained', 'training', 'features', 'adaptation', 'labeled'] , ['corpus', 'zeroshot', 'annotation', 'annotate', 'annotated', 'nlp', 'classification', 'shot', 'nlg', 'sentences']","['  There is a growing interest in the learning-to-learn paradigm, also known as\nmeta-learning, where models infer on new tasks using a few training examples.\nRecently, meta-learning based methods have been widely used in few-shot\nclassification, regression, reinforcement learning, and domain adaptation. The\nmodel-agnostic meta-learning (MAML) algorithm is a well-known algorithm that\nobtains model parameter initialization at meta-training phase. In the meta-test\nphase, this initialization is rapidly adapted to new tasks by using gradient\ndescent. However, meta-learning models are prone to overfitting since there are\ninsufficient training tasks resulting in over-parameterized models with poor\ngeneralization performance for unseen tasks. In this paper, we propose a\nBayesian neural network based MAML algorithm, which we refer to as the B-SMALL\nalgorithm. The proposed framework incorporates a sparse variational loss term\nalongside the loss function of MAML, which uses a sparsifying approximated KL\ndivergence as a regularizer. We demonstrate the performance of B-MAML using\nclassification and regression tasks, and highlight that training a sparsifying\nBNN using MAML indeed improves the parameter footprint of the model while\nperforming at par or even outperforming the MAML approach. We also illustrate\napplicability of our approach in distributed sensor networks, where sparsity\nand meta-learning can be beneficial.\n', ""  As a subset of machine learning, meta-learning, or learning to learn, aims at\nimproving the model's capabilities by employing prior knowledge and experience.\nA meta-learning paradigm can appropriately tackle the conventional challenges\nof traditional learning approaches, such as insufficient number of samples,\ndomain shifts, and generalization. These unique characteristics position\nmeta-learning as a suitable choice for developing influential solutions in\nvarious healthcare contexts, where the available data is often insufficient,\nand the data collection methodologies are different. This survey discusses\nmeta-learning broad applications in the healthcare domain to provide insight\ninto how and where it can address critical healthcare challenges. We first\ndescribe the theoretical foundations and pivotal methods of meta-learning. We\nthen divide the employed meta-learning approaches in the healthcare domain into\ntwo main categories of multi/single-task learning and many/few-shot learning\nand survey the studies. Finally, we highlight the current challenges in\nmeta-learning research, discuss the potential solutions, and provide future\nperspectives on meta-learning in healthcare.\n"", '  Few-shot learning, a challenging task in machine learning, aims to learn a\nclassifier adaptable to recognize new, unseen classes with limited labeled\nexamples. Meta-learning has emerged as a prominent framework for few-shot\nlearning. Its training framework is originally a task-level learning method,\nsuch as Model-Agnostic Meta-Learning (MAML) and Prototypical Networks. And a\nrecently proposed training paradigm called Meta-Baseline, which consists of\nsequential pre-training and meta-training stages, gains state-of-the-art\nperformance. However, as a non-end-to-end training method, indicating the\nmeta-training stage can only begin after the completion of pre-training,\nMeta-Baseline suffers from higher training cost and suboptimal performance due\nto the inherent conflicts of the two training stages. To address these\nlimitations, we propose an end-to-end training paradigm consisting of two\nalternative loops. In the outer loop, we calculate cross entropy loss on the\nentire training set while updating only the final linear layer. In the inner\nloop, we employ the original meta-learning training mode to calculate the loss\nand incorporate gradients from the outer loss to guide the parameter updates.\nThis training paradigm not only converges quickly but also outperforms existing\nbaselines, indicating that information from the overall training set and the\nmeta-learning training paradigm could mutually reinforce one another. Moreover,\nbeing model-agnostic, our framework achieves significant performance gains,\nsurpassing the baseline systems by approximate 1%.\n'] , [""  Few-shot learning (FSL) is a challenging machine learning problem due to a\nscarcity of labeled data. The ability to generalize effectively on both novel\nand training tasks is a significant barrier to FSL. This paper proposes a novel\nsolution that can generalize to both training and novel tasks while also\nutilizing unlabeled samples. The method refines the embedding model before\nupdating the outer loop using unsupervised techniques as ``meta-tasks''. The\nexperimental results show that our proposed method performs well on novel and\ntraining tasks, with faster and better convergence, lower generalization, and\nstandard deviation error, indicating its potential for practical applications\nin FSL. The experimental results show that the proposed method outperforms\nprototypical networks by 3.9%.\n"", '  In Few-Shot Learning (FSL), models are trained to recognise unseen objects\nfrom a query set, given a few labelled examples from a support set. In standard\nFSL, models are evaluated on query instances sampled from the same class\ndistribution of the support set. In this work, we explore the more nuanced and\npractical challenge of Open-Set Few-Shot Recognition (OSFSL). Unlike standard\nFSL, OSFSL incorporates unknown classes into the query set, thereby requiring\nthe model not only to classify known classes but also to identify outliers.\nBuilding on the groundwork laid by previous studies, we define a novel\ntransductive inference technique that leverages the InfoMax principle to\nexploit the unlabelled query set. We called our approach the Enhanced Outlier\nLogit (EOL) method. EOL refines class prototype representations through model\ncalibration, effectively balancing the inlier-outlier ratio. This calibration\nenhances pseudo-label accuracy for the query set and improves the optimisation\nobjective within the transductive inference process. We provide a comprehensive\nempirical evaluation demonstrating that EOL consistently surpasses traditional\nmethods, recording performance improvements ranging from approximately $+1.3%$\nto $+6.3%$ across a variety of classification and outlier detection metrics and\nbenchmarks, even in the presence of inlier-outlier imbalance.\n', '  Few-shot-learning (FSL) commonly requires a model to identify images\n(queries) that belong to classes unseen during training, based on a few labeled\nsamples of the new classes (support set) as reference. So far, plenty of\nalgorithms involve training data augmentation to improve the generalization\ncapability of FSL models, but outlier queries or support images during\ninference can still pose great generalization challenges. In this work, to\nreduce the bias caused by the outlier samples, we generate additional\ntest-class samples by combining original samples with suitable train-class\nsamples via a generative image combiner. Then, we obtain averaged features via\nan augmentor, which leads to more typical representations through the\naveraging. We experimentally and theoretically demonstrate the effectiveness of\nour method, e.g., obtaining a test accuracy improvement proportion of around\n10% (e.g., from 46.86% to 53.28%) for trained FSL models. Importantly, given\npretrained image combiner, our method is training-free for off-the-shelf FSL\nmodels, whose performance can be improved without extra datasets nor further\ntraining of the models themselves.\n'] , ['  In recent years, few-shot and zero-shot learning, which learn to predict\nlabels with limited annotated instances, have garnered significant attention.\nTraditional approaches often treat frequent-shot (freq-shot; labels with\nabundant instances), few-shot, and zero-shot learning as distinct challenges,\noptimizing systems for just one of these scenarios. Yet, in real-world\nsettings, label occurrences vary greatly. Some of them might appear thousands\nof times, while others might only appear sporadically or not at all. For\npractical deployment, it is crucial that a system can adapt to any label\noccurrence. We introduce a novel classification challenge: X-shot, reflecting a\nreal-world context where freq-shot, few-shot, and zero-shot labels co-occur\nwithout predefined limits. Here, X can span from 0 to positive infinity. The\ncrux of X-shot centers on open-domain generalization and devising a system\nversatile enough to manage various label scenarios. To solve X-shot, we propose\nBinBin (Binary INference Based on INstruction following) that leverages the\nIndirect Supervision from a large collection of NLP tasks via instruction\nfollowing, bolstered by Weak Supervision provided by large language models.\nBinBin surpasses previous state-of-the-art techniques on three benchmark\ndatasets across multiple domains. To our knowledge, this is the first work\naddressing X-shot learning, where X remains variable.\n', ""  Large language models (LLMs) offer impressive performance in various\nzero-shot and few-shot tasks. However, their success in zero-shot and few-shot\nsettings may be affected by task contamination, a potential limitation that has\nnot been thoroughly examined. This paper investigates how zero-shot and\nfew-shot performance of LLMs has changed chronologically over time. Utilizing\nGPT-3 series models and several other recent open-sourced LLMs, and controlling\nfor dataset difficulty, we find that on datasets released before the LLM\ntraining data creation date, LLMs perform surprisingly better than on datasets\nreleased after. This strongly indicates that, for many LLMs, there exists task\ncontamination on zero-shot and few-shot evaluation for datasets released prior\nto the LLMs' training data creation date. Additionally, we utilize training\ndata inspection, task example extraction, and a membership inference attack,\nwhich reveal further evidence of task contamination. Importantly, we find that\nfor classification tasks with no possibility of task contamination, LLMs rarely\ndemonstrate statistically significant improvements over simple majority\nbaselines, in both zero and few-shot settings.\n"", '  Recent foundational language models have shown state-of-the-art performance\nin many NLP tasks in zero- and few-shot settings. An advantage of these models\nover more standard approaches based on fine-tuning is the ability to understand\ninstructions written in natural language (prompts), which helps them generalise\nbetter to different tasks and domains without the need for specific training\ndata. This makes them suitable for addressing text classification problems for\ndomains with limited amounts of annotated instances. However, existing research\nis limited in scale and lacks understanding of how text generation models\ncombined with prompting techniques compare to more established methods for text\nclassification such as fine-tuning masked language models. In this paper, we\naddress this research gap by performing a large-scale evaluation study for 16\ntext classification datasets covering binary, multiclass, and multilabel\nproblems. In particular, we compare zero- and few-shot approaches of large\nlanguage models to fine-tuning smaller language models. We also analyse the\nresults by prompt, classification type, domain, and number of labels. In\ngeneral, the results show how fine-tuning smaller and more efficient language\nmodels can still outperform few-shot approaches of larger language models,\nwhich have room for improvement when it comes to text classification.\n']",Meta-Learning and Few-Shot Learning in Machine Learning and NLP,Few-Shot Learning Methods
223,"Semi-Supervised Learning with Pseudo-Labeling , Self-Supervised Learning (SSL) Paradigms , Self-Supervised Learning and Representation , ""Manifold-based Self-Supervised Learning Methods""","['labeling', 'labeled', 'supervised', 'labels', 'label', 'classification', 'unlabelled', 'unlabeled', 'classifier', 'regularization'] , ['ssl', 'ssl4ns', 'supervised', 'imagenet', 'learning', 'memorization', 'ssfa', 'encoders', 'learned', 'regularization'] , ['supervised', 'labeling', 'classification', 'imagenet', 'labeled', 'labels', 'embeddings', 'learning', 'embedding', 'label'] , ['deepinfomax', 'supervised', 'dimensionality', 'embedding', 'embeddings', 'regularization', 'dimensional', 'learning', 'representations', 'dimension']","['  Self-supervised pretraining on unlabeled data followed by supervised\nfine-tuning on labeled data is a popular paradigm for learning from limited\nlabeled examples. We extend this paradigm to the classical positive unlabeled\n(PU) setting, where the task is to learn a binary classifier given only a few\nlabeled positive samples, and (often) a large amount of unlabeled samples\n(which could be positive or negative).\n  We first propose a simple extension of standard infoNCE family of contrastive\nlosses, to the PU setting; and show that this learns superior representations,\nas compared to existing unsupervised and supervised approaches. We then develop\na simple methodology to pseudo-label the unlabeled samples using a new\nPU-specific clustering scheme; these pseudo-labels can then be used to train\nthe final (positive vs. negative) classifier. Our method handily outperforms\nstate-of-the-art PU methods over several standard PU benchmark datasets, while\nnot requiring a-priori knowledge of any class prior (which is a common\nassumption in other PU methods). We also provide a simple theoretical analysis\nthat motivates our methods.\n', '  Deep neural models have achieved state of the art performance on a wide range\nof problems in computer science, especially in computer vision. However, deep\nneural networks often require large datasets of labeled samples to generalize\neffectively, and an important area of active research is semi-supervised\nlearning, which attempts to instead utilize large quantities of (easily\nacquired) unlabeled samples. One family of methods in this space is\npseudo-labeling, a class of algorithms that use model outputs to assign labels\nto unlabeled samples which are then used as labeled samples during training.\nSuch assigned labels, called pseudo-labels, are most commonly associated with\nthe field of semi-supervised learning. In this work we explore a broader\ninterpretation of pseudo-labels within both self-supervised and unsupervised\nmethods. By drawing the connection between these areas we identify new\ndirections when advancements in one area would likely benefit others, such as\ncurriculum learning and self-supervised regularization.\n', '  Real-world datasets usually are class-imbalanced and corrupted by label\nnoise. To solve the joint issue of long-tailed distribution and label noise,\nmost previous works usually aim to design a noise detector to distinguish the\nnoisy and clean samples. Despite their effectiveness, they may be limited in\nhandling the joint issue effectively in a unified way. In this work, we develop\na novel pseudo labeling method using class prototypes from the perspective of\ndistribution matching, which can be solved with optimal transport (OT). By\nsetting a manually-specific probability measure and using a learned transport\nplan to pseudo-label the training samples, the proposed method can reduce the\nside-effects of noisy and long-tailed data simultaneously. Then we introduce a\nsimple yet effective filter criteria by combining the observed labels and\npseudo labels to obtain a more balanced and less noisy subset for a robust\nmodel training. Extensive experiments demonstrate that our method can extract\nthis class-balanced subset with clean labels, which brings effective\nperformance gains for long-tailed classification with label noise.\n'] , [""  In recent years, the rise of generative self-supervised learning (SSL)\nparadigms has exhibited impressive performance across visual, language, and\nmulti-modal domains. While the varied designs of generative SSL objectives lead\nto distinct properties in downstream tasks, a theoretical understanding of\nthese differences remains largely unexplored. In this paper, we establish the\nfirst theoretical comparisons between two leading generative SSL paradigms:\nautoregressive SSL and masked SSL. Through establishing theoretical frameworks,\nwe elucidate the strengths and limitations of autoregressive and masked SSL\nwithin the primary evaluation tasks of classification and content generation.\nOur findings demonstrate that in classification tasks, the flexibility of\ntargeted tokens in masked SSL fosters more inter-sample connections compared to\nthe fixed position of target tokens in autoregressive SSL, which yields\nsuperior clustering performance. In content generation tasks, the misalignment\nbetween the flexible lengths of test samples and the fixed length of unmasked\ntexts in masked SSL (vs. flexible lengths of conditional texts in\nautoregressive SSL) hinders its generation performance. To leverage each\nother's strengths and mitigate weaknesses, we propose diversity-enhanced\nautoregressive and variable-length masked objectives, which substantially\nimprove the classification performance of autoregressive SSL and the generation\nperformance of masked SSL. Code is available at\nhttps://github.com/PKU-ML/LookAheadLookAround.\n"", '  Deep supervised learning algorithms typically require a large volume of\nlabeled data to achieve satisfactory performance. However, the process of\ncollecting and labeling such data can be expensive and time-consuming.\nSelf-supervised learning (SSL), a subset of unsupervised learning, aims to\nlearn discriminative features from unlabeled data without relying on\nhuman-annotated labels. SSL has garnered significant attention recently,\nleading to the development of numerous related algorithms. However, there is a\ndearth of comprehensive studies that elucidate the connections and evolution of\ndifferent SSL variants. This paper presents a review of diverse SSL methods,\nencompassing algorithmic aspects, application domains, three key trends, and\nopen research questions. Firstly, we provide a detailed introduction to the\nmotivations behind most SSL algorithms and compare their commonalities and\ndifferences. Secondly, we explore representative applications of SSL in domains\nsuch as image processing, computer vision, and natural language processing.\nLastly, we discuss the three primary trends observed in SSL research and\nhighlight the open questions that remain. A curated collection of valuable\nresources can be accessed at https://github.com/guijiejie/SSL.\n', '  Self-supervised learning (SSL) has recently achieved impressive performance\non various time series tasks. The most prominent advantage of SSL is that it\nreduces the dependence on labeled data. Based on the pre-training and\nfine-tuning strategy, even a small amount of labeled data can achieve high\nperformance. Compared with many published self-supervised surveys on computer\nvision and natural language processing, a comprehensive survey for time series\nSSL is still missing. To fill this gap, we review current state-of-the-art SSL\nmethods for time series data in this article. To this end, we first\ncomprehensively review existing surveys related to SSL and time series, and\nthen provide a new taxonomy of existing time series SSL methods by summarizing\nthem from three perspectives: generative-based, contrastive-based, and\nadversarial-based. These methods are further divided into ten subcategories\nwith detailed reviews and discussions about their key intuitions, main\nframeworks, advantages and disadvantages. To facilitate the experiments and\nvalidation of time series SSL methods, we also summarize datasets commonly used\nin time series forecasting, classification, anomaly detection, and clustering\ntasks. Finally, we present the future directions of SSL for time series\nanalysis.\n'] , ['  In self-supervised learning (SSL), representations are learned via an\nauxiliary task without annotated labels. A common task is to classify\naugmentations or different modalities of the data, which share semantic content\n(e.g. an object in an image) but differ in style (e.g. the object\'s location).\nMany approaches to self-supervised learning have been proposed, e.g. SimCLR,\nCLIP, and VicREG, which have recently gained much attention for their\nrepresentations achieving downstream performance comparable to supervised\nlearning. However, a theoretical understanding of self-supervised methods\neludes. Addressing this, we present a generative latent variable model for\nself-supervised learning and show that several families of discriminative SSL,\nincluding contrastive methods, induce a comparable distribution over\nrepresentations, providing a unifying theoretical framework for these methods.\nThe proposed model also justifies connections drawn to mutual information and\nthe use of a ""projection head"". Learning representations by fitting the model\ngeneratively (termed SimVAE) improves performance over discriminative and other\nVAE-based methods on simple image benchmarks and significantly narrows the gap\nbetween generative and discriminative representation learning in more complex\nsettings. Importantly, as our analysis predicts, SimVAE outperforms\nself-supervised learning where style information is required, taking an\nimportant step toward understanding self-supervised methods and achieving\ntask-agnostic representations.\n', '  The rapid advancement in self-supervised learning (SSL) has highlighted its\npotential to leverage unlabeled data for learning rich visual representations.\nHowever, the existing SSL techniques, particularly those employing different\naugmentations of the same image, often rely on a limited set of simple\ntransformations that are not representative of real-world data variations. This\nconstrains the diversity and quality of samples, which leads to sub-optimal\nrepresentations. In this paper, we introduce a novel framework that enriches\nthe SSL paradigm by utilizing generative models to produce semantically\nconsistent image augmentations. By directly conditioning generative models on a\nsource image representation, our method enables the generation of diverse\naugmentations while maintaining the semantics of the source image, thus\noffering a richer set of data for self-supervised learning. Our extensive\nexperimental results on various SSL methods demonstrate that our framework\nsignificantly enhances the quality of learned visual representations by up to\n10\\% Top-1 accuracy in downstream tasks. This research demonstrates that\nincorporating generative models into the SSL workflow opens new avenues for\nexploring the potential of synthetic data. This development paves the way for\nmore robust and versatile representation learning techniques.\n', '  In this work, we propose a novel supervised contrastive loss that enables the\nintegration of taxonomic hierarchy information during the representation\nlearning process. A supervised contrastive loss operates by enforcing that\nimages with the same class label (positive samples) project closer to each\nother than images with differing class labels (negative samples). The advantage\nof this approach is that it directly penalizes the structure of the\nrepresentation space itself. This enables greater flexibility with respect to\nencoding semantic concepts. However, the standard supervised contrastive loss\nonly enforces semantic structure based on the downstream task (i.e. the class\nlabel). In reality, the class label is only one level of a \\emph{hierarchy of\ndifferent semantic relationships known as a taxonomy}. For example, the class\nlabel is oftentimes the species of an animal, but between different classes\nthere are higher order relationships such as all animals with wings being\n``birds"". We show that by explicitly accounting for these relationships with a\nweighting penalty in the contrastive loss we can out-perform the supervised\ncontrastive loss. Additionally, we demonstrate the adaptability of the notion\nof a taxonomy by integrating our loss into medical and noise-based settings\nthat show performance improvements by as much as 7%.\n'] , ['  Maximum Manifold Capacity Representations (MMCR) is a recent multi-view\nself-supervised learning (MVSSL) method that matches or surpasses other leading\nMVSSL methods. MMCR is intriguing because it does not fit neatly into any of\nthe commonplace MVSSL lineages, instead originating from a statistical\nmechanical perspective on the linear separability of data manifolds. In this\npaper, we seek to improve our understanding and our utilization of MMCR. To\nbetter understand MMCR, we leverage tools from high dimensional probability to\ndemonstrate that MMCR incentivizes alignment and uniformity of learned\nembeddings. We then leverage tools from information theory to show that such\nembeddings maximize a well-known lower bound on mutual information between\nviews, thereby connecting the geometric perspective of MMCR to the\ninformation-theoretic perspective commonly discussed in MVSSL. To better\nutilize MMCR, we mathematically predict and experimentally confirm\nnon-monotonic changes in the pretraining loss akin to double descent but with\nrespect to atypical hyperparameters. We also discover compute scaling laws that\nenable predicting the pretraining loss as a function of gradients steps, batch\nsize, embedding dimension and number of views. We then show that MMCR,\noriginally applied to image data, is performant on multimodal image-text data.\nBy more deeply understanding the theoretical and empirical behavior of MMCR,\nour work reveals insights on improving MVSSL methods.\n', '  The manifold hypothesis posits that high-dimensional data often lies on a\nlower-dimensional manifold and that utilizing this manifold as the target space\nyields more efficient representations. While numerous traditional\nmanifold-based techniques exist for dimensionality reduction, their application\nin self-supervised learning has witnessed slow progress. The recent MSimCLR\nmethod combines manifold encoding with SimCLR but requires extremely low target\nencoding dimensions to outperform SimCLR, limiting its applicability. This\npaper introduces a novel learning paradigm using an unbalanced atlas (UA),\ncapable of surpassing state-of-the-art self-supervised learning approaches. We\ninvestigated and engineered the DeepInfomax with an unbalanced atlas (DIM-UA)\nmethod by adapting the Spatiotemporal DeepInfomax (ST-DIM) framework to align\nwith our proposed UA paradigm. The efficacy of DIM-UA is demonstrated through\ntraining and evaluation on the Atari Annotated RAM Interface (AtariARI)\nbenchmark, a modified version of the Atari 2600 framework that produces\nannotated image samples for representation learning. The UA paradigm improves\nexisting algorithms significantly as the number of target encoding dimensions\ngrows. For instance, the mean F1 score averaged over categories of DIM-UA is\n~75% compared to ~70% of ST-DIM when using 16384 hidden units.\n', '  The expanding research on manifold-based self-supervised learning (SSL)\nbuilds on the manifold hypothesis, which suggests that the inherent complexity\nof high-dimensional data can be unraveled through lower-dimensional manifold\nembeddings. Capitalizing on this, DeepInfomax with an unbalanced atlas (DIM-UA)\nhas emerged as a powerful tool and yielded impressive results for state\nrepresentations in reinforcement learning. Meanwhile, Maximum Manifold Capacity\nRepresentation (MMCR) presents a new frontier for SSL by optimizing class\nseparability via manifold compression. However, MMCR demands extensive input\nviews, resulting in significant computational costs and protracted pre-training\ndurations. Bridging this gap, we present an innovative integration of MMCR into\nexisting SSL methods, incorporating a discerning regularization strategy that\nenhances the lower bound of mutual information. We also propose a novel state\nrepresentation learning method extending DIM-UA, embedding a nuclear norm loss\nto enforce manifold consistency robustly. On experimentation with the Atari\nAnnotated RAM Interface, our method improves DIM-UA significantly with the same\nnumber of target encoding dimensions. The mean F1 score averaged over\ncategories is 78% compared to 75% of DIM-UA. There are also compelling gains\nwhen implementing SimCLR and Barlow Twins. This supports our SSL innovation as\na paradigm shift, enabling more nuanced high-dimensional data representations.\n']",Self-Supervised Learning and Representation Learning,Self-Supervised Learning and Representation
224,"Decision Trees and Ensemble Methods , Ensemble Methods for Deep Learning , Fuzzy Ensemble Classifiers for Feature Learning","['ensembles', 'treeshap', 'ensemble', 'boosting', 'forests', 'forest', 'trees', 'classification', 'tree', 'metatree'] , ['ensemble', 'ensembles', 'ensembling', 'diversity', 'classification', 'trained', 'fusionshot', 'fusion', 'models', 'generalization'] , ['fuzzy', 'classifier', 'classification', 'intuitionistic', 'feature', 'features', 'ensemble', 'prediction', 'datasets', 'kernelshap']","['  Decision trees are a popular tool in machine learning and yield\neasy-to-understand models. Several techniques have been proposed in the\nliterature for learning a decision tree classifier, with different techniques\nworking well for data from different domains. In this work, we develop\napproaches to design decision tree learning algorithms given repeated access to\ndata from the same domain. We propose novel parameterized classes of node\nsplitting criteria in top-down algorithms, which interpolate between popularly\nused entropy and Gini impurity based criteria, and provide theoretical bounds\non the number of samples needed to learn the splitting function appropriate for\nthe data at hand. We also study the sample complexity of tuning prior\nparameters in Bayesian decision tree learning, and extend our results to\ndecision tree regression. We further consider the problem of tuning\nhyperparameters in pruning the decision tree for classical pruning algorithms\nincluding min-cost complexity pruning. We also study the interpretability of\nthe learned decision trees and introduce a data-driven approach for optimizing\nthe explainability versus accuracy trade-off using decision trees. Finally, we\ndemonstrate the significance of our approach on real world datasets by learning\ndata-specific decision trees which are simultaneously more accurate and\ninterpretable.\n', '  A decision tree is one of the most popular approaches in machine learning\nfields. However, it suffers from the problem of overfitting caused by overly\ndeepened trees. Then, a meta-tree is recently proposed. It solves the problem\nof overfitting caused by overly deepened trees. Moreover, the meta-tree\nguarantees statistical optimality based on Bayes decision theory. Therefore,\nthe meta-tree is expected to perform better than the decision tree. In contrast\nto a single decision tree, it is known that ensembles of decision trees, which\nare typically constructed boosting algorithms, are more effective in improving\npredictive performance. Thus, it is expected that ensembles of meta-trees are\nmore effective in improving predictive performance than a single meta-tree, and\nthere are no previous studies that construct multiple meta-trees in boosting.\nTherefore, in this study, we propose a method to construct multiple meta-trees\nusing a boosting approach. Through experiments with synthetic and benchmark\ndatasets, we conduct a performance comparison between the proposed methods and\nthe conventional methods using ensembles of decision trees. Furthermore, while\nensembles of decision trees can cause overfitting as well as a single decision\ntree, experiments confirmed that ensembles of meta-trees can prevent\noverfitting due to the tree depth.\n', '  Decades after their inception, random forests continue to provide\nstate-of-the-art accuracy in a variety of learning problems, outperforming in\nthis respect alternative machine learning algorithms such as decision trees or\neven neural networks. However, being an ensemble method, the one aspect where\nrandom forests tend to severely underperform decision trees is\ninterpretability. In the present work, we propose a post-hoc approach that aims\nto have the best of both worlds: the accuracy of random forests and the\ninterpretability of decision trees. To this end, we present two forest-pruning\nmethods to find an optimal sub-forest within a given random forest, and then,\nwhen applicable, combine the selected trees into one. Our first method relies\non constrained exhaustive search, while our second method is based on an\nadaptation of the LASSO methodology. Extensive experiments over synthetic and\nreal world datasets show that, in the majority of scenarios, at least one of\nthe two methods proposed is more accurate than the original random forest,\nwhile just using a small fraction of the trees, aiding result interpretability.\nCompared to current state-of-the-art forest pruning methods, namely sequential\nforward selection and (a variation of) sequential backward selection, our\nmethods tend to outperform both of them, whether in terms of accuracy, number\nof trees employed, or both.\n'] , ['  This paper presents FusionShot, a focal diversity optimized few-shot ensemble\nlearning approach for boosting the robustness and generalization performance of\npre-trained few-shot models. The paper makes three original contributions.\nFirst, we explore the unique characteristics of few-shot learning to ensemble\nmultiple few-shot (FS) models by creating three alternative fusion channels.\nSecond, we introduce the concept of focal error diversity to learn the most\nefficient ensemble teaming strategy, rather than assuming that an ensemble of a\nlarger number of base models will outperform those sub-ensembles of smaller\nsize. We develop a focal-diversity ensemble pruning method to effectively prune\nout the candidate ensembles with low ensemble error diversity and recommend\ntop-$K$ FS ensembles with the highest focal error diversity. Finally, we\ncapture the complex non-linear patterns of ensemble few-shot predictions by\ndesigning the learn-to-combine algorithm, which can learn the diverse weight\nassignments for robust ensemble fusion over different member models. Extensive\nexperiments on representative few-shot benchmarks show that the top-K ensembles\nrecommended by FusionShot can outperform the representative SOTA few-shot\nmodels on novel tasks (different distributions and unknown at training), and\ncan prevail over existing few-shot learners in both cross-domain settings and\nadversarial settings. For reproducibility purposes, FusionShot trained models,\nresults, and code are made available at https://github.com/sftekin/fusionshot\n', '  The performance of deep neural networks is enhanced by ensemble methods,\nwhich average the output of several models. However, this comes at an increased\ncost at inference. Weight averaging methods aim at balancing the generalization\nof ensembling and the inference speed of a single model by averaging the\nparameters of an ensemble of models. Yet, naive averaging results in poor\nperformance as models converge to different loss basins, and aligning the\nmodels to improve the performance of the average is challenging. Alternatively,\ninspired by distributed training, methods like DART and PAPA have been proposed\nto train several models in parallel such that they will end up in the same\nbasin, resulting in good averaging accuracy. However, these methods either\ncompromise ensembling accuracy or demand significant communication between\nmodels during training. In this paper, we introduce WASH, a novel distributed\nmethod for training model ensembles for weight averaging that achieves\nstate-of-the-art image classification accuracy. WASH maintains models within\nthe same basin by randomly shuffling a small percentage of weights during\ntraining, resulting in diverse models and lower communication costs compared to\nstandard parameter averaging methods.\n', '  Classic results establish that encouraging predictive diversity improves\nperformance in ensembles of low-capacity models, e.g. through bagging or\nboosting. Here we demonstrate that these intuitions do not apply to\nhigh-capacity neural network ensembles (deep ensembles), and in fact the\nopposite is often true. In a large scale study of nearly 600 neural network\nclassification ensembles, we examine a variety of interventions that trade off\ncomponent model performance for predictive diversity. While such interventions\ncan improve the performance of small neural network ensembles (in line with\nstandard intuitions), they harm the performance of the large neural network\nensembles most often used in practice. Surprisingly, we also find that\ndiscouraging predictive diversity is often benign in large-network ensembles,\nfully inverting standard intuitions. Even when diversity-promoting\ninterventions do not sacrifice component model performance (e.g. using\nheterogeneous architectures and training paradigms), we observe an opportunity\ncost associated with pursuing increased predictive diversity. Examining over\n1000 ensembles, we observe that the performance benefits of diverse\narchitectures/training procedures are easily dwarfed by the benefits of simply\nusing higher-capacity models, despite the fact that such higher capacity models\noften yield significantly less predictive diversity. Overall, our findings\ndemonstrate that standard intuitions around predictive diversity, originally\ndeveloped for low-capacity ensembles, do not directly apply to modern\nhigh-capacity deep ensembles. This work clarifies fundamental challenges to the\ngoal of improving deep ensembles by making them more diverse, while suggesting\nan alternative path: simply forming ensembles from ever more powerful (and less\ndiverse) component models.\n'] , ['  The ensemble deep random vector functional link (edRVFL) neural network has\ndemonstrated the ability to address the limitations of conventional artificial\nneural networks. However, since edRVFL generates features for its hidden layers\nthrough random projection, it can potentially lose intricate features or fail\nto capture certain non-linear features in its base models (hidden layers). To\nenhance the feature learning capabilities of edRVFL, we propose a novel edRVFL\nbased on fuzzy inference system (edRVFL-FIS). The proposed edRVFL-FIS leverages\nthe capabilities of two emerging domains, namely deep learning and ensemble\napproaches, with the intrinsic IF-THEN properties of fuzzy inference system\n(FIS) and produces rich feature representation to train the ensemble model.\nEach base model of the proposed edRVFL-FIS encompasses two key feature\naugmentation components: a) unsupervised fuzzy layer features and b) supervised\ndefuzzified features. The edRVFL-FIS model incorporates diverse clustering\nmethods (R-means, K-means, Fuzzy C-means) to establish fuzzy layer rules,\nresulting in three model variations (edRVFL-FIS-R, edRVFL-FIS-K, edRVFL-FIS-C)\nwith distinct fuzzified features and defuzzified features. Within the framework\nof edRVFL-FIS, each base model utilizes the original, hidden layer and\ndefuzzified features to make predictions. Experimental results, statistical\ntests, discussions and analyses conducted across UCI and NDC datasets\nconsistently demonstrate the superior performance of all variations of the\nproposed edRVFL-FIS model over baseline models. The source codes of the\nproposed models are available at https://github.com/mtanveer1/edRVFL-FIS.\n', '  Classification is essential to the applications in the field of data mining,\nartificial intelligence, and fault detection. There exists a strong need in\ndeveloping accurate, suitable, and efficient classification methods and\nalgorithms with broad applicability. Random forest is a general algorithm that\nis often used for classification under complex conditions. Although it has been\nwidely adopted, its combination with diverse fuzzy theory is still worth\nexploring. In this paper, we propose the intuitionistic fuzzy random forest\n(IFRF), a new random forest ensemble of intuitionistic fuzzy decision trees\n(IFDT). Such trees in forest use intuitionistic fuzzy information gain to\nselect features and consider hesitation in information transmission. The\nproposed method enjoys the power of the randomness from bootstrapped sampling\nand feature selection, the flexibility of fuzzy logic and fuzzy sets, and the\nrobustness of multiple classifier systems. Extensive experiments demonstrate\nthat the IFRF has competitative and superior performance compared to other\nstate-of-the-art fuzzy and ensemble algorithms. IFDT is more suitable for\nensemble learning with outstanding classification accuracy. This study is the\nfirst to propose a random forest ensemble based on the intuitionistic fuzzy\ntheory.\n', '  An important constraint of Fuzzy Inference Systems (FIS) is their structured\nrules defined based on evaluating all input variables. Indeed, the length of\nall fuzzy rules and the number of input variables are equal. However, in many\ndecision-making problems evaluating some conditions on a limited set of input\nvariables is sufficient to decide properly (unstructured rules). Therefore,\nthis constraint limits the performance, generalization, and interpretability of\nthe FIS. To address this issue, this paper presents a neuro-fuzzy inference\nsystem for classification applications that can select different sets of input\nvariables for constructing each fuzzy rule. To realize this capability, a new\nfuzzy selector neuron with an adaptive parameter is proposed that can select\ninput variables in the antecedent part of each fuzzy rule. Moreover, in this\npaper, the consequent part of the Takagi-Sugeno-Kang FIS is also changed\nproperly to consider only the selected set of input variables. To learn the\nparameters of the proposed architecture, a trust-region-based learning method\n(General quasi-Levenberg-Marquardt (GqLM)) is proposed to minimize\ncross-entropy in multiclass problems. The performance of the proposed method is\ncompared with some related previous approaches in some real-world\nclassification problems. Based on these comparisons the proposed method has\nbetter or very close performance with a parsimonious structure consisting of\nunstructured fuzzy.\n']",Ensemble Methods and Classification Techniques,Decision Trees and Ensemble Methods
225,"""Feature Selection Methods for Machine Learning"" , Feature Selection for Classification , Feature Selection and Optimization in Predictive Analytics","['feature', 'features', 'supervised', 'selection', 'lasso', 'predictive', 'datasets', 'embedding', 'evaluator', 'selecting'] , ['feature', 'supervised', 'classifiers', 'features', 'classification', 'labeling', 'selection', 'prediction', 'learning', 'discriminative'] , ['feature', 'features', 'classification', 'biomarker', 'ensemble', 'selection', 'datasets', 'prediction', 'predictive', 'optimization']","['  Feature selection prepares the AI-readiness of data by eliminating redundant\nfeatures. Prior research falls into two primary categories: i) Supervised\nFeature Selection, which identifies the optimal feature subset based on their\nrelevance to the target variable; ii) Unsupervised Feature Selection, which\nreduces the feature space dimensionality by capturing the essential information\nwithin the feature set instead of using target variable. However, SFS\napproaches suffer from time-consuming processes and limited generalizability\ndue to the dependence on the target variable and downstream ML tasks. UFS\nmethods are constrained by the deducted feature space is latent and\nuntraceable. To address these challenges, we introduce an innovative framework\nfor feature selection, which is guided by knockoff features and optimized\nthrough reinforcement learning, to identify the optimal and effective feature\nsubset. In detail, our method involves generating ""knockoff"" features that\nreplicate the distribution and characteristics of the original features but are\nindependent of the target variable. Each feature is then assigned a pseudo\nlabel based on its correlation with all the knockoff features, serving as a\nnovel metric for feature evaluation. Our approach utilizes these pseudo labels\nto guide the feature selection process in 3 novel ways, optimized by a single\nreinforced agent: 1). A deep Q-network, pre-trained with the original features\nand their corresponding pseudo labels, is employed to improve the efficacy of\nthe exploration process in feature selection. 2). We introduce unsupervised\nrewards to evaluate the feature subset quality based on the pseudo labels and\nthe feature space reconstruction loss to reduce dependencies on the target\nvariable. 3). A new {\\epsilon}-greedy strategy is used, incorporating insights\nfrom the pseudo labels to make the feature selection process more effective.\n', '  Feature selection aims to identify the most pattern-discriminative feature\nsubset. In prior literature, filter (e.g., backward elimination) and embedded\n(e.g., Lasso) methods have hyperparameters (e.g., top-K, score thresholding)\nand tie to specific models, thus, hard to generalize; wrapper methods search a\nfeature subset in a huge discrete space and is computationally costly. To\ntransform the way of feature selection, we regard a selected feature subset as\na selection decision token sequence and reformulate feature selection as a deep\nsequential generative learning task that distills feature knowledge and\ngenerates decision sequences. Our method includes three steps: (1) We develop a\ndeep variational transformer model over a joint of sequential reconstruction,\nvariational, and performance evaluator losses. Our model can distill feature\nselection knowledge and learn a continuous embedding space to map feature\nselection decision sequences into embedding vectors associated with utility\nscores. (2) We leverage the trained feature subset utility evaluator as a\ngradient provider to guide the identification of the optimal feature subset\nembedding;(3) We decode the optimal feature subset embedding to\nautoregressively generate the best feature selection decision sequence with\nautostop. Extensive experimental results show this generative perspective is\neffective and generic, without large discrete search space and expert-specific\nhyperparameters.\n', '  Feature selection aims to identify the optimal feature subset for enhancing\ndownstream models. Effective feature selection can remove redundant features,\nsave computational resources, accelerate the model learning process, and\nimprove the model overall performance. However, existing works are often\ntime-intensive to identify the effective feature subset within high-dimensional\nfeature spaces. Meanwhile, these methods mainly utilize a single downstream\ntask performance as the selection criterion, leading to the selected subsets\nthat are not only redundant but also lack generalizability. To bridge these\ngaps, we reformulate feature selection through a neuro-symbolic lens and\nintroduce a novel generative framework aimed at identifying short and effective\nfeature subsets. More specifically, we found that feature ID tokens of the\nselected subset can be formulated as symbols to reflect the intricate\ncorrelations among features. Thus, in this framework, we first create a data\ncollector to automatically collect numerous feature selection samples\nconsisting of feature ID tokens, model performance, and the measurement of\nfeature subset redundancy. Building on the collected data, an\nencoder-decoder-evaluator learning paradigm is developed to preserve the\nintelligence of feature selection into a continuous embedding space for\nefficient search. Within the learned embedding space, we leverage a\nmulti-gradient search algorithm to find more robust and generalized embeddings\nwith the objective of improving model performance and reducing feature subset\nredundancy. These embeddings are then utilized to reconstruct the feature ID\ntokens for executing the final feature selection. Ultimately, comprehensive\nexperiments and case studies are conducted to validate the effectiveness of the\nproposed framework.\n'] , ['  Under missing-not-at-random (MNAR) sample selection bias, the performance of\na prediction model is often degraded. This paper focuses on one classic\ninstance of MNAR sample selection bias where a subset of samples have\nnon-randomly missing outcomes. The Heckman selection model and its variants\nhave commonly been used to handle this type of sample selection bias. The\nHeckman model uses two separate equations to model the prediction and selection\nof samples, where the selection features include all prediction features. When\nusing the Heckman model, the prediction features must be properly chosen from\nthe set of selection features. However, choosing the proper prediction features\nis a challenging task for the Heckman model. This is especially the case when\nthe number of selection features is large. Existing approaches that use the\nHeckman model often provide a manually chosen set of prediction features. In\nthis paper, we propose Heckman-FA as a novel data-driven framework for\nobtaining prediction features for the Heckman model. Heckman-FA first trains an\nassignment function that determines whether or not a selection feature is\nassigned as a prediction feature. Using the parameters of the trained function,\nthe framework extracts a suitable set of prediction features based on the\ngoodness-of-fit of the prediction model given the chosen prediction features\nand the correlation between noise terms of the prediction and selection\nequations. Experimental results on real-world datasets show that Heckman-FA\nproduces a robust regression model under MNAR sample selection bias.\n', '  In the last decade, embedded multi-label feature selection methods,\nincorporating the search for feature subsets into model optimization, have\nattracted considerable attention in accurately evaluating the importance of\nfeatures in multi-label classification tasks. Nevertheless, the\nstate-of-the-art embedded multi-label feature selection algorithms based on\nleast square regression usually cannot preserve sufficient discriminative\ninformation in multi-label data. To tackle the aforementioned challenge, a\nnovel embedded multi-label feature selection method, termed global redundancy\nand relevance optimization in orthogonal regression (GRROOR), is proposed to\nfacilitate the multi-label feature selection. The method employs orthogonal\nregression with feature weighting to retain sufficient statistical and\nstructural information related to local label correlations of the multi-label\ndata in the feature learning process. Additionally, both global feature\nredundancy and global label relevancy information have been considered in the\northogonal regression model, which could contribute to the search for\ndiscriminative and non-redundant feature subsets in the multi-label data. The\ncost function of GRROOR is an unbalanced orthogonal Procrustes problem on the\nStiefel manifold. A simple yet effective scheme is utilized to obtain an\noptimal solution. Extensive experimental results on ten multi-label data sets\ndemonstrate the effectiveness of GRROOR.\n', '  Semi-supervised multi-label feature selection has recently been developed to\nsolve the curse of dimensionality problem in high-dimensional multi-label data\nwith certain samples missing labels. Although many efforts have been made, most\nexisting methods use a predefined graph approach to capture the sample\nsimilarity or the label correlation. In this manner, the presence of noise and\noutliers within the original feature space can undermine the reliability of the\nresulting sample similarity graph. It also fails to precisely depict the label\ncorrelation due to the existence of unknown labels. Besides, these methods only\nconsider the discriminative power of selected features, while neglecting their\nredundancy. In this paper, we propose an Adaptive Collaborative Correlation\nlEarning-based Semi-Supervised Multi-label Feature Selection (Access-MFS)\nmethod to address these issues. Specifically, a generalized regression model\nequipped with an extended uncorrelated constraint is introduced to select\ndiscriminative yet irrelevant features and maintain consistency between\npredicted and ground-truth labels in labeled data, simultaneously. Then, the\ninstance correlation and label correlation are integrated into the proposed\nregression model to adaptively learn both the sample similarity graph and the\nlabel similarity graph, which mutually enhance feature selection performance.\nExtensive experimental results demonstrate the superiority of the proposed\nAccess-MFS over other state-of-the-art methods.\n'] , ['  Understanding how multiple features are associated and contribute to a\nspecific objective is as important as understanding how each feature\ncontributes to a particular outcome. Interpretability of a single feature in a\nprediction may be handled in multiple ways; however, in a multi-objective\nprediction, it is difficult to obtain interpretability of a combination of\nfeature values. To address this issue, we propose an objective specific feature\ninteraction design using multi-labels to find the optimal combination of\nfeatures in agricultural settings. One of the novel aspects of this design is\nthe identification of a method that integrates feature explanations with global\nsensitivity analysis in order to ensure combinatorial optimization in\nmulti-objective settings. We have demonstrated in our preliminary experiments\nthat an approximate combination of feature values can be found to achieve the\ndesired outcome using two agricultural datasets: one with pre-harvest poultry\nfarm practices for multi-drug resistance presence, and one with post-harvest\npoultry farm practices for food-borne pathogens. In our combinatorial\noptimization approach, all three pathogens are taken into consideration\nsimultaneously to account for the interaction between conditions that favor\ndifferent types of pathogen growth. These results indicate that\nexplanation-based approaches are capable of identifying combinations of\nfeatures that reduce pathogen presence in fewer iterations than a baseline.\n', ""  The challenge in biomarker discovery using machine learning from omics data\nlies in the abundance of molecular features but scarcity of samples. Most\nfeature selection methods in machine learning require evaluating various sets\nof features (models) to determine the most effective combination. This process,\ntypically conducted using a validation dataset, involves testing different\nfeature sets to optimize the model's performance. Evaluations have performance\nestimation error and when the selection involves many models the best ones are\nalmost certainly overestimated. Biomarker identification with feature selection\nmethods can be addressed as a multi-objective problem with trade-offs between\npredictive ability and parsimony in the number of features. Genetic algorithms\nare a popular tool for multi-objective optimization but they evolve numerous\nsolutions thus are prone to overestimation. Methods have been proposed to\nreduce the overestimation after a model has already been selected in\nsingle-objective problems, but no algorithm existed capable of reducing the\noverestimation during the optimization, improving model selection, or applied\nin the more general multi-objective domain. We propose DOSA-MO, a novel\nmulti-objective optimization wrapper algorithm that learns how the original\nestimation, its variance, and the feature set size of the solutions predict the\noverestimation. DOSA-MO adjusts the expectation of the performance during the\noptimization, improving the composition of the solution set. We verify that\nDOSA-MO improves the performance of a state-of-the-art genetic algorithm on\nleft-out or external sample sets, when predicting cancer subtypes and/or\npatient overall survival, using three transcriptomics datasets for kidney and\nbreast cancer.\n"", '  In this study, we investigated the application of bio-inspired optimization\nalgorithms, including Genetic Algorithm, Particle Swarm Optimization, and Whale\nOptimization Algorithm, for feature selection in chronic disease prediction.\nThe primary goal was to enhance the predictive accuracy of models streamline\ndata dimensionality, and make predictions more interpretable and actionable.\n  The research encompassed a comparative analysis of the three bio-inspired\nfeature selection approaches across diverse chronic diseases, including\ndiabetes, cancer, kidney, and cardiovascular diseases. Performance metrics such\nas accuracy, precision, recall, and f1 score are used to assess the\neffectiveness of the algorithms in reducing the number of features needed for\naccurate classification.\n  The results in general demonstrate that the bio-inspired optimization\nalgorithms are effective in reducing the number of features required for\naccurate classification. However, there have been variations in the performance\nof the algorithms on different datasets.\n  The study highlights the importance of data pre-processing and cleaning in\nensuring the reliability and effectiveness of the analysis.\n  This study contributes to the advancement of predictive analytics in the\nrealm of chronic diseases. The potential impact of this work extends to early\nintervention, precision medicine, and improved patient outcomes, providing new\navenues for the delivery of healthcare services tailored to individual needs.\nThe findings underscore the potential benefits of using bio-inspired\noptimization algorithms for feature selection in chronic disease prediction,\noffering valuable insights for improving healthcare outcomes.\n']",Feature Selection and Optimization in Machine Learning,"""Feature Selection Methods for Machine Learning"""
226,"""Mitigating Spurious Correlations in Machine Learning Classifiers"" , Spurious Correlations in Neural Network Learning","['classifiers', 'classifier', 'annotations', 'spuriousness', 'generalization', 'misclassifies', 'attributes', 'features', 'spurious', 'labels'] , ['bias', 'biases', 'learning', 'neural', 'deeppoly', 'benchmark', 'debiasing', 'classifier', 'generalization', 'networks']","['  Machine learning models are known to learn spurious correlations, i.e.,\nfeatures having strong relations with class labels but no causal relation.\nRelying on those correlations leads to poor performance in the data groups\nwithout these correlations and poor generalization ability. To improve the\nrobustness of machine learning models to spurious correlations, we propose an\napproach to extract a subnetwork from a fully trained network that does not\nrely on spurious correlations. The subnetwork is found by the assumption that\ndata points with the same spurious attribute will be close to each other in the\nrepresentation space when training with ERM, then we employ supervised\ncontrastive loss in a novel way to force models to unlearn the spurious\nconnections. The increase in the worst-group performance of our approach\ncontributes to strengthening the hypothesis that there exists a subnetwork in a\nfully trained dense network that is responsible for using only invariant\nfeatures in classification tasks, therefore erasing the influence of spurious\nfeatures even in the setup of multi spurious attributes and no prior knowledge\nof attributes labels.\n', '  Standard empirical risk minimization (ERM) models may prioritize learning\nspurious correlations between spurious features and true labels, leading to\npoor accuracy on groups where these correlations do not hold. Mitigating this\nissue often requires expensive spurious attribute (group) labels or relies on\ntrained ERM models to infer group labels when group information is unavailable.\nHowever, the significant performance gap in worst-group accuracy between using\npseudo group labels and using oracle group labels inspires us to consider\nfurther improving group robustness through preciser group inference. Therefore,\nwe propose GIC, a novel method that accurately infers group labels, resulting\nin improved worst-group performance. GIC trains a spurious attribute classifier\nbased on two key properties of spurious correlations: (1) high correlation\nbetween spurious attributes and true labels, and (2) variability in this\ncorrelation between datasets with different group distributions. Empirical\nstudies on multiple datasets demonstrate the effectiveness of GIC in inferring\ngroup labels, and combining GIC with various downstream invariant learning\nmethods improves worst-group accuracy, showcasing its powerful flexibility.\nAdditionally, through analyzing the misclassifications in GIC, we identify an\ninteresting phenomenon called semantic consistency, which may contribute to\nbetter decoupling the association between spurious attributes and labels,\nthereby mitigating spurious correlation. The code for GIC is available at\nhttps://github.com/yujinhanml/GIC.\n', '  Deep neural classifiers tend to rely on spurious correlations between\nspurious attributes of inputs and targets to make predictions, which could\njeopardize their generalization capability. Training classifiers robust to\nspurious correlations typically relies on annotations of spurious correlations\nin data, which are often expensive to get. In this paper, we tackle an\nannotation-free setting and propose a self-guided spurious correlation\nmitigation framework. Our framework automatically constructs fine-grained\ntraining labels tailored for a classifier obtained with empirical risk\nminimization to improve its robustness against spurious correlations. The\nfine-grained training labels are formulated with different prediction behaviors\nof the classifier identified in a novel spuriousness embedding space. We\nconstruct the space with automatically detected conceptual attributes and a\nnovel spuriousness metric which measures how likely a class-attribute\ncorrelation is exploited for predictions. We demonstrate that training the\nclassifier to distinguish different prediction behaviors reduces its reliance\non spurious correlations without knowing them a priori and outperforms prior\nmethods on five real-world datasets.\n'] , ['  Despite the rapid development of machine learning algorithms for domain\ngeneralization (DG), there is no clear empirical evidence that the existing DG\nalgorithms outperform the classic empirical risk minimization (ERM) across\nstandard benchmarks. To better understand this phenomenon, we investigate\nwhether there are benefits of DG algorithms over ERM through the lens of label\nnoise. Specifically, our finite-sample analysis reveals that label noise\nexacerbates the effect of spurious correlations for ERM, undermining\ngeneralization. Conversely, we illustrate that DG algorithms exhibit implicit\nlabel-noise robustness during finite-sample training even when spurious\ncorrelation is present. Such desirable property helps mitigate spurious\ncorrelations and improve generalization in synthetic experiments. However,\nadditional comprehensive experiments on real-world benchmark datasets indicate\nthat label-noise robustness does not necessarily translate to better\nperformance compared to ERM. We conjecture that the failure mode of ERM arising\nfrom spurious correlations may be less pronounced in practice.\n', ""  Neural networks trained with (stochastic) gradient descent have an inductive\nbias towards learning simpler solutions. This makes them highly prone to\nlearning spurious correlations in the training data, that may not hold at test\ntime. In this work, we provide the first theoretical analysis of the effect of\nsimplicity bias on learning spurious correlations. Notably, we show that\nexamples with spurious features are provably separable based on the model's\noutput early in training. We further illustrate that if spurious features have\na small enough noise-to-signal ratio, the network's output on the majority of\nexamples is almost exclusively determined by the spurious features, leading to\npoor worst-group test accuracy. Finally, we propose SPARE, which identifies\nspurious correlations early in training and utilizes importance sampling to\nalleviate their effect. Empirically, we demonstrate that SPARE outperforms\nstate-of-the-art methods by up to 21.1% in worst-group accuracy, while being up\nto 12x faster. We also show that SPARE is a highly effective but lightweight\nmethod to discover spurious correlations.\n"", '  Existing research often posits spurious features as easier to learn than core\nfeatures in neural network optimization, but the impact of their relative\nsimplicity remains under-explored. Moreover, studies mainly focus on end\nperformance rather than the learning dynamics of feature learning. In this\npaper, we propose a theoretical framework and an associated synthetic dataset\ngrounded in boolean function analysis. This setup allows for fine-grained\ncontrol over the relative complexity (compared to core features) and\ncorrelation strength (with respect to the label) of spurious features to study\nthe dynamics of feature learning under spurious correlations. Our findings\nuncover several interesting phenomena: (1) stronger spurious correlations or\nsimpler spurious features slow down the learning rate of the core features, (2)\ntwo distinct subnetworks are formed to learn core and spurious features\nseparately, (3) learning phases of spurious and core features are not always\nseparable, (4) spurious features are not forgotten even after core features are\nfully learned. We demonstrate that our findings justify the success of\nretraining the last layer to remove spurious correlation and also identifies\nlimitations of popular debiasing algorithms that exploit early learning of\nspurious features. We support our empirical findings with theoretical analyses\nfor the case of learning XOR features with a one-hidden-layer ReLU network.\n']",Mitigating Spurious Correlations in Machine Learning,"""Mitigating Spurious Correlations in Machine Learning Classifiers"""
227,"Data Valuation with Machine Learning , SHAP Scores and Feature Attributions for Machine Learning , ""Deep Learning Attribution Methods""","['valuation', 'valuing', 'shapley', 'value', 'data', 'values', 'ml', 'quantifying', 'usefulness', 'estimate'] , ['shap', 'shapley', 'ranking', 'feature', 'generalising', 'probabilistic', 'kernelshap', 'attributions', 'rank', 'complexity'] , ['attributions', 'attribution', 'classifiers', 'classification', 'neural', 'learning', 'deep', 'datasets', 'ensemble', 'deeprepviz']","['  As data plays an increasingly pivotal role in decision-making, the emergence\nof data markets underscores the growing importance of data valuation. Within\nthe machine learning landscape, Data Shapley stands out as a widely embraced\nmethod for data valuation. However, a limitation of Data Shapley is its\nassumption of a fixed dataset, contrasting with the dynamic nature of\nreal-world applications where data constantly evolves and expands. This paper\nestablishes the relationship between Data Shapley and infinite-order\nU-statistics and addresses this limitation by quantifying the uncertainty of\nData Shapley with changes in data distribution from the perspective of\nU-statistics. We make statistical inferences on data valuation to obtain\nconfidence intervals for the estimations. We construct two different algorithms\nto estimate this uncertainty and provide recommendations for their applicable\nsituations. We also conduct a series of experiments on various datasets to\nverify asymptotic normality and propose a practical trading scenario enabled by\nthis method.\n', '  Data valuation has garnered increasing attention in recent years, given the\ncritical role of high-quality data in various applications, particularly in\nmachine learning tasks. There are diverse technical avenues to quantify the\nvalue of data within a corpus. While Shapley value-based methods are among the\nmost widely used techniques in the literature due to their solid theoretical\nfoundation, the accurate calculation of Shapley values is often intractable,\nleading to the proposal of numerous approximated calculation methods. Despite\nsignificant progress, nearly all existing methods overlook the utilization of\ndistribution information of values within a data corpus. In this paper, we\ndemonstrate that both global and local statistical information of value\ndistributions hold significant potential for data valuation within the context\nof machine learning. Firstly, we explore the characteristics of both global and\nlocal value distributions across several simulated and real data corpora.\nUseful observations and clues are obtained. Secondly, we propose a new data\nvaluation method that estimates Shapley values by incorporating the explored\ndistribution characteristics into an existing method, AME. Thirdly, we present\na new path to address the dynamic data valuation problem by formulating an\noptimization problem that integrates information of both global and local value\ndistributions. Extensive experiments are conducted on Shapley value estimation,\nvalue-based data removal/adding, mislabeled data detection, and\nincremental/decremental data valuation. The results showcase the effectiveness\nand efficiency of our proposed methodologies, affirming the significant\npotential of global and local value distributions in data valuation.\n', '  Measuring the value of individual samples is critical for many data-driven\ntasks, e.g., the training of a deep learning model. Recent literature witnesses\nthe substantial efforts in developing data valuation methods. The primary data\nvaluation methodology is based on the Shapley value from game theory, and\nvarious methods are proposed along this path. {Even though Shapley value-based\nvaluation has solid theoretical basis, it is entirely an experiment-based\napproach and no valuation model has been constructed so far.} In addition,\ncurrent data valuation methods ignore the interpretability of the output\nvalues, despite an interptable data valuation method is of great helpful for\napplications such as data pricing. This study aims to answer an important\nquestion: is data valuation learnable and interpretable? A learned valuation\nmodel have several desirable merits such as fixed number of parameters and\nknowledge reusability. An intrepretable data valuation model can explain why a\nsample is valuable or invaluable. To this end, two new data value modeling\nframeworks are proposed, in which a multi-layer perception~(MLP) and a new\nregression tree are utilized as specific base models for model training and\ninterpretability, respectively. Extensive experiments are conducted on\nbenchmark datasets. {The experimental results provide a positive answer for the\nquestion.} Our study opens up a new technical path for the assessing of data\nvalues. Large data valuation models can be built across many different\ndata-driven tasks, which can promote the widespread application of data\nvaluation.\n'] , ['  Recent work uncovered examples of classifiers for which SHAP scores yield\nmisleading feature attributions. While such examples might be perceived as\nsuggesting the inadequacy of Shapley values for explainability, this paper\nshows that the source of the identified shortcomings of SHAP scores resides\nelsewhere. Concretely, the paper makes the case that the failings of SHAP\nscores result from the characteristic functions used in earlier works.\nFurthermore, the paper identifies a number of properties that characteristic\nfunctions ought to respect, and proposes several novel characteristic\nfunctions, each exhibiting one or more of the desired properties. More\nimportantly, some of the characteristic functions proposed in this paper are\nguaranteed not to exhibit any of the shortcomings uncovered by earlier work.\nThe paper also investigates the impact of the new characteristic functions on\nthe complexity of computing SHAP scores. Finally, the paper proposes\nmodifications to the tool SHAP to use instead one of our novel characteristic\nfunctions, thereby eliminating some of the limitations reported for SHAP\nscores.\n', '  Attribution scores reflect how important the feature values in an input\nentity are for the output of a machine learning model. One of the most popular\nattribution scores is the SHAP score, which is an instantiation of the general\nShapley value used in coalition game theory. The definition of this score\nrelies on a probability distribution on the entity population. Since the exact\ndistribution is generally unknown, it needs to be assigned subjectively or be\nestimated from data, which may lead to misleading feature scores. In this\npaper, we propose a principled framework for reasoning on SHAP scores under\nunknown entity population distributions. In our framework, we consider an\nuncertainty region that contains the potential distributions, and the SHAP\nscore of a feature becomes a function defined over this region. We study the\nbasic problems of finding maxima and minima of this function, which allows us\nto determine tight ranges for the SHAP scores of all features. In particular,\nwe pinpoint the complexity of these problems, and other related ones, showing\nthem to be NP-complete. Finally, we present experiments on a real-world\ndataset, showing that our framework may contribute to a more robust feature\nscoring.\n', '  Several works propose various post-hoc, model-agnostic explanations for the\ntask of ranking, i.e. the task of ordering a set of documents, via feature\nattribution methods. However, these attributions are seen to weakly correlate\nand sometimes contradict each other. In classification/regression, several\nworks focus on \\emph{axiomatic characterization} of feature attribution\nmethods, showing that a certain method uniquely satisfies a set of desirable\nproperties. However, no such efforts have been taken in the space of feature\nattributions for the task of ranking. We take an axiomatic game-theoretic\napproach, popular in the feature attribution community, to identify candidate\nattribution methods for ranking tasks. We first define desirable axioms:\nRank-Efficiency, Rank-Missingness, Rank-Symmetry and Rank-Monotonicity, all\nvariants of the classical Shapley axioms. Next, we introduce Rank-SHAP, a\nfeature attribution algorithm for the general ranking task, which is an\nextension to classical Shapley values. We identify a polynomial-time algorithm\nfor computing approximate Rank-SHAP values and evaluate the computational\nefficiency and accuracy of our algorithm under various scenarios. We also\nevaluate its alignment with human intuition with a user study. Lastly, we\ntheoretically examine popular rank attribution algorithms, EXS and Rank-LIME,\nand evaluate their capacity to satisfy the classical Shapley axioms.\n'] , ['  Local data attribution (or influence estimation) techniques aim at estimating\nthe impact that individual data points seen during training have on particular\npredictions of an already trained Machine Learning model during test time.\nPrevious methods either do not perform well consistently across different\nevaluation criteria from literature, are characterized by a high computational\ndemand, or suffer from both. In this work we present DualView, a novel method\nfor post-hoc data attribution based on surrogate modelling, demonstrating both\nhigh computational efficiency, as well as good evaluation results. With a focus\non neural networks, we evaluate our proposed technique using suitable\nquantitative evaluation strategies from the literature against related\nprincipal local data attribution methods. We find that DualView requires\nconsiderably lower computational resources than other methods, while\ndemonstrating comparable performance to competing approaches across evaluation\nmetrics. Futhermore, our proposed method produces sparse explanations, where\nsparseness can be tuned via a hyperparameter. Finally, we showcase that with\nDualView, we can now render explanations from local data attributions\ncompatible with established local feature attribution methods: For each\nprediction on (test) data points explained in terms of impactful samples from\nthe training set, we are able to compute and visualize how the prediction on\n(test) sample relates to each influential training sample in terms of features\nrecognized and by the model. We provide an Open Source implementation of\nDualView online, together with implementations for all other local data\nattribution methods we compare against, as well as the metrics reported here,\nfor full reproducibility.\n', ""  Deep neural networks are very successful on many vision tasks, but hard to\ninterpret due to their black box nature. To overcome this, various post-hoc\nattribution methods have been proposed to identify image regions most\ninfluential to the models' decisions. Evaluating such methods is challenging\nsince no ground truth attributions exist. We thus propose three novel\nevaluation schemes to more reliably measure the faithfulness of those methods,\nto make comparisons between them more fair, and to make visual inspection more\nsystematic. To address faithfulness, we propose a novel evaluation setting\n(DiFull) in which we carefully control which parts of the input can influence\nthe output in order to distinguish possible from impossible attributions. To\naddress fairness, we note that different methods are applied at different\nlayers, which skews any comparison, and so evaluate all methods on the same\nlayers (ML-Att) and discuss how this impacts their performance on quantitative\nmetrics. For more systematic visualizations, we propose a scheme (AggAtt) to\nqualitatively evaluate the methods on complete datasets. We use these\nevaluation schemes to study strengths and shortcomings of some widely used\nattribution methods. Finally, we propose a post-processing smoothing step that\nsignificantly improves the performance of some attribution methods, and discuss\nits applicability.\n"", ""  Deep neural networks are very successful on many vision tasks, but hard to\ninterpret due to their black box nature. To overcome this, various post-hoc\nattribution methods have been proposed to identify image regions most\ninfluential to the models' decisions. Evaluating such methods is challenging\nsince no ground truth attributions exist. We thus propose three novel\nevaluation schemes to more reliably measure the faithfulness of those methods,\nto make comparisons between them more fair, and to make visual inspection more\nsystematic. To address faithfulness, we propose a novel evaluation setting\n(DiFull) in which we carefully control which parts of the input can influence\nthe output in order to distinguish possible from impossible attributions. To\naddress fairness, we note that different methods are applied at different\nlayers, which skews any comparison, and so evaluate all methods on the same\nlayers (ML-Att) and discuss how this impacts their performance on quantitative\nmetrics. For more systematic visualizations, we propose a scheme (AggAtt) to\nqualitatively evaluate the methods on complete datasets. We use these\nevaluation schemes to study strengths and shortcomings of some widely used\nattribution methods over a wide range of models. Finally, we propose a\npost-processing smoothing step that significantly improves the performance of\nsome attribution methods, and discuss its applicability.\n""]",Interpretable Machine Learning and Data Valuation,SHAP Scores and Feature Attributions for Machine Learning
228,"Machine Learning Theory and Generalization , Compositional Learning and Generalization in AI Models","['classifiers', 'learning', 'classification', 'classifier', 'generalization', 'regularization', 'optimal', 'memorization', 'minimax', 'distributionally'] , ['compositionality', 'compositional', 'compositionally', 'compositions', 'composition', 'generalization', 'neural', 'representations', 'generalize', 'cognition']","[""  This paper introduces General Distribution Learning (GD learning), a novel\ntheoretical learning framework designed to address a comprehensive range of\nmachine learning and statistical tasks, including classification, regression,\nand parameter estimation. GD learning focuses on estimating the true underlying\nprobability distribution of dataset and using models to fit the estimated\nparameters of the distribution. The learning error in GD learning is thus\ndecomposed into two distinct categories: estimation error and fitting error.\nThe estimation error, which stems from the constraints of finite sampling,\nlimited prior knowledge, and the estimation algorithm's inherent limitations,\nquantifies the discrepancy between the true distribution and its estimate. The\nfitting error can be attributed to model's capacity limitation and the\nperformance limitation of the optimization algorithm, which evaluates the\ndeviation of the model output from the fitted objective. To address the\nchallenge of non-convexity in the optimization of learning error, we introduce\nthe standard loss function and demonstrate that, when employing this function,\nglobal optimal solutions in non-convex optimization can be approached by\nminimizing the gradient norm and the structural error. Moreover, we demonstrate\nthat the estimation error is determined by the uncertainty of the estimate $q$,\nand propose the minimum uncertainty principle to obtain an optimal estimate of\nthe true distribution. We further provide upper bounds for the estimation\nerror, fitting error, and learning error within the GD learning framework.\nUltimately, our findings are applied to offer theoretical explanations for\nseveral unanswered questions on deep learning, including overparameterization,\nnon-convex optimization, flat minima, dynamic isometry condition and other\ntechniques in deep learning.\n"", '  Deep neural networks (DNNs) trained with the logistic loss (i.e., the cross\nentropy loss) have made impressive advancements in various binary\nclassification tasks. However, generalization analysis for binary\nclassification with DNNs and logistic loss remains scarce. The unboundedness of\nthe target function for the logistic loss is the main obstacle to deriving\nsatisfactory generalization bounds. In this paper, we aim to fill this gap by\nestablishing a novel and elegant oracle-type inequality, which enables us to\ndeal with the boundedness restriction of the target function, and using it to\nderive sharp convergence rates for fully connected ReLU DNN classifiers trained\nwith logistic loss. In particular, we obtain optimal convergence rates (up to\nlog factors) only requiring the H\\""older smoothness of the conditional class\nprobability $\\eta$ of data. Moreover, we consider a compositional assumption\nthat requires $\\eta$ to be the composition of several vector-valued functions\nof which each component function is either a maximum value function or a\nH\\""older smooth function only depending on a small number of its input\nvariables. Under this assumption, we derive optimal convergence rates (up to\nlog factors) which are independent of the input dimension of data. This result\nexplains why DNN classifiers can perform well in practical high-dimensional\nclassification problems. Besides the novel oracle-type inequality, the sharp\nconvergence rates given in our paper also owe to a tight error bound for\napproximating the natural logarithm function near zero (where it is unbounded)\nby ReLU DNNs. In addition, we justify our claims for the optimality of rates by\nproving corresponding minimax lower bounds. All these results are new in the\nliterature and will deepen our theoretical understanding of classification with\nDNNs.\n', ""  Transfer learning, or domain adaptation, is concerned with machine learning\nproblems in which training and testing data come from possibly different\nprobability distributions. In this work, we give an information-theoretic\nanalysis of the generalization error and excess risk of transfer learning\nalgorithms. Our results suggest, perhaps as expected, that the Kullback-Leibler\n(KL) divergence $D(\\mu\\|\\mu')$ plays an important role in the characterizations\nwhere $\\mu$ and $\\mu'$ denote the distribution of the training data and the\ntesting data, respectively. Specifically, we provide generalization error and\nexcess risk upper bounds for learning algorithms where data from both\ndistributions are available in the training phase. Recognizing that the bounds\ncould be sub-optimal in general, we provide improved excess risk upper bounds\nfor a certain class of algorithms, including the empirical risk minimization\n(ERM) algorithm, by making stronger assumptions through the \\textit{central\ncondition}. To demonstrate the usefulness of the bounds, we further extend the\nanalysis to the Gibbs algorithm and the noisy stochastic gradient descent\nmethod. We then generalize the mutual information bound with other divergences\nsuch as $\\phi$-divergence and Wasserstein distance, which may lead to tighter\nbounds and can handle the case when $\\mu$ is not absolutely continuous with\nrespect to $\\mu'$. Several numerical results are provided to demonstrate our\ntheoretical findings. Lastly, to address the problem that the bounds are often\nnot directly applicable in practice due to the absence of the distributional\nknowledge of the data, we develop an algorithm (called InfoBoost) that\ndynamically adjusts the importance weights for both source and target data\nbased on certain information measures. The empirical results show the\neffectiveness of the proposed algorithm.\n""] , ['  Compositional learning, mastering the ability to combine basic concepts and\nconstruct more intricate ones, is crucial for human cognition, especially in\nhuman language comprehension and visual perception. This notion is tightly\nconnected to generalization over unobserved situations. Despite its integral\nrole in intelligence, there is a lack of systematic theoretical and\nexperimental research methodologies, making it difficult to analyze the\ncompositional learning abilities of computational models. In this paper, we\nsurvey the literature on compositional learning of AI models and the\nconnections made to cognitive studies. We identify abstract concepts of\ncompositionality in cognitive and linguistic studies and connect these to the\ncomputational challenges faced by language and vision models in compositional\nreasoning. We overview the formal definitions, tasks, evaluation benchmarks,\nvariety of computational models, and theoretical findings. We cover modern\nstudies on large language models to provide a deeper understanding of the\ncutting-edge compositional capabilities exhibited by state-of-the-art AI models\nand pinpoint important directions for future research.\n', '  Compositional generalization (the ability to respond correctly to novel\ncombinations of familiar components) is thought to be a cornerstone of\nintelligent behavior. Compositionally structured (e.g. disentangled)\nrepresentations are essential for this; however, the conditions under which\nthey yield compositional generalization remain unclear. To address this gap, we\npresent a general theory of compositional generalization in kernel models with\nfixed, potentially nonlinear representations (which also applies to neural\nnetworks in the ""lazy regime""). We prove that these models are functionally\nlimited to adding up values assigned to conjunctions/combinations of components\nthat have been seen during training (""conjunction-wise additivity""), and\nidentify novel compositionality failure modes that arise from the data and\nmodel structure, even for disentangled inputs. For models in the representation\nlearning (or ""rich"") regime, we show that networks can generalize on an\nimportant non-additive task (associative inference), and give a mechanistic\nexplanation for why. Finally, we validate our theory empirically, showing that\nit captures the behavior of deep neural networks trained on a set of\ncompositional tasks. In sum, our theory characterizes the principles giving\nrise to compositional generalization in kernel models and shows how\nrepresentation learning can overcome their limitations. We further provide a\nformally grounded, novel generalization class for compositional tasks that\nhighlights fundamental differences in the required learning mechanisms\n(conjunction-wise additivity).\n', '  Compositionality is thought to be a key component of language, and various\ncompositional benchmarks have been developed to empirically probe the\ncompositional generalization of existing sequence processing models. These\nbenchmarks often highlight failures of existing models, but it is not clear why\nthese models fail in this way. In this paper, we seek to theoretically\nunderstand the role the compositional structure of the models plays in these\nfailures and how this structure relates to their expressivity and sample\ncomplexity. We propose a general neuro-symbolic definition of compositional\nfunctions and their compositional complexity. We then show how various existing\ngeneral and special purpose sequence processing models (such as recurrent,\nconvolution and attention-based ones) fit this definition and use it to analyze\ntheir compositional complexity. Finally, we provide theoretical guarantees for\nthe expressivity and systematic generalization of compositional models that\nexplicitly depend on our proposed definition and highlighting factors which\ndrive poor empirical performance.\n']",Machine Learning Foundations and Generalization,Machine Learning Theory and Generalization
229,"Machine Learning Model Development and Dataset Documentation , Machine Learning in Software Engineering","['documentation', 'datasets', 'dataset', 'researchers', 'developers', 'categories', 'repositories', 'frameworks', 'tools', 'guidelines'] , ['learnware', 'software', 'tools', 'mlops', 'projects', 'ai', 'development', 'engineering', 'tooling', 'learning']","['  Hugging Face (HF) has established itself as a crucial platform for the\ndevelopment and sharing of machine learning (ML) models. This repository mining\nstudy, which delves into more than 380,000 models using data gathered via the\nHF Hub API, aims to explore the community engagement, evolution, and\nmaintenance around models hosted on HF, aspects that have yet to be\ncomprehensively explored in the literature. We first examine the overall growth\nand popularity of HF, uncovering trends in ML domains, framework usage, authors\ngrouping and the evolution of tags and datasets used. Through text analysis of\nmodel card descriptions, we also seek to identify prevalent themes and insights\nwithin the developer community. Our investigation further extends to the\nmaintenance aspects of models, where we evaluate the maintenance status of ML\nmodels, classify commit messages into various categories (corrective,\nperfective, and adaptive), analyze the evolution across development stages of\ncommits metrics and introduce a new classification system that estimates the\nmaintenance status of models based on multiple attributes. This study aims to\nprovide valuable insights about ML model maintenance and evolution that could\ninform future model development strategies on platforms like HF.\n', ""  Advances in machine learning are closely tied to the creation of datasets.\nWhile data documentation is widely recognized as essential to the reliability,\nreproducibility, and transparency of ML, we lack a systematic empirical\nunderstanding of current dataset documentation practices. To shed light on this\nquestion, here we take Hugging Face -- one of the largest platforms for sharing\nand collaborating on ML models and datasets -- as a prominent case study. By\nanalyzing all 7,433 dataset documentation on Hugging Face, our investigation\nprovides an overview of the Hugging Face dataset ecosystem and insights into\ndataset documentation practices, yielding 5 main findings: (1) The dataset card\ncompletion rate shows marked heterogeneity correlated with dataset popularity.\n(2) A granular examination of each section within the dataset card reveals that\nthe practitioners seem to prioritize Dataset Description and Dataset Structure\nsections, while the Considerations for Using the Data section receives the\nlowest proportion of content. (3) By analyzing the subsections within each\nsection and utilizing topic modeling to identify key topics, we uncover what is\ndiscussed in each section, and underscore significant themes encompassing both\ntechnical and social impacts, as well as limitations within the Considerations\nfor Using the Data section. (4) Our findings also highlight the need for\nimproved accessibility and reproducibility of datasets in the Usage sections.\n(5) In addition, our human annotation evaluation emphasizes the pivotal role of\ncomprehensive dataset content in shaping individuals' perceptions of a dataset\ncard's overall quality. Overall, our study offers a unique perspective on\nanalyzing dataset documentation through large-scale data science analysis and\nunderlines the need for more thorough dataset documentation in machine learning\nresearch.\n"", '  The rapidly evolving fields of Machine Learning (ML) and Artificial\nIntelligence have witnessed the emergence of platforms like Hugging Face (HF)\nas central hubs for model development and sharing. This experience report\nsynthesizes insights from two comprehensive studies conducted on HF, focusing\non carbon emissions and the evolutionary and maintenance aspects of ML models.\nOur objective is to provide a practical guide for future researchers embarking\non mining software repository studies within the HF ecosystem to enhance the\nquality of these studies. We delve into the intricacies of the replication\npackage used in our studies, highlighting the pivotal tools and methodologies\nthat facilitated our analysis. Furthermore, we propose a nuanced stratified\nsampling strategy tailored for the diverse HF Hub dataset, ensuring a\nrepresentative and comprehensive analytical approach. The report also\nintroduces preliminary guidelines, transitioning from repository mining to\ncohort studies, to establish causality in repository mining studies,\nparticularly within the ML model of HF context. This transition is inspired by\nexisting frameworks and is adapted to suit the unique characteristics of the HF\nmodel ecosystem. Our report serves as a guiding framework for researchers,\ncontributing to the responsible and sustainable advancement of ML, and\nfostering a deeper understanding of the broader implications of ML models.\n'] , ['  Continuous Integration (CI) is a well-established practice in traditional\nsoftware development, but its nuances in the domain of Machine Learning (ML)\nprojects remain relatively unexplored. Given the distinctive nature of ML\ndevelopment, understanding how CI practices are adopted in this context is\ncrucial for tailoring effective approaches. In this study, we conduct a\ncomprehensive analysis of 185 open-source projects on GitHub (93 ML and 92\nnon-ML projects). Our investigation comprises both quantitative and qualitative\ndimensions, aiming to uncover differences in CI adoption between ML and non-ML\nprojects. Our findings indicate that ML projects often require longer build\ndurations, and medium-sized ML projects exhibit lower test coverage compared to\nnon-ML projects. Moreover, small and medium-sized ML projects show a higher\nprevalence of increasing build duration trends compared to their non-ML\ncounterparts. Additionally, our qualitative analysis illuminates the\ndiscussions around CI in both ML and non-ML projects, encompassing themes like\nCI Build Execution and Status, CI Testing, and CI Infrastructure. These\ninsights shed light on the unique challenges faced by ML projects in adopting\nCI practices effectively.\n', '  Software engineering (SE) is a dynamic field that involves multiple phases\nall of which are necessary to develop sustainable software systems. Machine\nlearning (ML), a branch of artificial intelligence (AI), has drawn a lot of\nattention in recent years thanks to its ability to analyze massive volumes of\ndata and extract useful patterns from data. Several studies have focused on\nexamining, categorising, and assessing the application of ML in SE processes.\nWe conducted a literature review on primary studies to address this gap. The\nstudy was carried out following the objective and the research questions to\nexplore the current state of the art in applying machine learning techniques in\nsoftware engineering processes. The review identifies the key areas within\nsoftware engineering where ML has been applied, including software quality\nassurance, software maintenance, software comprehension, and software\ndocumentation. It also highlights the specific ML techniques that have been\nleveraged in these domains, such as supervised learning, unsupervised learning,\nand deep learning.\n  Keywords: machine learning, deep learning, software engineering, natural\nlanguage processing, source code\n', '  The rise of machine learning (ML) and its embedding in systems has\ndrastically changed the engineering of software-intensive systems.\nTraditionally, software engineering focuses on manually created artifacts such\nas source code and the process of creating them, as well as best practices for\nintegrating them, i.e., software architectures. In contrast, the development of\nML artifacts, i.e. ML models, comes from data science and focuses on the ML\nmodels and their training data. However, to deliver value to end users, these\nML models must be embedded in traditional software, often forming complex\ntopologies. In fact, ML-enabled software can easily incorporate many different\nML models. While the challenges and practices of building ML-enabled systems\nhave been studied to some extent, beyond isolated examples, little is known\nabout the characteristics of real-world ML-enabled systems. Properly embedding\nML models in systems so that they can be easily maintained or reused is far\nfrom trivial. We need to improve our empirical understanding of such systems,\nwhich we address by presenting the first large-scale study of real ML-enabled\nsoftware systems, covering over 2,928 open source systems on GitHub. We\nclassified and analyzed them to determine their characteristics, as well as\ntheir practices for reusing ML models and related code, and the architecture of\nthese systems. Our findings provide practitioners and researchers with insight\ninto practices for embedding and integrating ML models, bringing data science\nand software engineering closer together.\n']",Machine Learning in Software Development and Engineering,Machine Learning Model Development and Dataset Documentation
230,"Stochastic Gradient Descent Optimizers , Distributed Optimization with Gradient Compression , Distributed Deep Learning Optimization , Asynchronous Distributed Stochastic Gradient Descent","['optimizers', 'sgd', 'optimizer', 'gradient', 'gradients', 'adaptive', 'optimization', 'stochastic', 'adam', 'hessian'] , ['distributed', 'compression', 'optimization', 'sgd', 'minimizer', 'compressed', 'gradient', 'bottleneck', 'gradients', 'optimal'] , ['parallelization', 'unifiednn', 'distributed', 'throughput', 'bottleneck', 'cloud', 'streaming', 'memory', 'network', 'deep'] , ['sgd', 'asynch', 'distributed', 'asynchronous', 'synchronization', 'stragglers', 'synchronous', 'sgbdt', 'parallelism', 'gradient']","['  For nonconvex objective functions, including deep neural networks, stochastic\ngradient descent (SGD) with momentum has fast convergence and excellent\ngeneralizability, but a theoretical explanation for this is lacking. In\ncontrast to previous studies that defined the stochastic noise that occurs\nduring optimization as the variance of the stochastic gradient, we define it as\nthe gap between the search direction of the optimizer and the steepest descent\ndirection and show that its level dominates generalizability of the model. We\nalso show that the stochastic noise in SGD with momentum smoothes the objective\nfunction, the degree of which is determined by the learning rate, the batch\nsize, the momentum factor, the variance of the stochastic gradient, and the\nupper bound of the gradient norm. By numerically deriving the stochastic noise\nlevel in SGD and SGD with momentum, we provide theoretical findings that help\nexplain the training dynamics of SGD with momentum, which were not explained by\nprevious studies on convergence and stability. We also provide experimental\nresults supporting our assertion that model generalizability depends on the\nstochastic noise level.\n', '  It is known that the standard stochastic gradient descent (SGD) optimization\nmethod, as well as accelerated and adaptive SGD optimization methods such as\nthe Adam optimizer fail to converge if the learning rates do not converge to\nzero (as, for example, in the situation of constant learning rates). Numerical\nsimulations often use human-tuned deterministic learning rate schedules or\nsmall constant learning rates. The default learning rate schedules for SGD\noptimization methods in machine learning implementation frameworks such as\nTensorFlow and Pytorch are constant learning rates. In this work we propose and\nstudy a learning-rate-adaptive approach for SGD optimization methods in which\nthe learning rate is adjusted based on empirical estimates for the values of\nthe objective function of the considered optimization problem (the function\nthat one intends to minimize). In particular, we propose a\nlearning-rate-adaptive variant of the Adam optimizer and implement it in case\nof several neural network learning problems, particularly, in the context of\ndeep learning approximation methods for partial differential equations such as\ndeep Kolmogorov methods, physics-informed neural networks, and deep Ritz\nmethods. In each of the presented learning problems the proposed\nlearning-rate-adaptive variant of the Adam optimizer faster reduces the value\nof the objective function than the Adam optimizer with the default learning\nrate. For a simple class of quadratic minimization problems we also rigorously\nprove that a learning-rate-adaptive variant of the SGD optimization method\nconverges to the minimizer of the considered minimization problem. Our\nconvergence proof is based on an analysis of the laws of invariant measures of\nthe SGD method as well as on a more general convergence analysis for SGD with\nrandom but predictable learning rates which we develop in this work.\n', '  Adaptive gradient-descent optimizers are the standard choice for training\nneural network models. Despite their faster convergence than gradient-descent\nand remarkable performance in practice, the adaptive optimizers are not as well\nunderstood as vanilla gradient-descent. A reason is that the dynamic update of\nthe learning rate that helps in faster convergence of these methods also makes\ntheir analysis intricate. Particularly, the simple gradient-descent method\nconverges at a linear rate for a class of optimization problems, whereas the\npractically faster adaptive gradient methods lack such a theoretical guarantee.\nThe Polyak-{\\L}ojasiewicz (PL) inequality is the weakest known class, for which\nlinear convergence of gradient-descent and its momentum variants has been\nproved. Therefore, in this paper, we prove that AdaGrad and Adam, two\nwell-known adaptive gradient methods, converge linearly when the cost function\nis smooth and satisfies the PL inequality. Our theoretical framework follows a\nsimple and unified approach, applicable to both batch and stochastic gradients,\nwhich can potentially be utilized in analyzing linear convergence of other\nvariants of Adam.\n'] , ['  In the last few years, various communication compression techniques have\nemerged as an indispensable tool helping to alleviate the communication\nbottleneck in distributed learning. However, despite the fact biased\ncompressors often show superior performance in practice when compared to the\nmuch more studied and understood unbiased compressors, very little is known\nabout them. In this work we study three classes of biased compression\noperators, two of which are new, and their performance when applied to\n(stochastic) gradient descent and distributed (stochastic) gradient descent. We\nshow for the first time that biased compressors can lead to linear convergence\nrates both in the single node and distributed settings. We prove that\ndistributed compressed SGD method, employed with error feedback mechanism,\nenjoys the ergodic rate $O\\left( \\delta L \\exp \\left[-\\frac{\\mu K}{\\delta\nL}\\right] + \\frac{(C + \\delta D)}{K\\mu}\\right)$, where $\\delta\\ge 1$ is a\ncompression parameter which grows when more compression is applied, $L$ and\n$\\mu$ are the smoothness and strong convexity constants, $C$ captures\nstochastic gradient noise ($C=0$ if full gradients are computed on each node)\nand $D$ captures the variance of the gradients at the optimum ($D=0$ for\nover-parameterized models). Further, via a theoretical study of several\nsynthetic and empirical distributions of communicated gradients, we shed light\non why and by how much biased compressors outperform their unbiased variants.\nFinally, we propose several new biased compressors with promising theoretical\nguarantees and practical performance.\n', '  Communication compression is a common technique in distributed optimization\nthat can alleviate communication overhead by transmitting compressed gradients\nand model parameters. However, compression can introduce information\ndistortion, which slows down convergence and incurs more communication rounds\nto achieve desired solutions. Given the trade-off between lower per-round\ncommunication costs and additional rounds of communication, it is unclear\nwhether communication compression reduces the total communication cost.\n  This paper explores the conditions under which unbiased compression, a widely\nused form of compression, can reduce the total communication cost, as well as\nthe extent to which it can do so. To this end, we present the first theoretical\nformulation for characterizing the total communication cost in distributed\noptimization with communication compression. We demonstrate that unbiased\ncompression alone does not necessarily save the total communication cost, but\nthis outcome can be achieved if the compressors used by all workers are further\nassumed independent. We establish lower bounds on the communication rounds\nrequired by algorithms using independent unbiased compressors to minimize\nsmooth convex functions and show that these lower bounds are tight by refining\nthe analysis for ADIANA. Our results reveal that using independent unbiased\ncompression can reduce the total communication cost by a factor of up to\n$\\Theta(\\sqrt{\\min\\{n, \\kappa\\}})$ when all local smoothness constants are\nconstrained by a common upper bound, where $n$ is the number of workers and\n$\\kappa$ is the condition number of the functions being minimized. These\ntheoretical findings are supported by experimental results.\n', '  We propose a novel algorithm for distributed stochastic gradient descent\n(SGD) with compressed gradient communication in the parameter-server framework.\nOur gradient compression technique, named flattened one-bit stochastic gradient\ndescent (FO-SGD), relies on two simple algorithmic ideas: (i) a one-bit\nquantization procedure leveraging the technique of dithering, and (ii) a\nrandomized fast Walsh-Hadamard transform to flatten the stochastic gradient\nbefore quantization. As a result, the approximation of the true gradient in\nthis scheme is biased, but it prevents commonly encountered algorithmic\nproblems, such as exploding variance in the one-bit compression regime,\ndeterioration of performance in the case of sparse gradients, and restrictive\nassumptions on the distribution of the stochastic gradients. In fact, we show\nSGD-like convergence guarantees under mild conditions. The compression\ntechnique can be used in both directions of worker-server communication,\ntherefore admitting distributed optimization with full communication\ncompression.\n'] , ['  With rapidly increasing distributed deep learning workloads in large-scale\ndata centers, efficient distributed deep learning framework strategies for\nresource allocation and workload scheduling have become the key to\nhigh-performance deep learning. The large-scale environment with large volumes\nof datasets, models, and computational and communication resources raises\nvarious unique challenges for resource allocation and workload scheduling in\ndistributed deep learning, such as scheduling complexity, resource and workload\nheterogeneity, and fault tolerance. To uncover these challenges and\ncorresponding solutions, this survey reviews the literature, mainly from 2019\nto 2024, on efficient resource allocation and workload scheduling strategies\nfor large-scale distributed DL. We explore these strategies by focusing on\nvarious resource types, scheduling granularity levels, and performance goals\nduring distributed training and inference processes. We highlight critical\nchallenges for each topic and discuss key insights of existing technologies. To\nillustrate practical large-scale resource allocation and workload scheduling in\nreal distributed deep learning scenarios, we use a case study of training large\nlanguage models. This survey aims to encourage computer science, artificial\nintelligence, and communications researchers to understand recent advances and\nexplore future research directions for efficient framework strategies for\nlarge-scale distributed deep learning.\n', '  The past few years have witnessed the flourishing of large-scale deep neural\nnetwork models with ever-growing parameter numbers. Training such large-scale\nmodels typically requires massive memory and computing resources that exceed\nthose of a single GPU, necessitating distributed training. As GPU performance\nhas rapidly evolved in recent years, computation time has shrunk, thereby\nincreasing the proportion of communication in the overall training time.\nTherefore, optimizing communication for distributed training has become an\nurgent issue. In this article, we briefly introduce the general architecture of\ndistributed deep neural network training and analyze relationships among\nParallelization Strategy, Collective Communication Library, and Network from\nthe perspective of communication optimization, which forms a three-layer\nparadigm. We then review current representative research advances with this\nthree-layer paradigm. We find that layers in the current three-layer paradigm\nare relatively independent, but there is a rich design space for cross-layer\ncollaborative optimization in distributed training scenarios. Therefore, we\nfurther advocate a communication-efficient five-layer paradigm underlining\nopportunities for collaboration designs and look forward to the perspectives of\n""Vertical"", ""Horizontal"", ""Intra-Inter"" and ""Host-Net"" collaboration designs.\nWe hope this article can shed some light on future research on communication\noptimization for distributed training.\n', '  With the rapid growth in the volume of data sets, models, and devices in the\ndomain of deep learning, there is increasing attention on large-scale\ndistributed deep learning. In contrast to traditional distributed deep\nlearning, the large-scale scenario poses new challenges that include fault\ntolerance, scalability of algorithms and infrastructures, and heterogeneity in\ndata sets, models, and resources. Due to intensive synchronization of models\nand sharing of data across GPUs and computing nodes during distributed training\nand inference processes, communication efficiency becomes the bottleneck for\nachieving high performance at a large scale. This article surveys the\nliterature over the period of 2018-2023 on algorithms and technologies aimed at\nachieving efficient communication in large-scale distributed deep learning at\nvarious levels, including algorithms, frameworks, and infrastructures.\nSpecifically, we first introduce efficient algorithms for model synchronization\nand communication data compression in the context of large-scale distributed\ntraining. Next, we introduce efficient strategies related to resource\nallocation and task scheduling for use in distributed training and inference.\nAfter that, we present the latest technologies pertaining to modern\ncommunication infrastructures used in distributed deep learning with a focus on\nexamining the impact of the communication overhead in a large-scale and\nheterogeneous setting. Finally, we conduct a case study on the distributed\ntraining of large language models at a large scale to illustrate how to apply\nthese technologies in real cases. This article aims to offer researchers a\ncomprehensive understanding of the current landscape of large-scale distributed\ndeep learning and to reveal promising future research directions toward\ncommunication-efficient solutions in this scope.\n'] , [""  Distributed stochastic gradient descent (SGD) has attracted considerable\nrecent attention due to its potential for scaling computational resources,\nreducing training time, and helping protect user privacy in machine learning.\nHowever, the staggers and limited bandwidth may induce random\ncomputational/communication delays, thereby severely hindering the learning\nprocess. Therefore, how to accelerate asynchronous SGD by efficiently\nscheduling multiple workers is an important issue. In this paper, a unified\nframework is presented to analyze and optimize the convergence of asynchronous\nSGD based on stochastic delay differential equations (SDDEs) and the Poisson\napproximation of aggregated gradient arrivals. In particular, we present the\nrun time and staleness of distributed SGD without a memorylessness assumption\non the computation times. Given the learning rate, we reveal the relevant\nSDDE's damping coefficient and its delay statistics, as functions of the number\nof activated clients, staleness threshold, the eigenvalues of the Hessian\nmatrix of the objective function, and the overall computational/communication\ndelay. The formulated SDDE allows us to present both the distributed SGD's\nconvergence condition and speed by calculating its characteristic roots,\nthereby optimizing the scheduling policies for asynchronous/event-triggered\nSGD. It is interestingly shown that increasing the number of activated workers\ndoes not necessarily accelerate distributed SGD due to staleness. Moreover, a\nsmall degree of staleness does not necessarily slow down the convergence, while\na large degree of staleness will result in the divergence of distributed SGD.\nNumerical results demonstrate the potential of our SDDE framework, even in\ncomplex learning tasks with non-convex objective functions.\n"", ""  Local stochastic gradient descent (Local-SGD), also referred to as federated\naveraging, is an approach to distributed optimization where each device\nperforms more than one SGD update per communication. This work presents an\nempirical study of {\\it asynchronous} Local-SGD for training language models;\nthat is, each worker updates the global parameters as soon as it has finished\nits SGD steps. We conduct a comprehensive investigation by examining how worker\nhardware heterogeneity, model size, number of workers, and optimizer could\nimpact the learning performance. We find that with naive implementations,\nasynchronous Local-SGD takes more iterations to converge than its synchronous\ncounterpart despite updating the (global) model parameters more frequently. We\nidentify momentum acceleration on the global parameters when worker gradients\nare stale as a key challenge. We propose a novel method that utilizes a delayed\nNesterov momentum update and adjusts the workers' local training steps based on\ntheir computation speed. This approach, evaluated with models up to 150M\nparameters on the C4 dataset, matches the performance of synchronous Local-SGD\nin terms of perplexity per update step, and significantly surpasses it in terms\nof wall clock time.\n"", ""  We consider the distributed learning problem with data dispersed across\nmultiple workers under the orchestration of a central server. Asynchronous\nStochastic Gradient Descent (SGD) has been widely explored in such a setting to\nreduce the synchronization overhead associated with parallelization. However,\nthe performance of asynchronous SGD algorithms often depends on a bounded\ndissimilarity condition among the workers' local data, a condition that can\ndrastically affect their efficiency when the workers' data are highly\nheterogeneous. To overcome this limitation, we introduce the\n\\textit{dual-delayed asynchronous SGD (DuDe-ASGD)} algorithm designed to\nneutralize the adverse effects of data heterogeneity. DuDe-ASGD makes full use\nof stale stochastic gradients from all workers during asynchronous training,\nleading to two distinct time lags in the model parameters and data samples\nutilized in the server's iterations. Furthermore, by adopting an incremental\naggregation strategy, DuDe-ASGD maintains a per-iteration computational cost\nthat is on par with traditional asynchronous SGD algorithms. Our analysis\ndemonstrates that DuDe-ASGD achieves a near-minimax-optimal convergence rate\nfor smooth nonconvex problems, even when the data across workers are extremely\nheterogeneous. Numerical experiments indicate that DuDe-ASGD compares favorably\nwith existing asynchronous and synchronous SGD-based algorithms.\n""]",Optimization Methods for Distributed Deep Learning,Distributed Optimization with Gradient Compression
231,"Wasserstein Distance Metrics , Optimal Transport and Regularization Methods , ""Wasserstein Gradient Flows for Optimization""","['wasserstein', 'metrics', 'distances', 'metric', 'distance', 'measures', 'gaussian', 'distributions', 'optimal', 'embedding'] , ['transport', 'regularization', 'transportation', 'optimal', 'optimization', 'regularized', 'distributions', 'maps', 'divergence', 'densities'] , ['wasserstein', 'gradient', 'variational', 'divergence', 'divergences', 'generative', 'flow', 'minimization', 'stochastic', 'langevin']","[""  The $2$-Wasserstein distance is sensitive to minor geometric differences\nbetween distributions, making it a very powerful dissimilarity metric. However,\ndue to this sensitivity, a small outlier mass can also cause a significant\nincrease in the $2$-Wasserstein distance between two similar distributions.\nSimilarly, sampling discrepancy can cause the empirical $2$-Wasserstein\ndistance on $n$ samples in $\\mathbb{R}^2$ to converge to the true distance at a\nrate of $n^{-1/4}$, which is significantly slower than the rate of $n^{-1/2}$\nfor $1$-Wasserstein distance. We introduce a new family of distances\nparameterized by $k \\ge 0$, called $k$-RPW that is based on computing the\npartial $2$-Wasserstein distance. We show that (1) $k$-RPW satisfies the metric\nproperties, (2) $k$-RPW is robust to small outlier mass while retaining the\nsensitivity of $2$-Wasserstein distance to minor geometric differences, and (3)\nwhen $k$ is a constant, $k$-RPW distance between empirical distributions on $n$\nsamples in $\\mathbb{R}^2$ converges to the true distance at a rate of\n$n^{-1/3}$, which is faster than the convergence rate of $n^{-1/4}$ for the\n$2$-Wasserstein distance. Using the partial $p$-Wasserstein distance, we extend\nour distance to any $p \\in [1,\\infty]$. By setting parameters $k$ or $p$\nappropriately, we can reduce our distance to the total variation,\n$p$-Wasserstein, and the L\\'evy-Prokhorov distances. Experiments show that our\ndistance function achieves higher accuracy in comparison to the\n$1$-Wasserstein, $2$-Wasserstein, and TV distances for image retrieval tasks on\nnoisy real-world data sets.\n"", '  The sliced Wasserstein (SW) distances between two probability measures are\ndefined as the expectation of the Wasserstein distance between two\none-dimensional projections of the two measures. The randomness comes from a\nprojecting direction that is used to project the two input measures to one\ndimension. Due to the intractability of the expectation, Monte Carlo\nintegration is performed to estimate the value of the SW distance. Despite\nhaving various variants, there has been no prior work that improves the Monte\nCarlo estimation scheme for the SW distance in terms of controlling its\nvariance. To bridge the literature on variance reduction and the literature on\nthe SW distance, we propose computationally efficient control variates to\nreduce the variance of the empirical estimation of the SW distance. The key\nidea is to first find Gaussian approximations of projected one-dimensional\nmeasures, then we utilize the closed-form of the Wasserstein-2 distance between\ntwo Gaussian distributions to design the control variates. In particular, we\npropose using a lower bound and an upper bound of the Wasserstein-2 distance\nbetween two fitted Gaussians as two computationally efficient control variates.\nWe empirically show that the proposed control variate estimators can help to\nreduce the variance considerably when comparing measures over images and\npoint-clouds. Finally, we demonstrate the favorable performance of the proposed\ncontrol variate estimators in gradient flows to interpolate between two\npoint-clouds and in deep generative modeling on standard image datasets, such\nas CIFAR10 and CelebA.\n', '  While many Machine Learning methods were developed or transposed on\nRiemannian manifolds to tackle data with known non Euclidean geometry, Optimal\nTransport (OT) methods on such spaces have not received much attention. The\nmain OT tool on these spaces is the Wasserstein distance which suffers from a\nheavy computational burden. On Euclidean spaces, a popular alternative is the\nSliced-Wasserstein distance, which leverages a closed-form solution of the\nWasserstein distance in one dimension, but which is not readily available on\nmanifolds. In this work, we derive general constructions of Sliced-Wasserstein\ndistances on Cartan-Hadamard manifolds, Riemannian manifolds with non-positive\ncurvature, which include among others Hyperbolic spaces or the space of\nSymmetric Positive Definite matrices. Then, we propose different applications.\nAdditionally, we derive non-parametric schemes to minimize these new distances\nby approximating their Wasserstein gradient flows.\n'] , ['  Within the field of optimal transport (OT), the choice of ground cost is\ncrucial to ensuring that the optimality of a transport map corresponds to\nusefulness in real-world applications. It is therefore desirable to use known\ninformation to tailor cost functions and hence learn OT maps which are adapted\nto the problem at hand. By considering a class of neural ground costs whose\nMonge maps have a known form, we construct a differentiable Monge map estimator\nwhich can be optimized to be consistent with known information about an OT map.\nIn doing so, we simultaneously learn both an OT map estimator and a\ncorresponding adapted cost function. Through suitable choices of loss function,\nour method provides a general approach for incorporating prior information\nabout the Monge map itself when learning adapted OT maps and cost functions.\n', '  Entropic optimal transport (OT) and the Sinkhorn algorithm have made it\npractical for machine learning practitioners to perform the fundamental task of\ncalculating transport distance between statistical distributions. In this work,\nwe focus on a general class of OT problems under a combination of equality and\ninequality constraints. We derive the corresponding entropy regularization\nformulation and introduce a Sinkhorn-type algorithm for such constrained OT\nproblems supported by theoretical guarantees. We first bound the approximation\nerror when solving the problem through entropic regularization, which reduces\nexponentially with the increase of the regularization parameter. Furthermore,\nwe prove a sublinear first-order convergence rate of the proposed Sinkhorn-type\nalgorithm in the dual space by characterizing the optimization procedure with a\nLyapunov function. To achieve fast and higher-order convergence under weak\nentropy regularization, we augment the Sinkhorn-type algorithm with dynamic\nregularization scheduling and second-order acceleration. Overall, this work\nsystematically combines recent theoretical and numerical advances in entropic\noptimal transport with the constrained case, allowing practitioners to derive\napproximate transport plans in complex scenarios.\n', '  Optimal Transport (OT) problem investigates a transport map that bridges two\ndistributions while minimizing a given cost function. In this regard, OT\nbetween tractable prior distribution and data has been utilized for generative\nmodeling tasks. However, OT-based methods are susceptible to outliers and face\noptimization challenges during training. In this paper, we propose a novel\ngenerative model based on the semi-dual formulation of Unbalanced Optimal\nTransport (UOT). Unlike OT, UOT relaxes the hard constraint on distribution\nmatching. This approach provides better robustness against outliers, stability\nduring training, and faster convergence. We validate these properties\nempirically through experiments. Moreover, we study the theoretical upper-bound\nof divergence between distributions in UOT. Our model outperforms existing\nOT-based generative models, achieving FID scores of 2.97 on CIFAR-10 and 6.36\non CelebA-HQ-256. The code is available at\n\\url{https://github.com/Jae-Moo/UOTM}.\n'] , ['  We consider the optimization problem of minimizing a functional defined over\na family of probability distributions, where the objective functional is\nassumed to possess a variational form. Such a distributional optimization\nproblem arises widely in machine learning and statistics, with Monte-Carlo\nsampling, variational inference, policy optimization, and generative\nadversarial network as examples. For this problem, we propose a novel\nparticle-based algorithm, dubbed as variational transport, which approximately\nperforms Wasserstein gradient descent over the manifold of probability\ndistributions via iteratively pushing a set of particles. Specifically, we\nprove that moving along the geodesic in the direction of functional gradient\nwith respect to the second-order Wasserstein distance is equivalent to applying\na pushforward mapping to a probability distribution, which can be approximated\naccurately by pushing a set of particles. Specifically, in each iteration of\nvariational transport, we first solve the variational problem associated with\nthe objective functional using the particles, whose solution yields the\nWasserstein gradient direction. Then we update the current distribution by\npushing each particle along the direction specified by such a solution. By\ncharacterizing both the statistical error incurred in estimating the\nWasserstein gradient and the progress of the optimization algorithm, we prove\nthat when the objective function satisfies a functional version of the\nPolyak-\\L{}ojasiewicz (PL) (Polyak, 1963) and smoothness conditions,\nvariational transport converges linearly to the global minimum of the objective\nfunctional up to a certain statistical error, which decays to zero sublinearly\nas the number of particles goes to infinity.\n', ""  We study the convergence of gradient flow for the training of deep neural\nnetworks. If Residual Neural Networks are a popular example of very deep\narchitectures, their training constitutes a challenging optimization problem\ndue notably to the non-convexity and the non-coercivity of the objective. Yet,\nin applications, those tasks are successfully solved by simple optimization\nalgorithms such as gradient descent. To better understand this phenomenon, we\nfocus here on a ``mean-field'' model of infinitely deep and arbitrarily wide\nResNet, parameterized by probability measures over the product set of layers\nand parameters and with constant marginal on the set of layers. Indeed, in the\ncase of shallow neural networks, mean field models have proven to benefit from\nsimplified loss-landscapes and good theoretical guarantees when trained with\ngradient flow for the Wasserstein metric on the set of probability measures.\nMotivated by this approach, we propose to train our model with gradient flow\nw.r.t. the conditional Optimal Transport distance: a restriction of the\nclassical Wasserstein distance which enforces our marginal condition. Relying\non the theory of gradient flows in metric spaces we first show the\nwell-posedness of the gradient flow equation and its consistency with the\ntraining of ResNets at finite width. Performing a local Polyak-\\L{}ojasiewicz\nanalysis, we then show convergence of the gradient flow for well-chosen\ninitializations: if the number of features is finite but sufficiently large and\nthe risk is sufficiently small at initialization, the gradient flow converges\ntowards a global minimizer. This is the first result of this type for\ninfinitely deep and arbitrarily wide ResNets.\n"", '  Recently, optimization on the Riemannian manifold has provided new insights\nto the optimization community. In this regard, the manifold taken as the\nprobability measure metric space equipped with the second-order Wasserstein\ndistance is of particular interest, since optimization on it can be linked to\npractical sampling processes. In general, the standard (continuous)\noptimization method on Wasserstein space is Riemannian gradient flow (i.e.,\nLangevin dynamics when minimizing KL divergence). In this paper, we aim to\nenrich the continuous optimization methods in the Wasserstein space, by\nextending the gradient flow on it into the stochastic gradient descent (SGD)\nflow and stochastic variance reduction gradient (SVRG) flow. The two flows in\nEuclidean space are standard continuous stochastic methods, while their\nRiemannian counterparts are unexplored. By leveraging the property of\nWasserstein space, we construct stochastic differential equations (SDEs) to\napproximate the corresponding discrete dynamics of desired Riemannian\nstochastic methods in Euclidean space. Then, our probability measures flows are\nobtained by the Fokker-Planck equation. Finally, the convergence rates of our\nRiemannian stochastic flows are proven, which match the results in Euclidean\nspace.\n']",Optimal Transport and Wasserstein Metrics for Machine Learning and Optimization,"""Wasserstein Gradient Flows for Optimization"""
232,"Bayesian Optimization for High-Dimensional Problems , Online Convex Optimization , Submodular Maximization and Optimization Algorithms , Distributionally Robust Optimization , Sparse Linear Regression and Convex Optimization , Optimization Algorithms for Matrix Problems , Optimization under Uncertainty and Risk","['optimizing', 'optimize', 'optimization', 'optimisation', 'optimal', 'hyperparameters', 'optimum', 'hyperparameter', 'algorithms', 'prior'] , ['optimal', 'optimization', 'bandit', 'minimize', 'convex', 'minimization', 'adversary', 'convexity', 'regret', 'guarantees'] , ['submodularity', 'submodular', 'knapsack', 'optimization', 'maximizing', 'algorithms', 'maximization', 'subroutine', 'greedyml', 'complexity'] , ['robust', 'minimizes', 'optimization', 'minimax', 'distributional', 'distributionally', 'stochastic', 'convex', 'regularized', 'wasserstein'] , ['lasso', 'sparse', 'regularization', 'optimization', 'convexity', 'convex', 'regularized', 'sparsity', 'regularizer', 'solvers'] , ['sparse', 'lasso', 'optimal', 'iterative', 'optimization', 'algorithms', 'matrix', 'matrices', 'spectral', 'complexity'] , ['optimality', 'optimal', 'optimize', 'optimization', 'risk', 'regret', 'stochastic', 'predict', 'predictive', 'portfolios']","[""  There has been a long-standing and widespread belief that Bayesian\nOptimization (BO) with standard Gaussian process (GP), referred to as standard\nBO, is ineffective in high-dimensional optimization problems. While this belief\nsounds reasonable, strong empirical evidence is lacking. In this paper, we\nsystematically investigated BO with standard GP regression across a variety of\nsynthetic and real-world benchmark problems for high-dimensional optimization.\nWe found that, surprisingly, when using Mat\\'ern kernels and Upper Confidence\nBound (UCB), standard BO consistently achieves top-tier performance, often\noutperforming other BO methods specifically designed for high-dimensional\noptimization. Contrary to the stereotype, we found that standard GP equipped\nwith Mat\\'ern kernels can serve as a capable surrogate for learning\nhigh-dimensional functions. Without strong structural assumptions, BO with\nstandard GP not only excels in high-dimensional optimization but also is robust\nin accommodating various structures within target functions. Furthermore, with\nstandard GP, achieving promising optimization performance is possible via\nmaximum a posterior (MAP) estimation with diffuse priors or merely maximum\nlikelihood estimation, eliminating the need for expensive Markov-Chain Monte\nCarlo (MCMC) sampling that might be required by more complex surrogate models.\nIn parallel, we also investigated and analyzed alternative popular settings in\nrunning standard BO, which, however, often fail in high-dimensional\noptimization. This might link to the a few failure cases reported in\nliterature. We thus advocate for a re-evaluation and in-depth study of the\npotential of standard BO in addressing high-dimensional problems.\n"", '  Bayesian Optimization (BO) is a method for globally optimizing black-box\nfunctions. While BO has been successfully applied to many scenarios, developing\neffective BO algorithms that scale to functions with high-dimensional domains\nis still a challenge. Optimizing such functions by vanilla BO is extremely\ntime-consuming. Alternative strategies for high-dimensional BO that are based\non the idea of embedding the high-dimensional space to the one with low\ndimension are sensitive to the choice of the embedding dimension, which needs\nto be pre-specified. We develop a new computationally efficient\nhigh-dimensional BO method that exploits variable selection. Our method is able\nto automatically learn axis-aligned sub-spaces, i.e. spaces containing selected\nvariables, without the demand of any pre-specified hyperparameters. We\ntheoretically analyze the computational complexity of our algorithm and derive\nthe regret bound. We empirically show the efficacy of our method on several\nsynthetic and real problems.\n', ""  Gaussian process (GP) based Bayesian optimization (BO) is a powerful method\nfor optimizing black-box functions efficiently. The practical performance and\ntheoretical guarantees of this approach depend on having the correct GP\nhyperparameter values, which are usually unknown in advance and need to be\nestimated from the observed data. However, in practice, these estimations could\nbe incorrect due to biased data sampling strategies used in BO. This can lead\nto degraded performance and break the sub-linear global convergence guarantee\nof BO. To address this issue, we propose a new BO method that can sub-linearly\nconverge to the objective function's global optimum even when the true GP\nhyperparameters are unknown in advance and need to be estimated from the\nobserved data. Our method uses a multi-armed bandit technique (EXP3) to add\nrandom data points to the BO process, and employs a novel training loss\nfunction for the GP hyperparameter estimation process that ensures consistent\nestimation. We further provide theoretical analysis of our proposed method.\nFinally, we demonstrate empirically that our method outperforms existing\napproaches on various synthetic and real-world problems.\n""] , ['  In this paper, we analyze the problem of online convex optimization in\ndifferent settings. We show that any algorithm for online linear optimization\nwith fully adaptive adversaries is an algorithm for online convex optimization.\nWe also show that any such algorithm that requires full-information feedback\nmay be transformed to an algorithm with semi-bandit feedback with comparable\nregret bound. We further show that algorithms that are designed for fully\nadaptive adversaries using deterministic semi-bandit feedback can obtain\nsimilar bounds using only stochastic semi-bandit feedback when facing oblivious\nadversaries. We use this to describe general meta-algorithms to convert first\norder algorithms to zeroth order algorithms with comparable regret bounds. Our\nframework allows us to analyze online optimization in various settings, such\nfull-information feedback, bandit feedback, stochastic regret, adversarial\nregret and various forms of non-stationary regret.\n', '  Online convex optimization (OCO) is a widely used framework in online\nlearning. In each round, the learner chooses a decision in a convex set and an\nadversary chooses a convex loss function, and then the learner suffers the loss\nassociated with their current decision. However, in many applications the\nlearner\'s loss depends not only on the current decision but on the entire\nhistory of decisions until that point. The OCO framework and its existing\ngeneralizations do not capture this, and they can only be applied to many\nsettings of interest after a long series of approximation arguments. They also\nleave open the question of whether the dependence on memory is tight because\nthere are no non-trivial lower bounds. In this work we introduce a\ngeneralization of the OCO framework, ""Online Convex Optimization with Unbounded\nMemory"", that captures long-term dependence on past decisions. We introduce the\nnotion of $p$-effective memory capacity, $H_p$, that quantifies the maximum\ninfluence of past decisions on present losses. We prove an $O(\\sqrt{H_p T})$\nupper bound on the policy regret and a matching (worst-case) lower bound. As a\nspecial case, we prove the first non-trivial lower bound for OCO with finite\nmemory \\citep{anavaHM2015online}, which could be of independent interest, and\nalso improve existing upper bounds. We demonstrate the broad applicability of\nour framework by using it to derive regret bounds, and to improve and simplify\nexisting regret bound derivations, for a variety of online learning problems\nincluding online linear control and an online variant of performative\nprediction.\n', '  In this paper, we explore online convex optimization (OCO) and introduce a\nnew analysis that provides fast rates by exploiting the curvature of feasible\nsets. In online linear optimization, it is known that if the average gradient\nof loss functions is larger than a certain value, the curvature of feasible\nsets can be exploited by the follow-the-leader (FTL) algorithm to achieve a\nlogarithmic regret. This paper reveals that algorithms adaptive to the\ncurvature of loss functions can also leverage the curvature of feasible sets.\nWe first prove that if an optimal decision is on the boundary of a feasible set\nand the gradient of an underlying loss function is non-zero, then the algorithm\nachieves a regret upper bound of $O(\\rho \\log T)$ in stochastic environments.\nHere, $\\rho > 0$ is the radius of the smallest sphere that includes the optimal\ndecision and encloses the feasible set. Our approach, unlike existing ones, can\nwork directly with convex loss functions, exploiting the curvature of loss\nfunctions simultaneously, and can achieve the logarithmic regret only with a\nlocal property of feasible sets. Additionally, it achieves an $O(\\sqrt{T})$\nregret even in adversarial environments where FTL suffers an $\\Omega(T)$\nregret, and attains an $O(\\rho \\log T + \\sqrt{C \\rho \\log T})$ regret bound in\ncorrupted stochastic environments with corruption level $C$. Furthermore, by\nextending our analysis, we establish a regret upper bound of\n$O\\Big(T^{\\frac{q-2}{2(q-1)}} (\\log T)^{\\frac{q}{2(q-1)}}\\Big)$ for\n$q$-uniformly convex feasible sets, where uniformly convex sets include\nstrongly convex sets and $\\ell_p$-balls for $p \\in [1,\\infty)$. This bound\nbridges the gap between the $O(\\log T)$ regret bound for strongly convex sets\n($q=2$) and the $O(\\sqrt{T})$ regret bound for non-curved sets ($q\\to\\infty$).\n'] , ['  Non-monotone constrained submodular maximization plays a crucial role in\nvarious machine learning applications. However, existing algorithms often\nstruggle with a trade-off between approximation guarantees and practical\nefficiency. The current state-of-the-art is a recent $0.401$-approximation\nalgorithm, but its computational complexity makes it highly impractical. The\nbest practical algorithms for the problem only guarantee $1/e$-approximation.\nIn this work, we present a novel algorithm for submodular maximization subject\nto a cardinality constraint that combines a guarantee of $0.385$-approximation\nwith a low and practical query complexity of $O(n+k^2)$. Furthermore, we\nevaluate the empirical performance of our algorithm in experiments based on\nvarious machine learning applications, including Movie Recommendation, Image\nSummarization, and more. These experiments demonstrate the efficacy of our\napproach.\n', '  Constrained submodular maximization problems encompass a wide variety of\napplications, including personalized recommendation, team formation, and\nrevenue maximization via viral marketing. The massive instances occurring in\nmodern day applications can render existing algorithms prohibitively slow,\nwhile frequently, those instances are also inherently stochastic. Focusing on\nthese challenges, we revisit the classic problem of maximizing a (possibly\nnon-monotone) submodular function subject to a knapsack constraint. We present\na simple randomized greedy algorithm that achieves a $5.83$ approximation and\nruns in $O(n \\log n)$ time, i.e., at least a factor $n$ faster than other\nstate-of-the-art algorithms. The robustness of our approach allows us to\nfurther transfer it to a stochastic version of the problem. There, we obtain a\n9-approximation to the best adaptive policy, which is the first constant\napproximation for non-monotone objectives. Experimental evaluation of our\nalgorithms showcases their improved performance on real and synthetic data.\n', '  Submodular optimization is a fundamental problem with many applications in\nmachine learning, often involving decision-making over datasets with sensitive\nattributes such as gender or age. In such settings, it is often desirable to\nproduce a diverse solution set that is fairly distributed with respect to these\nattributes. Motivated by this, we initiate the study of Fair Submodular Cover\n(FSC), where given a ground set $U$, a monotone submodular function\n$f:2^U\\to\\mathbb{R}_{\\ge 0}$, a threshold $\\tau$, the goal is to find a\nbalanced subset of $S$ with minimum cardinality such that $f(S)\\ge\\tau$. We\nfirst introduce discrete algorithms for FSC that achieve a bicriteria\napproximation ratio of $(\\frac{1}{\\epsilon}, 1-O(\\epsilon))$. We then present a\ncontinuous algorithm that achieves a $(\\ln\\frac{1}{\\epsilon},\n1-O(\\epsilon))$-bicriteria approximation ratio, which matches the best\napproximation guarantee of submodular cover without a fairness constraint.\nFinally, we complement our theoretical results with a number of empirical\nevaluations that demonstrate the effectiveness of our algorithms on instances\nof maximum coverage.\n'] , ['  The goal of this paper is to develop distributionally robust optimization\n(DRO) estimators, specifically for multidimensional Extreme Value Theory (EVT)\nstatistics. EVT supports using semi-parametric models called max-stable\ndistributions built from spatial Poisson point processes. While powerful, these\nmodels are only asymptotically valid for large samples. However, since extreme\ndata is by definition scarce, the potential for model misspecification error is\ninherent to these applications, thus DRO estimators are natural. In order to\nmitigate over-conservative estimates while enhancing out-of-sample performance,\nwe study DRO estimators informed by semi-parametric max-stable constraints in\nthe space of point processes. We study both tractable convex formulations for\nsome problems of interest (e.g. CVaR) and more general neural network based\nestimators. Both approaches are validated using synthetically generated data,\nrecovering prescribed characteristics, and verifying the efficacy of the\nproposed techniques. Additionally, the proposed method is applied to a real\ndata set of financial returns for comparison to a previous analysis. We\nestablished the proposed model as a novel formulation in the multivariate EVT\ndomain, and innovative with respect to performance when compared to relevant\nalternate proposals.\n', '  We consider the penalized distributionally robust optimization (DRO) problem\nwith a closed, convex uncertainty set, a setting that encompasses the $f$-DRO,\nWasserstein-DRO, and spectral/$L$-risk formulations used in practice. We\npresent Drago, a stochastic primal-dual algorithm that achieves a\nstate-of-the-art linear convergence rate on strongly convex-strongly concave\nDRO problems. The method combines both randomized and cyclic components with\nmini-batching, which effectively handles the unique asymmetric nature of the\nprimal and dual problems in DRO. We support our theoretical results with\nnumerical benchmarks in classification and regression.\n', '  Distributionally robust optimization (DRO) is a powerful framework for\ntraining robust models against data distribution shifts. This paper focuses on\nconstrained DRO, which has an explicit characterization of the robustness\nlevel. Existing studies on constrained DRO mostly focus on convex loss\nfunction, and exclude the practical and challenging case with non-convex loss\nfunction, e.g., neural network. This paper develops a stochastic algorithm and\nits performance analysis for non-convex constrained DRO. The computational\ncomplexity of our stochastic algorithm at each iteration is independent of the\noverall dataset size, and thus is suitable for large-scale applications. We\nfocus on the general Cressie-Read family divergence defined uncertainty set\nwhich includes $\\chi^2$-divergences as a special case. We prove that our\nalgorithm finds an $\\epsilon$-stationary point with a computational complexity\nof $\\mathcal O(\\epsilon^{-3k_*-5})$, where $k_*$ is the parameter of the\nCressie-Read divergence. The numerical results indicate that our method\noutperforms existing methods.} Our method also applies to the smoothed\nconditional value at risk (CVaR) DRO.\n'] , ['  Sparse linear regression (SLR) is a well-studied problem in statistics where\none is given a design matrix $X\\in\\mathbb{R}^{m\\times n}$ and a response vector\n$y=X\\theta^*+w$ for a $k$-sparse vector $\\theta^*$ (that is,\n$\\|\\theta^*\\|_0\\leq k$) and small, arbitrary noise $w$, and the goal is to find\na $k$-sparse $\\widehat{\\theta} \\in \\mathbb{R}^n$ that minimizes the mean\nsquared prediction error $\\frac{1}{m}\\|X\\widehat{\\theta}-X\\theta^*\\|^2_2$.\nWhile $\\ell_1$-relaxation methods such as basis pursuit, Lasso, and the Dantzig\nselector solve SLR when the design matrix is well-conditioned, no general\nalgorithm is known, nor is there any formal evidence of hardness in an\naverage-case setting with respect to all efficient algorithms.\n  We give evidence of average-case hardness of SLR w.r.t. all efficient\nalgorithms assuming the worst-case hardness of lattice problems. Specifically,\nwe give an instance-by-instance reduction from a variant of the bounded\ndistance decoding (BDD) problem on lattices to SLR, where the condition number\nof the lattice basis that defines the BDD instance is directly related to the\nrestricted eigenvalue condition of the design matrix, which characterizes some\nof the classical statistical-computational gaps for sparse linear regression.\nAlso, by appealing to worst-case to average-case reductions from the world of\nlattices, this shows hardness for a distribution of SLR instances; while the\ndesign matrices are ill-conditioned, the resulting SLR instances are in the\nidentifiable regime.\n  Furthermore, for well-conditioned (essentially) isotropic Gaussian design\nmatrices, where Lasso is known to behave well in the identifiable regime, we\nshow hardness of outputting any good solution in the unidentifiable regime\nwhere there are many solutions, assuming the worst-case hardness of standard\nand well-studied lattice problems.\n', '  Convex programming plays a fundamental role in machine learning, data\nscience, and engineering. Testing convexity structure in nonlinear programs\nrelies on verifying the convexity of objectives and constraints.\n\\citet{grant2006disciplined} introduced a framework, Disciplined Convex\nProgramming (DCP), for automating this verification task for a wide range of\nconvex functions that can be decomposed into basic convex functions (atoms)\nusing convexity-preserving compositions and transformations (rules). However,\nthe restriction to Euclidean convexity concepts can limit the applicability of\nthe framework. For instance, many notable instances of statistical estimators\nand matrix-valued (sub)routines in machine learning applications are Euclidean\nnon-convex, but exhibit geodesic convexity through a more general Riemannian\nlens. In this work, we extend disciplined programming to this setting by\nintroducing Disciplined Geodesically Convex Programming (DGCP). We determine\nconvexity-preserving compositions and transformations for geodesically convex\nfunctions on general Cartan-Hadamard manifolds, as well as for the special case\nof symmetric positive definite matrices, a common setting in matrix-valued\noptimization. For the latter, we also define a basic set of atoms. Our paper is\naccompanied by a Julia package SymbolicAnalysis.jl, which provides\nfunctionality for testing and certifying DGCP-compliant expressions. Our\nlibrary interfaces with manifold optimization software, which allows for\ndirectly solving verified geodesically convex programs.\n', '  The generalized minimax concave (GMC) penalty is a nonconvex sparse\nregularizer which can preserve the overall-convexity of the regularized\nleast-squares problem. In this paper, we focus on a significant instance of the\nGMC model termed scaled GMC (sGMC), and present various notable findings on its\nsolution-set geometry and regularization path. Our investigation indicates that\nwhile the sGMC penalty is a nonconvex extension of the LASSO penalty (i.e., the\n$\\ell_1$-norm), the sGMC model preserves many celebrated properties of the\nLASSO model, hence can serve as a less biased surrogate of LASSO without losing\nits advantages. Specifically, for a fixed regularization parameter $\\lambda$,\nwe show that the solution-set geometry, solution uniqueness and sparseness of\nthe sGMC model can be characterized in a similar elegant way to the LASSO model\n(see, e.g., Osborne et al. 2000, R. J. Tibshirani 2013). For a varying\n$\\lambda$, we prove that the sGMC solution set is a continuous polytope-valued\nmapping of $\\lambda$. Most noticeably, our study indicates that similar to\nLASSO, the minimum $\\ell_2$-norm regularization path of the sGMC model is\ncontinuous and piecewise linear in $\\lambda$. Based on these theoretical\nresults, an efficient regularization path algorithm is proposed for the sGMC\nmodel, extending the well-known least angle regression (LARS) algorithm for\nLASSO. We prove the correctness and finite termination of the proposed\nalgorithm under a mild assumption, and confirm its\ncorrectness-in-general-situation, efficiency, and practical utility through\nnumerical experiments. Many results in this study also contribute to the\ntheoretical research of LASSO.\n'] , ['  We give a stochastic optimization algorithm that solves a dense $n\\times n$\nreal-valued linear system $Ax=b$, returning $\\tilde x$ such that $\\|A\\tilde\nx-b\\|\\leq \\epsilon\\|b\\|$ in time: $$\\tilde\nO((n^2+nk^{\\omega-1})\\log1/\\epsilon),$$ where $k$ is the number of singular\nvalues of $A$ larger than $O(1)$ times its smallest positive singular value,\n$\\omega < 2.372$ is the matrix multiplication exponent, and $\\tilde O$ hides a\npoly-logarithmic in $n$ factor. When $k=O(n^{1-\\theta})$ (namely, $A$ has a\nflat-tailed spectrum, e.g., due to noisy data or regularization), this improves\non both the cost of solving the system directly, as well as on the cost of\npreconditioning an iterative method such as conjugate gradient. In particular,\nour algorithm has an $\\tilde O(n^2)$ runtime when $k=O(n^{0.729})$. We further\nadapt this result to sparse positive semidefinite matrices and least squares\nregression.\n  Our main algorithm can be viewed as a randomized block coordinate descent\nmethod, where the key challenge is simultaneously ensuring good convergence and\nfast per-iteration time. In our analysis, we use theory of majorization for\nelementary symmetric polynomials to establish a sharp convergence guarantee\nwhen coordinate blocks are sampled using a determinantal point process. We then\nuse a Markov chain coupling argument to show that similar convergence can be\nattained with a cheaper sampling scheme, and accelerate the block coordinate\ndescent update via matrix sketching.\n', '  The problem of structured matrix estimation has been studied mostly under\nstrong noise dependence assumptions. This paper considers a general framework\nof noisy low-rank-plus-sparse matrix recovery, where the noise matrix may come\nfrom any joint distribution with arbitrary dependence across entries. We\npropose an incoherent-constrained least-square estimator and prove its\ntightness both in the sense of deterministic lower bound and matching minimax\nrisks under various noise distributions. To attain this, we establish a novel\nresult asserting that the difference between two arbitrary low-rank incoherent\nmatrices must spread energy out across its entries, in other words cannot be\ntoo sparse, which sheds light on the structure of incoherent low-rank matrices\nand may be of independent interest. We then showcase the applications of our\nframework to several important statistical machine learning problems. In the\nproblem of estimating a structured Markov transition kernel, the proposed\nmethod achieves the minimax optimality and the result can be extended to\nestimating the conditional mean operator, a crucial component in reinforcement\nlearning. The applications to multitask regression and structured covariance\nestimation are also presented. We propose an alternating minimization algorithm\nto approximately solve the potentially hard optimization problem. Numerical\nresults corroborate the effectiveness of our method which typically converges\nin a few steps.\n', '  We consider sketching algorithms which first compress data by multiplication\nwith a random sketch matrix, and then apply the sketch to quickly solve an\noptimization problem, e.g., low-rank approximation and regression. In the\nlearning-based sketching paradigm proposed by~\\cite{indyk2019learning}, the\nsketch matrix is found by choosing a random sparse matrix, e.g., CountSketch,\nand then the values of its non-zero entries are updated by running gradient\ndescent on a training data set. Despite the growing body of work on this\nparadigm, a noticeable omission is that the locations of the non-zero entries\nof previous algorithms were fixed, and only their values were learned. In this\nwork, we propose the first learning-based algorithms that also optimize the\nlocations of the non-zero entries. Our first proposed algorithm is based on a\ngreedy algorithm. However, one drawback of the greedy algorithm is its slower\ntraining time. We fix this issue and propose approaches for learning a\nsketching matrix for both low-rank approximation and Hessian approximation for\nsecond order optimization. The latter is helpful for a range of constrained\noptimization problems, such as LASSO and matrix estimation with a nuclear norm\nconstraint. Both approaches achieve good accuracy with a fast running time.\nMoreover, our experiments suggest that our algorithm can still reduce the error\nsignificantly even if we only have a very limited number of training matrices.\n'] , ['  Optimization models used to make discrete decisions often contain uncertain\nparameters that are context-dependent and estimated through prediction. To\naccount for the quality of the decision made based on the prediction,\ndecision-focused learning (end-to-end predict-then-optimize) aims at training\nthe predictive model to minimize regret, i.e., the loss incurred by making a\nsuboptimal decision. Despite the challenge of the gradient of this loss w.r.t.\nthe predictive model parameters being zero almost everywhere for optimization\nproblems with a linear objective, effective gradient-based learning approaches\nhave been proposed to minimize the expected loss, using the empirical loss as a\nsurrogate. However, empirical regret can be an ineffective surrogate because\nempirical optimal decisions can vary substantially from expected optimal\ndecisions. To understand the impact of this deficiency, we evaluate the effect\nof aleatoric and epistemic uncertainty on the accuracy of empirical regret as a\nsurrogate. Next, we propose three novel loss functions that approximate\nexpected regret more robustly. Experimental results show that training two\nstate-of-the-art decision-focused learning approaches using robust regret\nlosses improves test-sample empirical regret in general while keeping\ncomputational time equivalent relative to the number of training epochs.\n', '  Stochastic dominance models risk-averse preferences for decision making with\nuncertain outcomes, which naturally captures the intrinsic structure of the\nunderlying uncertainty, in contrast to simply resorting to the expectations.\nDespite theoretically appealing, the application of stochastic dominance in\nmachine learning has been scarce, due to the following challenges:\n$\\textbf{i)}$, the original concept of stochastic dominance only provides a\n$\\textit{partial order}$, therefore, is not amenable to serve as an optimality\ncriterion; and $\\textbf{ii)}$, an efficient computational recipe remains\nlacking due to the continuum nature of evaluating stochastic dominance.%, which\nbarriers its application for machine learning.\n  In this work, we make the first attempt towards establishing a general\nframework of learning with stochastic dominance. We first generalize the\nstochastic dominance concept to enable feasible comparisons between any\narbitrary pair of random variables. We next develop a simple and\ncomputationally efficient approach for finding the optimal solution in terms of\nstochastic dominance, which can be seamlessly plugged into many learning tasks.\nNumerical experiments demonstrate that the proposed method achieves comparable\nperformance as standard risk-neutral strategies and obtains better trade-offs\nagainst risk across a variety of applications including supervised learning,\nreinforcement learning, and portfolio optimization.\n', '  Many real-world optimization problems contain parameters that are unknown\nbefore deployment time, either due to stochasticity or to lack of information\n(e.g., demand or travel times in delivery problems). A common strategy in such\ncases is to estimate said parameters via machine learning (ML) models trained\nto minimize the prediction error, which however is not necessarily aligned with\nthe downstream task-level error. The decision-focused learning (DFL) paradigm\novercomes this limitation by training to directly minimize a task loss, e.g.\nregret. Since the latter has non-informative gradients for combinatorial\nproblems, state-of-the-art DFL methods introduce surrogates and approximations\nthat enable training. But these methods exploit specific assumptions about the\nproblem structures (e.g., convex or linear problems, unknown parameters only in\nthe objective function). We propose an alternative method that makes no such\nassumptions, it combines stochastic smoothing with score function gradient\nestimation which works on any task loss. This opens up the use of DFL methods\nto nonlinear objectives, uncertain parameters in the problem constraints, and\neven two-stage stochastic optimization. Experiments show that it typically\nrequires more epochs, but that it is on par with specialized methods and\nperforms especially well for the difficult case of problems with uncertainty in\nthe constraints, in terms of solution quality, scalability, or both.\n']",Optimization Methods and Algorithms,Bayesian Optimization for High-Dimensional Problems
233,"Multi-Objective Optimization Methods , Multi-Objective Optimization Methods","['optimizing', 'optimizer', 'optimization', 'optimisation', 'optimal', 'metaheuristics', 'objective', 'pareto', 'metaheuristic', 'objectives'] , ['optimizers', 'optimization', 'optimize', 'hyperparameters', 'minimization', 'hyperparameter', 'minimizing', 'optimal', 'objectives', 'constraints']","['  Recently, Pareto Set Learning (PSL) has been proposed for learning the entire\nPareto set using a neural network. PSL employs preference vectors to scalarize\nmultiple objectives, facilitating the learning of mappings from preference\nvectors to specific Pareto optimal solutions. Previous PSL methods have shown\ntheir effectiveness in solving artificial multi-objective optimization problems\n(MOPs) with uniform preference vector sampling. The quality of the learned\nPareto set is influenced by the sampling strategy of the preference vector, and\nthe sampling of the preference vector needs to be decided based on the Pareto\nfront shape. However, a fixed preference sampling strategy cannot\nsimultaneously adapt the Pareto front of multiple MOPs. To address this\nlimitation, this paper proposes an Evolutionary Preference Sampling (EPS)\nstrategy to efficiently sample preference vectors. Inspired by evolutionary\nalgorithms, we consider preference sampling as an evolutionary process to\ngenerate preference vectors for neural network training. We integrate the EPS\nstrategy into five advanced PSL methods. Extensive experiments demonstrate that\nour proposed method has a faster convergence speed than baseline algorithms on\n7 testing problems. Our implementation is available at\nhttps://github.com/rG223/EPS.\n', '  Multi-objective optimization problems can be found in many real-world\napplications, where the objectives often conflict each other and cannot be\noptimized by a single solution. In the past few decades, numerous methods have\nbeen proposed to find Pareto solutions that represent optimal trade-offs among\nthe objectives for a given problem. However, these existing methods could have\nhigh computational complexity or may not have good theoretical properties for\nsolving a general differentiable multi-objective optimization problem. In this\nwork, by leveraging the smooth optimization technique, we propose a lightweight\nand efficient smooth Tchebycheff scalarization approach for gradient-based\nmulti-objective optimization. It has good theoretical properties for finding\nall Pareto solutions with valid trade-off preferences, while enjoying\nsignificantly lower computational complexity compared to other methods.\nExperimental results on various real-world application problems fully\ndemonstrate the effectiveness of our proposed method.\n', '  Real-world scenarios frequently involve multi-objective data-driven\noptimization problems, characterized by unknown problem coefficients and\nmultiple conflicting objectives. Traditional two-stage methods independently\napply a machine learning model to estimate problem coefficients, followed by\ninvoking a solver to tackle the predicted optimization problem. The independent\nuse of optimization solvers and prediction models may lead to suboptimal\nperformance due to mismatches between their objectives. Recent efforts have\nfocused on end-to-end training of predictive models that use decision loss\nderived from the downstream optimization problem. However, these methods have\nprimarily focused on single-objective optimization problems, thus limiting\ntheir applicability. We aim to propose a multi-objective decision-focused\napproach to address this gap. In order to better align with the inherent\nproperties of multi-objective optimization problems, we propose a set of novel\nloss functions. These loss functions are designed to capture the discrepancies\nbetween predicted and true decision problems, considering solution space,\nobjective space, and decision quality, named landscape loss, Pareto set loss,\nand decision loss, respectively. Our experimental results demonstrate that our\nproposed method significantly outperforms traditional two-stage methods and\nmost current decision-focused methods.\n'] , ['  Solving multi-objective optimization problems for large deep neural networks\nis a challenging task due to the complexity of the loss landscape and the\nexpensive computational cost of training and evaluating models. Efficient\nPareto front approximation of large models enables multi-objective optimization\nfor various tasks such as multi-task learning and trade-off analysis. Existing\nalgorithms for learning Pareto set, including (1) evolutionary, hypernetworks,\nand hypervolume-maximization methods, are computationally expensive and have\nrestricted scalability to large models; (2) Scalarization algorithms, where a\nseparate model is trained for each objective ray, which is inefficient for\nlearning the entire Pareto set and fails to capture the objective trade-offs\neffectively. Inspired by the recent success of model merging, we propose a\npractical and scalable approach to Pareto set learning problem via mixture of\nexperts (MoE) based model fusion. By ensembling the weights of specialized\nsingle-task models, the MoE module can effectively capture the trade-offs\nbetween multiple objectives and closely approximate the entire Pareto set of\nlarge neural networks. Once the routers are learned and a preference vector is\nset, the MoE module can be unloaded, thus no additional computational cost is\nintroduced during inference. We conduct extensive experiments on vision and\nlanguage tasks using large-scale models such as CLIP-ViT and GPT-2. The\nexperimental results demonstrate that our method efficiently approximates the\nentire Pareto front of large models. Using only hundreds of trainable\nparameters of the MoE routers, our method even has lower memory usage compared\nto linear scalarization and algorithms that learn a single Pareto optimal\nsolution, and are scalable to both the number of objectives and the size of the\nmodel.\n', ""  In the pursuit of efficient optimization of expensive-to-evaluate systems,\nthis paper investigates a novel approach to Bayesian multi-objective and\nmulti-fidelity (MOMF) optimization. Traditional optimization methods, while\neffective, often encounter prohibitively high costs in multi-dimensional\noptimizations of one or more objectives. Multi-fidelity approaches offer\npotential remedies by utilizing multiple, less costly information sources, such\nas low-resolution simulations. However, integrating these two strategies\npresents a significant challenge. We suggest the innovative use of a trust\nmetric to support simultaneous optimization of multiple objectives and data\nsources. Our method modifies a multi-objective optimization policy to\nincorporate the trust gain per evaluation cost as one objective in a Pareto\noptimization problem, enabling simultaneous MOMF at lower costs. We present and\ncompare two MOMF optimization methods: a holistic approach selecting both the\ninput parameters and the trust parameter jointly, and a sequential approach for\nbenchmarking. Through benchmarks on synthetic test functions, our approach is\nshown to yield significant cost reductions - up to an order of magnitude\ncompared to pure multi-objective optimization. Furthermore, we find that joint\noptimization of the trust and objective domains outperforms addressing them in\nsequential manner. We validate our results using the use case of optimizing\nlaser-plasma acceleration simulations, demonstrating our method's potential in\nPareto optimization of high-cost black-box functions. Implementing these\nmethods in existing Bayesian frameworks is simple, and they can be readily\nextended to batch optimization. With their capability to handle various\ncontinuous or discrete fidelity dimensions, our techniques offer broad\napplicability in solving simulation problems in fields such as plasma physics\nand fluid dynamics.\n"", '  This paper focuses on integrating the networks and adversarial training into\nconstrained optimization problems to develop a framework algorithm for\nconstrained optimization problems. For such problems, we first transform them\ninto minimax problems using the augmented Lagrangian method and then use two\n(or several) deep neural networks(DNNs) to represent the primal and dual\nvariables respectively. The parameters in the neural networks are then trained\nby an adversarial process. The proposed architecture is relatively insensitive\nto the scale of values of different constraints when compared to penalty based\ndeep learning methods. Through this type of training, the constraints are\nimposed better based on the augmented Lagrangian multipliers. Extensive\nexamples for optimization problems with scalar constraints, nonlinear\nconstraints, partial differential equation constraints, and inequality\nconstraints are considered to show the capability and robustness of the\nproposed method, with applications ranging from Ginzburg--Landau energy\nminimization problems, partition problems, fluid-solid topology optimization,\nto obstacle problems.\n']",Multi-Objective Optimization Techniques,Multi-Objective Optimization Methods
234,"""Surrogate Losses for Multiclass Classification"" , Multi-Fidelity Surrogate Modeling","['classification', 'misclassification', 'losses', 'surrogate', 'loss', 'learning', 'generalization', 'prediction', 'learnability', 'multiclass'] , ['fidelity', 'surrogate', 'modeling', 'prediction', 'modelling', 'surrogates', 'regression', 'neural', 'model', 'predictive']","['  We present a detailed study of top-$k$ classification, the task of predicting\nthe $k$ most probable classes for an input, extending beyond single-class\nprediction. We demonstrate that several prevalent surrogate loss functions in\nmulti-class classification, such as comp-sum and constrained losses, are\nsupported by $H$-consistency bounds with respect to the top-$k$ loss. These\nbounds guarantee consistency in relation to the hypothesis set $H$, providing\nstronger guarantees than Bayes-consistency due to their non-asymptotic and\nhypothesis-set specific nature. To address the trade-off between accuracy and\ncardinality $k$, we further introduce cardinality-aware loss functions through\ninstance-dependent cost-sensitive learning. For these functions, we derive\ncost-sensitive comp-sum and constrained surrogate losses, establishing their\n$H$-consistency bounds and Bayes-consistency. Minimizing these losses leads to\nnew cardinality-aware algorithms for top-$k$ classification. We report the\nresults of extensive experiments on CIFAR-100, ImageNet, CIFAR-10, and SVHN\ndatasets demonstrating the effectiveness and benefit of these algorithms.\n', '  We present a detailed study of surrogate losses and algorithms for\nmulti-label learning, supported by $H$-consistency bounds. We first show that,\nfor the simplest form of multi-label loss (the popular Hamming loss), the\nwell-known consistent binary relevance surrogate suffers from a sub-optimal\ndependency on the number of labels in terms of $H$-consistency bounds, when\nusing smooth losses such as logistic losses. Furthermore, this loss function\nfails to account for label correlations. To address these drawbacks, we\nintroduce a novel surrogate loss, multi-label logistic loss, that accounts for\nlabel correlations and benefits from label-independent $H$-consistency bounds.\nWe then broaden our analysis to cover a more extensive family of multi-label\nlosses, including all common ones and a new extension defined based on\nlinear-fractional functions with respect to the confusion matrix. We also\nextend our multi-label logistic losses to more comprehensive multi-label\ncomp-sum losses, adapting comp-sum losses from standard classification to the\nmulti-label learning. We prove that this family of surrogate losses benefits\nfrom $H$-consistency bounds, and thus Bayes-consistency, across any general\nmulti-label loss. Our work thus proposes a unified surrogate loss framework\nbenefiting from strong consistency guarantees for any multi-label loss,\nsignificantly expanding upon previous work which only established\nBayes-consistency and for specific loss functions. Additionally, we adapt\nconstrained losses from standard classification to multi-label constrained\nlosses in a similar way, which also benefit from $H$-consistency bounds and\nthus Bayes-consistency for any multi-label loss. We further describe efficient\ngradient computation algorithms for minimizing the multi-label logistic loss.\n', '  We present a study of surrogate losses and algorithms for the general problem\nof learning to defer with multiple experts. We first introduce a new family of\nsurrogate losses specifically tailored for the multiple-expert setting, where\nthe prediction and deferral functions are learned simultaneously. We then prove\nthat these surrogate losses benefit from strong $H$-consistency bounds. We\nillustrate the application of our analysis through several examples of\npractical surrogate losses, for which we give explicit guarantees. These loss\nfunctions readily lead to the design of new learning to defer algorithms based\non their minimization. While the main focus of this work is a theoretical\nanalysis, we also report the results of several experiments on SVHN and\nCIFAR-10 datasets.\n'] , [""  Multifidelity surrogate modelling combines data of varying accuracy and cost\nfrom different sources. It strategically uses low-fidelity models for rapid\nevaluations, saving computational resources, and high-fidelity models for\ndetailed refinement. It improves decision-making by addressing uncertainties\nand surpassing the limits of single-fidelity models, which either oversimplify\nor are computationally intensive. Blending high-fidelity data for detailed\nresponses with frequent low-fidelity data for quick approximations facilitates\ndesign optimisation in various domains.\n  Despite progress in interpolation, regression, enhanced sampling, error\nestimation, variable fidelity, and data fusion techniques, challenges persist\nin selecting fidelity levels and developing efficient data fusion methods. This\nstudy proposes a new fusion approach to construct multi-fidelity surrogate\nmodels by constructing gradient-only surrogates that use only gradients to\nconstruct regression surfaces. Results are demonstrated on foundational example\nproblems that isolate and illustrate the fusion approach's efficacy, avoiding\nthe need for complex examples that obfuscate the main concept.\n"", '  Multi-fidelity machine learning methods address the accuracy-efficiency\ntrade-off by integrating scarce, resource-intensive high-fidelity data with\nabundant but less accurate low-fidelity data. We propose a practical\nmulti-fidelity strategy for problems spanning low- and high-dimensional\ndomains, integrating a non-probabilistic regression model for the low-fidelity\nwith a Bayesian model for the high-fidelity. The models are trained in a\nstaggered scheme, where the low-fidelity model is transfer-learned to the\nhigh-fidelity data and a Bayesian model is trained for the residual. This\nthree-model strategy -- deterministic low-fidelity, transfer learning, and\nBayesian residual -- leads to a prediction that includes uncertainty\nquantification both for noisy and noiseless multi-fidelity data. The strategy\nis general and unifies the topic, highlighting the expressivity trade-off\nbetween the transfer-learning and Bayesian models (a complex transfer-learning\nmodel leads to a simpler Bayesian model, and vice versa). We propose modeling\nchoices for two scenarios, and argue in favor of using a linear\ntransfer-learning model that fuses 1) kernel ridge regression for low-fidelity\nwith Gaussian processes for high-fidelity; or 2) deep neural network for\nlow-fidelity with a Bayesian neural network for high-fidelity. We demonstrate\nthe effectiveness and efficiency of the proposed strategies and contrast them\nwith the state-of-the-art based on various numerical examples. The simplicity\nof these formulations makes them practical for a broad scope of future\nengineering applications.\n', '  In this work, we consider the general problem of constructing a neural\nnetwork surrogate model using multi-fidelity information. Motivated by rigorous\nerror and complexity estimates for ReLU neural networks, given an inexpensive\nlow-fidelity and an expensive high-fidelity computational model, we present a\nresidual multi-fidelity computational framework that formulates the correlation\nbetween models as a residual function, a possibly non-linear mapping between 1)\nthe shared input space of the models together with the low-fidelity model\noutput and 2) the discrepancy between the two model outputs. To accomplish\nthis, we train two neural networks to work in concert. The first network learns\nthe residual function on a small set of high-fidelity and low-fidelity data.\nOnce trained, this network is used to generate additional synthetic\nhigh-fidelity data, which is used in the training of a second network. This\nsecond network, once trained, acts as our surrogate for the high-fidelity\nquantity of interest. We present three numerical examples to demonstrate the\npower of the proposed framework. In particular, we show that dramatic savings\nin computational cost may be achieved when the output predictions are desired\nto be accurate within small tolerances.\n']",Surrogate Modeling for Machine Learning and Prediction,"""Surrogate Losses for Multiclass Classification"""
235,"Machine Unlearning and Forgetting in AI Models , Machine Unlearning and Forgetting in AI","['unlearning', 'unlearn', 'forgetting', 'unlearned', 'erase', 'forget', 'erasure', 'remembering', 'forgotten', 'amnesiac'] , ['unlearning', 'memorization', 'forgetting', 'memorize', 'unlearned', 'dataset', 'overfitting', 'trained', 'forget', 'training']","['  To comply with AI and data regulations, the need to forget private or\ncopyrighted information from trained machine learning models is increasingly\nimportant. The key challenge in unlearning is forgetting the necessary data in\na timely manner, while preserving model performance. In this work, we address\nthe zero-shot unlearning scenario, whereby an unlearning algorithm must be able\nto remove data given only a trained model and the data to be forgotten. We\nexplore unlearning from an information theoretic perspective, connecting the\ninfluence of a sample to the information gain a model receives by observing it.\nFrom this, we derive a simple but principled zero-shot unlearning method based\non the geometry of the model. Our approach takes the form of minimising the\ngradient of a learned function with respect to a small neighbourhood around a\ntarget forget point. This induces a smoothing effect, causing forgetting by\nmoving the boundary of the classifier. We explore the intuition behind why this\napproach can jointly unlearn forget samples while preserving general model\nperformance through a series of low-dimensional experiments. We perform\nextensive empirical evaluation of our method over a range of contemporary\nbenchmarks, verifying that our method is competitive with state-of-the-art\nperformance under the strict constraints of zero-shot unlearning.\n', '  Machine unlearning is an emerging technology that has come to attract\nwidespread attention. A number of factors, including regulations and laws,\nprivacy, and usability concerns, have resulted in this need to allow a trained\nmodel to forget some of its training data. Existing studies of machine\nunlearning mainly focus on unlearning requests that forget a cluster of\ninstances or all instances from one class. While these approaches are effective\nin removing instances, they do not scale to scenarios where partial targets\nwithin an instance need to be forgotten. For example, one would like to only\nunlearn a person from all instances that simultaneously contain the person and\nother targets. Directly migrating instance-level unlearning to target-level\nunlearning will reduce the performance of the model after the unlearning\nprocess, or fail to erase information completely. To address these concerns, we\nhave proposed a more effective and efficient unlearning scheme that focuses on\nremoving partial targets from the model, which we name ""target unlearning"".\nSpecifically, we first construct an essential graph data structure to describe\nthe relationships between all important parameters that are selected based on\nthe model explanation method. After that, we simultaneously filter parameters\nthat are also important for the remaining targets and use the pruning-based\nunlearning method, which is a simple but effective solution to remove\ninformation about the target that needs to be forgotten. Experiments with\ndifferent training models on various datasets demonstrate the effectiveness of\nthe proposed approach.\n', '  In response to recent data regulation requirements, machine unlearning (MU)\nhas emerged as a critical process to remove the influence of specific examples\nfrom a given model. Although exact unlearning can be achieved through complete\nmodel retraining using the remaining dataset, the associated computational\ncosts have driven the development of efficient, approximate unlearning\ntechniques. Moving beyond data-centric MU approaches, our study introduces a\nnovel model-based perspective: model sparsification via weight pruning, which\nis capable of reducing the gap between exact unlearning and approximate\nunlearning. We show in both theory and practice that model sparsity can boost\nthe multi-criteria unlearning performance of an approximate unlearner, closing\nthe approximation gap, while continuing to be efficient. This leads to a new MU\nparadigm, termed prune first, then unlearn, which infuses a sparse model prior\ninto the unlearning process. Building on this insight, we also develop a\nsparsity-aware unlearning method that utilizes sparsity regularization to\nenhance the training process of approximate unlearning. Extensive experiments\nshow that our proposals consistently benefit MU in various unlearning\nscenarios. A notable highlight is the 77% unlearning efficacy gain of\nfine-tuning (one of the simplest unlearning methods) when using sparsity-aware\nunlearning. Furthermore, we demonstrate the practical impact of our proposed MU\nmethods in addressing other machine learning challenges, such as defending\nagainst backdoor attacks and enhancing transfer learning. Codes are available\nat https://github.com/OPTML-Group/Unlearn-Sparse.\n'] , [""  With the increasing emphasis on data privacy, the significance of machine\nunlearning has grown substantially. Class unlearning, which involves enabling a\ntrained model to forget data belonging to a specific class learned before, is\nimportant as classification tasks account for the majority of today's machine\nlearning as a service (MLaaS). Retraining the model on the original data,\nexcluding the data to be forgotten (a.k.a forgetting data), is a common\napproach to class unlearning. However, the availability of original data during\nthe unlearning phase is not always guaranteed, leading to the exploration of\nclass unlearning with restricted data access. While current unlearning methods\nwith restricted data access usually generate proxy sample via the trained\nneural network classifier, they typically focus on training and forgetting\nbalanced data. However, the imbalanced original data can cause trouble for\nthese proxies and unlearning, particularly when the forgetting data consists\npredominantly of the majority class. To address this issue, we propose the\nGENerative Imbalanced Unlearning (GENIU) framework. GENIU utilizes a\nVariational Autoencoder (VAE) to concurrently train a proxy generator alongside\nthe original model. These generated proxies accurately represent each class and\nare leveraged in the unlearning phase, eliminating the reliance on the original\ntraining data. To further mitigate the performance degradation resulting from\nforgetting the majority class, we introduce an in-batch tuning strategy that\nworks with the generated proxies. GENIU is the first practical framework for\nclass unlearning in imbalanced data settings and restricted data access,\nensuring the preservation of essential information for future unlearning.\nExperimental results confirm the superiority of GENIU over existing methods,\nestablishing its effectiveness in empirical scenarios.\n"", '  With the implementation of personal data privacy regulations, the field of\nmachine learning (ML) faces the challenge of the ""right to be forgotten"".\nMachine unlearning has emerged to address this issue, aiming to delete data and\nreduce its impact on models according to user requests. Despite the widespread\ninterest in machine unlearning, comprehensive surveys on its latest\nadvancements, especially in the field of Large Language Models (LLMs) is\nlacking. This survey aims to fill this gap by providing an in-depth exploration\nof machine unlearning, including the definition, classification and evaluation\ncriteria, as well as challenges in different environments and their solutions.\nSpecifically, this paper categorizes and investigates unlearning on both\ntraditional models and LLMs, and proposes methods for evaluating the\neffectiveness and efficiency of unlearning, and standards for performance\nmeasurement. This paper reveals the limitations of current unlearning\ntechniques and emphasizes the importance of a comprehensive unlearning\nevaluation to avoid arbitrary forgetting. This survey not only summarizes the\nkey concepts of unlearning technology but also points out its prominent issues\nand feasible directions for future research, providing valuable guidance for\nscholars in the field.\n', ""  By adopting a more flexible definition of unlearning and adjusting the model\ndistribution to simulate training without the targeted data, approximate\nmachine unlearning provides a less resource-demanding alternative to the more\nlaborious exact unlearning methods. Yet, the unlearning completeness of target\nsamples-even when the approximate algorithms are executed faithfully without\nexternal threats-remains largely unexamined, raising questions about those\napproximate algorithms' ability to fulfill their commitment of unlearning\nduring the lifecycle.\n  In this paper, we introduce the task of Lifecycle Unlearning Commitment\nManagement (LUCM) for approximate unlearning and outline its primary\nchallenges. We propose an efficient metric designed to assess the sample-level\nunlearning completeness. Our empirical results demonstrate its superiority over\nmembership inference techniques in two key areas: the strong correlation of its\nmeasurements with unlearning completeness across various unlearning tasks, and\nits computational efficiency, making it suitable for real-time applications.\nAdditionally, we show that this metric is able to serve as a tool for\nmonitoring unlearning anomalies throughout the unlearning lifecycle, including\nboth under-unlearning and over-unlearning.\n  We apply this metric to evaluate the unlearning commitments of current\napproximate algorithms. Our analysis, conducted across multiple unlearning\nbenchmarks, reveals that these algorithms inconsistently fulfill their\nunlearning commitments due to two main issues: 1) unlearning new data can\nsignificantly affect the unlearning utility of previously requested data, and\n2) approximate algorithms fail to ensure equitable unlearning utility across\ndifferent groups. These insights emphasize the crucial importance of LUCM\nthroughout the unlearning lifecycle. We will soon open-source our newly\ndeveloped benchmark.\n""]",Machine Unlearning and Forgetting in Artificial Intelligence,Machine Unlearning and Forgetting in AI Models
236,"Continual Learning and Catastrophic Forgetting , Continual Learning in Neural Information Retrieval , Continual Graph Learning , Continual Learning and Forgetting in AI Models","['forgetting', 'continual', 'learning', 'retention', 'learned', 'memory', 'forget', 'learn', 'continually', 'retaining'] , ['forgetting', 'memorization', 'continual', 'learning', 'memory', 'retrieval', 'retaining', 'lifelong', 'continually', 'incrementally'] , ['forgetting', 'continual', 'graphs', 'learning', 'memory', 'graph', 'forget', 'nodes', 'replaying', 'continually'] , ['forgetting', 'continual', 'learning', 'memory', 'learned', 'forgotten', 'adaptive', 'regularization', 'incremental', 'trained']","['  One of the objectives of continual learning is to prevent catastrophic\nforgetting in learning multiple tasks sequentially, and the existing solutions\nhave been driven by the conceptualization of the plasticity-stability dilemma.\nHowever, the convergence of continual learning for each sequential task is less\nstudied so far. In this paper, we provide a convergence analysis of\nmemory-based continual learning with stochastic gradient descent and empirical\nevidence that training current tasks causes the cumulative degradation of\nprevious tasks. We propose an adaptive method for nonconvex continual learning\n(NCCL), which adjusts step sizes of both previous and current tasks with the\ngradients. The proposed method can achieve the same convergence rate as the SGD\nmethod when the catastrophic forgetting term which we define in the paper is\nsuppressed at each iteration. Further, we demonstrate that the proposed\nalgorithm improves the performance of continual learning over existing methods\nfor several image classification tasks.\n', ""  Continual learning (CL) aims to incrementally learn different tasks (such as\nclassification) in a non-stationary data stream without forgetting old ones.\nMost CL works focus on tackling catastrophic forgetting under a\nlearning-from-scratch paradigm. However, with the increasing prominence of\nfoundation models, pre-trained models equipped with informative representations\nhave become available for various downstream requirements. Several CL methods\nbased on pre-trained models have been explored, either utilizing pre-extracted\nfeatures directly (which makes bridging distribution gaps challenging) or\nincorporating adaptors (which may be subject to forgetting). In this paper, we\npropose a concise and effective approach for CL with pre-trained models. Given\nthat forgetting occurs during parameter updating, we contemplate an alternative\napproach that exploits training-free random projectors and class-prototype\naccumulation, which thus bypasses the issue. Specifically, we inject a frozen\nRandom Projection layer with nonlinear activation between the pre-trained\nmodel's feature representations and output head, which captures interactions\nbetween features with expanded dimensionality, providing enhanced linear\nseparability for class-prototype-based CL. We also demonstrate the importance\nof decorrelating the class-prototypes to reduce the distribution disparity when\nusing pre-trained representations. These techniques prove to be effective and\ncircumvent the problem of forgetting for both class- and domain-incremental\ncontinual learning. Compared to previous methods applied to pre-trained\nViT-B/16 models, we reduce final error rates by between 20% and 62% on seven\nclass-incremental benchmarks, despite not using any rehearsal memory. We\nconclude that the full potential of pre-trained models for simple, effective,\nand fast CL has not hitherto been fully tapped. Code is at\ngithub.com/RanPAC/RanPAC.\n"", '  A key challenge for machine intelligence is to learn new visual concepts\nwithout forgetting the previously acquired knowledge. Continual learning is\naimed towards addressing this challenge. However, there is a gap between\nexisting supervised continual learning and human-like intelligence, where human\nis able to learn from both labeled and unlabeled data. How unlabeled data\naffects learning and catastrophic forgetting in the continual learning process\nremains unknown. To explore these issues, we formulate a new semi-supervised\ncontinual learning method, which can be generically applied to existing\ncontinual learning models. Specifically, a novel gradient learner learns from\nlabeled data to predict gradients on unlabeled data. Hence, the unlabeled data\ncould fit into the supervised continual learning method. Different from\nconventional semi-supervised settings, we do not hypothesize that the\nunderlying classes, which are associated to the unlabeled data, are known to\nthe learning process. In other words, the unlabeled data could be very distinct\nfrom the labeled data. We evaluate the proposed method on mainstream continual\nlearning, adversarial continual learning, and semi-supervised learning tasks.\nThe proposed method achieves state-of-the-art performance on classification\naccuracy and backward transfer in the continual learning setting while\nachieving desired performance on classification accuracy in the semi-supervised\nlearning setting. This implies that the unlabeled images can enhance the\ngeneralizability of continual learning models on the predictive ability on\nunseen data and significantly alleviate catastrophic forgetting. The code is\navailable at \\url{https://github.com/luoyan407/grad_prediction.git}.\n'] , ['  Continual learning refers to the capability of a machine learning model to\nlearn and adapt to new information, without compromising its performance on\npreviously learned tasks. Although several studies have investigated continual\nlearning methods for information retrieval tasks, a well-defined task\nformulation is still lacking, and it is unclear how typical learning strategies\nperform in this context. To address this challenge, a systematic task\nformulation of continual neural information retrieval is presented, along with\na multiple-topic dataset that simulates continuous information retrieval. A\ncomprehensive continual neural information retrieval framework consisting of\ntypical retrieval models and continual learning strategies is then proposed.\nEmpirical evaluations illustrate that the proposed framework can successfully\nprevent catastrophic forgetting in neural information retrieval and enhance\nperformance on previously learned tasks. The results indicate that\nembedding-based retrieval models experience a decline in their continual\nlearning performance as the topic shift distance and dataset volume of new\ntasks increase. In contrast, pretraining-based models do not show any such\ncorrelation. Adopting suitable learning strategies can mitigate the effects of\ntopic shift and data augmentation.\n', '  This paper studies the evolving domain of Continual Learning (CL) in large\nlanguage models (LLMs), with a focus on developing strategies for efficient and\nsustainable training. Our primary emphasis is on continual domain-adaptive\npretraining, a process designed to equip LLMs with the ability to integrate new\ninformation from various domains while retaining previously learned knowledge\nand enhancing cross-domain knowledge transfer without relying on\ndomain-specific identification. Unlike previous studies, which mostly\nconcentrate on a limited selection of tasks or domains and primarily aim to\naddress the issue of forgetting, our research evaluates the adaptability and\ncapabilities of LLMs to changing data landscapes in practical scenarios. To\nthis end, we introduce a new benchmark designed to measure the adaptability of\nLLMs to these evolving data environments, offering a comprehensive framework\nfor evaluation. We examine the impact of model size on learning efficacy and\nforgetting, as well as how the progression and similarity of emerging domains\naffect the knowledge transfer within these models. Our findings uncover several\nkey insights: (i) when the sequence of domains shows semantic similarity,\ncontinual pretraining enables LLMs to better specialize in the current domain\ncompared to stand-alone fine-tuning, (ii) training across a diverse range of\ndomains enhances both backward and forward knowledge transfer, and (iii)\nsmaller models are particularly sensitive to continual pretraining, showing the\nmost significant rates of both forgetting and learning. We posit that our\nresearch marks a shift towards establishing a more realistic benchmark for\ninvestigating CL in LLMs, and has the potential to play a key role in guiding\nthe direction of future research in the field.\n', '  Recently, foundation language models (LMs) have marked significant\nachievements in the domains of natural language processing (NLP) and computer\nvision (CV). Unlike traditional neural network models, foundation LMs obtain a\ngreat ability for transfer learning by acquiring rich commonsense knowledge\nthrough pre-training on extensive unsupervised datasets with a vast number of\nparameters. However, they still can not emulate human-like continuous learning\ndue to catastrophic forgetting. Consequently, various continual learning\n(CL)-based methodologies have been developed to refine LMs, enabling them to\nadapt to new tasks without forgetting previous knowledge. However, a systematic\ntaxonomy of existing approaches and a comparison of their performance are still\nlacking, which is the gap that our survey aims to fill. We delve into a\ncomprehensive review, summarization, and classification of the existing\nliterature on CL-based approaches applied to foundation language models, such\nas pre-trained language models (PLMs), large language models (LLMs) and\nvision-language models (VLMs). We divide these studies into offline CL and\nonline CL, which consist of traditional methods, parameter-efficient-based\nmethods, instruction tuning-based methods and continual pre-training methods.\nOffline CL encompasses domain-incremental learning, task-incremental learning,\nand class-incremental learning, while online CL is subdivided into hard task\nboundary and blurry task boundary settings. Additionally, we outline the\ntypical datasets and metrics employed in CL research and provide a detailed\nanalysis of the challenges and future work for LMs-based continual learning.\n'] , [""  Continual learning (CL) is the research field that aims to build machine\nlearning models that can accumulate knowledge continuously over different tasks\nwithout retraining from scratch. Previous studies have shown that pre-training\ngraph neural networks (GNN) may lead to negative transfer (Hu et al., 2020)\nafter fine-tuning, a setting which is closely related to CL. Thus, we focus on\nstudying GNN in the continual graph learning (CGL) setting. We propose the\nfirst continual graph learning benchmark for spatio-temporal graphs and use it\nto benchmark well-known CGL methods in this novel setting. The benchmark is\nbased on the N-UCLA and NTU-RGB+D datasets for skeleton-based action\nrecognition. Beyond benchmarking for standard performance metrics, we study the\nclass and task-order sensitivity of CGL methods, i.e., the impact of learning\norder on each class/task's performance, and the architectural sensitivity of\nCGL methods with backbone GNN at various widths and depths. We reveal that\ntask-order robust methods can still be class-order sensitive and observe\nresults that contradict previous empirical observations on architectural\nsensitivity in CL.\n"", '  When handling streaming graphs, existing graph representation learning models\nencounter a catastrophic forgetting problem, where previously learned knowledge\nof these models is easily overwritten when learning with newly incoming graphs.\nIn response, Continual Graph Learning (CGL) emerges as a novel paradigm\nenabling graph representation learning from streaming graphs. Our prior work,\nCondense and Train (CaT) is a replay-based CGL framework with a balanced\ncontinual learning procedure, which designs a small yet effective memory bankn\nfor replaying. Although the CaT alleviates the catastrophic forgetting problem,\nthere exist three issues: (1) The graph condensation only focuses on labelled\nnodes while neglecting abundant information carried by unlabelled nodes; (2)\nThe continual training scheme of the CaT overemphasises on the previously\nlearned knowledge, limiting the model capacity to learn from newly added\nmemories; (3) Both the condensation process and replaying process of the CaT\nare time-consuming. In this paper, we propose a PsUdo-label guided Memory bAnk\n(PUMA) CGL framework, extending from the CaT to enhance its efficiency and\neffectiveness by overcoming the above-mentioned weaknesses and limits. To fully\nexploit the information in a graph, PUMA expands the coverage of nodes during\ngraph condensation with both labelled and unlabelled nodes. Furthermore, a\ntraining-from-scratch strategy is proposed to upgrade the previous continual\nlearning scheme for a balanced training between the historical and the new\ngraphs. Besides, PUMA uses a one-time prorogation and wide graph encoders to\naccelerate the graph condensation and the graph encoding process in the\ntraining stage to improve the efficiency of the whole framework. Extensive\nexperiments on six datasets for the node classification task demonstrate the\nstate-of-the-art performance and efficiency over existing methods.\n', '  Continual learning on graph data has recently attracted paramount attention\nfor its aim to resolve the catastrophic forgetting problem on existing tasks\nwhile adapting the sequentially updated model to newly emerged graph tasks.\nWhile there have been efforts to summarize progress on continual learning\nresearch over Euclidean data, e.g., images and texts, a systematic review of\nprogress in continual learning on graphs, a.k.a, continual graph learning (CGL)\nor lifelong graph learning, is still demanding. Graph data are far more complex\nin terms of data structures and application scenarios, making CGL task\nsettings, model designs, and applications extremely challenging. To bridge the\ngap, we provide a comprehensive review of existing continual graph learning\n(CGL) algorithms by elucidating the different task settings and categorizing\nthe existing methods based on their characteristics. We compare the CGL methods\nwith traditional continual learning techniques and analyze the applicability of\nthe traditional continual learning techniques to CGL tasks. Additionally, we\nreview the benchmark works that are crucial to CGL research. Finally, we\ndiscuss the remaining challenges and propose several future directions. We will\nmaintain an up-to-date GitHub repository featuring a comprehensive list of CGL\nalgorithms, accessible at\nhttps://github.com/UConn-DSIS/Survey-of-Continual-Learning-on-Graphs.\n'] , ['  Recent works demonstrate a remarkable ability to customize text-to-image\ndiffusion models while only providing a few example images. What happens if you\ntry to customize such models using multiple, fine-grained concepts in a\nsequential (i.e., continual) manner? In our work, we show that recent\nstate-of-the-art customization of text-to-image models suffer from catastrophic\nforgetting when new concepts arrive sequentially. Specifically, when adding a\nnew concept, the ability to generate high quality images of past, similar\nconcepts degrade. To circumvent this forgetting, we propose a new method,\nC-LoRA, composed of a continually self-regularized low-rank adaptation in cross\nattention layers of the popular Stable Diffusion model. Furthermore, we use\ncustomization prompts which do not include the word of the customized object\n(i.e., ""person"" for a human face dataset) and are initialized as completely\nrandom embeddings. Importantly, our method induces only marginal additional\nparameter costs and requires no storage of user data for replay. We show that\nC-LoRA not only outperforms several baselines for our proposed setting of\ntext-to-image continual customization, which we refer to as Continual\nDiffusion, but that we achieve a new state-of-the-art in the well-established\nrehearsal-free continual learning setting for image classification. The high\nachieving performance of C-LoRA in two separate domains positions it as a\ncompelling solution for a wide range of applications, and we believe it has\nsignificant potential for practical impact. Project page:\nhttps://jamessealesmith.github.io/continual-diffusion/\n', ""  Foundational vision-language models have shown impressive performance on\nvarious downstream tasks. Yet, there is still a pressing need to update these\nmodels later as new tasks or domains become available. Ongoing Continual\nLearning (CL) research provides techniques to overcome catastrophic forgetting\nof previous information when new knowledge is acquired. To date, CL techniques\nfocus only on the supervised training sessions. This results in significant\nforgetting yielding inferior performance to even the prior model zero shot\nperformance. In this work, we argue that test-time data hold great information\nthat can be leveraged in a self supervised manner to refresh the model's memory\nof previous learned tasks and hence greatly reduce forgetting at no extra\nlabelling cost. We study how unsupervised data can be employed online to\nimprove models' performance on prior tasks upon encountering representative\nsamples. We propose a simple yet effective student-teacher model with gradient\nbased sparse parameters updates and show significant performance improvements\nand reduction in forgetting, which could alleviate the role of an offline\nepisodic memory/experience replay buffer.\n"", '  The recent success of large language models (LLMs) trained on static,\npre-collected, general datasets has sparked numerous research directions and\napplications. One such direction addresses the non-trivial challenge of\nintegrating pre-trained LLMs into dynamic data distributions, task structures,\nand user preferences. Pre-trained LLMs, when tailored for specific needs, often\nexperience significant performance degradation in previous knowledge domains --\na phenomenon known as ""catastrophic forgetting"". While extensively studied in\nthe continual learning (CL) community, it presents new manifestations in the\nrealm of LLMs. In this survey, we provide a comprehensive overview of the\ncurrent research progress on LLMs within the context of CL. This survey is\nstructured into four main sections: we first describe an overview of\ncontinually learning LLMs, consisting of two directions of continuity: vertical\ncontinuity (or vertical continual learning), i.e., continual adaptation from\ngeneral to specific capabilities, and horizontal continuity (or horizontal\ncontinual learning), i.e., continual adaptation across time and domains\n(Section 3). We then summarize three stages of learning LLMs in the context of\nmodern CL: Continual Pre-Training (CPT), Domain-Adaptive Pre-training (DAP),\nand Continual Fine-Tuning (CFT) (Section 4). Then we provide an overview of\nevaluation protocols for continual learning with LLMs, along with the current\navailable data sources (Section 5). Finally, we discuss intriguing questions\npertaining to continual learning for LLMs (Section 6). The full list of papers\nexamined in this survey is available at\nhttps://github.com/Wang-ML-Lab/llm-continual-learning-survey.\n']",Continual Learning and Catastrophic Forgetting,Continual Learning and Catastrophic Forgetting
237,"Langevin MCMC and Stochastic Sampling Methods , Stochastic Dynamics and Langevin Equations , Stochastic Differential Equations Modeling , Stochastic Approximation with Markovian Data","['langevin', 'mcmc', 'stochastic', 'sampling', 'diffusion', 'dmc', 'mc', 'subsampling', 'hessian', 'samplers'] , ['stochastic', 'langevin', 'markovian', 'markovianity', 'dynamics', 'drift', 'trajectories', 'entropy', 'odinger', 'observations'] , ['stochastic', 'brownian', 'discretization', 'diffusion', 'gaussian', 'sde', 'drift', 'probabilistic', 'sdes', 'mcmc'] , ['markovian', 'stochastic', 'martingale', 'markov', 'approximation', 'ergodic', 'asynchronous', 'algorithms', 'asymptotic', 'optimal']","['  We study the task of efficiently sampling from a Gibbs distribution $d \\pi^*\n= e^{-h} d {vol}_g$ over a Riemannian manifold $M$ via (geometric) Langevin\nMCMC; this algorithm involves computing exponential maps in random Gaussian\ndirections and is efficiently implementable in practice. The key to our\nanalysis of Langevin MCMC is a bound on the discretization error of the\ngeometric Euler-Murayama scheme, assuming $\\nabla h$ is Lipschitz and $M$ has\nbounded sectional curvature. Our error bound matches the error of Euclidean\nEuler-Murayama in terms of its stepsize dependence. Combined with a contraction\nguarantee for the geometric Langevin Diffusion under Kendall-Cranston coupling,\nwe prove that the Langevin MCMC iterates lie within $\\epsilon$-Wasserstein\ndistance of $\\pi^*$ after $\\tilde{O}(\\epsilon^{-2})$ steps, which matches the\niteration complexity for Euclidean Langevin MCMC. Our results apply in general\nsettings where $h$ can be nonconvex and $M$ can have negative Ricci curvature.\nUnder additional assumptions that the Riemannian curvature tensor has bounded\nderivatives, and that $\\pi^*$ satisfies a $CD(\\cdot,\\infty)$ condition, we\nanalyze the stochastic gradient version of Langevin MCMC, and bound its\niteration complexity by $\\tilde{O}(\\epsilon^{-2})$ as well.\n', '  Understanding the dimension dependency of computational complexity in\nhigh-dimensional sampling problem is a fundamental problem, both from a\npractical and theoretical perspective. Compared with samplers with unbiased\nstationary distribution, e.g., Metropolis-adjusted Langevin algorithm (MALA),\nbiased samplers, e.g., Underdamped Langevin Dynamics (ULD), perform better in\nlow-accuracy cases just because a lower dimension dependency in their\ncomplexities. Along this line, Freund et al. (2022) suggest that the modified\nLangevin algorithm with prior diffusion is able to converge dimension\nindependently for strongly log-concave target distributions. Nonetheless, it\nremains open whether such property establishes for more general cases. In this\npaper, we investigate the prior diffusion technique for the target\ndistributions satisfying log-Sobolev inequality (LSI), which covers a much\nbroader class of distributions compared to the strongly log-concave ones. In\nparticular, we prove that the modified Langevin algorithm can also obtain the\ndimension-independent convergence of KL divergence with different step size\nschedules. The core of our proof technique is a novel construction of an\ninterpolating SDE, which significantly helps to conduct a more accurate\ncharacterization of the discrete updates of the overdamped Langevin dynamics.\nOur theoretical analysis demonstrates the benefits of prior diffusion for a\nbroader class of target distributions and provides new insights into developing\nfaster sampling algorithms.\n', '  Stochastic gradients have been widely integrated into Langevin-based methods\nto improve their scalability and efficiency in solving large-scale sampling\nproblems. However, the proximal sampler, which exhibits much faster convergence\nthan Langevin-based algorithms in the deterministic setting Lee et al. (2021),\nhas yet to be explored in its stochastic variants. In this paper, we study the\nStochastic Proximal Samplers (SPS) for sampling from non-log-concave\ndistributions. We first establish a general framework for implementing\nstochastic proximal samplers and establish the convergence theory accordingly.\nWe show that the convergence to the target distribution can be guaranteed as\nlong as the second moment of the algorithm trajectory is bounded and restricted\nGaussian oracles can be well approximated. We then provide two implementable\nvariants based on Stochastic gradient Langevin dynamics (SGLD) and\nMetropolis-adjusted Langevin algorithm (MALA), giving rise to SPS-SGLD and\nSPS-MALA. We further show that SPS-SGLD and SPS-MALA can achieve\n$\\epsilon$-sampling error in total variation (TV) distance within\n$\\tilde{\\mathcal{O}}(d\\epsilon^{-2})$ and\n$\\tilde{\\mathcal{O}}(d^{1/2}\\epsilon^{-2})$ gradient complexities, which\noutperform the best-known result by at least an $\\tilde{\\mathcal{O}}(d^{1/3})$\nfactor. This enhancement in performance is corroborated by our empirical\nstudies on synthetic data with various dimensions, demonstrating the efficiency\nof our proposed algorithm.\n'] , ['  Trajectory inference seeks to recover the temporal dynamics of a population\nfrom snapshots of its (uncoupled) temporal marginals, i.e. where observed\nparticles are not tracked over time. Lavenant et al. arXiv:2102.09204 addressed\nthis challenging problem under a stochastic differential equation (SDE) model\nwith a gradient-driven drift in the observed space, introducing a minimum\nentropy estimator relative to the Wiener measure. Chizat et al.\narXiv:2205.07146 then provided a practical grid-free mean-field Langevin (MFL)\nalgorithm using Schr\\""odinger bridges. Motivated by the overwhelming success of\nobservable state space models in the traditional paired trajectory inference\nproblem (e.g. target tracking), we extend the above framework to a class of\nlatent SDEs in the form of observable state space models. In this setting, we\nuse partial observations to infer trajectories in the latent space under a\nspecified dynamics model (e.g. the constant velocity/acceleration models from\ntarget tracking). We introduce PO-MFL to solve this latent trajectory inference\nproblem and provide theoretical guarantees by extending the results of\narXiv:2102.09204 to the partially observed setting. We leverage the MFL\nframework of arXiv:2205.07146, yielding an algorithm based on entropic OT\nbetween dynamics-adjusted adjacent time marginals. Experiments validate the\nrobustness of our method and the exponential convergence of the MFL dynamics,\nand demonstrate significant outperformance over the latent-free method of\narXiv:2205.07146 in key scenarios.\n', ""  In the last few years, the dynamical characterization of the power output of\na wind turbine by means of a Langevin equation has been well established. For\nthis approach, temporally highly resolved measurements of wind speed and power\noutput are used to obtain the drift and diffusion coefficients of the energy\nconversion process. These coefficients fully determine a Langevin stochastic\ndifferential equation with Gaussian white noise. The drift term specifies the\ndeterministic behavior of the system whereas the diffusion term describes the\nstochastic behavior of the system. A precise estimation of these coefficients\nis essential to understand the dynamics of the power conversion process of the\nwind turbine. We show that the dynamics of the power output of a wind turbine\nhave a hidden dependency on turbine's different operational states. Here, we\nuse an approach based on clustering Pearson correlation matrices for different\nobservables on a moving time window to identify different operational states.\nWe have identified five operational states in total, for example the state of\nrated power. Those different operational states distinguish non-stationary\nbehavior in the mutual dependencies and represent different turbine control\nsettings. As a next step, we condition our Langevin analysis on these different\nstates to reveal distinctly different behaviors of the power conversion process\nfor each operational state. Moreover, in our new representation hysteresis\neffects which have typically appeared in the Langevin dynamics of wind turbines\nseem to be resolved. We assign these typically observed hysteresis effects\nclearly to the change of the wind energy system between our estimated different\noperational states. In this contribution, we discuss further consequences for\nthe meaning of hysteric switching and detection of malbehaviors in wind\nturbines.\n"", '  Pervasive across diverse domains, stochastic systems exhibit fluctuations in\nprocesses ranging from molecular dynamics to climate phenomena. The Langevin\nequation has served as a common mathematical model for studying such systems,\nenabling predictions of their temporal evolution and analyses of thermodynamic\nquantities, including absorbed heat, work done on the system, and entropy\nproduction. However, inferring the Langevin equation from observed trajectories\nremains challenging, particularly for nonlinear and high-dimensional systems.\nIn this study, we present a comprehensive framework that employs Bayesian\nneural networks for inferring Langevin equations in both overdamped and\nunderdamped regimes. Our framework first provides the drift force and diffusion\nmatrix separately and then combines them to construct the Langevin equation. By\nproviding a distribution of predictions instead of a single value, our approach\nallows us to assess prediction uncertainties, which can prevent potential\nmisunderstandings and erroneous decisions about the system. We demonstrate the\neffectiveness of our framework in inferring Langevin equations for various\nscenarios including a neuron model and microscopic engine, highlighting its\nversatility and potential impact.\n'] , [""  Recently, Gaussian processes have been used to model the vector field of\ncontinuous dynamical systems, referred to as GPODEs, which are characterized by\na probabilistic ODE equation. Bayesian inference for these models has been\nextensively studied and applied in tasks such as time series prediction.\nHowever, the use of standard GPs with basic kernels like squared exponential\nkernels has been common in GPODE research, limiting the model's ability to\nrepresent complex scenarios. To address this limitation, we introduce\nnormalizing flows to reparameterize the ODE vector field, resulting in a\ndata-driven prior distribution, thereby increasing flexibility and expressive\npower. We develop a data-driven variational learning algorithm that utilizes\nanalytically tractable probability density functions of normalizing flows,\nenabling simultaneous learning and inference of unknown continuous dynamics.\nAdditionally, we also apply normalizing flows to the posterior inference of GP\nODEs to resolve the issue of strong mean-field assumptions in posterior\ninference. By applying normalizing flows in both these ways, our model improves\naccuracy and uncertainty estimates for Bayesian Gaussian Process ODEs. We\nvalidate the effectiveness of our approach on simulated dynamical systems and\nreal-world human motion data, including time series prediction and missing data\nrecovery tasks. Experimental results show that our proposed method effectively\ncaptures model uncertainty while improving accuracy.\n"", ""  The Fokker-Planck (FP) equation is a foundational PDE in stochastic\nprocesses. However, curse of dimensionality (CoD) poses challenge when dealing\nwith high-dimensional FP PDEs. Although Monte Carlo and vanilla\nPhysics-Informed Neural Networks (PINNs) have shown the potential to tackle\nCoD, both methods exhibit numerical errors in high dimensions when dealing with\nthe probability density function (PDF) associated with Brownian motion. The\npoint-wise PDF values tend to decrease exponentially as dimension increases,\nsurpassing the precision of numerical simulations and resulting in substantial\nerrors. Moreover, due to its massive sampling, Monte Carlo fails to offer fast\nsampling. Modeling the logarithm likelihood (LL) via vanilla PINNs transforms\nthe FP equation into a difficult HJB equation, whose error grows rapidly with\ndimension. To this end, we propose a novel approach utilizing a score-based\nsolver to fit the score function in SDEs. The score function, defined as the\ngradient of the LL, plays a fundamental role in inferring LL and PDF and\nenables fast SDE sampling. Three fitting methods, Score Matching (SM), Sliced\nSM (SSM), and Score-PINN, are introduced. The proposed score-based SDE solver\noperates in two stages: first, employing SM, SSM, or Score-PINN to acquire the\nscore; and second, solving the LL via an ODE using the obtained score.\nComparative evaluations across these methods showcase varying trade-offs. The\nproposed method is evaluated across diverse SDEs, including anisotropic OU\nprocesses, geometric Brownian, and Brownian with varying eigenspace. We also\ntest various distributions, including Gaussian, Log-normal, Laplace, and\nCauchy. The numerical results demonstrate the score-based SDE solver's\nstability, speed, and performance across different settings, solidifying its\npotential as a solution to CoD for high-dimensional FP equations.\n"", ""  With the rapid increase of observational, experimental and simulated data for\nstochastic systems, tremendous efforts have been devoted to identifying\ngoverning laws underlying the evolution of these systems. Despite the broad\napplications of non-Gaussian fluctuations in numerous physical phenomena, the\ndata-driven approaches to extracting stochastic dynamics with L\\'{e}vy noise\nare relatively few. In this work, we propose a Weak Collocation Regression\n(WCR) to explicitly reveal unknown stochastic dynamical systems, i.e., the\nStochastic Differential Equation (SDE) with both $\\alpha$-stable L\\'{e}vy noise\nand Gaussian noise, from discrete aggregate data. This method utilizes the\nevolution equation of the probability distribution function, i.e., the\nFokker-Planck (FP) equation. With the weak form of the FP equation, the WCR\nconstructs a linear system of unknown parameters where all integrals are\nevaluated by Monte Carlo method with the observations. Then, the unknown\nparameters are obtained by a sparse linear regression. For a SDE with L\\'{e}vy\nnoise, the corresponding FP equation is a partial integro-differential equation\n(PIDE), which contains nonlocal terms, and is difficult to deal with. The weak\nform can avoid complicated multiple integrals. Our approach can simultaneously\ndistinguish mixed noise types, even in multi-dimensional problems. Numerical\nexperiments demonstrate that our method is accurate and computationally\nefficient.\n""] , ['  In this work, we investigate stochastic approximation (SA) with Markovian\ndata and nonlinear updates under constant stepsize $\\alpha>0$. Existing work\nhas primarily focused on either i.i.d. data or linear update rules. We take a\nnew perspective and carefully examine the simultaneous presence of Markovian\ndependency of data and nonlinear update rules, delineating how the interplay\nbetween these two structures leads to complications that are not captured by\nprior techniques. By leveraging the smoothness and recurrence properties of the\nSA updates, we develop a fine-grained analysis of the correlation between the\nSA iterates $\\theta_k$ and Markovian data $x_k$. This enables us to overcome\nthe obstacles in existing analysis and establish for the first time the weak\nconvergence of the joint process $(x_k, \\theta_k)_{k\\geq0}$. Furthermore, we\npresent a precise characterization of the asymptotic bias of the SA iterates,\ngiven by\n$\\mathbb{E}[\\theta_\\infty]-\\theta^\\ast=\\alpha(b_\\text{m}+b_\\text{n}+b_\\text{c})+O(\\alpha^{3/2})$.\nHere, $b_\\text{m}$ is associated with the Markovian noise, $b_\\text{n}$ is tied\nto the nonlinearity, and notably, $b_\\text{c}$ represents a multiplicative\ninteraction between the Markovian noise and nonlinearity, which is absent in\nprevious works. As a by-product of our analysis, we derive finite-time bounds\non higher moment $\\mathbb{E}[\\|\\theta_k-\\theta^\\ast\\|^{2p}]$ and present\nnon-asymptotic geometric convergence rates for the iterates, along with a\nCentral Limit Theorem.\n', '  We study stochastic approximation procedures for approximately solving a\n$d$-dimensional linear fixed point equation based on observing a trajectory of\nlength $n$ from an ergodic Markov chain. We first exhibit a non-asymptotic\nbound of the order $t_{\\mathrm{mix}} \\tfrac{d}{n}$ on the squared error of the\nlast iterate of a standard scheme, where $t_{\\mathrm{mix}}$ is a mixing time.\nWe then prove a non-asymptotic instance-dependent bound on a suitably averaged\nsequence of iterates, with a leading term that matches the local asymptotic\nminimax limit, including sharp dependence on the parameters $(d,\nt_{\\mathrm{mix}})$ in the higher order terms. We complement these upper bounds\nwith a non-asymptotic minimax lower bound that establishes the\ninstance-optimality of the averaged SA estimator. We derive corollaries of\nthese results for policy evaluation with Markov noise -- covering the\nTD($\\lambda$) family of algorithms for all $\\lambda \\in [0, 1)$ -- and linear\nautoregressive models. Our instance-dependent characterizations open the door\nto the design of fine-grained model selection procedures for hyperparameter\ntuning (e.g., choosing the value of $\\lambda$ when running the TD($\\lambda$)\nalgorithm).\n', ""  We study a family of distributed stochastic optimization algorithms where\ngradients are sampled by a token traversing a network of agents in random-walk\nfashion. Typically, these random-walks are chosen to be Markov chains that\nasymptotically sample from a desired target distribution, and play a critical\nrole in the convergence of the optimization iterates. In this paper, we take a\nnovel approach by replacing the standard linear Markovian token by one which\nfollows a nonlinear Markov chain - namely the Self-Repellent Radom Walk (SRRW).\nDefined for any given 'base' Markov chain, the SRRW, parameterized by a\npositive scalar {\\alpha}, is less likely to transition to states that were\nhighly visited in the past, thus the name. In the context of MCMC sampling on a\ngraph, a recent breakthrough in Doshi et al. (2023) shows that the SRRW\nachieves O(1/{\\alpha}) decrease in the asymptotic variance for sampling. We\npropose the use of a 'generalized' version of the SRRW to drive token\nalgorithms for distributed stochastic optimization in the form of stochastic\napproximation, termed SA-SRRW. We prove that the optimization iterate errors of\nthe resulting SA-SRRW converge to zero almost surely and prove a central limit\ntheorem, deriving the explicit form of the resulting asymptotic covariance\nmatrix corresponding to iterate errors. This asymptotic covariance is always\nsmaller than that of an algorithm driven by the base Markov chain and decreases\nat rate O(1/{\\alpha}^2) - the performance benefit of using SRRW thereby\namplified in the stochastic optimization context. Empirical results support our\ntheoretical findings.\n""]",Stochastic Methods for Sampling and Dynamics,Stochastic Dynamics and Langevin Equations
238,"Conformal Prediction and Uncertainty Quantification , Belief Revision and Epistemic Uncertainty , ""Uncertainty Quantification in Classification""","['prediction', 'conformity', 'conformalized', 'conformal', 'predictions', 'predictors', 'predictive', 'predictor', 'classification', 'regression'] , ['epistemological', 'epistemic', 'probabilistic', 'belief', 'uncertainty', 'beliefs', 'information', 'isopignistic', 'hypothesis', 'credibility'] , ['classifiers', 'prediction', 'probabilistic', 'bayes', 'predictive', 'uncertainties', 'uncertainty', 'predictions', 'boosting', 'probabilities']","['  Conformal prediction is a non-parametric technique for constructing\nprediction intervals or sets from arbitrary predictive models under the\nassumption that the data is exchangeable. It is popular as it comes with\ntheoretical guarantees on the marginal coverage of the prediction sets and the\nsplit conformal prediction variant has a very low computational cost compared\nto model training. We study the robustness of split conformal prediction in a\ndata contamination setting, where we assume a small fraction of the calibration\nscores are drawn from a different distribution than the bulk. We quantify the\nimpact of the corrupted data on the coverage and efficiency of the constructed\nsets when evaluated on ""clean"" test points, and verify our results with\nnumerical experiments. Moreover, we propose an adjustment in the classification\nsetting which we call Contamination Robust Conformal Prediction, and verify the\nefficacy of our approach using both synthetic and real datasets.\n', ""  Given the growing significance of reliable, trustworthy, and explainable\nmachine learning, the requirement of uncertainty quantification for anomaly\ndetection systems has become increasingly important. In this context,\neffectively controlling Type I error rates ($\\alpha$) without compromising the\nstatistical power ($1-\\beta$) of these systems can build trust and reduce costs\nrelated to false discoveries, particularly when follow-up procedures are\nexpensive. Leveraging the principles of conformal prediction emerges as a\npromising approach for providing respective statistical guarantees by\ncalibrating a model's uncertainty. This work introduces a novel framework for\nanomaly detection, termed cross-conformal anomaly detection, building upon\nwell-known cross-conformal methods designed for prediction tasks. With that, it\naddresses a natural research gap by extending previous works in the context of\ninductive conformal anomaly detection, relying on the split-conformal approach\nfor model calibration. Drawing on insights from conformal prediction, we\ndemonstrate that the derived methods for calculating cross-conformal $p$-values\nstrike a practical compromise between statistical efficiency (full-conformal)\nand computational efficiency (split-conformal) for uncertainty-quantified\nanomaly detection on benchmark datasets.\n"", '  Conformal Prediction (CP) is a distribution-free uncertainty estimation\nframework that constructs prediction sets guaranteed to contain the true answer\nwith a user-specified probability. Intuitively, the size of the prediction set\nencodes a general notion of uncertainty, with larger sets associated with\nhigher degrees of uncertainty. In this work, we leverage information theory to\nconnect conformal prediction to other notions of uncertainty. More precisely,\nwe prove three different ways to upper bound the intrinsic uncertainty, as\ndescribed by the conditional entropy of the target variable given the inputs,\nby combining CP with information theoretical inequalities. Moreover, we\ndemonstrate two direct and useful applications of such connection between\nconformal prediction and information theory: (i) more principled and effective\nconformal training objectives that generalize previous approaches and enable\nend-to-end training of machine learning models from scratch, and (ii) a natural\nmechanism to incorporate side information into conformal prediction. We\nempirically validate both applications in centralized and federated learning\nsettings, showing our theoretical results translate to lower inefficiency\n(average prediction set size) for popular CP methods.\n'] , ['  In belief revision, agents typically modify their beliefs when they receive\nsome new piece of information that is in conflict with them. The guiding\nprinciple behind most belief revision frameworks is that of minimalism, which\nadvocates minimal changes to existing beliefs. However, minimalism may not\nnecessarily capture the nuanced ways in which human agents reevaluate and\nmodify their beliefs. In contrast, the explanatory hypothesis indicates that\npeople are inherently driven to seek explanations for inconsistencies, thereby\nstriving for explanatory coherence rather than minimal changes when revising\nbeliefs. Our contribution in this paper is two-fold. Motivated by the\nexplanatory hypothesis, we first present a novel, yet simple belief revision\noperator that, given a belief base and an explanation for an explanandum, it\nrevises the belief bases in a manner that preserves the explanandum and is not\nnecessarily minimal. We call this operator explanation-based belief revision.\nSecond, we conduct two human-subject studies to empirically validate our\napproach and investigate belief revision behavior in real-world scenarios. Our\nfindings support the explanatory hypothesis and provide insights into the\nstrategies people employ when resolving inconsistencies.\n', '  Knowledge Measures (KMs) aim at quantifying the amount of\nknowledge/information that a knowledge base carries. On the other hand, Belief\nChange (BC) is the process of changing beliefs (in our case, in terms of\ncontraction, expansion and revision) taking into account a new piece of\nknowledge, which possibly may be in contradiction with the current belief. We\npropose a new quantitative BC framework that is based on KMs by defining belief\nchange operators that try to minimise, from an information-theoretic point of\nview, the surprise that the changed belief carries. To this end, we introduce\nthe principle of minimal surprise. In particular, our contributions are (i) a\ngeneral information-theoretic approach to KMs for which [1] is a special case;\n(ii) KM-based BC operators that satisfy the so-called AGM postulates; and (iii)\na characterisation of any BC operator that satisfies the AGM postulates as a\nKM-based BC operator, i.e., any BC operator satisfying the AGM postulates can\nbe encoded within our quantitative BC framework. We also introduce quantitative\nmeasures that account for the information loss of contraction, information gain\nof expansion and information change of revision. We also give a succinct look\ninto the problem of iterated revision, which deals with the application of a\nsequence of revision operations in our framework, and also illustrate how one\nmay build from our KM-based contraction operator also one not satisfying the\n(in)famous recovery postulate, by focusing on the so-called severe withdrawal\nmodel as an illustrative example.\n', '  Developing a general information processing model in uncertain environments\nis fundamental for the advancement of explainable artificial intelligence.\nDempster-Shafer theory of evidence is a well-known and effective reasoning\nmethod for representing epistemic uncertainty, which is closely related to\nsubjective probability theory and possibility theory. Although they can be\ntransformed to each other under some particular belief structures, there\nremains a lack of a clear and interpretable transformation process, as well as\na unified approach for information processing. In this paper, we aim to address\nthese issues from the perspectives of isopignistic belief functions and the\nhyper-cautious transferable belief model. Firstly, we propose an isopignistic\ntransformation based on the belief evolution network. This transformation\nallows for the adjustment of the information granule while retaining the\npotential decision outcome. The isopignistic transformation is integrated with\na hyper-cautious transferable belief model to establish a new canonical\ndecomposition. This decomposition offers a reverse path between the possibility\ndistribution and its isopignistic mass functions. The result of the canonical\ndecomposition, called isopignistic function, is an identical information\ncontent distribution to reflect the propensity and relative commitment degree\nof the BPA. Furthermore, this paper introduces a method to reconstruct the\nbasic belief assignment by adjusting the isopignistic function. It explores the\nadvantages of this approach in modeling and handling uncertainty within the\nhyper-cautious transferable belief model. More general, this paper establishes\na theoretical basis for building general models of artificial intelligence\nbased on probability theory, Dempster-Shafer theory, and possibility theory.\n'] , ['  We present a novel approach to uncertainty quantification in classification\ntasks based on label-wise decomposition of uncertainty measures. This\nlabel-wise perspective allows uncertainty to be quantified at the individual\nclass level, thereby improving cost-sensitive decision-making and helping\nunderstand the sources of uncertainty. Furthermore, it allows to define total,\naleatoric, and epistemic uncertainty on the basis of non-categorical measures\nsuch as variance, going beyond common entropy-based measures. In particular,\nvariance-based measures address some of the limitations associated with\nestablished methods that have recently been discussed in the literature. We\nshow that our proposed measures adhere to a number of desirable properties.\nThrough empirical evaluation on a variety of benchmark data sets -- including\napplications in the medical domain where accurate uncertainty quantification is\ncrucial -- we establish the effectiveness of label-wise uncertainty\nquantification.\n', '  In binary classification tasks, accurate representation of probabilistic\npredictions is essential for various real-world applications such as predicting\npayment defaults or assessing medical risks. The model must then be\nwell-calibrated to ensure alignment between predicted probabilities and actual\noutcomes. However, when score heterogeneity deviates from the underlying data\nprobability distribution, traditional calibration metrics lose reliability,\nfailing to align score distribution with actual probabilities. In this study,\nwe highlight approaches that prioritize optimizing the alignment between\npredicted scores and true probability distributions over minimizing traditional\nperformance or calibration metrics. When employing tree-based models such as\nRandom Forest and XGBoost, our analysis emphasizes the flexibility these models\noffer in tuning hyperparameters to minimize the Kullback-Leibler (KL)\ndivergence between predicted and true distributions. Through extensive\nempirical analysis across 10 UCI datasets and simulations, we demonstrate that\noptimizing tree-based models based on KL divergence yields superior alignment\nbetween predicted scores and actual probabilities without significant\nperformance loss. In real-world scenarios, the reference probability is\ndetermined a priori as a Beta distribution estimated through maximum\nlikelihood. Conversely, minimizing traditional calibration metrics may lead to\nsuboptimal results, characterized by notable performance declines and inferior\nKL values. Our findings reveal limitations in traditional calibration metrics,\nwhich could undermine the reliability of predictive models for critical\ndecision-making.\n', ""  Most machine learning classifiers are designed to output posterior\nprobabilities for the classes given the input sample. These probabilities may\nbe used to make the categorical decision on the class of the sample; provided\nas input to a downstream system; or provided to a human for interpretation.\nEvaluating the quality of the posteriors generated by these system is an\nessential problem which was addressed decades ago with the invention of proper\nscoring rules (PSRs). Unfortunately, much of the recent machine learning\nliterature uses calibration metrics -- most commonly, the expected calibration\nerror (ECE) -- as a proxy to assess posterior performance. The problem with\nthis approach is that calibration metrics reflect only one aspect of the\nquality of the posteriors, ignoring the discrimination performance. For this\nreason, we argue that calibration metrics should play no role in the assessment\nof posterior quality. Expected PSRs should instead be used for this job,\npreferably normalized for ease of interpretation. In this work, we first give a\nbrief review of PSRs from a practical perspective, motivating their definition\nusing Bayes decision theory. We discuss why expected PSRs provide a principled\nmeasure of the quality of a system's posteriors and why calibration metrics are\nnot the right tool for this job. We argue that calibration metrics, while not\nuseful for performance assessment, may be used as diagnostic tools during\nsystem development. With this purpose in mind, we discuss a simple and\npractical calibration metric, called calibration loss, derived from a\ndecomposition of expected PSRs. We compare this metric with the ECE and with\nthe expected score divergence calibration metric from the PSR literature and\nargue, using theoretical and empirical evidence, that calibration loss is\nsuperior to these two metrics.\n""]",Uncertainty Estimation and Quantification in Machine Learning,"""Uncertainty Quantification in Classification"""
239,"Bayesian Neural Networks and Deep Learning , ""Bayesian Neural Networks on Memristor-based Hardware"" , Bayesian Neural Networks for Uncertainty Estimation","['priors', 'bayesian', 'posteriors', 'bayes', 'neural', 'posterior', 'deep', 'prior', 'likelihood', 'inference'] , ['memristor', 'memristors', 'memory', 'memristive', 'baynn', 'hardware', 'neural', 'baynns', 'bayesnns', 'spintronics'] , ['bayesian', 'probabilistic', 'neural', 'ensembles', 'optimization', 'posterior', 'deep', 'estimation', 'sbnn', 'networks']","['  Uncertainty quantification is an important task in machine learning - a task\nin which standardneural networks (NNs) have traditionally not excelled. This\ncan be a limitation for safety-critical applications, where uncertainty-aware\nmethods like Gaussian processes or Bayesian linear regression are often\npreferred. Bayesian neural networks are an approach to address this limitation.\nThey assume probability distributions for all parameters and yield distributed\npredictions. However, training and inference are typically intractable and\napproximations must be employed. A promising approximation is NNs with Bayesian\nlast layer (BLL). They assume distributed weights only in the linear output\nlayer and yield a normally distributed prediction. To approximate the\nintractable Bayesian neural network, point estimates of the distributed weights\nin all but the last layer should be obtained by maximizing the marginal\nlikelihood. This has previously been challenging, as the marginal likelihood is\nexpensive to evaluate in this setting. We present a reformulation of the\nlog-marginal likelihood of a NN with BLL which allows for efficient training\nusing backpropagation. Furthermore, we address the challenge of uncertainty\nquantification for extrapolation points. We provide a metric to quantify the\ndegree of extrapolation and derive a method to improve the uncertainty\nquantification for these points. Our methods are derived for the multivariate\ncase and demonstrated in a simulation study. In comparison to Bayesian linear\nregression with fixed features, and a Bayesian neural network trained with\nvariational inference, our proposed method achieves the highest log-predictive\ndensity on test data.\n', '  Bayesian neural networks (BNNs) have recently gained popularity due to their\nability to quantify model uncertainty. However, specifying a prior for BNNs\nthat captures relevant domain knowledge is often extremely challenging. In this\nwork, we propose a framework for integrating general forms of domain knowledge\n(i.e., any knowledge that can be represented by a loss function) into a BNN\nprior through variational inference, while enabling computationally efficient\nposterior inference and sampling. Specifically, our approach results in a prior\nover neural network weights that assigns high probability mass to models that\nbetter align with our domain knowledge, leading to posterior samples that also\nexhibit this behavior. We show that BNNs using our proposed domain knowledge\npriors outperform those with standard priors (e.g., isotropic Gaussian,\nGaussian process), successfully incorporating diverse types of prior\ninformation such as fairness, physics rules, and healthcare knowledge and\nachieving better predictive performance. We also present techniques for\ntransferring the learned priors across different model architectures,\ndemonstrating their broad utility across various settings.\n', '  Deep learning has revolutionized the last decade, being at the forefront of\nextraordinary advances in a wide range of tasks including computer vision,\nnatural language processing, and reinforcement learning, to name but a few.\nHowever, it is well-known that deep models trained via maximum likelihood\nestimation tend to be overconfident and give poorly-calibrated predictions.\nBayesian deep learning attempts to address this by placing priors on the model\nparameters, which are then combined with a likelihood to perform posterior\ninference. Unfortunately, for deep models, the true posterior is intractable,\nforcing the user to resort to approximations. In this thesis, we explore the\nuse of variational inference (VI) as an approximation, as it is unique in\nsimultaneously approximating the posterior and providing a lower bound to the\nmarginal likelihood. If tight enough, this lower bound can be used to optimize\nhyperparameters and to facilitate model selection. However, this capacity has\nrarely been used to its full extent for Bayesian neural networks, likely\nbecause the approximate posteriors typically used in practice can lack the\nflexibility to effectively bound the marginal likelihood. We therefore explore\nthree aspects of Bayesian learning for deep models: 1) we ask whether it is\nnecessary to perform inference over as many parameters as possible, or whether\nit is reasonable to treat many of them as optimizable hyperparameters; 2) we\npropose a variational posterior that provides a unified view of inference in\nBayesian neural networks and deep Gaussian processes; 3) we demonstrate how VI\ncan be improved in certain deep Gaussian process models by analytically\nremoving symmetries from the posterior, and performing inference on Gram\nmatrices instead of features. We hope that our contributions will provide a\nstepping stone to fully realize the promises of VI in the future.\n'] , ['  Internet of Things (IoT) and smart wearable devices for personalized\nhealthcare will require storing and computing ever-increasing amounts of data.\nThe key requirements for these devices are ultra-low-power, high-processing\ncapabilities, autonomy at low cost, as well as reliability and accuracy to\nenable Green AI at the edge. Artificial Intelligence (AI) models, especially\nBayesian Neural Networks (BayNNs) are resource-intensive and face challenges\nwith traditional computing architectures due to the memory wall problem.\nComputing-in-Memory (CIM) with emerging resistive memories offers a solution by\ncombining memory blocks and computing units for higher efficiency and lower\npower consumption. However, implementing BayNNs on CIM hardware, particularly\nwith spintronic technologies, presents technical challenges due to variability\nand manufacturing defects. The NeuSPIN project aims to address these challenges\nthrough full-stack hardware and software co-design, developing novel\nalgorithmic and circuit design approaches to enhance the performance,\nenergy-efficiency and robustness of BayNNs on sprintronic-based CIM platforms.\n', '  The performance of deep learning algorithms such as neural networks (NNs) has\nincreased tremendously recently, and they can achieve state-of-the-art\nperformance in many domains. However, due to memory and computation resource\nconstraints, implementing NNs on edge devices is a challenging task. Therefore,\nhardware accelerators such as computation-in-memory (CIM) with memristive\ndevices have been developed to accelerate the most common operations, i.e.,\nmatrix-vector multiplication. However, due to inherent device properties,\nexternal environmental factors such as temperature, and an immature fabrication\nprocess, memristors suffer from various non-idealities, including defects and\nvariations occurring during manufacturing and runtime. Consequently, there is a\nlack of complete confidence in the predictions made by the model. To improve\nconfidence in NN predictions made by hardware accelerators in the presence of\ndevice non-idealities, in this paper, we propose a Bayesian test vector\ngeneration framework that can estimate the model uncertainty of NNs implemented\non memristor-based CIM hardware. Compared to the conventional point estimate\ntest vector generation method, our method is more generalizable across\ndifferent model dimensions and requires storing only one test Bayesian vector\nin the hardware. Our method is evaluated on different model dimensions, tasks,\nfault rates, and variation noise to show that it can consistently achieve\n$100\\%$ coverage with only $0.024$ MB of memory overhead.\n', '  Uncertainty estimation in Neural Networks (NNs) is vital in improving\nreliability and confidence in predictions, particularly in safety-critical\napplications. Bayesian Neural Networks (BayNNs) with Dropout as an\napproximation offer a systematic approach to quantifying uncertainty, but they\ninherently suffer from high hardware overhead in terms of power, memory, and\ncomputation. Thus, the applicability of BayNNs to edge devices with limited\nresources or to high-performance applications is challenging. Some of the\ninherent costs of BayNNs can be reduced by accelerating them in hardware on a\nComputation-In-Memory (CIM) architecture with spintronic memories and\nbinarizing their parameters. However, numerous stochastic units are required to\nimplement conventional dropout-based BayNN. In this paper, we propose the Scale\nDropout, a novel regularization technique for Binary Neural Networks (BNNs),\nand Monte Carlo-Scale Dropout (MC-Scale Dropout)-based BayNNs for efficient\nuncertainty estimation. Our approach requires only one stochastic unit for the\nentire model, irrespective of the model size, leading to a highly scalable\nBayesian NN. Furthermore, we introduce a novel Spintronic memory-based CIM\narchitecture for the proposed BayNN that achieves more than $100\\times$ energy\nsavings compared to the state-of-the-art. We validated our method to show up to\na $1\\%$ improvement in predictive performance and superior uncertainty\nestimates compared to related works.\n'] , ['  Bayesian Neural Networks (BNNs) have become one of the promising approaches\nfor uncertainty estimation due to the solid theorical foundations. However, the\nperformance of BNNs is affected by the ability of catching uncertainty. Instead\nof only seeking the distribution of neural network weights by in-distribution\n(ID) data, in this paper, we propose a new Bayesian Neural Network with an\nAttached structure (ABNN) to catch more uncertainty from out-of-distribution\n(OOD) data. We first construct a mathematical description for the uncertainty\nof OOD data according to the prior distribution, and then develop an attached\nBayesian structure to integrate the uncertainty of OOD data into the backbone\nnetwork. ABNN is composed of an expectation module and several distribution\nmodules. The expectation module is a backbone deep network which focuses on the\noriginal task, and the distribution modules are mini Bayesian structures which\nserve as attachments of the backbone. In particular, the distribution modules\naim at extracting the uncertainty from both ID and OOD data. We further provide\ntheoretical analysis for the convergence of ABNN, and experimentally validate\nits superiority by comparing with some state-of-the-art uncertainty estimation\nmethods Code will be made available.\n', '  Bayesian optimization is a highly efficient approach to optimizing objective\nfunctions which are expensive to query. These objectives are typically\nrepresented by Gaussian process (GP) surrogate models which are easy to\noptimize and support exact inference. While standard GP surrogates have been\nwell-established in Bayesian optimization, Bayesian neural networks (BNNs) have\nrecently become practical function approximators, with many benefits over\nstandard GPs such as the ability to naturally handle non-stationarity and learn\nrepresentations for high-dimensional data. In this paper, we study BNNs as\nalternatives to standard GP surrogates for optimization. We consider a variety\nof approximate inference procedures for finite-width BNNs, including\nhigh-quality Hamiltonian Monte Carlo, low-cost stochastic MCMC, and heuristics\nsuch as deep ensembles. We also consider infinite-width BNNs, linearized\nLaplace approximations, and partially stochastic models such as deep kernel\nlearning. We evaluate this collection of surrogate models on diverse problems\nwith varying dimensionality, number of objectives, non-stationarity, and\ndiscrete and continuous inputs. We find: (i) the ranking of methods is highly\nproblem dependent, suggesting the need for tailored inductive biases; (ii) HMC\nis the most successful approximate inference procedure for fully stochastic\nBNNs; (iii) full stochasticity may be unnecessary as deep kernel learning is\nrelatively competitive; (iv) deep ensembles perform relatively poorly; (v)\ninfinite-width BNNs are particularly promising, especially in high dimensions.\n', '  In the drug discovery process, where experiments can be costly and\ntime-consuming, computational models that predict drug-target interactions are\nvaluable tools to accelerate the development of new therapeutic agents.\nEstimating the uncertainty inherent in these neural network predictions\nprovides valuable information that facilitates optimal decision-making when\nrisk assessment is crucial. However, such models can be poorly calibrated,\nwhich results in unreliable uncertainty estimates that do not reflect the true\npredictive uncertainty. In this study, we compare different metrics, including\naccuracy and calibration scores, used for model hyperparameter tuning to\ninvestigate which model selection strategy achieves well-calibrated models.\nFurthermore, we propose to use a computationally efficient Bayesian uncertainty\nestimation method named Bayesian Linear Probing (BLP), which generates\nHamiltonian Monte Carlo (HMC) trajectories to obtain samples for the parameters\nof a Bayesian Logistic Regression fitted to the hidden layer of the baseline\nneural network. We report that BLP improves model calibration and achieves the\nperformance of common uncertainty quantification methods by combining the\nbenefits of uncertainty estimation and probability calibration methods.\nFinally, we show that combining post hoc calibration method with\nwell-performing uncertainty quantification approaches can boost model accuracy\nand calibration.\n']",Bayesian Neural Networks,Bayesian Neural Networks and Deep Learning
240,"Markov Chain Monte Carlo Methods , Monte-Carlo Tree Search Algorithms","['mcmc', 'bayesian', 'probabilistic', 'posteriors', 'markov', 'posterior', 'stochastic', 'bayesmbar', 'monte', 'dmc'] , ['exploration', 'heuristic', 'algorithms', 'search', 'subtrees', 'monte', 'mcts', 'tree', 'pathfinding', 'randomized']","['  In this paper, we study sampling from a posterior derived from a neural\nnetwork. We propose a new probabilistic model consisting of adding noise at\nevery pre- and post-activation in the network, arguing that the resulting\nposterior can be sampled using an efficient Gibbs sampler. For small models,\nthe Gibbs sampler attains similar performances as the state-of-the-art Markov\nchain Monte Carlo (MCMC) methods, such as the Hamiltonian Monte Carlo (HMC) or\nthe Metropolis adjusted Langevin algorithm (MALA), both on real and synthetic\ndata. By framing our analysis in the teacher-student setting, we introduce a\nthermalization criterion that allows us to detect when an algorithm, when run\non data with synthetic labels, fails to sample from the posterior. The\ncriterion is based on the fact that in the teacher-student setting we can\ninitialize an algorithm directly at equilibrium.\n', '  Numerous applications in biology, statistics, science, and engineering\nrequire generating samples from high-dimensional probability distributions. In\nrecent years, the Hamiltonian Monte Carlo (HMC) method has emerged as a\nstate-of-the-art Markov chain Monte Carlo technique, exploiting the shape of\nsuch high-dimensional target distributions to efficiently generate samples.\nDespite its impressive empirical success and increasing popularity, its\nwide-scale adoption remains limited due to the high computational cost of\ngradient calculation. Moreover, applying this method is impossible when the\ngradient of the posterior cannot be computed (for example, with black-box\nsimulators). To overcome these challenges, we propose a novel two-stage\nHamiltonian Monte Carlo algorithm with a surrogate model. In this\nmulti-fidelity algorithm, the acceptance probability is computed in the first\nstage via a standard HMC proposal using an inexpensive differentiable surrogate\nmodel, and if the proposal is accepted, the posterior is evaluated in the\nsecond stage using the high-fidelity (HF) numerical solver. Splitting the\nstandard HMC algorithm into these two stages allows for approximating the\ngradient of the posterior efficiently, while producing accurate posterior\nsamples by using HF numerical solvers in the second stage. We demonstrate the\neffectiveness of this algorithm for a range of problems, including linear and\nnonlinear Bayesian inverse problems with in-silico data and experimental data.\nThe proposed algorithm is shown to seamlessly integrate with various\nlow-fidelity and HF models, priors, and datasets. Remarkably, our proposed\nmethod outperforms the traditional HMC algorithm in both computational and\nstatistical efficiency by several orders of magnitude, all while retaining or\nimproving the accuracy in computed posterior statistics.\n', '  This paper is intended to appear as a chapter for the Handbook of Markov\nChain Monte Carlo. The goal of this chapter is to unify various problems at the\nintersection of Markov chain Monte Carlo (MCMC) and machine\nlearning$\\unicode{x2014}$which includes black-box variational inference,\nadaptive MCMC, normalizing flow construction and transport-assisted MCMC,\nsurrogate-likelihood MCMC, coreset construction for MCMC with big data, Markov\nchain gradient descent, Markovian score climbing, and\nmore$\\unicode{x2014}$within one common framework. By doing so, the theory and\nmethods developed for each may be translated and generalized.\n'] , ['  Monte-Carlo Tree Search (MCTS) is a widely-used strategy for online planning\nthat combines Monte-Carlo sampling with forward tree search. Its success relies\non the Upper Confidence bound for Trees (UCT) algorithm, an extension of the\nUCB method for multi-arm bandits. However, the theoretical foundation of UCT is\nincomplete due to an error in the logarithmic bonus term for action selection,\nleading to the development of Fixed-Depth-MCTS with a polynomial exploration\nbonus to balance exploration and exploitation~\\citep{shah2022journal}. Both UCT\nand Fixed-Depth-MCTS suffer from biased value estimation: the weighted sum\nunderestimates the optimal value, while the maximum valuation overestimates\nit~\\citep{coulom2006efficient}. The power mean estimator offers a balanced\nsolution, lying between the average and maximum values.\nPower-UCT~\\citep{dam2019generalized} incorporates this estimator for more\naccurate value estimates but its theoretical analysis remains incomplete. This\npaper introduces Stochastic-Power-UCT, an MCTS algorithm using the power mean\nestimator and tailored for stochastic MDPs. We analyze its polynomial\nconvergence in estimating root node values and show that it shares the same\nconvergence rate of $\\mathcal{O}(n^{-1/2})$, with $n$ is the number of visited\ntrajectories, as Fixed-Depth-MCTS, with the latter being a special case of the\nformer. Our theoretical results are validated with empirical tests across\nvarious stochastic MDP environments.\n', '  Monte-Carlo tree search (MCTS) is an effective anytime algorithm with a vast\namount of applications. It strategically allocates computational resources to\nfocus on promising segments of the search tree, making it a very attractive\nsearch algorithm in large search spaces. However, it often expends its limited\nresources on reevaluating previously explored regions when they remain the most\npromising path. Our proposed methodology, denoted as AmEx-MCTS, solves this\nproblem by introducing a novel MCTS formulation. Central to AmEx-MCTS is the\ndecoupling of value updates, visit count updates, and the selected path during\nthe tree search, thereby enabling the exclusion of already explored subtrees or\nleaves. This segregation preserves the utility of visit counts for both\nexploration-exploitation balancing and quality metrics within MCTS. The\nresultant augmentation facilitates in a considerably broader search using\nidentical computational resources, preserving the essential characteristics of\nMCTS. The expanded coverage not only yields more precise estimations but also\nproves instrumental in larger and more complex problems. Our empirical\nevaluation demonstrates the superior performance of AmEx-MCTS, surpassing\nclassical MCTS and related approaches by a substantial margin.\n', '  Monte Carlo Tree Search (MCTS) is an immensely popular search-based framework\nused for decision making. It is traditionally applied to domains where a\nperfect simulation model of the environment is available. We study and improve\nMCTS in the context where the environment model is given but imperfect. We show\nthat the discrepancy between the model and the actual environment can lead to\nsignificant performance degradation with standard MCTS. We therefore develop\nUncertainty Adapted MCTS (UA-MCTS), a more robust algorithm within the MCTS\nframework. We estimate the transition uncertainty in the given model, and\ndirect the search towards more certain transitions in the state space. We\nmodify all four MCTS phases to improve the search behavior by considering these\nestimates. We prove, in the corrupted bandit case, that adding uncertainty\ninformation to adapt UCB leads to tighter regret bound than standard UCB.\nEmpirically, we evaluate UA-MCTS and its individual components on the\ndeterministic domains from the MinAtar test suite. Our results demonstrate that\nUA-MCTS strongly improves MCTS in the presence of model transition errors.\n']",Probabilistic Methods for Sampling and Decision Making,Monte-Carlo Tree Search Algorithms
241,"Bayesian Probabilistic Graphical Models , Probabilistic Circuits and Tractable Models","['bayesian', 'probabilistic', 'inference', 'bayes', 'graphs', 'posterior', 'prior', 'learning', 'likelihood', 'causal'] , ['probabilistic', 'inferences', 'bayesian', 'circuits', 'inference', 'inceptionpcs', 'learnspn', 'tractability', 'computing', 'tractable']","['  Gaussian Process Networks (GPNs) are a class of directed graphical models\nwhich employ Gaussian processes as priors for the conditional expectation of\neach variable given its parents in the network. The model allows the\ndescription of continuous joint distributions in a compact but flexible manner\nwith minimal parametric assumptions on the dependencies between variables.\nBayesian structure learning of GPNs requires computing the posterior over\ngraphs of the network and is computationally infeasible even in low dimensions.\nThis work implements Monte Carlo and Markov Chain Monte Carlo methods to sample\nfrom the posterior distribution of network structures. As such, the approach\nfollows the Bayesian paradigm, comparing models via their marginal likelihood\nand computing the posterior probability of the GPN features. Simulation studies\nshow that our method outperforms state-of-the-art algorithms in recovering the\ngraphical structure of the network and provides an accurate approximation of\nits posterior distribution.\n', '  Bayesian causal structure learning aims to learn a posterior distribution\nover directed acyclic graphs (DAGs), and the mechanisms that define the\nrelationship between parent and child variables. By taking a Bayesian approach,\nit is possible to reason about the uncertainty of the causal model. The notion\nof modelling the uncertainty over models is particularly crucial for causal\nstructure learning since the model could be unidentifiable when given only a\nfinite amount of observational data. In this paper, we introduce a novel method\nto jointly learn the structure and mechanisms of the causal model using\nVariational Bayes, which we call Variational Bayes-DAG-GFlowNet (VBG). We\nextend the method of Bayesian causal structure learning using GFlowNets to\nlearn not only the posterior distribution over the structure, but also the\nparameters of a linear-Gaussian model. Our results on simulated data suggest\nthat VBG is competitive against several baselines in modelling the posterior\nover DAGs and mechanisms, while offering several advantages over existing\nmethods, including the guarantee to sample acyclic graphs, and the flexibility\nto generalize to non-linear causal mechanisms.\n', '  Estimating the structure of a Bayesian network, in the form of a directed\nacyclic graph (DAG), from observational data is a statistically and\ncomputationally hard problem with essential applications in areas such as\ncausal discovery. Bayesian approaches are a promising direction for solving\nthis task, as they allow for uncertainty quantification and deal with\nwell-known identifiability issues. From a probabilistic inference perspective,\nthe main challenges are (i) representing distributions over graphs that satisfy\nthe DAG constraint and (ii) estimating a posterior over the underlying\ncombinatorial space. We propose an approach that addresses these challenges by\nformulating a joint distribution on an augmented space of DAGs and\npermutations. We carry out posterior estimation via variational inference,\nwhere we exploit continuous relaxations of discrete distributions. We show that\nour approach performs competitively when compared with a wide range of Bayesian\nand non-Bayesian benchmarks on a range of synthetic and real datasets.\n'] , ['  Probabilistic models based on continuous latent spaces, such as variational\nautoencoders, can be understood as uncountable mixture models where components\ndepend continuously on the latent code. They have proven to be expressive tools\nfor generative and probabilistic modelling, but are at odds with tractable\nprobabilistic inference, that is, computing marginals and conditionals of the\nrepresented probability distribution. Meanwhile, tractable probabilistic models\nsuch as probabilistic circuits (PCs) can be understood as hierarchical discrete\nmixture models, and thus are capable of performing exact inference efficiently\nbut often show subpar performance in comparison to continuous latent-space\nmodels. In this paper, we investigate a hybrid approach, namely continuous\nmixtures of tractable models with a small latent dimension. While these models\nare analytically intractable, they are well amenable to numerical integration\nschemes based on a finite set of integration points. With a large enough number\nof integration points the approximation becomes de-facto exact. Moreover, for a\nfinite set of integration points, the integration method effectively compiles\nthe continuous mixture into a standard PC. In experiments, we show that this\nsimple scheme proves remarkably effective, as PCs learnt this way set new state\nof the art for tractable models on many standard density estimation benchmarks.\n', '  We present a comprehensive survey of the advancements and techniques in the\nfield of tractable probabilistic generative modeling, primarily focusing on\nProbabilistic Circuits (PCs). We provide a unified perspective on the inherent\ntrade-offs between expressivity and tractability, highlighting the design\nprinciples and algorithmic extensions that have enabled building expressive and\nefficient PCs, and provide a taxonomy of the field. We also discuss recent\nefforts to build deep and hybrid PCs by fusing notions from deep neural models,\nand outline the challenges and open questions that can guide future research in\nthis evolving field.\n', '  Zhang et al. (ICML 2021, PLMR 139, pp. 12447-1245) introduced probabilistic\ngenerating circuits (PGCs) as a probabilistic model to unify probabilistic\ncircuits (PCs) and determinantal point processes (DPPs). At a first glance,\nPGCs store a distribution in a very different way, they compute the probability\ngenerating polynomial instead of the probability mass function and it seems\nthat this is the main reason why PGCs are more powerful than PCs or DPPs.\nHowever, PGCs also allow for negative weights, whereas classical PCs assume\nthat all weights are nonnegative. One of the main insights of our paper is that\nthe negative weights are responsible for the power of PGCs and not the\ndifferent representation. PGCs are PCs in disguise, in particular, we show how\nto transform any PGC into a PC with negative weights with only polynomial\nblowup.\n  PGCs were defined by Zhang et al. only for binary random variables. As our\nsecond main result, we show that there is a good reason for this: we prove that\nPGCs for categorial variables with larger image size do not support tractable\nmarginalization unless NP = P. On the other hand, we show that we can model\ncategorial variables with larger image size as PC with negative weights\ncomputing set-multilinear polynomials. These allow for tractable\nmarginalization. In this sense, PCs with negative weights strictly subsume\nPGCs.\n']",Probabilistic Modeling and Inference,Bayesian Probabilistic Graphical Models
242,"Community Detection in Graphs and Hypergraphs , Topological Data Analysis for Graph Classification , Graph Anomaly Detection Techniques , Graph Clustering and Signal Processing","['communities', 'hypergraph', 'hypergraphs', 'subgraph', 'graphs', 'cluster', 'nodes', 'adjacency', 'clusters', 'clustering'] , ['topological', 'topologically', 'topology', 'homeomorphic', 'networks', 'manifolds', 'homology', 'classification', 'graphcodes', 'neural'] , ['anomaly', 'anomalyllm', 'outliers', 'outlier', 'graphs', 'anomalies', 'anomalous', 'graph', 'nodes', 'detecting'] , ['graphs', 'graphon', 'graph', 'adjacency', 'spectral', 'nodes', 'networks', 'vertex', 'edges', 'clustering']","['  The study of complex networks has significantly advanced our understanding of\ncommunity structures which serves as a crucial feature of real-world graphs.\nDetecting communities in graphs is a challenging problem with applications in\nsociology, biology, and computer science. Despite the efforts of an\ninterdisciplinary community of scientists, a satisfactory solution to this\nproblem has not yet been achieved. This review article delves into the topic of\ncommunity detection in graphs, which serves as a thorough exposition of various\ncommunity detection methods from perspectives of modularity-based method,\nspectral clustering, probabilistic modelling, and deep learning. Along with the\nmethods, a new community detection method designed by us is also presented.\nAdditionally, the performance of these methods on the datasets with and without\nground truth is compared. In conclusion, this comprehensive review provides a\ndeep understanding of community detection in graphs.\n', '  Graph clustering is an important unsupervised learning technique for\npartitioning graphs with attributes and detecting communities. However, current\nmethods struggle to accurately capture true community structures and\nintra-cluster relations, be computationally efficient, and identify smaller\ncommunities. We address these challenges by integrating coarsening and\nmodularity maximization, effectively leveraging both adjacency and node\nfeatures to enhance clustering accuracy. We propose a loss function\nincorporating log-determinant, smoothness, and modularity components using a\nblock majorization-minimization technique, resulting in superior clustering\noutcomes. The method is theoretically consistent under the Degree-Corrected\nStochastic Block Model (DC-SBM), ensuring asymptotic error-free performance and\ncomplete label recovery. Our provably convergent and time-efficient algorithm\nseamlessly integrates with graph neural networks (GNNs) and variational graph\nautoencoders (VGAEs) to learn enhanced node features and deliver exceptional\nclustering performance. Extensive experiments on benchmark datasets demonstrate\nits superiority over existing state-of-the-art methods for both attributed and\nnon-attributed graphs.\n', '  Hypergraphs are a representation of complex systems involving interactions\namong more than two entities and allow to investigation of higher-order\nstructure and dynamics in real-world complex systems. Community structure is a\ncommon property observed in empirical networks in various domains. Stochastic\nblock models have been employed to investigate community structure in networks.\nNode attribute data, often accompanying network data, has been found to\npotentially enhance the learning of community structure in dyadic networks. In\nthis study, we develop a statistical framework that incorporates node attribute\ndata into the learning of community structure in a hypergraph, employing a\nstochastic block model. We demonstrate that our model, which we refer to as\nHyperNEO, enhances the learning of community structure in synthetic and\nempirical hypergraphs when node attributes are sufficiently associated with the\ncommunities. Furthermore, we found that applying a dimensionality reduction\nmethod, UMAP, to the learned representations obtained using stochastic block\nmodels, including our model, maps nodes into a two-dimensional vector space\nwhile largely preserving community structure in empirical hypergraphs. We\nexpect that our framework will broaden the investigation and understanding of\nhigher-order community structure in real-world complex systems.\n'] , ['  Persistent homology, a technique from computational topology, has recently\nshown strong empirical performance in the context of graph classification.\nBeing able to capture long range graph properties via higher-order topological\nfeatures, such as cycles of arbitrary length, in combination with multi-scale\ntopological descriptors, has improved predictive performance for data sets with\nprominent topological structures, such as molecules. At the same time, the\ntheoretical properties of persistent homology have not been formally assessed\nin this context. This paper intends to bridge the gap between computational\ntopology and graph machine learning by providing a brief introduction to\npersistent homology in the context of graphs, as well as a theoretical\ndiscussion and empirical analysis of its expressivity for graph learning tasks.\n', '  Topological Data Analysis (TDA) allows us to extract powerful topological and\nhigher-order information on the global shape of a data set or point cloud.\nTools like Persistent Homology or the Euler Transform give a single complex\ndescription of the global structure of the point cloud. However, common machine\nlearning applications like classification require point-level information and\nfeatures to be available. In this paper, we bridge this gap and propose a novel\nmethod to extract node-level topological features from complex point clouds\nusing discrete variants of concepts from algebraic topology and differential\ngeometry. We verify the effectiveness of these topological point features\n(TOPF) on both synthetic and real-world data and study their robustness under\nnoise.\n', ""  We specialize techniques from topological data analysis to the problem of\ncharacterizing the topological complexity (as defined in the body of the paper)\nof a multi-class data set. As a by-product, a topological classifier is defined\nthat uses an open sub-covering of the data set. This sub-covering can be used\nto construct a simplicial complex whose topological features (e.g., Betti\nnumbers) provide information about the classification problem. We use these\ntopological constructs to study the impact of topological complexity on\nlearning in feedforward deep neural networks (DNNs). We hypothesize that\ntopological complexity is negatively correlated with the ability of a fully\nconnected feedforward deep neural network to learn to classify data correctly.\nWe evaluate our topological classification algorithm on multiple constructed\nand open source data sets. We also validate our hypothesis regarding the\nrelationship between topological complexity and learning in DNN's on multiple\ndata sets.\n""] , ['  Graph Anomaly Detection (GAD) is a technique used to identify abnormal nodes\nwithin graphs, finding applications in network security, fraud detection,\nsocial media spam detection, and various other domains. A common method for GAD\nis Graph Auto-Encoders (GAEs), which encode graph data into node\nrepresentations and identify anomalies by assessing the reconstruction quality\nof the graphs based on these representations. However, existing GAE models are\nprimarily optimized for direct link reconstruction, resulting in nodes\nconnected in the graph being clustered in the latent space. As a result, they\nexcel at detecting cluster-type structural anomalies but struggle with more\ncomplex structural anomalies that do not conform to clusters. To address this\nlimitation, we propose a novel solution called GAD-NR, a new variant of GAE\nthat incorporates neighborhood reconstruction for graph anomaly detection.\nGAD-NR aims to reconstruct the entire neighborhood of a node, encompassing the\nlocal structure, self-attributes, and neighbor attributes, based on the\ncorresponding node representation. By comparing the neighborhood reconstruction\nloss between anomalous nodes and normal nodes, GAD-NR can effectively detect\nany anomalies. Extensive experimentation conducted on six real-world datasets\nvalidates the effectiveness of GAD-NR, showcasing significant improvements (by\nup to 30% in AUC) over state-of-the-art competitors. The source code for GAD-NR\nis openly available. Importantly, the comparative analysis reveals that the\nexisting methods perform well only in detecting one or two types of anomalies\nout of the three types studied. In contrast, GAD-NR excels at detecting all\nthree types of anomalies across the datasets, demonstrating its comprehensive\nanomaly detection capabilities.\n', '  This paper considers an important Graph Anomaly Detection (GAD) task, namely\nopen-set GAD, which aims to train a detection model using a small number of\nnormal and anomaly nodes (referred to as seen anomalies) to detect both seen\nanomalies and unseen anomalies (i.e., anomalies that cannot be illustrated the\ntraining anomalies). The availability of those labelled training data provides\ncrucial prior knowledge about abnormalities for GAD models, enabling\nsubstantially reduced detection errors. However, current methods tend to\nover-emphasise fitting the seen anomalies, leading to a weak generalisation\nability to detect the unseen anomalies. Further, they were introduced to handle\nEuclidean data, failing to effectively capture important information on graph\nstructure and node attributes for GAD. In this work, we propose a novel\nopen-set GAD approach, namely Normal Structure Regularisation (NSReg) to\nachieve generalised detection ability to unseen anomalies, while maintaining\nits effectiveness on detecting seen anomalies. The key idea in NSReg is to\nintroduce a regularisation term that enforces the learning of compact,\nsemantically-rich representations of normal nodes based on their structural\nrelations to other nodes. When being optimised with supervised anomaly\ndetection losses, the regularisation term helps incorporate strong normality\ninto the modelling, and thus, it effectively avoids the overfitting the seen\nanomalies solely. In doing so, it helps learn better normality decision\nboundary, reducing the errors of detecting unseen anomalies as normal.\nExtensive empirical results on seven real-world datasets show the superiority\nof NSReg for open-set GAD.\n', '  Real-world graphs are complex to process for performing effective analysis,\nsuch as anomaly detection. However, recently, there have been several research\nefforts addressing the issues surrounding graph-based anomaly detection. In\nthis paper, we discuss a comprehensive overview of anomaly detection techniques\non graph data. We also discuss the various application domains which use those\nanomaly detection techniques. We present a new taxonomy that categorizes the\ndifferent state-of-the-art anomaly detection methods based on assumptions and\ntechniques. Within each category, we discuss the fundamental research ideas\nthat have been done to improve anomaly detection. We further discuss the\nadvantages and disadvantages of current anomaly detection techniques. Finally,\nwe present potential future research directions in anomaly detection on\ngraph-structured data.\n'] , ['  Time-evolving graphs arise frequently when modeling complex dynamical systems\nsuch as social networks, traffic flow, and biological processes. Developing\ntechniques to identify and analyze communities in these time-varying graph\nstructures is an important challenge. In this work, we generalize existing\nspectral clustering algorithms from static to dynamic graphs using canonical\ncorrelation analysis (CCA) to capture the temporal evolution of clusters. Based\non this extended canonical correlation framework, we define the dynamic graph\nLaplacian and investigate its spectral properties. We connect these concepts to\ndynamical systems theory via transfer operators, and illustrate the advantages\nof our method on benchmark graphs by comparison with existing methods. We show\nthat the dynamic graph Laplacian allows for a clear interpretation of cluster\nstructure evolution over time for directed and undirected graphs.\n', '  Graph signal processing (GSP) is a prominent framework for analyzing signals\non non-Euclidean domains. The graph Fourier transform (GFT) uses the\ncombinatorial graph Laplacian matrix to reveal the spectral decomposition of\nsignals in the graph frequency domain. However, a common challenge in applying\nGSP methods is that in many scenarios the underlying graph of a system is\nunknown. A solution in such cases is to construct the unobserved graph from\navailable data, which is commonly referred to as graph or network inference.\nAlthough different graph inference methods exist, these are restricted to\nlearning from either smooth graph signals or simple additive Gaussian noise.\nOther types of noisy data, such as discrete counts or binary digits, are rather\ncommon in real-world applications, yet are underexplored in graph inference. In\nthis paper, we propose a versatile graph inference framework for learning from\ngraph signals corrupted by exponential family noise. Our framework generalizes\nprevious methods from continuous smooth graph signals to various data types. We\npropose an alternating algorithm that jointly estimates the graph Laplacian and\nthe unobserved smooth representation from the noisy signals. We also extend our\napproach to a variational form to account for the inherent stochasticity of the\nlatent smooth representation. Finally, since real-world graph signals are\nfrequently non-independent and temporally correlated, we further adapt our\noriginal setting to a time-vertex formulation. We demonstrate on synthetic and\nreal-world data that our new algorithms outperform competing Laplacian\nestimation methods that suffer from noise model mismatch.\n', '  Many real-world systems can be represented as graphs where the different\nentities in the system are presented by nodes and their interactions by edges.\nAn important task in studying large datasets with graphical structure is graph\nclustering. While there has been a lot of work on graph clustering using the\nconnectivity between the nodes, many real-world networks also have node\nattributes. Clustering attributed graphs requires joint modeling of graph\nstructure and node attributes. Recent work has focused on combining these two\ncomplementary sources of information through graph convolutional networks and\ngraph filtering. However, these methods are mostly limited to lowpass filtering\nand do not explicitly learn the filter parameters for the clustering task. In\nthis paper, we introduce a graph signal processing based approach, where we\nlearn the parameters of Finite Impulse Response (FIR) and Autoregressive Moving\nAverage (ARMA) graph filters optimized for clustering. The proposed approach is\nformulated as a two-step iterative optimization problem, focusing on learning\ninterpretable graph filters that are optimal for the given data and that\nmaximize the separation between different clusters. The proposed approach is\nevaluated on attributed networks and compared to the state-of-the-art methods.\n']",Graph Analysis and Processing Techniques,Graph Clustering and Signal Processing
243,"Graph Prompt Learning for GNNs , ""Scalable Graph Neural Networks (GNNs) Training Systems"" , Graph Condensation for Efficient GNN Training , Graph Neural Networks (GNNs) for Graph Data Analysis","['graphprompt', 'graphs', 'graph4gui', 'subgraph', 'graph', 'networks', 'pretraining', 'pretext', 'pretexts', 'nodes'] , ['networks', 'graphs', 'gnn', 'vertex', 'gpus', 'graphscale', 'graph', 'bottleneck', 'gnns', 'gpu'] , ['graphslim', 'graphs', 'graph', 'tinygraph', 'gnn', 'nodes', 'condense', 'gnns', 'node', 'condenses'] , ['graphnas', 'networks', 'graphs', 'subgraph', 'graphstorm', 'nodes', 'graph', 'gnn', 'gnns', 'neural']","[""  In recent years, prompt tuning has sparked a research surge in adapting\npre-trained models. Unlike the unified pre-training strategy employed in the\nlanguage field, the graph field exhibits diverse pre-training strategies,\nposing challenges in designing appropriate prompt-based tuning methods for\ngraph neural networks. While some pioneering work has devised specialized\nprompting functions for models that employ edge prediction as their\npre-training tasks, these methods are limited to specific pre-trained GNN\nmodels and lack broader applicability. In this paper, we introduce a universal\nprompt-based tuning method called Graph Prompt Feature (GPF) for pre-trained\nGNN models under any pre-training strategy. GPF operates on the input graph's\nfeature space and can theoretically achieve an equivalent effect to any form of\nprompting function. Consequently, we no longer need to illustrate the prompting\nfunction corresponding to each pre-training strategy explicitly. Instead, we\nemploy GPF to obtain the prompted graph for the downstream task in an adaptive\nmanner. We provide rigorous derivations to demonstrate the universality of GPF\nand make guarantee of its effectiveness. The experimental results under various\npre-training strategies indicate that our method performs better than\nfine-tuning, with an average improvement of about 1.4% in full-shot scenarios\nand about 3.2% in few-shot scenarios. Moreover, our method significantly\noutperforms existing specialized prompt-based tuning methods when applied to\nmodels utilizing the pre-training strategy they specialize in. These numerous\nadvantages position our method as a compelling alternative to fine-tuning for\ndownstream adaptations.\n"", '  In recent years, graph prompt learning/tuning has garnered increasing\nattention in adapting pre-trained models for graph representation learning. As\na kind of universal graph prompt learning method, Graph Prompt Feature (GPF)\nhas achieved remarkable success in adapting pre-trained models for Graph Neural\nNetworks (GNNs). By fixing the parameters of a pre-trained GNN model, the aim\nof GPF is to modify the input graph data by adding some (learnable) prompt\nvectors into graph node features to better align with the downstream tasks on\nthe smaller dataset. However, existing GPFs generally suffer from two main\nlimitations. First, GPFs generally focus on node prompt learning which ignore\nthe prompting for graph edges. Second, existing GPFs generally conduct the\nprompt learning on all nodes equally which fails to capture the importances of\ndifferent nodes and may perform sensitively w.r.t noisy nodes in aligning with\nthe downstream tasks. To address these issues, in this paper, we propose a new\nunified Graph Selective Prompt Feature learning (GSPF) for GNN fine-tuning. The\nproposed GSPF integrates the prompt learning on both graph node and edge\ntogether, which thus provides a unified prompt model for the graph data.\nMoreover, it conducts prompt learning selectively on nodes and edges by\nconcentrating on the important nodes and edges for prompting which thus make\nour model be more reliable and compact. Experimental results on many benchmark\ndatasets demonstrate the effectiveness and advantages of the proposed GSPF\nmethod.\n', '  Graphs have emerged as a natural choice to represent and analyze the\nintricate patterns and rich information of the Web, enabling applications such\nas online page classification and social recommendation. The prevailing\n""pre-train, fine-tune"" paradigm has been widely adopted in graph machine\nlearning tasks, particularly in scenarios with limited labeled nodes. However,\nthis approach often exhibits a misalignment between the training objectives of\npretext tasks and those of downstream tasks. This gap can result in the\n""negative transfer"" problem, wherein the knowledge gained from pre-training\nadversely affects performance in the downstream tasks. The surge in\nprompt-based learning within Natural Language Processing (NLP) suggests the\npotential of adapting a ""pre-train, prompt"" paradigm to graphs as an\nalternative. However, existing graph prompting techniques are tailored to\nhomogeneous graphs, neglecting the inherent heterogeneity of Web graphs. To\nbridge this gap, we propose HetGPT, a general post-training prompting framework\nto improve the predictive performance of pre-trained heterogeneous graph neural\nnetworks (HGNNs). The key is the design of a novel prompting function that\nintegrates a virtual class prompt and a heterogeneous feature prompt, with the\naim to reformulate downstream tasks to mirror pretext tasks. Moreover, HetGPT\nintroduces a multi-view neighborhood aggregation mechanism, capturing the\ncomplex neighborhood structure in heterogeneous graphs. Extensive experiments\non three benchmark datasets demonstrate HetGPT\'s capability to enhance the\nperformance of state-of-the-art HGNNs on semi-supervised node classification.\n'] , ['  Graph neural networks have been shown successful in recent years. While\ndifferent GNN architectures and training systems have been developed, GNN\ntraining on large-scale real-world graphs still remains challenging. Existing\ndistributed systems load the entire graph in memory for graph partitioning,\nrequiring a huge memory space to process large graphs and thus hindering GNN\ntraining on such large graphs using commodity workstations. In this paper, we\npropose CATGNN, a cost-efficient and scalable distributed GNN training system\nwhich focuses on scaling GNN training to billion-scale or larger graphs under\nlimited computational resources. Among other features, it takes a stream of\nedges as input, instead of loading the entire graph in memory, for\npartitioning. We also propose a novel streaming partitioning algorithm named\nSPRING for distributed GNN training. We verify the correctness and\neffectiveness of CATGNN with SPRING on 16 open datasets. In particular, we\ndemonstrate that CATGNN can handle the largest publicly available dataset with\nlimited memory, which would have been infeasible without increasing the memory\nspace. SPRING also outperforms state-of-the-art partitioning algorithms\nsignificantly, with a 50% reduction in replication factor on average.\n', '  Graph Neural Networks (GNNs) are widely used today in recommendation systems,\nfraud detection, and node/link classification tasks. Real world GNNs continue\nto scale in size and require a large memory footprint for storing graphs and\nembeddings that often exceed the memory capacities of the target GPUs used for\ntraining. To address limited memory capacities, traditional GNN training\napproaches use graph partitioning and sharding techniques to scale up across\nmultiple GPUs within a node and/or scale out across multiple nodes. However,\nthis approach suffers from the high computational costs of graph partitioning\nalgorithms and inefficient communication across GPUs.\n  To address these overheads, we propose Large-scale Storage-based Multi-GPU\nGNN framework (LSM-GNN), a storagebased approach to train GNN models that\nutilizes a novel communication layer enabling GPU software caches to function\nas a system-wide shared cache with low overheads.LSM-GNN incorporates a hybrid\neviction policy that intelligently manages cache space by using both static and\ndynamic node information to significantly enhance cache performance.\nFurthermore, we introduce the Preemptive Victim-buffer Prefetcher (PVP), a\nmechanism for prefetching node feature data from a Victim Buffer located in CPU\npinned-memory to further reduce the pressure on the storage devices.\nExperimental results show that despite the lower compute capabilities and\nmemory capacities, LSM-GNN in a single node with two GPUs offers superior\nperformance over two-node-four-GPU Dist-DGL baseline and provides up to 3.75x\nspeed up on end-to-end epoch time while running large-scale GNN training\n', '  Recently, graph neural networks (GNNs) have gained much attention as a\ngrowing area of deep learning capable of learning on graph-structured data.\nHowever, the computational and memory requirements for training GNNs on\nlarge-scale graphs make it necessary to distribute the training. A prerequisite\nfor distributed GNN training is to partition the input graph into smaller parts\nthat are distributed among multiple machines of a compute cluster. Although\ngraph partitioning has been studied with regard to graph analytics and graph\ndatabases, its effect on GNN training performance is largely unexplored. As a\nconsequence, it is unclear whether investing computational efforts into\nhigh-quality graph partitioning would pay off in GNN training scenarios.\n  In this paper, we study the effectiveness of graph partitioning for\ndistributed GNN training. Our study aims to understand how different factors\nsuch as GNN parameters, mini-batch size, graph type, features size, and\nscale-out factor influence the effectiveness of graph partitioning. We conduct\nexperiments with two different GNN systems using vertex and edge partitioning.\nWe found that high-quality graph partitioning is a very effective optimization\nto speed up GNN training and to reduce memory consumption. Furthermore, our\nresults show that invested partitioning time can quickly be amortized by\nreduced GNN training time, making it a relevant optimization for most GNN\nscenarios. Compared to research on distributed graph processing, our study\nreveals that graph partitioning plays an even more significant role in\ndistributed GNN training, which motivates further research on the graph\npartitioning problem.\n'] , ['  Graph condensation (GC) is an emerging technique designed to learn a\nsignificantly smaller graph that retains the essential information of the\noriginal graph. This condensed graph has shown promise in accelerating graph\nneural networks while preserving performance comparable to those achieved with\nthe original, larger graphs. Additionally, this technique facilitates\ndownstream applications such as neural architecture search and enhances our\nunderstanding of redundancy in large graphs. Despite the rapid development of\nGC methods, a systematic evaluation framework remains absent, which is\nnecessary to clarify the critical designs for particular evaluative aspects.\nFurthermore, several meaningful questions have not been investigated, such as\nwhether GC inherently preserves certain graph properties and offers robustness\neven without targeted design efforts. In this paper, we introduce GC-Bench, a\ncomprehensive framework to evaluate recent GC methods across multiple\ndimensions and to generate new insights. Our experimental findings provide a\ndeeper insights into the GC process and the characteristics of condensed\ngraphs, guiding future efforts in enhancing performance and exploring new\napplications. Our code is available at\n\\url{https://github.com/Emory-Melody/GraphSlim/tree/main/benchmark}.\n', '  The increasing prevalence of large-scale graphs poses a significant challenge\nfor graph neural network training, attributed to their substantial\ncomputational requirements. In response, graph condensation (GC) emerges as a\npromising data-centric solution aiming to substitute the large graph with a\nsmall yet informative condensed graph to facilitate data-efficient GNN\ntraining. However, existing GC methods suffer from intricate optimization\nprocesses, necessitating excessive computing resources. In this paper, we\nrevisit existing GC optimization strategies and identify two pervasive issues:\n1. various GC optimization strategies converge to class-level node feature\nmatching between the original and condensed graphs, making the optimization\ntarget coarse-grained despite the complex computations; 2. to bridge the\noriginal and condensed graphs, existing GC methods rely on a Siamese graph\nnetwork architecture that requires time-consuming bi-level optimization with\niterative gradient computations. To overcome these issues, we propose a\ntraining-free GC framework termed Class-partitioned Graph Condensation (CGC),\nwhich refines the node feature matching from the class-to-class paradigm into a\nnovel class-to-node paradigm. Remarkably, this refinement also simplifies the\nGC optimization as a class partition problem, which can be efficiently solved\nby any clustering methods. Moreover, CGC incorporates a pre-defined graph\nstructure to enable a closed-form solution for condensed node features,\neliminating the back-and-forth gradient descent in existing GC approaches\nwithout sacrificing accuracy. Extensive experiments demonstrate that CGC\nachieves state-of-the-art performance with a more efficient condensation\nprocess. For instance, compared with the seminal GC method (i.e., GCond), CGC\ncondenses the largest Reddit graph within 10 seconds, achieving a 2,680X\nspeedup and a 1.4% accuracy increase.\n', '  The burgeoning volume of graph data presents significant computational\nchallenges in training graph neural networks (GNNs), critically impeding their\nefficiency in various applications. To tackle this challenge, graph\ncondensation (GC) has emerged as a promising acceleration solution, focusing on\nthe synthesis of a compact yet representative graph for efficiently training\nGNNs while retaining performance. Despite the potential to promote scalable use\nof GNNs, existing GC methods are limited to aligning the condensed graph with\nmerely the observed static graph distribution. This limitation significantly\nrestricts the generalization capacity of condensed graphs, particularly in\nadapting to dynamic distribution changes. In real-world scenarios, however,\ngraphs are dynamic and constantly evolving, with new nodes and edges being\ncontinually integrated. Consequently, due to the limited generalization\ncapacity of condensed graphs, applications that employ GC for efficient GNN\ntraining end up with sub-optimal GNNs when confronted with evolving graph\nstructures and distributions in dynamic real-world situations. To overcome this\nissue, we propose open-world graph condensation (OpenGC), a robust GC framework\nthat integrates structure-aware distribution shift to simulate evolving graph\npatterns and exploit the temporal environments for invariance condensation.\nThis approach is designed to extract temporal invariant patterns from the\noriginal graph, thereby enhancing the generalization capabilities of the\ncondensed graph and, subsequently, the GNNs trained on it. Extensive\nexperiments on both real-world and synthetic evolving graphs demonstrate that\nOpenGC outperforms state-of-the-art (SOTA) GC methods in adapting to dynamic\nchanges in open-world graph environments.\n'] , ['  Subgraph isomorphism counting is an important problem on graphs, as many\ngraph-based tasks exploit recurring subgraph patterns. Classical methods\nusually boil down to a backtracking framework that needs to navigate a huge\nsearch space with prohibitive computational costs. Some recent studies resort\nto graph neural networks (GNNs) to learn a low-dimensional representation for\nboth the query and input graphs, in order to predict the number of subgraph\nisomorphisms on the input graph. However, typical GNNs employ a node-centric\nmessage passing scheme that receives and aggregates messages on nodes, which is\ninadequate in complex structure matching for isomorphism counting. Moreover, on\nan input graph, the space of possible query graphs is enormous, and different\nparts of the input graph will be triggered to match different queries. Thus,\nexpecting a fixed representation of the input graph to match diversely\nstructured query graphs is unrealistic. In this paper, we propose a novel GNN\ncalled Count-GNN for subgraph isomorphism counting, to deal with the above\nchallenges. At the edge level, given that an edge is an atomic unit of encoding\ngraph structures, we propose an edge-centric message passing scheme, where\nmessages on edges are propagated and aggregated based on the edge adjacency to\npreserve fine-grained structural information. At the graph level, we modulate\nthe input graph representation conditioned on the query, so that the input\ngraph can be adapted to each query individually to improve their matching.\nFinally, we conduct extensive experiments on a number of benchmark datasets to\ndemonstrate the superior performance of Count-GNN.\n', ""  GNN-to-MLP distillation aims to utilize knowledge distillation (KD) to learn\ncomputationally-efficient multi-layer perceptron (student MLP) on graph data by\nmimicking the output representations of teacher GNN. Existing methods mainly\nmake the MLP to mimic the GNN predictions over a few class labels. However, the\nclass space may not be expressive enough for covering numerous diverse local\ngraph structures, thus limiting the performance of knowledge transfer from GNN\nto MLP. To address this issue, we propose to learn a new powerful graph\nrepresentation space by directly labeling nodes' diverse local structures for\nGNN-to-MLP distillation. Specifically, we propose a variant of VQ-VAE to learn\na structure-aware tokenizer on graph data that can encode each node's local\nsubstructure as a discrete code. The discrete codes constitute a codebook as a\nnew graph representation space that is able to identify different local graph\nstructures of nodes with the corresponding code indices. Then, based on the\nlearned codebook, we propose a new distillation target, namely soft code\nassignments, to directly transfer the structural knowledge of each node from\nGNN to MLP. The resulting framework VQGraph achieves new state-of-the-art\nperformance on GNN-to-MLP distillation in both transductive and inductive\nsettings across seven graph datasets. We show that VQGraph with better\nperformance infers faster than GNNs by 828x, and also achieves accuracy\nimprovement over GNNs and stand-alone MLPs by 3.90% and 28.05% on average,\nrespectively. Code: https://github.com/YangLing0818/VQGraph.\n"", '  Graph Neural Networks (GNNs) are deep-learning architectures designed for\ngraph-type data, where understanding relationships among individual\nobservations is crucial. However, achieving promising GNN performance,\nespecially on unseen data, requires comprehensive hyperparameter tuning and\nmeticulous training. Unfortunately, these processes come with high\ncomputational costs and significant human effort. Additionally, conventional\nsearching algorithms such as grid search may result in overfitting on\nvalidation data, diminishing generalization accuracy. To tackle these\nchallenges, we propose a graph conditional latent diffusion framework\n(GNN-Diff) to generate high-performing GNNs directly by learning from\ncheckpoints saved during a light-tuning coarse search. Our method: (1)\nunleashes GNN training from heavy tuning and complex search space design; (2)\nproduces GNN parameters that outperform those obtained through comprehensive\ngrid search; and (3) establishes higher-quality generation for GNNs compared to\ndiffusion frameworks designed for general neural networks.\n']",Graph Neural Networks (GNNs) and Graph Data Analysis,Graph Condensation for Efficient GNN Training
244,"Graph Neural Networks for Enhanced Representation Learning , Graph Contrastive Learning (GCL) Methods , Spectral Graph Neural Networks , Graph Neural Networks for Node Classification , Graph Clustering and Representation Learning , Graph Learning and Node Classification , Graph Neural Networks and Architectures","['graphs', 'subgraph', 'networks', 'subgraphs', 'graph', 'nodes', 'node', 'vertices', 'neural', 'edge'] , ['subgraph', 'hypergraph', 'graphs', 'graphmae', 'graphacl', 'graphlearner', 'hypergraphs', 'embeddings', 'graph', 'supervised'] , ['spectral', 'graphs', 'spectrally', 'networks', 'graph', 'filters', 'convolutions', 'filtering', 'nodes', 'filter'] , ['networks', 'graphs', 'nodes', 'prediction', 'graph', 'classification', 'node', 'neural', 'gnns', 'predictions'] , ['hypergraph', 'subgraph', 'graphs', 'hypergraphs', 'graph', 'adjacency', 'networks', 'nodes', 'vertices', 'embeddings'] , ['graphs', 'subgraph', 'nodes', 'networks', 'graph', 'subgraphs', 'supervised', 'node', 'learning', 'labeled'] , ['graphs', 'networks', 'graph', 'graphgpt', 'adjacency', 'neural', 'nodes', 'node', 'edges', 'network']","['  Graph neural networks (GNNs) have become the \\textit{de facto} standard for\nrepresentational learning in graphs, and have achieved state-of-the-art\nperformance in many graph-related tasks; however, it has been shown that the\nexpressive power of standard GNNs are equivalent maximally to 1-dimensional\nWeisfeiler-Lehman (1-WL) Test. Recently, there is a line of works aiming to\nenhance the expressive power of graph neural networks. One line of such works\naim at developing $K$-hop message-passing GNNs where node representation is\nupdated by aggregating information from not only direct neighbors but all\nneighbors within $K$-hop of the node. Another line of works leverages subgraph\ninformation to enhance the expressive power which is proven to be strictly more\npowerful than 1-WL test. In this work, we discuss the limitation of $K$-hop\nmessage-passing GNNs and propose \\textit{substructure encoding function} to\nuplift the expressive power of any $K$-hop message-passing GNN. We further\ninject contextualized substructure information to enhance the expressiveness of\n$K$-hop message-passing GNNs. Our method is provably more powerful than\nprevious works on $K$-hop graph neural networks and 1-WL subgraph GNNs, which\nis a specific type of subgraph based GNN models, and not less powerful than\n3-WL. Empirically, our proposed method set new state-of-the-art performance or\nachieves comparable performance for a variety of datasets. Our code is\navailable at \\url{https://github.com/tianyao-aka/Expresive_K_hop_GNNs}.\n', ""  Graph Neural Network (GNN), with the main idea of encoding graph structure\ninformation of graphs by propagation and aggregation, has developed rapidly. It\nachieved excellent performance in representation learning of multiple types of\ngraphs such as homogeneous graphs, heterogeneous graphs, and more complex\ngraphs like knowledge graphs. However, merely stacking GNN layers may not\nimprove the model's performance and can even be detrimental. For the phenomenon\nof performance degradation in deep GNNs, we propose a new perspective. Unlike\nthe popular explanations of over-smoothing or over-squashing, we think the\nissue arises from the interference of low-quality node representations during\nmessage propagation. We introduce a simple and general method, SF-GNN, to\naddress this problem. In SF-GNN, we define two representations for each node,\none is the node representation that represents the feature of the node itself,\nand the other is the message representation specifically for propagating\nmessages to neighbor nodes. A self-filter module evaluates the quality of the\nnode representation and decides whether to integrate it into the message\npropagation based on this quality assessment. Experiments on node\nclassification tasks for both homogeneous and heterogeneous graphs, as well as\nlink prediction tasks on knowledge graphs, demonstrate that our method can be\napplied to various GNN models and outperforms state-of-the-art baseline methods\nin addressing deep GNN degradation.\n"", '  Graph neural networks (GNNs) have achieved state-of-the-art performance in\ngraph representation learning. Message passing neural networks, which learn\nrepresentations through recursively aggregating information from each node and\nits neighbors, are among the most commonly-used GNNs. However, a wealth of\nstructural information of individual nodes and full graphs is often ignored in\nsuch process, which restricts the expressive power of GNNs. Various graph data\naugmentation methods that enable the message passing with richer structure\nknowledge have been introduced as one main way to tackle this issue, but they\nare often focused on individual structure features and difficult to scale up\nwith more structure features. In this work we propose a novel approach, namely\ncollective structure knowledge-augmented graph neural network (CoS-GNN), in\nwhich a new message passing method is introduced to allow GNNs to harness a\ndiverse set of node- and graph-level structure features, together with original\nnode features/attributes, in augmented graphs. In doing so, our approach\nlargely improves the structural knowledge modeling of GNNs in both node and\ngraph levels, resulting in substantially improved graph representations. This\nis justified by extensive empirical results where CoS-GNN outperforms\nstate-of-the-art models in various graph-level learning tasks, including graph\nclassification, anomaly detection, and out-of-distribution generalization.\n'] , [""  Graph contrastive learning (GCL) is a popular method for leaning graph\nrepresentations by maximizing the consistency of features across augmented\nviews. Traditional GCL methods utilize single-perspective i.e. data or\nmodel-perspective) augmentation to generate positive samples, restraining the\ndiversity of positive samples. In addition, these positive samples may be\nunreliable due to uncontrollable augmentation strategies that potentially alter\nthe semantic information. To address these challenges, this paper proposed a\ninnovative framework termed dual-perspective cross graph contrastive learning\n(DC-GCL), which incorporates three modifications designed to enhance positive\nsample diversity and reliability: 1) We propose dual-perspective augmentation\nstrategy that provide the model with more diverse training data, enabling the\nmodel effective learning of feature consistency across different views. 2) From\nthe data perspective, we slightly perturb the original graphs using\ncontrollable data augmentation, effectively preserving their semantic\ninformation. 3) From the model perspective, we enhance the encoder by utilizing\nmore powerful graph transformers instead of graph neural networks. Based on the\nmodel's architecture, we propose three pruning-based strategies to slightly\nperturb the encoder, providing more reliable positive samples. These\nmodifications collectively form the DC-GCL's foundation and provide more\ndiverse and reliable training inputs, offering significant improvements over\ntraditional GCL methods. Extensive experiments on various benchmarks\ndemonstrate that DC-GCL consistently outperforms different baselines on various\ndatasets and tasks.\n"", '  Graph contrastive learning (GCL) is an effective paradigm for node\nrepresentation learning in graphs. The key components hidden behind GCL are\ndata augmentation and positive-negative pair selection. Typical data\naugmentations in GCL, such as uniform deletion of edges, are generally blind\nand resort to local perturbation, which is prone to producing under-diversity\nviews. Additionally, there is a risk of making the augmented data traverse to\nother classes. Moreover, most methods always treat all other samples as\nnegatives. Such a negative pairing naturally results in sampling bias and\nlikewise may make the learned representation suffer from semantic drift.\nTherefore, to increase the diversity of the contrastive view, we propose two\nsimple and effective global topological augmentations to compensate current\nGCL. One is to mine the semantic correlation between nodes in the feature\nspace. The other is to utilize the algebraic properties of the adjacency matrix\nto characterize the topology by eigen-decomposition. With the help of both, we\ncan retain important edges to build a better view. To reduce the risk of\nsemantic drift, a prototype-based negative pair selection is further designed\nwhich can filter false negative samples. Extensive experiments on various tasks\ndemonstrate the advantages of the model compared to the state-of-the-art\nmethods.\n', '  Graph Contrastive Learning (GCL) has emerged as a popular training approach\nfor learning node embeddings from augmented graphs without labels. Despite the\nkey principle that maximizing the similarity between positive node pairs while\nminimizing it between negative node pairs is well established, some fundamental\nproblems are still unclear. Considering the complex graph structure, are some\nnodes consistently well-trained and following this principle even with\ndifferent graph augmentations? Or are there some nodes more likely to be\nuntrained across graph augmentations and violate the principle? How to\ndistinguish these nodes and further guide the training of GCL? To answer these\nquestions, we first present experimental evidence showing that the training of\nGCL is indeed imbalanced across all nodes. To address this problem, we propose\nthe metric ""node compactness"", which is the lower bound of how a node follows\nthe GCL principle related to the range of augmentations. We further derive the\nform of node compactness theoretically through bound propagation, which can be\nintegrated into binary cross-entropy as a regularization. To this end, we\npropose the PrOvable Training (POT) for GCL, which regularizes the training of\nGCL to encode node embeddings that follows the GCL principle better. Through\nextensive experiments on various benchmarks, POT consistently improves the\nexisting GCL approaches, serving as a friendly plugin.\n'] , ['  Spectral Graph Neural Networks (GNNs) have achieved tremendous success in\ngraph learning. As an essential part of spectral GNNs, spectral graph\nconvolution extracts crucial frequency information in graph data, leading to\nsuperior performance of spectral GNNs in downstream tasks. However, in this\npaper, we show that existing spectral GNNs remain critical drawbacks in\nperforming the spectral graph convolution. Specifically, considering the\nspectral graph convolution as a construction operation towards target output,\nwe prove that existing popular convolution paradigms cannot construct the\ntarget output with mild conditions on input graph signals, causing spectral\nGNNs to fall into suboptimal solutions. To address the issues, we rethink the\nspectral graph convolution from a more general two-dimensional (2-D) signal\nconvolution perspective and propose a new convolution paradigm, named 2-D graph\nconvolution. We prove that 2-D graph convolution unifies existing graph\nconvolution paradigms, and is capable to construct arbitrary target output.\nBased on the proposed 2-D graph convolution, we further propose ChebNet2D, an\nefficient and effective GNN implementation of 2-D graph convolution through\napplying Chebyshev interpolation. Extensive experiments on benchmark datasets\ndemonstrate both effectiveness and efficiency of the ChebNet2D.\n', '  Whilst spectral Graph Neural Networks (GNNs) are theoretically well-founded\nin the spectral domain, their practical reliance on polynomial approximation\nimplies a profound linkage to the spatial domain. As previous studies rarely\nexamine spectral GNNs from the spatial perspective, their spatial-domain\ninterpretability remains elusive, e.g., what information is essentially encoded\nby spectral GNNs in the spatial domain? In this paper, to answer this question,\nwe investigate the theoretical connection between spectral filtering and\nspatial aggregation, unveiling an intrinsic interaction that spectral filtering\nimplicitly leads the original graph to an adapted new graph, explicitly\ncomputed for spatial aggregation. Both theoretical and empirical investigations\nreveal that the adapted new graph not only exhibits non-locality but also\naccommodates signed edge weights to reflect label consistency among nodes.\nThese findings thus highlight the interpretable role of spectral GNNs in the\nspatial domain and inspire us to rethink graph spectral filters beyond the\nfixed-order polynomials, which neglect global information. Built upon the\ntheoretical findings, we revisit the state-of-the-art spectral GNNs and propose\na novel Spatially Adaptive Filtering (SAF) framework, which leverages the\nadapted new graph by spectral filtering for an auxiliary non-local aggregation.\nNotably, our SAF comprehensively models both node similarity and dissimilarity\nfrom a global perspective, therefore alleviating persistent deficiencies of\nGNNs related to long-range dependencies and graph heterophily. Extensive\nexperiments over 13 node classification benchmarks demonstrate the superiority\nof our proposed framework to the state-of-the-art methods.\n', '  With the recent advancements in graph neural networks (GNNs), spectral GNNs\nhave received increasing popularity by virtue of their specialty in capturing\ngraph signals in the frequency domain, demonstrating promising capability in\nspecific tasks. However, few systematic studies have been conducted on\nassessing their spectral characteristics. This emerging family of models also\nvaries in terms of designs and settings, leading to difficulties in comparing\ntheir performance and deciding on the suitable model for specific scenarios,\nespecially for large-scale tasks. In this work, we extensively benchmark\nspectral GNNs with a focus on the frequency perspective. We analyze and\ncategorize over 30 GNNs with 27 corresponding filters. Then, we implement these\nspectral models under a unified framework with dedicated graph computations and\nefficient training schemes. Thorough experiments are conducted on the spectral\nmodels with inclusive metrics on effectiveness and efficiency, offering\npractical guidelines on evaluating and selecting spectral GNNs with desirable\nperformance. Our implementation enables application on larger graphs with\ncomparable performance and less overhead, which is available at:\nhttps://github.com/gdmnl/Spectral-GNN-Benchmark.\n'] , ['  Graph Neural Networks have achieved remarkable accuracy in semi-supervised\nnode classification tasks. However, these results lack reliable uncertainty\nestimates. Conformal prediction methods provide a theoretical guarantee for\nnode classification tasks, ensuring that the conformal prediction set contains\nthe ground-truth label with a desired probability (e.g., 95%). In this paper,\nwe empirically show that for each node, aggregating the non-conformity scores\nof nodes with the same label can improve the efficiency of conformal prediction\nsets. This observation motivates us to propose a novel algorithm named\nSimilarity-Navigated Adaptive Prediction Sets (SNAPS), which aggregates the\nnon-conformity scores based on feature similarity and structural neighborhood.\nThe key idea behind SNAPS is that nodes with high feature similarity or direct\nconnections tend to have the same label. By incorporating adaptive similar\nnodes information, SNAPS can generate compact prediction sets and increase the\nsingleton hit ratio (correct prediction sets of size one). Moreover, we\ntheoretically provide a finite-sample coverage guarantee of SNAPS. Extensive\nexperiments demonstrate the superiority of SNAPS, improving the efficiency of\nprediction sets and singleton hit ratio while maintaining valid coverage.\n', '  While graph neural networks (GNNs) are widely used for node and graph\nrepresentation learning tasks, the reliability of GNN uncertainty estimates\nunder distribution shifts remains relatively under-explored. Indeed, while\npost-hoc calibration strategies can be used to improve in-distribution\ncalibration, they need not also improve calibration under distribution shift.\nHowever, techniques which produce GNNs with better intrinsic uncertainty\nestimates are particularly valuable, as they can always be combined with\npost-hoc strategies later. Therefore, in this work, we propose G-$\\Delta$UQ, a\nnovel training framework designed to improve intrinsic GNN uncertainty\nestimates. Our framework adapts the principle of stochastic data centering to\ngraph data through novel graph anchoring strategies, and is able to support\npartially stochastic GNNs. While, the prevalent wisdom is that fully stochastic\nnetworks are necessary to obtain reliable estimates, we find that the\nfunctional diversity induced by our anchoring strategies when sampling\nhypotheses renders this unnecessary and allows us to support G-$\\Delta$UQ on\npretrained models. Indeed, through extensive evaluation under covariate,\nconcept and graph size shifts, we show that G-$\\Delta$UQ leads to better\ncalibrated GNNs for node and graph classification. Further, it also improves\nperformance on the uncertainty-based tasks of out-of-distribution detection and\ngeneralization gap estimation. Overall, our work provides insights into\nuncertainty estimation for GNNs, and demonstrates the utility of G-$\\Delta$UQ\nin obtaining reliable estimates.\n', '  Graph Neural Networks (GNNs) have emerged as potent tools for predicting\noutcomes in graph-structured data. Despite their efficacy, a significant\ndrawback of GNNs lies in their limited ability to provide robust uncertainty\nestimates, posing challenges to their reliability in contexts where errors\ncarry significant consequences. Moreover, GNNs typically excel in\nin-distribution settings, assuming that training and test data follow identical\ndistributions a condition often unmet in real world graph data scenarios. In\nthis article, we leverage conformal prediction, a widely recognized statistical\ntechnique for quantifying uncertainty by transforming predictive model outputs\ninto prediction sets, to address uncertainty quantification in GNN predictions\namidst conditional shift\\footnote{Representing the change in conditional\nprobability distribution \\(P(label|input)\\) from source domain to target\ndomain.} in graph-based semi-supervised learning (SSL). Additionally, we\npropose a novel loss function aimed at refining model predictions by minimizing\nconditional shift in latent stages. Termed Conditional Shift Robust (CondSR)\nconformal prediction for GNNs, our approach CondSR is model-agnostic and\nadaptable to various classification models. We validate the effectiveness of\nour method on standard graph benchmark datasets, integrating it with\nstate-of-the-art GNNs in node classification tasks. Comprehensive evaluations\ndemonstrate that our approach consistently achieves any predefined target\nmarginal coverage, enhances the accuracy of state of the art GNN models by up\nto 12\\% under conditional shift, and reduces the prediction set size by up to\n48\\%. The code implementation is publicly available for further exploration and\nexperimentation.\n'] , ['  Graph-based clustering plays an important role in the clustering area. Recent\nstudies about graph convolution neural networks have achieved impressive\nsuccess on graph type data. However, in general clustering tasks, the graph\nstructure of data does not exist such that the strategy to construct a graph is\ncrucial for performance. Therefore, how to extend graph convolution networks\ninto general clustering tasks is an attractive problem. In this paper, we\npropose a graph auto-encoder for general data clustering, which constructs the\ngraph adaptively according to the generative perspective of graphs. The\nadaptive process is designed to induce the model to exploit the high-level\ninformation behind data and utilize the non-Euclidean structure sufficiently.\nWe further design a novel mechanism with rigorous analysis to avoid the\ncollapse caused by the adaptive construction. Via combining the generative\nmodel for network embedding and graph-based clustering, a graph auto-encoder\nwith a novel decoder is developed such that it performs well in weighted graph\nused scenarios. Extensive experiments prove the superiority of our model.\n', '  Clustering holds profound significance in data mining. In recent years, graph\nconvolutional network (GCN) has emerged as a powerful tool for deep clustering,\nintegrating both graph structural information and node attributes. However,\nmost existing methods ignore the higher-order structural information of the\ngraph. Evidently, nodes within the same cluster can establish distant\nconnections. Besides, recent deep clustering methods usually apply a\nself-supervised module to monitor the training process of their model, focusing\nsolely on node attributes without paying attention to graph structure. In this\npaper, we propose a novel graph clustering network to make full use of graph\nstructural information. To capture the higher-order structural information, we\ndesign a graph mutual infomax module, effectively maximizing mutual information\nbetween graph-level and node-level representations, and employ a trinary\nself-supervised module that includes modularity as a structural constraint. Our\nproposed model outperforms many state-of-the-art methods on various datasets,\ndemonstrating its superiority.\n', '  Hypergraphs, with their capacity to depict high-order relationships, have\nemerged as a significant extension of traditional graphs. Although Graph Neural\nNetworks (GNNs) have remarkable performance in graph representation learning,\ntheir extension to hypergraphs encounters challenges due to their intricate\nstructures. Furthermore, current hypergraph transformers, a special variant of\nGNN, utilize semantic feature-based self-attention, ignoring topological\nattributes of nodes and hyperedges. To address these challenges, we propose a\nTopology-guided Hypergraph Transformer Network (THTN). In this model, we first\nformulate a hypergraph from a graph while retaining its structural essence to\nlearn higher-order relations within the graph. Then, we design a simple yet\neffective structural and spatial encoding module to incorporate the topological\nand spatial information of the nodes into their representation. Further, we\npresent a structure-aware self-attention mechanism that discovers the important\nnodes and hyperedges from both semantic and structural viewpoints. By\nleveraging these two modules, THTN crafts an improved node representation,\ncapturing both local and global topological expressions. Extensive experiments\nconducted on node classification tasks demonstrate that the performance of the\nproposed model consistently exceeds that of the existing approaches.\n'] , ['  In this work, we study semi-supervised multi-label node classification\nproblem in attributed graphs. Classic solutions to multi-label node\nclassification follow two steps, first learn node embedding and then build a\nnode classifier on the learned embedding. To improve the discriminating power\nof the node embedding, we propose a novel collaborative graph walk, named\nMulti-Label-Graph-Walk, to finely tune node representations with the available\nlabel assignments in attributed graphs via reinforcement learning. The proposed\nmethod formulates the multi-label node classification task as simultaneous\ngraph walks conducted by multiple label-specific agents. Furthermore, policies\nof the label-wise graph walks are learned in a cooperative way to capture first\nthe predictive relation between node labels and structural attributes of\ngraphs; and second, the correlation among the multiple label-specific\nclassification tasks. A comprehensive experimental study demonstrates that the\nproposed method can achieve significantly better multi-label classification\nperformance than the state-of-the-art approaches and conduct more efficient\ngraph exploration.\n', '  Graph learning plays a pivotal role and has gained significant attention in\nvarious application scenarios, from social network analysis to recommendation\nsystems, for its effectiveness in modeling complex data relations represented\nby graph structural data. In reality, the real-world graph data typically show\ndynamics over time, with changing node attributes and edge structure, leading\nto the severe graph data distribution shift issue. This issue is compounded by\nthe diverse and complex nature of distribution shifts, which can significantly\nimpact the performance of graph learning methods in degraded generalization and\nadaptation capabilities, posing a substantial challenge to their effectiveness.\nIn this survey, we provide a comprehensive review and summary of the latest\napproaches, strategies, and insights that address distribution shifts within\nthe context of graph learning. Concretely, according to the observability of\ndistributions in the inference stage and the availability of sufficient\nsupervision information in the training stage, we categorize existing graph\nlearning methods into several essential scenarios, including graph domain\nadaptation learning, graph out-of-distribution learning, and graph continual\nlearning. For each scenario, a detailed taxonomy is proposed, with specific\ndescriptions and discussions of existing progress made in distribution-shifted\ngraph learning. Additionally, we discuss the potential applications and future\ndirections for graph learning under distribution shifts with a systematic\nanalysis of the current state in this field. The survey is positioned to\nprovide general guidance for the development of effective graph learning\nalgorithms in handling graph distribution shifts, and to stimulate future\nresearch and advancements in this area.\n', ""  Graph learning has become indispensable for interpreting and harnessing\nrelational data in diverse fields, ranging from recommendation systems to\nsocial network analysis. In this context, a variety of GNNs have emerged as\npromising methodologies for encoding the structural information of graphs. By\neffectively capturing the graph's underlying structure, these GNNs have shown\ngreat potential in enhancing performance in graph learning tasks, such as link\nprediction and node classification. However, despite their successes, a\nsignificant challenge persists: these advanced methods often face difficulties\nin generalizing to unseen graph data that significantly differs from the\ntraining instances. In this work, our aim is to advance the graph learning\nparadigm by developing a general graph foundation model. This model is designed\nto understand the complex topological patterns present in diverse graph data,\nenabling it to excel in zero-shot graph learning tasks across different\ndownstream datasets. To achieve this goal, we address several key technical\nchallenges in our OpenGraph model. Firstly, we propose a unified graph\ntokenizer to adapt our graph model to generalize well on unseen graph data,\neven when the underlying graph properties differ significantly from those\nencountered during training. Secondly, we develop a scalable graph transformer\nas the foundational encoder, which effectively captures node-wise dependencies\nwithin the global topological context. Thirdly, we introduce a data\naugmentation mechanism enhanced by a LLM to alleviate the limitations of data\nscarcity in real-world scenarios. Extensive experiments validate the\neffectiveness of our framework. By adapting our OpenGraph to new graph\ncharacteristics and comprehending the nuances of diverse graphs, our approach\nachieves remarkable zero-shot graph learning performance across various\nsettings and domains.\n""] , ['  Graph Neural Networks (GNNs) have excelled in predicting graph properties in\nvarious applications ranging from identifying trends in social networks to drug\ndiscovery and malware detection. With the abundance of new architectures and\nincreased complexity, GNNs are becoming highly specialized when tested on a few\nwell-known datasets. However, how the performance of GNNs depends on the\ntopological and features properties of graphs is still an open question. In\nthis work, we introduce a comprehensive benchmarking framework for graph\nmachine learning, focusing on the performance of GNNs across varied network\nstructures. Utilizing the geometric soft configuration model in hyperbolic\nspace, we generate synthetic networks with realistic topological properties and\nnode feature vectors. This approach enables us to assess the impact of network\nproperties, such as topology-feature correlation, degree distributions, local\ndensity of triangles (or clustering), and homophily, on the effectiveness of\ndifferent GNN architectures. Our results highlight the dependency of model\nperformance on the interplay between network structure and node features,\nproviding insights for model selection in various scenarios. This study\ncontributes to the field by offering a versatile tool for evaluating GNNs,\nthereby assisting in developing and selecting suitable models based on specific\ndata characteristics.\n', ""  Recently, transformer architectures for graphs emerged as an alternative to\nestablished techniques for machine learning with graphs, such as\n(message-passing) graph neural networks. So far, they have shown promising\nempirical results, e.g., on molecular prediction datasets, often attributed to\ntheir ability to circumvent graph neural networks' shortcomings, such as\nover-smoothing and over-squashing. Here, we derive a taxonomy of graph\ntransformer architectures, bringing some order to this emerging field. We\noverview their theoretical properties, survey structural and positional\nencodings, and discuss extensions for important graph classes, e.g., 3D\nmolecular graphs. Empirically, we probe how well graph transformers can recover\nvarious graph properties, how well they can deal with heterophilic graphs, and\nto what extent they prevent over-squashing. Further, we outline open challenges\nand research direction to stimulate future work. Our code is available at\nhttps://github.com/luis-mueller/probing-graph-transformers.\n"", ""  The irreducible complexity of natural phenomena has led Graph Neural Networks\nto be employed as a standard model to perform representation learning tasks on\ngraph-structured data. While their capacity to capture local and global\npatterns is remarkable, the implications associated with long-range and\nhigher-order dependencies pose considerable challenges to such models. This\nwork starts with a theoretical framework to reveal the impact of network's\nwidth, depth, and graph topology on the over-squashing phenomena in\nmessage-passing neural networks. Then, the work drifts towards, higher-order\ninteractions and multi-relational inductive biases via Topological Neural\nNetworks. Such models propagate messages through higher-dimensional structures,\nproviding shortcuts or additional routes for information flow. With this\nconstruction, the underlying computational graph is no longer coupled with the\ninput graph structure, thus mitigating the aforementioned bottlenecks while\naccounting also for higher-order interactions. Inspired by Graph Attention\nNetworks, two topological attention networks are proposed: Simplicial and Cell\nAttention Networks. The rationale behind these architecture is to leverage the\nextended notion of neighbourhoods provided by the arrangement of groups of\nnodes within a simplicial or cell complex to design anisotropic aggregations\nable to measure the importance of the information coming from different regions\nof the domain. By doing so, they capture dependencies that conventional Graph\nNeural Networks might miss. Finally, a multi-way communication scheme is\nintroduced with Enhanced Cellular Isomorphism Networks, which augment\ntopological message passing schemes to enable a direct interactions among\ngroups of nodes arranged in ring-like structures.\n""]",Graph Neural Networks and Deep Learning on Graph-Structured Data,Graph Neural Networks for Node Classification
245,"Neural Representations of 3D Surfaces , Sketch-based 3D Modeling and Representation Learning","['surfaces', 'surface', 'curvature', 'curvatures', 'shapes', 'mesh', '3d', 'neural', 'shape', 'meshes'] , ['sketches', 'sketching', 'sketch', 'sketchime', 'sketchinr', 'drawings', 'drawing', 'cadyface', 'abstraction', '3d']","['  Point clouds are popular 3D representations for real-life objects (such as in\nLiDAR and Kinect) due to their detailed and compact representation of\nsurface-based geometry. Recent approaches characterise the geometry of point\nclouds by bringing deep learning based techniques together with geometric\nfidelity metrics such as optimal transportation costs (e.g., Chamfer and\nWasserstein metrics). In this paper, we propose a new surface geometry\ncharacterisation within this realm, namely a neural varifold representation of\npoint clouds. Here the surface is represented as a measure/distribution over\nboth point positions and tangent spaces of point clouds. The varifold\nrepresentation quantifies not only the surface geometry of point clouds through\nthe manifold-based discrimination, but also subtle geometric consistencies on\nthe surface due to the combined product space. This study proposes neural\nvarifold algorithms to compute the varifold norm between two point clouds using\nneural networks on point clouds and their neural tangent kernel\nrepresentations. The proposed neural varifold is evaluated on three different\nsought-after tasks -- shape matching, few-shot shape classification and shape\nreconstruction. Detailed evaluation and comparison to the state-of-the-art\nmethods demonstrate that the proposed versatile neural varifold is superior in\nshape matching and few-shot shape classification, and is competitive for shape\nreconstruction.\n', '  Neural surfaces (e.g., neural map encoding, deep implicits and neural\nradiance fields) have recently gained popularity because of their generic\nstructure (e.g., multi-layer perceptron) and easy integration with modern\nlearning-based setups. Traditionally, we have a rich toolbox of geometry\nprocessing algorithms designed for polygonal meshes to analyze and operate on\nsurface geometry. However, neural representations are typically discretized and\nconverted into a mesh, before applying any geometry processing algorithm. This\nis unsatisfactory and, as we demonstrate, unnecessary. In this work, we propose\na spherical neural surface representation (a spherical parametrization) for\ngenus-0 surfaces and demonstrate how to compute core geometric operators\ndirectly on this representation. Namely, we show how to construct the normals\nand the first and second fundamental forms of the surface, and how to compute\nthe surface gradient, surface divergence and Laplace Beltrami operator on\nscalar/vector fields defined on the surface. These operators, in turn, enable\nus to create geometry processing tools that act directly on the neural\nrepresentations without any unnecessary meshing. We demonstrate illustrative\napplications in (neural) spectral analysis, heat flow and mean curvature flow,\nand our method shows robustness to isometric shape variations. We both propose\ntheoretical formulations and validate their numerical estimates. By\nsystematically linking neural surface representations with classical geometry\nprocessing algorithms, we believe this work can become a key ingredient in\nenabling neural geometry processing.\n', '  In the field of computer vision, the numerical encoding of 3D surfaces is\ncrucial. It is classical to represent surfaces with their Signed Distance\nFunctions (SDFs) or Unsigned Distance Functions (UDFs). For tasks like\nrepresentation learning, surface classification, or surface reconstruction,\nthis function can be learned by a neural network, called Neural Distance\nFunction. This network, and in particular its weights, may serve as a\nparametric and implicit representation for the surface. The network must\nrepresent the surface as accurately as possible. In this paper, we propose a\nmethod for learning UDFs that improves the fidelity of the obtained Neural UDF\nto the original 3D surface. The key idea of our method is to concentrate the\nlearning effort of the Neural UDF on surface edges. More precisely, we show\nthat sampling more training points around surface edges allows better local\naccuracy of the trained Neural UDF, and thus improves the global expressiveness\nof the Neural UDF in terms of Hausdorff distance. To detect surface edges, we\npropose a new statistical method based on the calculation of a $p$-value at\neach point on the surface. Our method is shown to detect surface edges more\naccurately than a commonly used local geometric descriptor.\n'] , ['  This paper, for the first time, marries large foundation models with human\nsketch understanding. We demonstrate what this brings -- a paradigm shift in\nterms of generalised sketch representation learning (e.g., classification).\nThis generalisation happens on two fronts: (i) generalisation across unknown\ncategories (i.e., open-set), and (ii) generalisation traversing abstraction\nlevels (i.e., good and bad sketches), both being timely challenges that remain\nunsolved in the sketch literature. Our design is intuitive and centred around\ntransferring the already stellar generalisation ability of CLIP to benefit\ngeneralised learning for sketches. We first ""condition"" the vanilla CLIP model\nby learning sketch-specific prompts using a novel auxiliary head of raster to\nvector sketch conversion. This importantly makes CLIP ""sketch-aware"". We then\nmake CLIP acute to the inherently different sketch abstraction levels. This is\nachieved by learning a codebook of abstraction-specific prompt biases, a\nweighted combination of which facilitates the representation of sketches across\nabstraction levels -- low abstract edge-maps, medium abstract sketches in\nTU-Berlin, and highly abstract doodles in QuickDraw. Our framework surpasses\npopular sketch representation learning algorithms in both zero-shot and\nfew-shot setups and in novel settings across different abstraction boundaries.\n', ""  We present SENS, a novel method for generating and editing 3D models from\nhand-drawn sketches, including those of abstract nature. Our method allows\nusers to quickly and easily sketch a shape, and then maps the sketch into the\nlatent space of a part-aware neural implicit shape architecture. SENS analyzes\nthe sketch and encodes its parts into ViT patch encoding, subsequently feeding\nthem into a transformer decoder that converts them to shape embeddings suitable\nfor editing 3D neural implicit shapes. SENS provides intuitive sketch-based\ngeneration and editing, and also succeeds in capturing the intent of the user's\nsketch to generate a variety of novel and expressive 3D shapes, even from\nabstract and imprecise sketches. Additionally, SENS supports refinement via\npart reconstruction, allowing for nuanced adjustments and artifact removal. It\nalso offers part-based modeling capabilities, enabling the combination of\nfeatures from multiple sketches to create more complex and customized 3D\nshapes. We demonstrate the effectiveness of our model compared to the\nstate-of-the-art using objective metric evaluation criteria and a user study,\nboth indicating strong performance on sketches with a medium level of\nabstraction. Furthermore, we showcase our method's intuitive sketch-based shape\nediting capabilities, and validate it through a usability study.\n"", '  Recently, text-to-3D approaches have achieved high-fidelity 3D content\ngeneration using text description. However, the generated objects are\nstochastic and lack fine-grained control. Sketches provide a cheap approach to\nintroduce such fine-grained control. Nevertheless, it is challenging to achieve\nflexible control from these sketches due to their abstraction and ambiguity. In\nthis paper, we present a multi-view sketch-guided text-to-3D generation\nframework (namely, Sketch2NeRF) to add sketch control to 3D generation.\nSpecifically, our method leverages pretrained 2D diffusion models (e.g., Stable\nDiffusion and ControlNet) to supervise the optimization of a 3D scene\nrepresented by a neural radiance field (NeRF). We propose a novel synchronized\ngeneration and reconstruction method to effectively optimize the NeRF. In the\nexperiments, we collected two kinds of multi-view sketch datasets to evaluate\nthe proposed method. We demonstrate that our method can synthesize 3D\nconsistent contents with fine-grained sketch control while being high-fidelity\nto text prompts. Extensive results show that our method achieves\nstate-of-the-art performance in terms of sketch similarity and text alignment.\n']",3D Geometry and Shape Representation Learning,Neural Representations of 3D Surfaces
246,"Masked Autoencoders for Image Representation Learning , Disentangled Representation Learning , Contrastive Learning for Representation","['autoencoders', 'autoencoder', 'masking', 'imagenet', 'mask', 'learning', 'masked', 'supervised', 'training', 'learned'] , ['disentangling', 'disentangled', 'disentanglement', 'representations', 'autoencoders', 'entangled', 'generative', 'representation', 'autoencoder', 'encode'] , ['imagenet', 'contrastive', 'learning', 'embeddings', 'unlearning', 'views', 'supervised', 'view', 'representations', 'unsupervised']","['  Masked Autoencoder (MAE) has demonstrated superior performance on various\nvision tasks via randomly masking image patches and reconstruction. However,\neffective data augmentation strategies for MAE still remain open questions,\ndifferent from those in contrastive learning that serve as the most important\npart. This paper studies the prevailing mixing augmentation for MAE. We first\ndemonstrate that naive mixing will in contrast degenerate model performance due\nto the increase of mutual information (MI). To address, we propose homologous\nrecognition, an auxiliary pretext task, not only to alleviate the MI\nincreasement by explicitly requiring each patch to recognize homologous\npatches, but also to perform object-aware self-supervised pre-training for\nbetter downstream dense perception performance. With extensive experiments, we\ndemonstrate that our proposed Mixed Autoencoder (MixedAE) achieves the\nstate-of-the-art transfer results among masked image modeling (MIM)\naugmentations on different downstream tasks with significant efficiency.\nSpecifically, our MixedAE outperforms MAE by +0.3% accuracy, +1.7 mIoU and +0.9\nAP on ImageNet-1K, ADE20K and COCO respectively with a standard ViT-Base.\nMoreover, MixedAE surpasses iBOT, a strong MIM method combined with instance\ndiscrimination, while accelerating training by 2x. To our best knowledge, this\nis the very first work to consider mixing for MIM from the perspective of\npretext task design. Code will be made available.\n', '  Masked image modeling has been demonstrated as a powerful pretext task for\ngenerating robust representations that can be effectively generalized across\nmultiple downstream tasks. Typically, this approach involves randomly masking\npatches (tokens) in input images, with the masking strategy remaining unchanged\nduring training. In this paper, we propose a curriculum learning approach that\nupdates the masking strategy to continually increase the complexity of the\nself-supervised reconstruction task. We conjecture that, by gradually\nincreasing the task complexity, the model can learn more sophisticated and\ntransferable representations. To facilitate this, we introduce a novel\nlearnable masking module that possesses the capability to generate masks of\ndifferent complexities, and integrate the proposed module into masked\nautoencoders (MAE). Our module is jointly trained with the MAE, while adjusting\nits behavior during training, transitioning from a partner to the MAE\n(optimizing the same reconstruction loss) to an adversary (optimizing the\nopposite loss), while passing through a neutral state. The transition between\nthese behaviors is smooth, being regulated by a factor that is multiplied with\nthe reconstruction loss of the masking module. The resulting training procedure\ngenerates an easy-to-hard curriculum. We train our Curriculum-Learned Masked\nAutoencoder (CL-MAE) on ImageNet and show that it exhibits superior\nrepresentation learning capabilities compared to MAE. The empirical results on\nfive downstream tasks confirm our conjecture, demonstrating that curriculum\nlearning can be successfully used to self-supervise masked autoencoders. We\nrelease our code at https://github.com/ristea/cl-mae.\n', '  Masked image modeling (MIM) has been recognized as a strong self-supervised\npre-training approach in the vision domain. However, the mechanism and\nproperties of the learned representations by such a scheme, as well as how to\nfurther enhance the representations are so far not well-explored. In this\npaper, we aim to explore an interactive Masked Autoencoders (i-MAE) framework\nto enhance the representation capability from two aspects: (1) employing a\ntwo-way image reconstruction and a latent feature reconstruction with\ndistillation loss to learn better features; (2) proposing a semantics-enhanced\nsampling strategy to boost the learned semantics in MAE. Upon the proposed\ni-MAE architecture, we can address two critical questions to explore the\nbehaviors of the learned representations in MAE: (1) Whether the separability\nof latent representations in Masked Autoencoders is helpful for model\nperformance? We study it by forcing the input as a mixture of two images\ninstead of one. (2) Whether we can enhance the representations in the latent\nfeature space by controlling the degree of semantics during sampling on Masked\nAutoencoders? To this end, we propose a sampling strategy within a mini-batch\nbased on the semantics of training samples to examine this aspect. Extensive\nexperiments are conducted on CIFAR-10/100, Tiny-ImageNet and ImageNet-1K to\nverify the observations we discovered. Furthermore, in addition to\nqualitatively analyzing the characteristics of the latent representations, we\nexamine the existence of linear separability and the degree of semantics in the\nlatent space by proposing two evaluation schemes. The surprising and consistent\nresults demonstrate that i-MAE is a superior framework design for understanding\nMAE frameworks, as well as achieving better representational ability. Code is\navailable at https://github.com/vision-learning-acceleration-lab/i-mae.\n'] , ['  Representation learning is an approach that allows to discover and extract\nthe factors of variation from the data. Intuitively, a representation is said\nto be disentangled if it separates the different factors of variation in a way\nthat is understandable to humans. Definitions of disentanglement and metrics to\nmeasure it usually assume that the factors of variation are independent of each\nother. However, this is generally false in the real world, which limits the use\nof these definitions and metrics to very specific and unrealistic scenarios. In\nthis paper we give a definition of disentanglement based on information theory\nthat is also valid when the factors of variation are not independent.\nFurthermore, we relate this definition to the Information Bottleneck Method.\nFinally, we propose a method to measure the degree of disentanglement from the\ngiven definition that works when the factors of variation are not independent.\nWe show through different experiments that the method proposed in this paper\ncorrectly measures disentanglement with non-independent factors of variation,\nwhile other methods fail in this scenario.\n', '  Current autoencoder-based disentangled representation learning methods\nachieve disentanglement by penalizing the (aggregate) posterior to encourage\nstatistical independence of the latent factors. This approach introduces a\ntrade-off between disentangled representation learning and reconstruction\nquality since the model does not have enough capacity to learn correlated\nlatent variables that capture detail information present in most image data. To\novercome this trade-off, we present a novel multi-stage modeling approach where\nthe disentangled factors are first learned using a penalty-based disentangled\nrepresentation learning method; then, the low-quality reconstruction is\nimproved with another deep generative model that is trained to model the\nmissing correlated latent variables, adding detail information while\nmaintaining conditioning on the previously learned disentangled factors. Taken\ntogether, our multi-stage modelling approach results in a single, coherent\nprobabilistic model that is theoretically justified by the principal of\nD-separation and can be realized with a variety of model classes including\nlikelihood-based models such as variational autoencoders, implicit models such\nas generative adversarial networks, and tractable models like normalizing flows\nor mixtures of Gaussians. We demonstrate that our multi-stage model has higher\nreconstruction quality than current state-of-the-art methods with equivalent\ndisentanglement performance across multiple standard benchmarks. In addition,\nwe apply the multi-stage model to generate synthetic tabular datasets,\nshowcasing an enhanced performance over benchmark models across a variety of\nmetrics. The interpretability analysis further indicates that the multi-stage\nmodel can effectively uncover distinct and meaningful features of variations\nfrom which the original distribution can be recovered.\n', '  In representation learning, a disentangled representation is highly desirable\nas it encodes generative factors of data in a separable and compact pattern.\nResearchers have advocated leveraging disentangled representations to complete\ndownstream tasks with encouraging empirical evidence. This paper further\ninvestigates the necessity of disentangled representation in downstream\napplications. Specifically, we show that dimension-wise disentangled\nrepresentations are unnecessary on a fundamental downstream task, abstract\nvisual reasoning. We provide extensive empirical evidence against the necessity\nof disentanglement, covering multiple datasets, representation learning\nmethods, and downstream network architectures. Furthermore, our findings\nsuggest that the informativeness of representations is a better indicator of\ndownstream performance than disentanglement. Finally, the positive correlation\nbetween informativeness and disentanglement explains the claimed usefulness of\ndisentangled representations in previous works. The source code is available at\nhttps://github.com/Richard-coder-Nai/disentanglement-lib-necessity.git.\n'] , ['  In contrastive learning, two views of an original image, generated by\ndifferent augmentations, are considered a positive pair, and their similarity\nis required to be high. Similarly, two views of distinct images form a negative\npair, with encouraged low similarity. Typically, a single similarity measure,\nprovided by a lone projection head, evaluates positive and negative sample\npairs. However, due to diverse augmentation strategies and varying intra-sample\nsimilarity, views from the same image may not always be similar. Additionally,\nowing to inter-sample similarity, views from different images may be more akin\nthan those from the same image. Consequently, enforcing high similarity for\npositive pairs and low similarity for negative pairs may be unattainable, and\nin some cases, such enforcement could detrimentally impact performance. To\naddress this challenge, we propose using multiple projection heads, each\nproducing a distinct set of features. Our pre-training loss function emerges\nfrom a solution to the maximum likelihood estimation over head-wise posterior\ndistributions of positive samples given observations. This loss incorporates\nthe similarity measure over positive and negative pairs, each re-weighted by an\nindividual adaptive temperature, regulated to prevent ill solutions. Our\napproach, Adaptive Multi-Head Contrastive Learning (AMCL), can be applied to\nand experimentally enhances several popular contrastive learning methods such\nas SimCLR, MoCo, and Barlow Twins. The improvement remains consistent across\nvarious backbones and linear probing epochs, and becomes more significant when\nemploying multiple augmentation methods.\n', '  In the era of big data and Artificial Intelligence, an emerging paradigm is\nto utilize contrastive self-supervised learning to model large-scale\nheterogeneous data. Many existing foundation models benefit from the\ngeneralization capability of contrastive self-supervised learning by learning\ncompact and high-quality representations without relying on any label\ninformation. Amidst the explosive advancements in foundation models across\nmultiple domains, including natural language processing and computer vision, a\nthorough survey on heterogeneous contrastive learning for the foundation model\nis urgently needed. In response, this survey critically evaluates the current\nlandscape of heterogeneous contrastive learning for foundation models,\nhighlighting the open challenges and future trends of contrastive learning. In\nparticular, we first present how the recent advanced contrastive learning-based\nmethods deal with view heterogeneity and how contrastive learning is applied to\ntrain and fine-tune the multi-view foundation models. Then, we move to\ncontrastive learning methods for task heterogeneity, including pretraining\ntasks and downstream tasks, and show how different tasks are combined with\ncontrastive learning loss for different purposes. Finally, we conclude this\nsurvey by discussing the open challenges and shedding light on the future\ndirections of contrastive learning.\n', '  Contrastive learning is a paradigm for learning representations from\nunlabelled data that has been highly successful for image and text data.\nSeveral recent works have examined contrastive losses to claim that contrastive\nmodels effectively learn spectral embeddings, while few works show relations\nbetween (wide) contrastive models and kernel principal component analysis\n(PCA). However, it is not known if trained contrastive models indeed correspond\nto kernel methods or PCA. In this work, we analyze the training dynamics of\ntwo-layer contrastive models, with non-linear activation, and answer when these\nmodels are close to PCA or kernel methods. It is well known in the supervised\nsetting that neural networks are equivalent to neural tangent kernel (NTK)\nmachines, and that the NTK of infinitely wide networks remains constant during\ntraining. We provide the first convergence results of NTK for contrastive\nlosses, and present a nuanced picture: NTK of wide networks remains almost\nconstant for cosine similarity based contrastive losses, but not for losses\nbased on dot product similarity. We further study the training dynamics of\ncontrastive models with orthogonality constraints on output layer, which is\nimplicitly assumed in works relating contrastive learning to spectral\nembedding. Our deviation bounds suggest that representations learned by\ncontrastive models are close to the principal components of a certain matrix\ncomputed from random features. We empirically show that our theoretical results\npossibly hold beyond two-layer networks.\n']",Self-Supervised Representation Learning,Disentangled Representation Learning
247,"Graph Embeddings with Text Attributes , Temporal Graph Embeddings and Link Prediction , Natural Language Embeddings and Detection","['graphllm', 'graphprompter', 'graphs', 'embeddings', 'textual', 'nodes', 'graph', 'networks', 'supervised', 'node'] , ['networks', 'graphs', 'nodes', 'rnns', 'graph', 'temporal', 'rnn', 'node', 'prediction', 'embeddings'] , ['embeddings', 'nlp', 'embedding', 'attention', 'textual', 'encoder', 'descriptors', 'detection', 'representations', 'semantic']","['  We present Simplified Text-Attributed Graph Embeddings (STAGE), a\nstraightforward yet effective method for enhancing node features in Graph\nNeural Network (GNN) models that encode Text-Attributed Graphs (TAGs). Our\napproach leverages Large-Language Models (LLMs) to generate embeddings for\ntextual attributes. STAGE achieves competitive results on various node\nclassification benchmarks while also maintaining a simplicity in implementation\nrelative to current state-of-the-art (SoTA) techniques. We show that utilizing\npre-trained LLMs as embedding generators provides robust features for ensemble\nGNN training, enabling pipelines that are simpler than current SoTA approaches\nwhich require multiple expensive training and prompting stages. We also\nimplement diffusion-pattern GNNs in an effort to make this pipeline scalable to\ngraphs beyond academic benchmarks.\n', '  The text-attributed graph (TAG) is one kind of important real-world\ngraph-structured data with each node associated with raw texts. For TAGs,\ntraditional few-shot node classification methods directly conduct training on\nthe pre-processed node features and do not consider the raw texts. The\nperformance is highly dependent on the choice of the feature pre-processing\nmethod. In this paper, we propose P2TAG, a framework designed for few-shot node\nclassification on TAGs with graph pre-training and prompting. P2TAG first\npre-trains the language model (LM) and graph neural network (GNN) on TAGs with\nself-supervised loss. To fully utilize the ability of language models, we adapt\nthe masked language modeling objective for our framework. The pre-trained model\nis then used for the few-shot node classification with a mixed prompt method,\nwhich simultaneously considers both text and graph information. We conduct\nexperiments on six real-world TAGs, including paper citation networks and\nproduct co-purchasing networks. Experimental results demonstrate that our\nproposed framework outperforms existing graph few-shot learning methods on\nthese datasets with +18.98% ~ +35.98% improvements.\n', ""  Foundation models like ChatGPT and GPT-4 have revolutionized artificial\nintelligence, exhibiting remarkable abilities to generalize across a wide array\nof tasks and applications beyond their initial training objectives. However,\nwhen this concept is applied to graph learning, a stark contrast emerges. Graph\nlearning has predominantly focused on single-graph models, tailored to specific\ntasks or datasets, lacking the ability to transfer learned knowledge to\ndifferent domains. This limitation stems from the inherent complexity and\ndiversity of graph structures, along with the different feature and label\nspaces specific to graph data. In this paper, we present our UniGraph\nframework, designed to train a graph foundation model capable of generalizing\nto unseen graphs and tasks across diverse domains. Unlike single-graph models\nthat use pre-computed node features of varying dimensions as input, our\napproach leverages Text-Attributed Graphs (TAGs) for unifying node\nrepresentations. We propose a cascaded architecture of Language Models (LMs)\nand Graph Neural Networks (GNNs) as backbone networks with a self-supervised\ntraining objective based on Masked Graph Modeling (MGM). We introduce graph\ninstruction tuning using Large Language Models (LLMs) to enable zero-shot\nprediction ability. Our comprehensive experiments across various graph learning\ntasks and domains demonstrate the model's effectiveness in self-supervised\nrepresentation learning on unseen graphs, few-shot in-context transfer, and\nzero-shot transfer, even surpassing or matching the performance of GNNs that\nhave undergone supervised training on target datasets.\n""] , [""  Graphs are a powerful representation tool in machine learning applications,\nwith link prediction being a key task in graph learning. Temporal link\nprediction in dynamic networks is of particular interest due to its potential\nfor solving complex scientific and real-world problems. Traditional approaches\nto temporal link prediction have focused on finding the aggregation of dynamics\nof the network as a unified output. In this study, we propose a novel\nperspective on temporal link prediction by defining nodes as Newtonian objects\nand incorporating the concept of velocity to predict network dynamics. By\ncomputing more specific dynamics of each node, rather than overall dynamics, we\nimprove both accuracy and explainability in predicting future connections. We\ndemonstrate the effectiveness of our approach using two datasets, including 17\nyears of co-authorship data from PubMed. Experimental results show that our\ntemporal graph embedding dynamics approach improves downstream classification\nmodels' ability to predict future collaboration efficacy in co-authorship\nnetworks by 17.34% (AUROC improvement relative to the baseline model).\nFurthermore, our approach offers an interpretable layer over traditional\napproaches to address the temporal link prediction problem.\n"", '  Inductive representation learning on temporal heterogeneous graphs is crucial\nfor scalable deep learning on heterogeneous information networks (HINs) which\nare time-varying, such as citation networks. However, most existing approaches\nare not inductive and thus cannot handle new nodes or edges. Moreover, previous\ntemporal graph embedding methods are often trained with the temporal link\nprediction task to simulate the link formation process of temporal graphs,\nwhile ignoring the evolution of high-order topological structures on temporal\ngraphs. To fill these gaps, we propose a Continuous-Time Representation\nLearning (CTRL) model on temporal HINs. To preserve heterogeneous node features\nand temporal structures, CTRL integrates three parts in a single layer, they\nare 1) a \\emph{heterogeneous attention} unit that measures the semantic\ncorrelation between nodes, 2) a \\emph{edge-based Hawkes process} to capture\ntemporal influence between heterogeneous nodes, and 3) \\emph{dynamic\ncentrality} that indicates the dynamic importance of a node. We train the CTRL\nmodel with a future event (a subgraph) prediction task to capture the evolution\nof the high-order network structure. Extensive experiments have been conducted\non three benchmark datasets. The results demonstrate that our model\nsignificantly boosts performance and outperforms various state-of-the-art\napproaches. Ablation studies are conducted to demonstrate the effectiveness of\nthe model design.\n', ""  Dynamic link prediction is an important problem considered by many recent\nworks proposing various approaches for learning temporal edge patterns. To\nassess their efficacy, models are evaluated on publicly available benchmark\ndatasets involving continuous-time and discrete-time temporal graphs. However,\nas we show in this work, the suitability of common batch-oriented evaluation\ndepends on the datasets' characteristics, which can cause two issues: First,\nfor continuous-time temporal graphs, fixed-size batches create time windows\nwith different durations, resulting in an inconsistent dynamic link prediction\ntask. Second, for discrete-time temporal graphs, the sequence of batches can\nadditionally introduce temporal dependencies that are not present in the data.\nIn this work, we empirically show that this common evaluation approach leads to\nskewed model performance and hinders the fair comparison of methods. We\nmitigate this problem by reformulating dynamic link prediction as a link\nforecasting task that better accounts for temporal information present in the\ndata. We provide implementations of our new evaluation method for commonly used\ngraph learning frameworks.\n""] , ['  For natural language understanding and generation, embedding concepts using\nan order-based representation is an essential task. Unlike traditional point\nvector based representation, an order-based representation imposes geometric\nconstraints on the representation vectors for explicitly capturing various\nsemantic relationships that may exist between a pair of concepts. In existing\nliterature, several approaches on order-based embedding have been proposed,\nmostly focusing on capturing hierarchical relationships; examples include\nvectors in Euclidean space, complex, Hyperbolic, order, and Box Embedding. Box\nembedding creates region-based rich representation of concepts, but along the\nprocess it sacrifices simplicity, requiring a custom-made optimization scheme\nfor learning the representation. Hyperbolic embedding improves embedding\nquality by exploiting the ever-expanding property of Hyperbolic space, but it\nalso suffers from the same fate as box embedding as gradient descent like\noptimization is not simple in the Hyperbolic space. In this work, we propose\nBinder, a novel approach for order-based representation. Binder uses binary\nvectors for embedding, so the embedding vectors are compact with an order of\nmagnitude smaller footprint than other methods. Binder uses a simple and\nefficient optimization scheme for learning representation vectors with a linear\ntime complexity. Our comprehensive experimental results show that Binder is\nvery accurate, yielding competitive results on the representation task. But\nBinder stands out from its competitors on the transitive closure link\nprediction task as it can learn concept embeddings just from the direct edges,\nwhereas all existing order-based approaches rely on the indirect edges.\n', '  Out-of-distribution (OOD) detection plays a crucial role in ensuring the\nsafety and reliability of deep neural networks in various applications. While\nthere has been a growing focus on OOD detection in visual data, the field of\ntextual OOD detection has received less attention. Only a few attempts have\nbeen made to directly apply general OOD detection methods to natural language\nprocessing (NLP) tasks, without adequately considering the characteristics of\ntextual data. In this paper, we delve into textual OOD detection with\nTransformers. We first identify a key problem prevalent in existing OOD\ndetection methods: the biased representation learned through the maximization\nof the conditional likelihood $p(y\\mid x)$ can potentially result in subpar\nperformance. We then propose a novel variational inference framework for OOD\ndetection (VI-OOD), which maximizes the likelihood of the joint distribution\n$p(x, y)$ instead of $p(y\\mid x)$. VI-OOD is tailored for textual OOD detection\nby efficiently exploiting the representations of pre-trained Transformers.\nThrough comprehensive experiments on various text classification tasks, VI-OOD\ndemonstrates its effectiveness and wide applicability. Our code has been\nreleased at \\url{https://github.com/liam0949/LLM-OOD}.\n', '  Out-of-distribution (OOD) detection plays a vital role in enhancing the\nreliability of machine learning (ML) models. The emergence of large language\nmodels (LLMs) has catalyzed a paradigm shift within the ML community,\nshowcasing their exceptional capabilities across diverse natural language\nprocessing tasks. While existing research has probed OOD detection with\nrelative small-scale Transformers like BERT, RoBERTa and GPT-2, the stark\ndifferences in scales, pre-training objectives, and inference paradigms call\ninto question the applicability of these findings to LLMs. This paper embarks\non a pioneering empirical investigation of OOD detection in the domain of LLMs,\nfocusing on LLaMA series ranging from 7B to 65B in size. We thoroughly evaluate\ncommonly-used OOD detectors, scrutinizing their performance in both zero-grad\nand fine-tuning scenarios. Notably, we alter previous discriminative\nin-distribution fine-tuning into generative fine-tuning, aligning the\npre-training objective of LLMs with downstream tasks. Our findings unveil that\na simple cosine distance OOD detector demonstrates superior efficacy,\noutperforming other OOD detectors. We provide an intriguing explanation for\nthis phenomenon by highlighting the isotropic nature of the embedding spaces of\nLLMs, which distinctly contrasts with the anisotropic property observed in\nsmaller BERT family models. The new insight enhances our understanding of how\nLLMs detect OOD data, thereby enhancing their adaptability and reliability in\ndynamic environments. We have released the source code at\n\\url{https://github.com/Awenbocc/LLM-OOD} for other researchers to reproduce\nour results.\n']",Graph and Text Embeddings for Representation Learning,Graph Embeddings with Text Attributes
248,"Hyperbolic Embeddings and Manifolds , ""Manifold Learning and Graph Laplacians"" , Manifold Learning with Autoencoders","['hyperbolic', 'embeddings', 'embedding', 'manifolds', 'manifold', 'dimensional', 'cnns', 'riemannian', 'classification', 'hessian'] , ['hypergraph', 'laplacian', 'graphs', 'manifolds', 'regularization', 'manifold', 'dimensionality', 'clustering', 'graphons', 'supervised'] , ['autoencoders', 'autoencoder', 'embeddings', 'encoder', 'dimensionality', 'dimensional', 'manifolds', 'manifold', 'representations', 'descriptors']","['  Hyperbolic embeddings have demonstrated their effectiveness in capturing\nmeasures of uncertainty and hierarchical relationships across various\ndeep-learning tasks, including image segmentation and active learning. However,\ntheir application in modern vision-language models (VLMs) has been limited. A\nnotable exception is MERU, which leverages the hierarchical properties of\nhyperbolic space in the CLIP ViT-large model, consisting of hundreds of\nmillions parameters. In our work, we address the challenges of scaling\nmulti-modal hyperbolic models by orders of magnitude in terms of parameters\n(billions) and training complexity using the BLIP-2 architecture. Although\nhyperbolic embeddings offer potential insights into uncertainty not present in\nEuclidean embeddings, our analysis reveals that scaling these models is\nparticularly difficult. We propose a novel training strategy for a hyperbolic\nversion of BLIP-2, which allows to achieve comparable performance to its\nEuclidean counterpart, while maintaining stability throughout the training\nprocess and showing a meaningful indication of uncertainty with each embedding.\n', '  The need to understand the structure of hierarchical or high-dimensional data\nis present in a variety of fields. Hyperbolic spaces have proven to be an\nimportant tool for embedding computations and analysis tasks as their\nnon-linear nature lends itself well to tree or graph data. Subsequently, they\nhave also been used in the visualization of high-dimensional data, where they\nexhibit increased embedding performance. However, none of the existing\ndimensionality reduction methods for embedding into hyperbolic spaces scale\nwell with the size of the input data. That is because the embeddings are\ncomputed via iterative optimization schemes and the computation cost of every\niteration is quadratic in the size of the input. Furthermore, due to the\nnon-linear nature of hyperbolic spaces, Euclidean acceleration structures\ncannot directly be translated to the hyperbolic setting. This paper introduces\nthe first acceleration structure for hyperbolic embeddings, building upon a\npolar quadtree. We compare our approach with existing methods and demonstrate\nthat it computes embeddings of similar quality in significantly less time.\nImplementation and scripts for the experiments can be found at\nhttps://graphics.tudelft.nl/accelerating-hyperbolic-tsne.\n', '  Hyperbolic geometry is gaining traction in machine learning for its\neffectiveness at capturing hierarchical structures in real-world data.\nHyperbolic spaces, where neighborhoods grow exponentially, offer substantial\nadvantages and consistently deliver state-of-the-art results across diverse\napplications. However, hyperbolic classifiers often grapple with computational\nchallenges. Methods reliant on Riemannian optimization frequently exhibit\nsluggishness, stemming from the increased computational demands of operations\non Riemannian manifolds. In response to these challenges, we present hyperDT, a\nnovel extension of decision tree algorithms into hyperbolic space. Crucially,\nhyperDT eliminates the need for computationally intensive Riemannian\noptimization, numerically unstable exponential and logarithmic maps, or\npairwise comparisons between points by leveraging inner products to adapt\nEuclidean decision tree algorithms to hyperbolic space. Our approach is\nconceptually straightforward and maintains constant-time decision complexity\nwhile mitigating the scalability issues inherent in high-dimensional Euclidean\nspaces. Building upon hyperDT we introduce hyperRF, a hyperbolic random forest\nmodel. Extensive benchmarking across diverse datasets underscores the superior\nperformance of these models, providing a swift, precise, accurate, and\nuser-friendly toolkit for hyperbolic data analysis.\n'] , [""  This paper extends the possibility to examine the underlying curvature of\ndata through the lens of topology by using the Betti curves, tools of\nPersistent Homology, as key topological descriptors, building on the clique\ntopology approach. It was previously shown that Betti curves distinguish random\nfrom Euclidean geometric matrices - i.e. distance matrices of points randomly\ndistributed in a cube with Euclidean distance. In line with previous\nexperiments, we consider their low-dimensional approximations named integral\nBetti values, or signatures that effectively distinguish not only Euclidean,\nbut also spherical and hyperbolic geometric matrices, both from purely random\nmatrices as well as among themselves. To prove this, we analyse the behaviour\nof Betti curves for various geometric matrices -- i.e. distance matrices of\npoints randomly distributed on manifolds of constant sectional curvature,\nconsidering the classical models of curvature 0, 1, -1, given by the Euclidean\nspace, the sphere, and the hyperbolic space. We further investigate the\ndependence of integral Betti signatures on factors including the sample size\nand dimension. This is important for assessment of real-world connectivity\nmatrices, as we show that the standard approach to network construction gives\nrise to (spurious) spherical geometry, with topology dependent on sample\ndimensions. Finally, we use the manifolds of constant curvature as comparison\nmodels to infer curvature underlying real-world datasets coming from\nneuroscience, finance and climate. Their associated topological features\nexhibit a hyperbolic character: the integral Betti signatures associated to\nthese datasets sit in between Euclidean and hyperbolic (of small curvature).\nThe potential confounding ``hyperbologenic effect'' of intrinsic low-rank\nmodular structures is also evaluated through simulations.\n"", '  Bi-stochastic normalization provides an alternative normalization of graph\nLaplacians in graph-based data analysis and can be computed efficiently by\nSinkhorn-Knopp (SK) iterations. This paper proves the convergence of\nbi-stochastically normalized graph Laplacian to manifold (weighted-)Laplacian\nwith rates, when $n$ data points are i.i.d. sampled from a general\n$d$-dimensional manifold embedded in a possibly high-dimensional space. Under\ncertain joint limit of $n \\to \\infty$ and kernel bandwidth $\\epsilon \\to 0$,\nthe point-wise convergence rate of the graph Laplacian operator (under 2-norm)\nis proved to be $ O( n^{-1/(d/2+3)})$ at finite large $n$ up to log factors,\nachieved at the scaling of $\\epsilon \\sim n^{-1/(d/2+3)} $. When the manifold\ndata are corrupted by outlier noise, we theoretically prove the graph Laplacian\npoint-wise consistency which matches the rate for clean manifold data plus an\nadditional term proportional to the boundedness of the inner-products of the\nnoise vectors among themselves and with data vectors. Motivated by our\nanalysis, which suggests that not exact bi-stochastic normalization but an\napproximate one will achieve the same consistency rate, we propose an\napproximate and constrained matrix scaling problem that can be solved by SK\niterations with early termination. Numerical experiments support our\ntheoretical results and show the robustness of bi-stochastically normalized\ngraph Laplacian to high-dimensional outlier noise.\n', ""  Graph Laplacian based algorithms for data lying on a manifold have been\nproven effective for tasks such as dimensionality reduction, clustering, and\ndenoising. In this work, we consider data sets whose data points lie on a\nmanifold that is closed under the action of a known unitary matrix Lie group G.\nWe propose to construct the graph Laplacian by incorporating the distances\nbetween all the pairs of points generated by the action of G on the data set.\nWe deem the latter construction the ``G-invariant Graph Laplacian'' (G-GL). We\nshow that the G-GL converges to the Laplace-Beltrami operator on the data\nmanifold, while enjoying a significantly improved convergence rate compared to\nthe standard graph Laplacian which only utilizes the distances between the\npoints in the given data set. Furthermore, we show that the G-GL admits a set\nof eigenfunctions that have the form of certain products between the group\nelements and eigenvectors of certain matrices, which can be estimated from the\ndata efficiently using FFT-type algorithms. We demonstrate our construction and\nits advantages on the problem of filtering data on a noisy manifold closed\nunder the action of the special unitary group SU(2).\n""] , ['  Representing a manifold of very high-dimensional data with generative models\nhas been shown to be computationally efficient in practice. However, this\nrequires that the data manifold admits a global parameterization. In order to\nrepresent manifolds of arbitrary topology, we propose to learn a mixture model\nof variational autoencoders. Here, every encoder-decoder pair represents one\nchart of a manifold. We propose a loss function for maximum likelihood\nestimation of the model weights and choose an architecture that provides us the\nanalytical expression of the charts and of their inverses. Once the manifold is\nlearned, we use it for solving inverse problems by minimizing a data fidelity\nterm restricted to the learned manifold. To solve the arising minimization\nproblem we propose a Riemannian gradient descent algorithm on the learned\nmanifold. We demonstrate the performance of our method for low-dimensional toy\nexamples as well as for deblurring and electrical impedance tomography on\ncertain image manifolds.\n', '  The Manifold Hypothesis is a widely accepted tenet of Machine Learning which\nasserts that nominally high-dimensional data are in fact concentrated near a\nlow-dimensional manifold, embedded in high-dimensional space. This phenomenon\nis observed empirically in many real world situations, has led to development\nof a wide range of statistical methods in the last few decades, and has been\nsuggested as a key factor in the success of modern AI technologies. We show\nthat rich and sometimes intricate manifold structure in data can emerge from a\ngeneric and remarkably simple statistical model -- the Latent Metric Model --\nvia elementary concepts such as latent variables, correlation and stationarity.\nThis establishes a general statistical explanation for why the Manifold\nHypothesis seems to hold in so many situations. Informed by the Latent Metric\nModel we derive procedures to discover and interpret the geometry of\nhigh-dimensional data, and explore hypotheses about the data generating\nmechanism. These procedures operate under minimal assumptions and make use of\nwell known, scaleable graph-analytic algorithms.\n', '  Autoencoders, which consist of an encoder and a decoder, are widely used in\nmachine learning for dimension reduction of high-dimensional data. The encoder\nembeds the input data manifold into a lower-dimensional latent space, while the\ndecoder represents the inverse map, providing a parametrization of the data\nmanifold by the manifold in latent space. A good regularity and structure of\nthe embedded manifold may substantially simplify further data processing tasks\nsuch as cluster analysis or data interpolation. We propose and analyze a novel\nregularization for learning the encoder component of an autoencoder: a loss\nfunctional that prefers isometric, extrinsically flat embeddings and allows to\ntrain the encoder on its own. To perform the training it is assumed that for\npairs of nearby points on the input manifold their local Riemannian distance\nand their local Riemannian average can be evaluated. The loss functional is\ncomputed via Monte Carlo integration with different sampling strategies for\npairs of points on the input manifold. Our main theorem identifies a geometric\nloss functional of the embedding map as the $\\Gamma$-limit of the\nsampling-dependent loss functionals. Numerical tests, using image data that\nencodes different explicitly given data manifolds, show that smooth manifold\nembeddings into latent space are obtained. Due to the promotion of extrinsic\nflatness, these embeddings are regular enough such that interpolation between\nnot too distant points on the manifold is well approximated by linear\ninterpolation in latent space as one possible postprocessing.\n']",Geometric Deep Learning on Manifolds,Manifold Learning with Autoencoders
249,"""Deep Learning for Inverse Problems and Imaging"" , ""Machine Learning for Inverse Problems and Kernel Methods""","['regularization', 'inversions', 'inverse', 'inversion', 'neural', 'deep', 'cnn', 'invertible', 'encoder', 'imaging'] , ['regularization', 'kernels', 'lasso', 'classification', 'learning', 'classifier', 'regularized', 'kernel', 'generalization', 'svm']","[""  Inverse scattering problems are inherently challenging, given the fact they\nare ill-posed and nonlinear. This paper presents a powerful deep learning-based\napproach that relies on generative adversarial networks to accurately and\nefficiently reconstruct randomly-shaped two-dimensional dielectric objects from\namplitudes of multi-frequency scattered electric fields. An adversarial\nautoencoder (AAE) is trained to learn to generate the scatterer's geometry from\na lower-dimensional latent representation constrained to adhere to the Gaussian\ndistribution. A cohesive inverse neural network (INN) framework is set up\ncomprising a sequence of appropriately designed dense layers, the\nalready-trained generator as well as a separately trained forward neural\nnetwork. The images reconstructed at the output of the inverse network are\nvalidated through comparison with outputs from the forward neural network,\naddressing the non-uniqueness challenge inherent to electromagnetic (EM)\nimaging problems. The trained INN demonstrates an enhanced robustness,\nevidenced by a mean binary cross-entropy (BCE) loss of $0.13$ and a structure\nsimilarity index (SSI) of $0.90$. The study not only demonstrates a significant\nreduction in computational load, but also marks a substantial improvement over\ntraditional objective-function-based methods. It contributes both to the fields\nof machine learning and EM imaging by offering a real-time quantitative imaging\napproach. The results obtained with the simulated data, for both training and\ntesting, yield promising results and may open new avenues for radio-frequency\ninverse imaging.\n"", '  Physics-Informed Neural Networks (PINNs) are a machine learning technique for\nsolving partial differential equations (PDEs) by incorporating PDEs as loss\nterms in neural networks and minimizing the loss function during training.\nTomographic imaging, a method to reconstruct internal properties from external\nmeasurement data, is highly complex and ill-posed, making it an inverse\nproblem. Recently, PINNs have shown significant potential in computational\nfluid dynamics (CFD) and have advantages in solving inverse problems. However,\nexisting research has primarily focused on semi-inverse Electrical Impedance\nTomography (EIT), where internal electric potentials are accessible. The\npractical full inverse EIT problem, where only boundary voltage measurements\nare available, remains challenging. To address this, we propose a two-stage\nhybrid learning framework combining Convolutional Neural Networks (CNNs) and\nPINNs to solve the full inverse EIT problem. This framework integrates\ndata-driven and model-driven approaches, combines supervised and unsupervised\nlearning, and decouples the forward and inverse problems within the PINN\nframework in EIT. Stage I: a U-Net constructs an end-to-end mapping from\nboundary voltage measurements to the internal potential distribution using\nsupervised learning. Stage II: a Multilayer Perceptron (MLP)-based PINN takes\nthe predicted internal potentials as input to solve for the conductivity\ndistribution through unsupervised learning.\n', '  Regularization is critical for solving ill-posed geophysical inverse\nproblems. Explicit regularization is often used, but there are opportunities to\nexplore the implicit regularization effects that are inherent in a Neural\nNetwork structure. Researchers have discovered that the Convolutional Neural\nNetwork (CNN) architecture inherently enforces a regularization that is\nadvantageous for addressing diverse inverse problems in computer vision,\nincluding de-noising and in-painting. In this study, we examine the\napplicability of this implicit regularization to geophysical inversions. The\nCNN maps an arbitrary vector to the model space. The predicted subsurface model\nis then fed into a forward numerical simulation to generate corresponding\npredicted measurements. Subsequently, the objective function value is computed\nby comparing these predicted measurements with the observed measurements. The\nbackpropagation algorithm is employed to update the trainable parameters of the\nCNN during the inversion. Note that the CNN in our proposed method does not\nrequire training before the inversion, rather, the CNN weights are estimated in\nthe inversion process, hence this is a test-time learning (TTL) approach. In\nthis study, we choose to focus on the Direct Current (DC) resistivity inverse\nproblem, which is representative of typical Tikhonov-style geophysical\ninversions (e.g. gravity, electromagnetic, etc.), to test our hypothesis. The\nexperimental results demonstrate that the implicit regularization can be useful\nin some DC resistivity inversions. We also provide a discussion of the\npotential sources of this implicit regularization introduced from the CNN\narchitecture and discuss some practical guides for applying the proposed method\nto other geophysical methods.\n'] , ['  In inverse problems, it is widely recognized that the incorporation of a\nsparsity prior yields a regularization effect on the solution. This approach is\ngrounded on the a priori assumption that the unknown can be appropriately\nrepresented in a basis with a limited number of significant components, while\nmost coefficients are close to zero. This occurrence is frequently observed in\nreal-world scenarios, such as with piecewise smooth signals. In this study, we\npropose a probabilistic sparsity prior formulated as a mixture of degenerate\nGaussians, capable of modeling sparsity with respect to a generic basis. Under\nthis premise, we design a neural network that can be interpreted as the Bayes\nestimator for linear inverse problems. Additionally, we put forth both a\nsupervised and an unsupervised training strategy to estimate the parameters of\nthis network. To evaluate the effectiveness of our approach, we conduct a\nnumerical comparison with commonly employed sparsity-promoting regularization\ntechniques, namely LASSO, group LASSO, iterative hard thresholding, and sparse\ncoding/dictionary learning. Notably, our reconstructions consistently exhibit\nlower mean square error values across all $1$D datasets utilized for the\ncomparisons, even in cases where the datasets significantly deviate from a\nGaussian mixture model.\n', '  This paper presents new and effective algorithms for learning kernels. In\nparticular, as shown by our empirical results, these algorithms consistently\noutperform the so-called uniform combination solution that has proven to be\ndifficult to improve upon in the past, as well as other algorithms for learning\nkernels based on convex combinations of base kernels in both classification and\nregression. Our algorithms are based on the notion of centered alignment which\nis used as a similarity measure between kernels or kernel matrices. We present\na number of novel algorithmic, theoretical, and empirical results for learning\nkernels based on our notion of centered alignment. In particular, we describe\nefficient algorithms for learning a maximum alignment kernel by showing that\nthe problem can be reduced to a simple QP and discuss a one-stage algorithm for\nlearning both a kernel and a hypothesis based on that kernel using an\nalignment-based regularization. Our theoretical results include a novel\nconcentration bound for centered alignment between kernel matrices, the proof\nof the existence of effective predictors for kernels with high alignment, both\nfor classification and for regression, and the proof of stability-based\ngeneralization bounds for a broad family of algorithms for learning kernels\nbased on centered alignment. We also report the results of experiments with our\ncentered alignment-based algorithms in both classification and regression.\n', '  Recent advances in machine learning have inspired a surge of research into\nreconstructing specific quantities of interest from measurements that comply\nwith certain physical laws. These efforts focus on inverse problems that are\ngoverned by partial differential equations (PDEs). In this work, we develop an\nasymptotic Sobolev norm learning curve for kernel ridge(less) regression when\naddressing (elliptical) linear inverse problems. Our results show that the PDE\noperators in the inverse problem can stabilize the variance and even behave\nbenign overfitting for fixed-dimensional problems, exhibiting different\nbehaviors from regression problems. Besides, our investigation also\ndemonstrates the impact of various inductive biases introduced by minimizing\ndifferent Sobolev norms as a form of implicit regularization. For the\nregularized least squares estimator, we find that all considered inductive\nbiases can achieve the optimal convergence rate, provided the regularization\nparameter is appropriately chosen. The convergence rate is actually independent\nto the choice of (smooth enough) inductive bias for both ridge and ridgeless\nregression. Surprisingly, our smoothness requirement recovered the condition\nfound in Bayesian setting and extend the conclusion to the minimum norm\ninterpolation estimators.\n']",Machine Learning for Inverse Problems,"""Deep Learning for Inverse Problems and Imaging"""
250,"Seismic Waveform Inversion with Deep Learning , ""Geological Exploration using Deep Learning and Drilling Data"" , ""Physics-based Deep Learning for 3D Shape Sensing and Dynamics""","['seismic', 'waveform', 'deep', 'inversion', 'predicting', 'forecasting', 'neural', 'prediction', 'deeponet', 'wave'] , ['cnn', 'cnns', 'drilling', 'boulders', 'geological', 'neural', 'deep', 'networks', 'learning', 'drill'] , ['neural', 'shape', '3d', 'shapes', 'deep', 'sensing', 'mechanics', 'rigid', 'dynamics', 'linearization']","['  Full waveform inversion (FWI) infers the subsurface structure information\nfrom seismic waveform data by solving a non-convex optimization problem.\nData-driven FWI has been increasingly studied with various neural network\narchitectures to improve accuracy and computational efficiency. Nevertheless,\nthe applicability of pre-trained neural networks is severely restricted by\npotential discrepancies between the source function used in the field survey\nand the one utilized during training. Here, we develop a Fourier-enhanced deep\noperator network (Fourier-DeepONet) for FWI with the generalization of seismic\nsources, including the frequencies and locations of sources. Specifically, we\nemploy the Fourier neural operator as the decoder of DeepONet, and we utilize\nsource parameters as one input of Fourier-DeepONet, facilitating the resolution\nof FWI with variable sources. To test Fourier-DeepONet, we develop three new\nand realistic FWI benchmark datasets (FWI-F, FWI-L, and FWI-FL) with varying\nsource frequencies, locations, or both. Our experiments demonstrate that\ncompared with existing data-driven FWI methods, Fourier-DeepONet obtains more\naccurate predictions of subsurface structures in a wide range of source\nparameters. Moreover, the proposed Fourier-DeepONet exhibits superior\nrobustness when handling data with Gaussian noise or missing traces and sources\nwith Gaussian noise, paving the way for more reliable and accurate subsurface\nimaging across diverse real conditions.\n', '  Full-waveform inversion (FWI) plays a vital role in geoscience to explore the\nsubsurface. It utilizes the seismic wave to image the subsurface velocity map.\nAs the machine learning (ML) technique evolves, the data-driven approaches\nusing ML for FWI tasks have emerged, offering enhanced accuracy and reduced\ncomputational cost compared to traditional physics-based methods. However, a\ncommon challenge in geoscience, the unprivileged data, severely limits ML\neffectiveness. The issue becomes even worse during model pruning, a step\nessential in geoscience due to environmental complexities. To tackle this, we\nintroduce the EdGeo toolkit, which employs a diffusion-based model guided by\nphysics principles to generate high-fidelity velocity maps. The toolkit uses\nthe acoustic wave equation to generate corresponding seismic waveform data,\nfacilitating the fine-tuning of pruned ML models. Our results demonstrate\nsignificant improvements in SSIM scores and reduction in both MAE and MSE\nacross various pruning ratios. Notably, the ML model fine-tuned using data\ngenerated by EdGeo yields superior quality of velocity maps, especially in\nrepresenting unprivileged features, outperforming other existing methods.\n', '  Full-Waveform Inversion (FWI) is a nonlinear iterative seismic imaging\ntechnique that, by reducing the misfit between recorded and predicted seismic\nwaveforms, can produce detailed estimates of subsurface geophysical properties.\nNevertheless, the strong nonlinearity of FWI can trap the optimization in local\nminima. This issue arises due to factors such as improper initial values, the\nabsence of low frequencies in the measurements, noise, and other related\nconsiderations. To address this challenge and with the advent of advanced\nmachine-learning techniques, data-driven methods, such as deep learning, have\nattracted significantly increasing attention in the geophysical community.\nFurthermore, the elastic wave equation should be included in FWI to represent\nelastic effects accurately. The intersection of data-driven techniques and\nelastic scattering theories presents opportunities and challenges. In this\npaper, by using the knowledge of elastic scattering (Physics of problem) and\nintegrating it with deep learning techniques, we propose methods for the\nsolution of time-harmonic FWI to enhance accuracy compared to pure data-driven\napproaches. Moreover, by modifying the structure of the Variational\nAutoencoder, we introduce a probabilistic deep learning method based on the\nphysics of the problem that enables us to explore the uncertainties of the\nsolution. According to the limited availability of datasets in this field and\nto assess the performance and accuracy of the proposed methods, we create a\ncomprehensive dataset close to reality and conduct a comparative analysis of\nthe presented approaches to it.\n'] , ['  Deep subsurface exploration is important for mining, oil and gas industries,\nas well as in the assessment of geological units for the disposal of chemical\nor nuclear waste, or the viability of geothermal energy systems. Typically,\ndetailed examinations of subsurface formations or units are performed on\ncuttings or core materials extracted during drilling campaigns, as well as on\ngeophysical borehole data, which provide detailed information about the\npetrophysical properties of the rocks. Depending on the volume of rock samples\nand the analytical program, the laboratory analysis and diagnostics can be very\ntime-consuming. This study investigates the potential of utilizing machine\nlearning, specifically convolutional neural networks (CNN), to assess the\nlithology and mineral content solely from analysis of drill core images, aiming\nto support and expedite the subsurface geological exploration. The paper\noutlines a comprehensive methodology, encompassing data preprocessing, machine\nlearning methods, and transfer learning techniques. The outcome reveals a\nremarkable 96.7% accuracy in the classification of drill core segments into\ndistinct formation classes. Furthermore, a CNN model was trained for the\nevaluation of mineral content using a learning data set from multidimensional\nlog analysis data (silicate, total clay, carbonate). When benchmarked against\nlaboratory XRD measurements on samples from the cores, both the advanced\nmultidimensional log analysis model and the neural network approach developed\nhere provide equally good performance. This work demonstrates that deep\nlearning and particularly transfer learning can support extracting\npetrophysical properties, including mineral content and formation\nclassification, from drill core images, thus offering a road map for enhancing\nmodel performance and data set quality in image-based analysis of drill cores.\n', ""  Current rock engineering design in drill and blast tunnelling primarily\nrelies on engineers' observational assessments. Measure While Drilling (MWD)\ndata, a high-resolution sensor dataset collected during tunnel excavation, is\nunderutilised, mainly serving for geological visualisation. This study aims to\nautomate the translation of MWD data into actionable metrics for rock\nengineering. It seeks to link data to specific engineering actions, thus\nproviding critical decision support for geological challenges ahead of the\ntunnel face. Leveraging a large and geologically diverse dataset of 500,000\ndrillholes from 15 tunnels, the research introduces models for accurate rock\nmass quality classification in a real-world tunnelling context. Both\nconventional machine learning and image-based deep learning are explored to\nclassify MWD data into Q-classes and Q-values, examples of metrics describing\nthe stability of the rock mass, using both tabular and image data. The results\nindicate that the K-nearest neighbours algorithm in an ensemble with tree-based\nmodels using tabular data, effectively classifies rock mass quality. It\nachieves a cross-validated balanced accuracy of 0.86 in classifying rock mass\ninto the Q-classes A, B, C, D, E1, E2, and 0.95 for a binary classification\nwith E versus the rest. Classification using a CNN with MWD-images for each\nblasting round resulted in a balanced accuracy of 0.82 for binary\nclassification. Regressing the Q-value from tabular MWD-data achieved\ncross-validated R2 and MSE scores of 0.80 and 0.18 for a similar ensemble model\nas in classification. High performance in regression and classification boosts\nconfidence in automated rock mass assessment. Applying advanced modelling on a\nunique dataset demonstrates MWD data's value in improving rock mass\nclassification accuracy and advancing data-driven rock engineering design,\nreducing manual intervention.\n"", '  Rock mass classification systems are crucial for assessing stability and risk\nin underground construction globally and guiding support and excavation design.\nHowever, systems developed primarily in the 1970s lack access to modern\nhigh-resolution data and advanced statistical techniques, limiting their\neffectiveness as decision-support systems. Initially, we outline the\nlimitations observed in this context and later describe how a data-driven\nsystem, based on drilling data as detailed in this study, can overcome these\nlimitations. Using extracted statistical information from thousands of MWD-data\nvalues in one-meter sections of a full tunnel profile, thus working as a\nsignature of the rock mass, we have demonstrated that it is possible to form\nwell-defined clusters that can act as a foundational basis for various rock\nmass classification systems. We reduced the dimensionality of 48-value vectors\nusing nonlinear manifold learning techniques (UMAP) and linear principal\ncomponent analysis (PCA) to enhance clustering. Unsupervised machine learning\nmethods (HDBSCAN, Agglomerative Clustering, K-means) were employed to cluster\nthe data, with hyperparameters optimised through multi-objective Bayesian\noptimisation for effective clustering. Using domain knowledge, we experienced\nimproved clustering and system tuning opportunities in adding extra features to\ncore clusters of MWD-data. We structured and correlated these clusters with\nphysical rock mass properties, including labels of rock type and rock quality,\nand analysed cumulative distributions of key MWD-parameters for rock mass\nassessment to determine if clusters meaningfully differentiate rock masses. The\nability of MWD data to form distinct rock mass clusters suggests substantial\npotential for future classification systems grounded in this objective,\ndata-driven methodology, free from human bias.\n'] , ['  In many real-world settings, image observations of freely rotating 3D rigid\nbodies may be available when low-dimensional measurements are not. However, the\nhigh-dimensionality of image data precludes the use of classical estimation\ntechniques to learn the dynamics. The usefulness of standard deep learning\nmethods is also limited, because an image of a rigid body reveals nothing about\nthe distribution of mass inside the body, which, together with initial angular\nvelocity, is what determines how the body will rotate. We present a\nphysics-based neural network model to estimate and predict 3D rotational\ndynamics from image sequences. We achieve this using a multi-stage prediction\npipeline that maps individual images to a latent representation homeomorphic to\n$\\mathbf{SO}(3)$, computes angular velocities from latent pairs, and predicts\nfuture latent states using the Hamiltonian equations of motion. We demonstrate\nthe efficacy of our approach on new rotating rigid-body datasets of sequences\nof synthetic images of rotating objects, including cubes, prisms and\nsatellites, with unknown uniform and non-uniform mass distributions. Our model\noutperforms competing baselines on our datasets, producing better qualitative\npredictions and reducing the error observed for the state-of-the-art\nHamiltonian Generative Network by a factor of 2.\n', ""  Fiber optic shape sensors have enabled unique advances in various navigation\ntasks, from medical tool tracking to industrial applications. Eccentric fiber\nBragg gratings (FBG) are cheap and easy-to-fabricate shape sensors that are\noften interrogated with simple setups. However, using low-cost interrogation\nsystems for such intensity-based quasi-distributed sensors introduces further\ncomplications to the sensor's signal. Therefore, eccentric FBGs have not been\nable to accurately estimate complex multi-bend shapes. Here, we present a novel\ntechnique to overcome these limitations and provide accurate and precise shape\nestimation in eccentric FBG sensors. We investigate the most important\nbending-induced effects in curved optical fibers that are usually eliminated in\nintensity-based fiber sensors. These effects contain shape deformation\ninformation with a higher spatial resolution that we are now able to extract\nusing deep learning techniques. We design a deep learning model based on a\nconvolutional neural network that is trained to predict shapes given the\nsensor's spectra. We also provide a visual explanation, highlighting wavelength\nelements whose intensities are more relevant in making shape predictions. These\nfindings imply that deep learning techniques benefit from the bending-induced\neffects that impact the desired signal in a complex manner. This is the first\nstep toward cheap yet accurate fiber shape sensing solutions.\n"", '  In this work, we propose a novel single-end morphing capacitive sensing\nmethod for shape tracking, FxC, by combining Folding origami structures and\nCapacitive sensing to detect the morphing structural motions using\nstate-of-the-art sensing circuits and deep learning. It was observed through\nembedding areas of origami structures with conductive materials as single-end\ncapacitive sensing patches, that the sensor signals change coherently with the\nmotion of the structure. Different from other origami capacitors where the\norigami structures are used in adjusting the thickness of the dielectric layer\nof double-plate capacitors, FxC uses only a single conductive plate per\nchannel, and the origami structure directly changes the geometry of the\nconductive plate. We examined the operation principle of morphing single-end\ncapacitors through 3D geometry simulation combined with physics theoretical\ndeduction, which deduced similar behaviour as observed in experimentation. Then\na software pipeline was developed to use the sensor signals to reconstruct the\ndynamic structural geometry through data-driven deep neural network regression\nof geometric primitives extracted from vision tracking. We created multiple\nfolding patterns to validate our approach, based on folding patterns including\nAccordion, Chevron, Sunray and V-Fold patterns with different layouts of\ncapacitive sensors using paper-based and textile-based materials.\nExperimentation results show that the geometry primitives predicted from the\ncapacitive signals have a strong correlation with the visual ground truth with\nR-squared value of up to 95% and tracking error of 6.5 mm for patches. The\nsimulation and machine learning constitute two-way information exchange between\nthe sensing signals and structural geometry.\n']",Deep Learning for Geophysical and Structural Analysis,"""Geological Exploration using Deep Learning and Drilling Data"""
251,"""Deep Learning for Medical Image Segmentation"" , ""Deep Learning for MRI Reconstruction and Imaging"" , Medical Image Analysis with Deep Learning , Deep Learning for Radio-Interferometric Imaging","['cnn', 'segmentation', 'cnns', 'segmentations', 'segmenting', 'supervised', 'deep', 'convolutional', 'attention', 'ultrasound'] , ['mri', 'imaging', 'denoising', 'tomography', 'reconstructed', 'undersampled', 'undersampling', 'deep', 'reconstruction', 'reconstructions'] , ['supervised', 'classification', 'learning', 'adversarial', 'labeling', 'deep', 'unlearning', 'trained', 'labeled', 'labels'] , ['imaging', 'deep', 'gans', 'telescope', 'iterative', 'regularization', 'astronomy', 'dnns', 'dnn', 'generative']","[""  In this study, the main objective is to develop an algorithm capable of\nidentifying and delineating tumor regions in breast ultrasound (BUS) and\nmammographic images. The technique employs two advanced deep learning\narchitectures, namely U-Net and pretrained SAM, for tumor segmentation. The\nU-Net model is specifically designed for medical image segmentation and\nleverages its deep convolutional neural network framework to extract meaningful\nfeatures from input images. On the other hand, the pretrained SAM architecture\nincorporates a mechanism to capture spatial dependencies and generate\nsegmentation results. Evaluation is conducted on a diverse dataset containing\nannotated tumor regions in BUS and mammographic images, covering both benign\nand malignant tumors. This dataset enables a comprehensive assessment of the\nalgorithm's performance across different tumor types. Results demonstrate that\nthe U-Net model outperforms the pretrained SAM architecture in accurately\nidentifying and segmenting tumor regions in both BUS and mammographic images.\nThe U-Net exhibits superior performance in challenging cases involving\nirregular shapes, indistinct boundaries, and high tumor heterogeneity. In\ncontrast, the pretrained SAM architecture exhibits limitations in accurately\nidentifying tumor areas, particularly for malignant tumors and objects with\nweak boundaries or complex shapes. These findings highlight the importance of\nselecting appropriate deep learning architectures tailored for medical image\nsegmentation. The U-Net model showcases its potential as a robust and accurate\ntool for tumor detection, while the pretrained SAM architecture suggests the\nneed for further improvements to enhance segmentation performance.\n"", '  Tumor segmentation from multi-modal brain MRI images is a challenging task\ndue to the limited samples, high variance in shapes and uneven distribution of\ntumor morphology. The performance of automated medical image segmentation has\nbeen significant improvement by the recent advances in deep learning. However,\nthe model predictions have not yet reached the desired level for clinical use\nin terms of accuracy and generalizability. In order to address the distinct\nproblems presented in Challenges 1, 2, and 3 of BraTS 2023, we have constructed\nan optimization framework based on a 3D U-Net model for brain tumor\nsegmentation. This framework incorporates a range of techniques, including\nvarious pre-processing and post-processing techniques, and transfer learning.\nOn the validation datasets, this multi-modality brain tumor segmentation\nframework achieves an average lesion-wise Dice score of 0.79, 0.72, 0.74 on\nChallenges 1, 2, 3 respectively.\n', '  Ultrasound imaging plays a critical role in the early detection of breast\ncancer. Accurate identification and segmentation of lesions are essential steps\nin clinical practice, requiring methods to assist physicians in lesion\nsegmentation. However, ultrasound lesion segmentation models based on\nsupervised learning require extensive manual labeling, which is both\ntime-consuming and labor-intensive. In this study, we present a novel framework\nfor weakly supervised lesion segmentation in early breast ultrasound images.\nOur method uses morphological enhancement and class activation map (CAM)-guided\nlocalization. Finally, we employ the Segment Anything Model (SAM), a computer\nvision foundation model, for detailed segmentation. This approach does not\nrequire pixel-level annotation, thereby reducing the cost of data annotation.\nThe performance of our method is comparable to supervised learning methods that\nrequire manual annotations, achieving a Dice score of 74.39% and outperforming\ncomparative supervised models in terms of Hausdorff distance in the BUSI\ndataset. These results demonstrate that our framework effectively integrates\nweakly supervised learning with SAM, providing a promising solution for breast\ncancer image analysis. The code for this study is available at:\nhttps://github.com/YueXin18/MorSeg-CAM-SAM.\n'] , [""  Image generative AI has garnered significant attention in recent years. In\nparticular, the diffusion model, a core component of recent generative AI,\nproduces high-quality images with rich diversity. In this study, we propose a\nnovel CT reconstruction method by combining the denoising diffusion\nprobabilistic model with iterative CT reconstruction. In sharp contrast to\nprevious studies, we optimize the fidelity loss of CT reconstruction with\nrespect to the latent variable of the diffusion model, instead of the image and\nmodel parameters. To suppress anatomical structure changes produced by the\ndiffusion model, we shallow the diffusion and reverse processes, and fix a set\nof added noises in the reverse process to make it deterministic during\ninference. We demonstrate the effectiveness of the proposed method through\nsparse view CT reconstruction of 1/10 view projection data. Despite the\nsimplicity of the implementation, the proposed method shows the capability of\nreconstructing high-quality images while preserving the patient's anatomical\nstructure, and outperforms existing methods including iterative reconstruction,\niterative reconstruction with total variation, and the diffusion model alone in\nterms of quantitative indices such as SSIM and PSNR. We also explore further\nsparse view CT using 1/20 view projection data with the same trained diffusion\nmodel. As the number of iterations increases, image quality improvement\ncomparable to that of 1/10 sparse view CT reconstruction is achieved. In\nprinciple, the proposed method can be widely applied not only to CT but also to\nother imaging modalities such as MRI, PET, and SPECT.\n"", ""  Motion artifacts in Magnetic Resonance Imaging (MRI) are one of the\nfrequently occurring artifacts due to patient movements during scanning. Motion\nis estimated to be present in approximately 30% of clinical MRI scans; however,\nmotion has not been explicitly modeled within deep learning image\nreconstruction models. Deep learning (DL) algorithms have been demonstrated to\nbe effective for both the image reconstruction task and the motion correction\ntask, but the two tasks are considered separately. The image reconstruction\ntask involves removing undersampling artifacts such as noise and aliasing\nartifacts, whereas motion correction involves removing artifacts including\nblurring, ghosting, and ringing. In this work, we propose a novel method to\nsimultaneously accelerate imaging and correct motion. This is achieved by\nintegrating a motion module into the deep learning-based MRI reconstruction\nprocess, enabling real-time detection and correction of motion. We model motion\nas a tightly integrated auxiliary layer in the deep learning model during\ntraining, making the deep learning model 'motion-informed'. During inference,\nimage reconstruction is performed from undersampled raw k-space data using a\ntrained motion-informed DL model. Experimental results demonstrate that the\nproposed motion-informed deep learning image reconstruction network\noutperformed the conventional image reconstruction network for motion-degraded\nMRI datasets.\n"", ""  In Magnetic Resonance Imaging (MRI), image acquisitions are often\nundersampled in the measurement domain to accelerate the scanning process, at\nthe expense of image quality. However, image quality is a crucial factor that\ninfluences the accuracy of clinical diagnosis; hence, high-quality image\nreconstruction from undersampled measurements has been a key area of research.\nRecently, deep learning (DL) methods have emerged as the state-of-the-art for\nMRI reconstruction, typically involving deep neural networks to transform\nundersampled MRI images into high-quality MRI images through data-driven\nprocesses. Nevertheless, there is clear and significant room for improvement in\nundersampled DL MRI reconstruction to meet the high standards required for\nclinical diagnosis, in terms of eliminating aliasing artifacts and reducing\nimage noise. In this paper, we introduce a self-supervised pretraining\nprocedure using contrastive learning to improve the accuracy of undersampled DL\nMRI reconstruction. We use contrastive learning to transform the MRI image\nrepresentations into a latent space that maximizes mutual information among\ndifferent undersampled representations and optimizes the information content at\nthe input of the downstream DL reconstruction models. Our experiments\ndemonstrate improved reconstruction accuracy across a range of acceleration\nfactors and datasets, both quantitatively and qualitatively. Furthermore, our\nextended experiments validate the proposed framework's robustness under\nadversarial conditions, such as measurement noise, different k-space sampling\npatterns, and pathological abnormalities, and also prove the transfer learning\ncapabilities on MRI datasets with completely different anatomy. Additionally,\nwe conducted experiments to visualize and analyze the properties of the\nproposed MRI contrastive learning latent space.\n""] , [""  The lack of annotated medical images limits the performance of deep learning\nmodels, which usually need large-scale labelled datasets. Few-shot learning\ntechniques can reduce data scarcity issues and enhance medical image analysis,\nespecially with meta-learning. This systematic review gives a comprehensive\noverview of few-shot learning in medical imaging. We searched the literature\nsystematically and selected 80 relevant articles published from 2018 to 2023.\nWe clustered the articles based on medical outcomes, such as tumour\nsegmentation, disease classification, and image registration; anatomical\nstructure investigated (i.e. heart, lung, etc.); and the meta-learning method\nused. For each cluster, we examined the papers' distributions and the results\nprovided by the state-of-the-art. In addition, we identified a generic pipeline\nshared among all the studies. The review shows that few-shot learning can\novercome data scarcity in most outcomes and that meta-learning is a popular\nchoice to perform few-shot learning because it can adapt to new tasks with few\nlabelled samples. In addition, following meta-learning, supervised learning and\nsemi-supervised learning stand out as the predominant techniques employed to\ntackle few-shot learning challenges in medical imaging and also best\nperforming. Lastly, we observed that the primary application areas\npredominantly encompass cardiac, pulmonary, and abdominal domains. This\nsystematic review aims to inspire further research to improve medical image\nanalysis and patient care.\n"", '  Objectives: Medical research faces substantial challenges from noisy labels\nattributed to factors like inter-expert variability and machine-extracted\nlabels. Despite this, the adoption of label noise management remains limited,\nand label noise is largely ignored. To this end, there is a critical need to\nconduct a scoping review focusing on the problem space. This scoping review\naims to comprehensively review label noise management in deep learning-based\nmedical prediction problems, which includes label noise detection, label noise\nhandling, and evaluation. Research involving label uncertainty is also\nincluded.\n  Methods: Our scoping review follows the Preferred Reporting Items for\nSystematic Reviews and Meta-Analyses (PRISMA) guidelines. We searched 4\ndatabases, including PubMed, IEEE Xplore, Google Scholar, and Semantic Scholar.\nOur search terms include ""noisy label AND medical / healthcare / clinical"",\n""un-certainty AND medical / healthcare / clinical"", and ""noise AND medical /\nhealthcare / clinical"".\n  Results: A total of 60 papers met inclusion criteria between 2016 and 2023. A\nseries of practical questions in medical research are investigated. These\ninclude the sources of label noise, the impact of label noise, the detection of\nlabel noise, label noise handling techniques, and their evaluation.\nCategorization of both label noise detection methods and handling techniques\nare provided.\n  Discussion: From a methodological perspective, we observe that the medical\ncommunity has been up to date with the broader deep-learning community, given\nthat most techniques have been evaluated on medical data. We recommend\nconsidering label noise as a standard element in medical research, even if it\nis not dedicated to handling noisy labels. Initial experiments can start with\neasy-to-implement methods, such as noise-robust loss functions, weighting, and\ncurriculum learning.\n', '  Black-box deep learning approaches have showcased significant potential in\nthe realm of medical image analysis. However, the stringent trustworthiness\nrequirements intrinsic to the medical field have catalyzed research into the\nutilization of Explainable Artificial Intelligence (XAI), with a particular\nfocus on concept-based methods. Existing concept-based methods predominantly\napply concept annotations from a single perspective (e.g., global level),\nneglecting the nuanced semantic relationships between sub-regions and concepts\nembedded within medical images. This leads to underutilization of the valuable\nmedical information and may cause models to fall short in harmoniously\nbalancing interpretability and performance when employing inherently\ninterpretable architectures such as Concept Bottlenecks. To mitigate these\nshortcomings, we propose a multi-modal explainable disease diagnosis framework\nthat meticulously aligns medical images and clinical-related concepts\nsemantically at multiple strata, encompassing the image level, token level, and\nconcept level. Moreover, our method allows for model intervention and offers\nboth textual and visual explanations in terms of human-interpretable concepts.\nExperimental results on three skin image datasets demonstrate that our method,\nwhile preserving model interpretability, attains high performance and label\nefficiency for concept detection and disease diagnosis.\n'] , ['  We propose a new approach for non-Cartesian magnetic resonance image\nreconstruction. While unrolled architectures provide robustness via\ndata-consistency layers, embedding measurement operators in Deep Neural Network\n(DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP)\napproaches, where the denoising DNNs are blind to the measurement setting, are\nnot affected by this limitation and have also proven effective, but their\nhighly iterative nature also affects scalability. To address this scalability\nchallenge, we leverage the ""Residual-to-Residual DNN series for high-Dynamic\nrange imaging (R2D2)"" approach recently introduced in astronomical imaging.\nR2D2\'s reconstruction is formed as a series of residual images, iteratively\nestimated as outputs of DNNs taking the previous iteration\'s image estimate and\nassociated data residual as inputs. The method can be interpreted as a learned\nversion of the Matching Pursuit algorithm. We demonstrate R2D2 in simulation,\nconsidering radial k-space sampling acquisition sequences. Our preliminary\nresults suggest that R2D2 achieves: (i) suboptimal performance compared to its\nunrolled incarnation R2D2-Net, which is however non-scalable due to the\nnecessary embedding of NUFFT-based data-consistency layers; (ii) superior\nreconstruction quality to a scalable version of R2D2-Net embedding an FFT-based\napproximation for data consistency; (iii) superior reconstruction quality to\nPnP, while only requiring few iterations.\n', '  Radio-interferometric (RI) imaging entails solving high-resolution\nhigh-dynamic range inverse problems from large data volumes. Recent image\nreconstruction techniques grounded in optimization theory have demonstrated\nremarkable capability for imaging precision, well beyond CLEAN\'s capability.\nThese range from advanced proximal algorithms propelled by handcrafted\nregularization operators, such as the SARA family, to hybrid plug-and-play\n(PnP) algorithms propelled by learned regularization denoisers, such as AIRI.\nOptimization and PnP structures are however highly iterative, which hinders\ntheir ability to handle the extreme data sizes expected from future\ninstruments. To address this scalability challenge, we introduce a novel deep\nlearning approach, dubbed ""Residual-to-Residual DNN series for high-Dynamic\nrange imaging"". R2D2\'s reconstruction is formed as a series of residual images,\niteratively estimated as outputs of Deep Neural Networks (DNNs) taking the\nprevious iteration\'s image estimate and associated data residual as inputs. It\nthus takes a hybrid structure between a PnP algorithm and a learned version of\nthe matching pursuit algorithm that underpins CLEAN. We present a comprehensive\nstudy of our approach, featuring its multiple incarnations distinguished by\ntheir DNN architectures. We provide a detailed description of its training\nprocess, targeting a telescope-specific approach. R2D2\'s capability to deliver\nhigh precision is demonstrated in simulation, across a variety of image and\nobservation settings using the Very Large Array (VLA). Its reconstruction speed\nis also demonstrated: with only few iterations required to clean data residuals\nat dynamic ranges up to 100000, R2D2 opens the door to fast precision imaging.\nR2D2 codes are available in the BASPLib library on GitHub.\n', ""  The ``Residual-to-Residual DNN series for high-Dynamic range imaging'' (R2D2)\napproach was recently introduced for Radio-Interferometric (RI) imaging in\nastronomy. R2D2's reconstruction is formed as a series of residual images,\niteratively estimated as outputs of Deep Neural Networks (DNNs) taking the\nprevious iteration's image estimate and associated data residual as inputs. In\nthis work, we investigate the robustness of the R2D2 image estimation process,\nby studying the uncertainty associated with its series of learned models.\nAdopting an ensemble averaging approach, multiple series can be trained,\narising from different random DNN initializations of the training process at\neach iteration. The resulting multiple R2D2 instances can also be leveraged to\ngenerate ``R2D2 samples'', from which empirical mean and standard deviation\nendow the algorithm with a joint estimation and uncertainty quantification\nfunctionality. Focusing on RI imaging, and adopting a telescope-specific\napproach, multiple R2D2 instances were trained to encompass the most general\nobservation setting of the Very Large Array (VLA). Simulations and real-data\nexperiments confirm that: (i) R2D2's image estimation capability is superior to\nthat of the state-of-the-art algorithms; (ii) its ultra-fast reconstruction\ncapability (arising from series with only few DNNs) makes the computation of\nmultiple reconstruction samples and of uncertainty maps practical even at large\nimage dimension; (iii) it is characterized by a very low model uncertainty.\n""]",Deep Learning for Medical Imaging and Analysis,"""Deep Learning for Medical Image Segmentation"""
252,"Alzheimer's Disease Diagnosis using Neuroimaging , Brain Network Analysis using fMRI and Deep Learning","['alzheimer', 'dementia', 'neurodegenerative', 'neuroimaging', 'neurodegeneration', 'mri', 'amyloid', 'predicting', 'fmri', 'mris'] , ['fmri', 'neuroimaging', 'networks', 'brain', 'connectome', 'connectivity', 'hippocampal', 'hippocampus', 'functional', 'embedding']","[""  Neurodegeneration as measured through magnetic resonance imaging (MRI) is\nrecognized as a potential biomarker for diagnosing Alzheimer's disease (AD),\nbut is generally considered less specific than amyloid or tau based biomarkers.\nDue to a large amount of variability in brain anatomy between different\nindividuals, we hypothesize that leveraging MRI time series can help improve\nspecificity, by treating each patient as their own baseline. Here we turn to\nconditional variational autoencoders to generate individualized MRI predictions\ngiven the subject's age, disease status and one previous scan. Using serial\nimaging data from the Alzheimer's Disease Neuroimaging Initiative, we train a\nnovel architecture to build a latent space distribution which can be sampled\nfrom to generate future predictions of changing anatomy. This enables us to\nextrapolate beyond the dataset and predict MRIs up to 10 years. We evaluated\nthe model on a held-out set from ADNI and an independent dataset (from Open\nAccess Series of Imaging Studies). By comparing to several alternatives, we\nshow that our model produces more individualized images with higher resolution.\nFurther, if an individual already has a follow-up MRI, we demonstrate a usage\nof our model to compute a likelihood ratio classifier for disease status. In\npractice, the model may be able to assist in early diagnosis of AD and provide\na counterfactual baseline trajectory for treatment effect estimation.\nFurthermore, it generates a synthetic dataset that can potentially be used for\ndownstream tasks such as anomaly detection and classification.\n"", ""  Objectives: The objectives of this narrative review are to summarize the\ncurrent state of AI applications in neuroimaging for early Alzheimer's disease\n(AD) prediction and to highlight the potential of AI techniques in improving\nearly AD diagnosis, prognosis, and management.\n  Methods: We conducted a narrative review of studies using AI techniques\napplied to neuroimaging data for early AD prediction. We examined\nsingle-modality studies using structural MRI and PET imaging, as well as\nmulti-modality studies integrating multiple neuroimaging techniques and\nbiomarkers. Furthermore, they reviewed longitudinal studies that model AD\nprogression and identify individuals at risk of rapid decline.\n  Results: Single-modality studies using structural MRI and PET imaging have\ndemonstrated high accuracy in classifying AD and predicting progression from\nmild cognitive impairment (MCI) to AD. Multi-modality studies, integrating\nmultiple neuroimaging techniques and biomarkers, have shown improved\nperformance and robustness compared to single-modality approaches. Longitudinal\nstudies have highlighted the value of AI in modeling AD progression and\nidentifying individuals at risk of rapid decline. However, challenges remain in\ndata standardization, model interpretability, generalizability, clinical\nintegration, and ethical considerations.\n  Conclusion: AI techniques applied to neuroimaging data have the potential to\nimprove early AD diagnosis, prognosis, and management. Addressing challenges\nrelated to data standardization, model interpretability, generalizability,\nclinical integration, and ethical considerations is crucial for realizing the\nfull potential of AI in AD research and clinical practice. Collaborative\nefforts among researchers, clinicians, and regulatory agencies are needed to\ndevelop reliable, robust, and ethical AI tools that can benefit AD patients and\nsociety.\n"", ""  Alzheimer's is a brain disease that gets worse over time and affects memory,\nthinking, and behavior. Alzheimer's disease (AD) can be treated and managed if\nit is diagnosed early, which can slow the progression of symptoms and improve\nquality of life. In this study, we suggested using the Visual Transformer (ViT)\nand bi-LSTM to process MRI images for diagnosing Alzheimer's disease. We used\nViT to extract features from the MRI and then map them to a feature sequence.\nThen, we used Bi-LSTM sequence modeling to keep the interdependencies between\nrelated features. In addition, we evaluated the performance of the proposed\nmodel for the binary classification of AD patients using data from the\nAlzheimer's Disease Neuroimaging Initiative (ADNI). Finally, we evaluated our\nmethod against other deep learning models in the literature. The proposed\nmethod performs well in terms of accuracy, precision, F-score, and recall for\nthe diagnosis of AD.\n""] , [""  The human brain is a complex, dynamic network, which is commonly studied\nusing functional magnetic resonance imaging (fMRI) and modeled as network of\nRegions of interest (ROIs) for understanding various brain functions. Recent\nstudies utilize deep learning approaches to learn the brain network\nrepresentation based on functional connectivity (FC) profile, broadly falling\ninto two main categories. The Fixed-FC approaches, utilizing the FC profile\nwhich represents the linear temporal relation within the brain network, are\nlimited by failing to capture informative brain temporal dynamics. On the other\nhand, the Dynamic-FC approaches, modeling the evolving FC profile over time,\noften exhibit less satisfactory performance due to challenges in handling the\ninherent noisy nature of fMRI data.\n  To address these challenges, we propose Brain Masked Auto-Encoder (BrainMAE)\nfor learning representations directly from fMRI time-series data. Our approach\nincorporates two essential components: a region-aware graph attention mechanism\ndesigned to capture the relationships between different brain ROIs, and a novel\nself-supervised masked autoencoding framework for effective model pre-training.\nThese components enable the model to capture rich temporal dynamics of brain\nactivity while maintaining resilience to inherent noise in fMRI data. Our\nexperiments demonstrate that BrainMAE consistently outperforms established\nbaseline methods by significant margins in four distinct downstream tasks.\nFinally, leveraging the model's inherent interpretability, our analysis of\nmodel-generated representations reveals findings that resonate with ongoing\nresearch in the field of neuroscience.\n"", '  Brain functional connectivity (FC) reveals biomarkers for identification of\nvarious neuropsychiatric disorders. Recent application of deep neural networks\n(DNNs) to connectome-based classification mostly relies on traditional\nconvolutional neural networks using input connectivity matrices on a regular\nEuclidean grid. We propose a graph deep learning framework to incorporate the\nnon-Euclidean information about graph structure for classifying functional\nmagnetic resonance imaging (fMRI)-derived brain networks in major depressive\ndisorder (MDD). We design a novel graph autoencoder (GAE) architecture based on\nthe graph convolutional networks (GCNs) to embed the topological structure and\nnode content of large-sized fMRI networks into low-dimensional latent\nrepresentations. In network construction, we employ the Ledoit-Wolf (LDW)\nshrinkage method to estimate the high-dimensional FC metrics efficiently from\nfMRI data. We consider both supervised and unsupervised approaches for the\ngraph embedding learning. The learned embeddings are then used as feature\ninputs for a deep fully-connected neural network (FCNN) to discriminate MDD\nfrom healthy controls. Evaluated on two resting-state fMRI (rs-fMRI) MDD\ndatasets, results show that the proposed GAE-FCNN model significantly\noutperforms several state-of-the-art methods for brain connectome\nclassification, achieving the best accuracy using the LDW-FC edges as node\nfeatures. The graph embeddings of fMRI FC networks learned by the GAE also\nreveal apparent group differences between MDD and HC. Our new framework\ndemonstrates feasibility of learning graph embeddings on brain networks to\nprovide discriminative information for diagnosis of brain disorders.\n', '  Resting-state functional magnetic resonance imaging (rs-fMRI) is a\nnoninvasive technique pivotal for understanding human neural mechanisms of\nintricate cognitive processes. Most rs-fMRI studies compute a single static\nfunctional connectivity matrix across brain regions of interest, or dynamic\nfunctional connectivity matrices with a sliding window approach. These\napproaches are at risk of oversimplifying brain dynamics and lack proper\nconsideration of the goal at hand. While deep learning has gained substantial\npopularity for modeling complex relational data, its application to uncovering\nthe spatiotemporal dynamics of the brain is still limited. We propose a novel\ninterpretable deep learning framework that learns goal-specific functional\nconnectivity matrix directly from time series and employs a specialized graph\nneural network for the final classification. Our model, DSAM, leverages\ntemporal causal convolutional networks to capture the temporal dynamics in both\nlow- and high-level feature representations, a temporal attention unit to\nidentify important time points, a self-attention unit to construct the\ngoal-specific connectivity matrix, and a novel variant of graph neural network\nto capture the spatial dynamics for downstream classification. To validate our\napproach, we conducted experiments on the Human Connectome Project dataset with\n1075 samples to build and interpret the model for the classification of sex\ngroup, and the Adolescent Brain Cognitive Development Dataset with 8520 samples\nfor independent testing. Compared our proposed framework with other\nstate-of-art models, results suggested this novel approach goes beyond the\nassumption of a fixed connectivity matrix and provides evidence of\ngoal-specific brain connectivity patterns, which opens up the potential to gain\ndeeper insights into how the human brain adapts its functional connectivity\nspecific to the task at hand.\n']",Neuroimaging and Deep Learning for Brain Disorder Diagnosis and Analysis,Alzheimer's Disease Diagnosis using Neuroimaging
253,"""Defect Detection using Deep Learning and Computer Vision"" , ""Deep Learning Library Testing and Bug Detection"" , ""Deep Neural Network Testing and Fault Localization""","['cnn', 'cnns', 'defects', 'features', 'feature', 'defect', 'inspection', 'classification', 'visual', 'yolov5'] , ['testing', 'bugs', 'apis', 'dllens', 'developers', 'deep', 'dl', 'reproducibility', 'projects', 'libraries'] , ['inceptionv3', 'dnns', 'dnn', 'deepfault', 'tensorflow', 'deep', 'testability', 'deepgd', 'faults', 'neuron']","['  With the rapid growth of the PCB manufacturing industry, there is an\nincreasing demand for computer vision inspection to detect defects during\nproduction. Improving the accuracy and generalization of PCB defect detection\nmodels remains a significant challenge. This paper proposes a high-precision,\nrobust, and real-time end-to-end method for PCB defect detection based on deep\nConvolutional Neural Networks (CNN). Traditional methods often suffer from low\naccuracy and limited applicability. We propose a novel approach combining\nYOLOv5 and multiscale modules for hierarchical residual-like connections. In\nPCB defect detection, noise can confuse the background and small targets. The\nYOLOv5 model provides a strong foundation with its real-time processing and\naccurate object detection capabilities. The multi-scale module extends\ntraditional approaches by incorporating hierarchical residual-like connections\nwithin a single block, enabling multiscale feature extraction. This\nplug-and-play module significantly enhances performance by extracting features\nat multiple scales and levels, which are useful for identifying defects of\nvarying sizes and complexities. Our multi-scale architecture integrates feature\nextraction, defect localization, and classification into a unified network.\nExperiments on a large-scale PCB dataset demonstrate significant improvements\nin precision, recall, and F1-score compared to existing methods. This work\nadvances computer vision inspection for PCB defect detection, providing a\nreliable solution for high-precision, robust, real-time, and domain-adaptive\ndefect detection in the PCB manufacturing industry.\n', '  Deep learning-based semiconductor defect inspection has gained traction in\nrecent years, offering a powerful and versatile approach that provides high\naccuracy, adaptability, and efficiency in detecting and classifying nano-scale\ndefects. However, semiconductor manufacturing processes are continually\nevolving, leading to the emergence of new types of defects over time. This\npresents a significant challenge for conventional supervised defect detectors,\nas they may suffer from catastrophic forgetting when trained on new defect\ndatasets, potentially compromising performance on previously learned tasks. An\nalternative approach involves the constant storage of previously trained\ndatasets alongside pre-trained model versions, which can be utilized for\n(re-)training from scratch or fine-tuning whenever encountering a new defect\ndataset. However, adhering to such a storage template is impractical in terms\nof size, particularly when considering High-Volume Manufacturing (HVM).\nAdditionally, semiconductor defect datasets, especially those encompassing\nstochastic defects, are often limited and expensive to obtain, thus lacking\nsufficient representation of the entire universal set of defectivity. This work\nintroduces a task-agnostic, meta-learning approach aimed at addressing this\nchallenge, which enables the incremental addition of new defect classes and\nscales to create a more robust and generalized model for semiconductor defect\ninspection. We have benchmarked our approach using real resist-wafer SEM\n(Scanning Electron Microscopy) datasets for two process steps, ADI and AEI,\ndemonstrating its superior performance compared to conventional supervised\ntraining methods.\n', '  As automation advances in manufacturing, the demand for precise and\nsophisticated defect detection technologies grows. Existing vision models for\ndefect recognition methods are insufficient for handling the complexities and\nvariations of defects in contemporary manufacturing settings. These models\nespecially struggle in scenarios involving limited or imbalanced defect data.\nIn this work, we introduce MemoryMamba, a novel memory-augmented state space\nmodel (SSM), designed to overcome the limitations of existing defect\nrecognition models. MemoryMamba integrates the state space model with the\nmemory augmentation mechanism, enabling the system to maintain and retrieve\nessential defect-specific information in training. Its architecture is designed\nto capture dependencies and intricate defect characteristics, which are crucial\nfor effective defect detection. In the experiments, MemoryMamba was evaluated\nacross four industrial datasets with diverse defect types and complexities. The\nmodel consistently outperformed other methods, demonstrating its capability to\nadapt to various defect recognition scenarios.\n'] , ['  Recent deep learning (DL) applications are mostly built on top of DL\nlibraries. The quality assurance of these libraries is critical to the\ndependable deployment of DL applications. Techniques have been proposed to\ngenerate various DL models and apply them to test these libraries. However,\ntheir test effectiveness is constrained by the diversity of layer API calls in\ntheir generated DL models. Our study reveals that these techniques can cover at\nmost 34.1% layer inputs, 25.9% layer parameter values, and 15.6% layer\nsequences. As a result, we find that many bugs arising from specific layer API\ncalls (i.e., specific layer inputs, parameter values, or layer sequences) can\nbe missed by existing techniques. Because of this limitation, we propose COMET\nto effectively generate DL models with diverse layer API calls for DL library\ntesting. COMET: (1) designs a set of mutation operators and a coverage-based\nsearch algorithm to diversify layer inputs, layer parameter values, and layer\nsequences in DL models. (2) proposes a model synthesis method to boost the test\nefficiency without compromising the layer API call diversity. Our evaluation\nresult shows that COMET outperforms baselines by covering twice as many layer\ninputs (69.7% vs. 34.1%), layer parameter values (50.2% vs. 25.9%), and layer\nsequences (39.0% vs. 15.6%) as those by the state-of-the-art. Moreover, COMET\ncovers 3.4% more library branches than those by existing techniques. Finally,\nCOMET detects 32 new bugs in the latest version of eight popular DL libraries,\nincluding TensorFlow and MXNet, with 21 of them confirmed by DL library\ndevelopers and 7 of those confirmed bugs have been fixed by developers.\n', ""  Testing is a major approach to ensuring the quality of deep learning (DL)\nlibraries. Existing testing techniques commonly adopt differential testing to\nrelieve the need for test oracle construction. However, these techniques are\nlimited in finding implementations that offer the same functionality and\ngenerating diverse test inputs for differential testing. This paper introduces\nDLLens, a novel differential testing technique for DL library testing. Our\ninsight is that APIs in different DL libraries are commonly designed to\naccomplish various computations for the same set of published DL algorithms.\nAlthough the mapping of these APIs is not often one-to-one, we observe that\ntheir computations can be mutually simulated after proper composition and\nadaptation. The use of these simulation counterparts facilitates differential\ntesting for the detection of functional DL library bugs. Leveraging the\ninsight, we propose DLLens as a novel mechanism that utilizes a large language\nmodel (LLM) to synthesize valid counterparts of DL library APIs. To generate\ndiverse test inputs, DLLens incorporates a static analysis method aided by LLM\nto extract path constraints from all execution paths in each API and its\ncounterpart's implementations. These path constraints are then used to guide\nthe generation of diverse test inputs. We evaluate DLLens on two popular DL\nlibraries, TensorFlow and PyTorch. Our evaluation shows that DLLens can\nsynthesize counterparts for more than twice as many APIs found by\nstate-of-the-art techniques on these libraries. Moreover, DLLens can extract\n26.7% more constraints and detect 2.5 times as many bugs as state-of-the-art\ntechniques. DLLens has successfully found 56 bugs in recent TensorFlow and\nPyTorch libraries. Among them, 41 are previously unknown, 39 of which have been\nconfirmed by developers after reporting, and 19 of those confirmed bugs have\nbeen fixed by developers.\n"", ""  In recent years, software systems powered by deep learning (DL) techniques\nhave significantly facilitated people's lives in many aspects. As the backbone\nof these DL systems, various DL libraries undertake the underlying optimization\nand computation. However, like traditional software, DL libraries are not\nimmune to bugs, which can pose serious threats to users' personal property and\nsafety. Studying the characteristics of DL libraries, their associated bugs,\nand the corresponding testing methods is crucial for enhancing the security of\nDL systems and advancing the widespread application of DL technology. This\npaper provides an overview of the testing research related to various DL\nlibraries, discusses the strengths and weaknesses of existing methods, and\nprovides guidance and reference for the application of the DL library. This\npaper first introduces the workflow of DL underlying libraries and the\ncharacteristics of three kinds of DL libraries involved, namely DL framework,\nDL compiler, and DL hardware library. It then provides definitions for DL\nunderlying library bugs and testing. Additionally, this paper summarizes the\nexisting testing methods and tools tailored to these DL libraries separately\nand analyzes their effectiveness and limitations. It also discusses the\nexisting challenges of DL library testing and outlines potential directions for\nfuture research.\n""] , ['  When deploying Deep Neural Networks (DNNs), developers often convert models\nfrom one deep learning framework to another (e.g., TensorFlow to PyTorch).\nHowever, this process is error-prone and can impact target model accuracy. To\nidentify the extent of such impact, we perform and briefly present a\ndifferential analysis against three DNNs widely used for image recognition\n(MobileNetV2, ResNet101, and InceptionV3) converted across four well-known deep\nlearning frameworks (PyTorch, Keras, TensorFlow (TF), and TFLite), which\nrevealed numerous model crashes and output label discrepancies of up to 100%.\nTo mitigate such errors, we present a novel approach towards fault localization\nand repair of buggy deep learning framework conversions, focusing on\npre-trained image recognition models. Our technique consists of four stages of\nanalysis: 1) conversion tools, 2) model parameters, 3) model hyperparameters,\nand 4) graph representation. In addition, we propose various strategies towards\nfault repair of the faults detected. We implement our technique on top of the\nApache TVM deep learning compiler, and we test it by conducting a preliminary\nfault localization analysis for the conversion of InceptionV3 from TF to\nTFLite. Our approach detected a fault in a common DNN converter tool, which\nintroduced precision errors in weights, reducing model accuracy. After our\nfault localization, we repaired the issue, reducing our conversion error to\nzero.\n', '  Deep neural networks (DNNs) are widely used in various application domains\nsuch as image processing, speech recognition, and natural language processing.\nHowever, testing DNN models may be challenging due to the complexity and size\nof their input domain. Particularly, testing DNN models often requires\ngenerating or exploring large unlabeled datasets. In practice, DNN test\noracles, which identify the correct outputs for inputs, often require expensive\nmanual effort to label test data, possibly involving multiple experts to ensure\nlabeling correctness. In this paper, we propose DeepGD, a black-box\nmulti-objective test selection approach for DNN models. It reduces the cost of\nlabeling by prioritizing the selection of test inputs with high fault revealing\npower from large unlabeled datasets. DeepGD not only selects test inputs with\nhigh uncertainty scores to trigger as many mispredicted inputs as possible but\nalso maximizes the probability of revealing distinct faults in the DNN model by\nselecting diverse mispredicted inputs. The experimental results conducted on\nfour widely used datasets and five DNN models show that in terms of\nfault-revealing ability: (1) White-box, coverage-based approaches fare poorly,\n(2) DeepGD outperforms existing black-box test selection approaches in terms of\nfault detection, and (3) DeepGD also leads to better guidance for DNN model\nretraining when using selected inputs to augment the training set.\n', ""  Despite deep learning's transformative impact on various domains, the\nreliability of Deep Neural Networks (DNNs) is still a pressing concern due to\ntheir complexity and data dependency. Traditional software fault localization\ntechniques, such as Spectrum-based Fault Localization (SBFL), have been adapted\nto DNNs with limited success. Existing methods like DeepFault utilize SBFL\nmeasures but fail to account for fault propagation across neural pathways,\nleading to suboptimal fault detection. Addressing this gap, we propose the\nNP-SBFL method, leveraging Layer-wise Relevance Propagation (LRP) to identify\nand verify critical neural pathways. Our innovative multi-stage gradient ascent\n(MGA) technique, an extension of gradient ascent (GA), activates neurons\nsequentially, enhancing fault detection efficacy. We evaluated the\neffectiveness of our method, i.e. NP-SBFL-MGA, on two commonly used datasets,\nMNIST and CIFAR-10, two baselines DeepFault and NP- SBFL-GA, and three\nsuspicious neuron measures, Tarantula, Ochiai, and Barinel. The empirical\nresults showed that NP-SBFL-MGA is statistically more effective than the\nbaselines at identifying suspicious paths and synthesizing adversarial inputs.\nParticularly, Tarantula on NP-SBFL-MGA had the highest fault detection rate at\n96.75%, surpassing DeepFault on Ochiai (89.90%) and NP-SBFL-GA on Ochiai\n(60.61%). Our approach also yielded results comparable to those of the\nbaselines in synthesizing naturalness inputs, and we found a positive\ncorrelation between the coverage of critical paths and the number of failed\ntests in DNN fault localization.\n""]",Deep Learning for Defect Detection and Testing,"""Deep Learning Library Testing and Bug Detection"""
254,"Deepfake Detection and Analysis , ""Deep Learning Watermarking and Protection"" , ""Efficient Checkpointing for Large-Scale Deep Learning""","['deepfake', 'deepfakes', 'multimodal', 'faceforensics', 'detecting', 'detection', 'embedders', 'dataset', 'datasets', 'faces'] , ['adversarial', 'watermarking', 'watermark', 'watermarked', 'watermarks', 'unmarker', 'deeptaster', 'protect', 'tampering', 'stealing'] , ['checkpointing', 'checkpoint', 'checkpoints', 'backup', 'queue', 'storage', 'writes', 'parallelism', 'saving', 'persistent']","['  We study universal deepfake detection. Our goal is to detect synthetic images\nfrom a range of generative AI approaches, particularly from emerging ones which\nare unseen during training of the deepfake detector. Universal deepfake\ndetection requires outstanding generalization capability. Motivated by recently\nproposed masked image modeling which has demonstrated excellent generalization\nin self-supervised pre-training, we make the first attempt to explore masked\nimage modeling for universal deepfake detection. We study spatial and frequency\ndomain masking in training deepfake detectors. Based on empirical analysis, we\npropose a novel deepfake detector via frequency masking. Our focus on frequency\ndomain is different from the majority, which primarily target spatial domain\ndetection. Our comparative analyses reveal substantial performance gains over\nexisting methods. Code and models are publicly available.\n', '  In recent years, the abuse of a face swap technique called deepfake has\nraised enormous public concerns. So far, a large number of deepfake videos\n(known as ""deepfakes"") have been crafted and uploaded to the internet, calling\nfor effective countermeasures. One promising countermeasure against deepfakes\nis deepfake detection. Several deepfake datasets have been released to support\nthe training and testing of deepfake detectors, such as DeepfakeDetection and\nFaceForensics++. While this has greatly advanced deepfake detection, most of\nthe real videos in these datasets are filmed with a few volunteer actors in\nlimited scenes, and the fake videos are crafted by researchers using a few\npopular deepfake softwares. Detectors developed on these datasets may become\nless effective against real-world deepfakes on the internet. To better support\ndetection against real-world deepfakes, in this paper, we introduce a new\ndataset WildDeepfake which consists of 7,314 face sequences extracted from 707\ndeepfake videos collected completely from the internet. WildDeepfake is a small\ndataset that can be used, in addition to existing datasets, to develop and test\nthe effectiveness of deepfake detectors against real-world deepfakes. We\nconduct a systematic evaluation of a set of baseline detection networks on both\nexisting and our WildDeepfake datasets, and show that WildDeepfake is indeed a\nmore challenging dataset, where the detection performance can decrease\ndrastically. We also propose two (eg. 2D and 3D) Attention-based Deepfake\nDetection Networks (ADDNets) to leverage the attention masks on real/fake faces\nfor improved detection. We empirically verify the effectiveness of ADDNets on\nboth existing datasets and WildDeepfake. The dataset is available at:\nhttps://github.com/OpenTAI/wild-deepfake.\n', '  Deepfake is a generative deep learning algorithm that creates or changes\nfacial features in a very realistic way making it hard to differentiate the\nreal from the fake features It can be used to make movies look better as well\nas to spread false information by imitating famous people In this paper many\ndifferent ways to make a Deepfake are explained analyzed and separated\ncategorically Using Deepfake datasets models are trained and tested for\nreliability through experiments Deepfakes are a type of facial manipulation\nthat allow people to change their entire faces identities attributes and\nexpressions The trends in the available Deepfake datasets are also discussed\nwith a focus on how they have changed Using Deep learning a general Deepfake\ndetection model is made Moreover the problems in making and detecting Deepfakes\nare also mentioned As a result of this survey it is expected that the\ndevelopment of new Deepfake based imaging tools will speed up in the future\nThis survey gives indepth review of methods for manipulating images of face and\nvarious techniques to spot altered face images Four types of facial\nmanipulation are specifically discussed which are attribute manipulation\nexpression swap entire face synthesis and identity swap Across every\nmanipulation category we yield information on manipulation techniques\nsignificant benchmarks for technical evaluation of counterfeit detection\ntechniques available public databases and a summary of the outcomes of all such\nanalyses From all of the topics in the survey we focus on the most recent\ndevelopment of Deepfake showing its advances and obstacles in detecting fake\nimages\n'] , ['  Recently, numerous highly-valuable Deep Neural Networks (DNNs) have been\ntrained using deep learning algorithms. To protect the Intellectual Property\n(IP) of the original owners over such DNN models, backdoor-based watermarks\nhave been extensively studied. However, most of such watermarks fail upon model\nextraction attack, which utilizes input samples to query the target model and\nobtains the corresponding outputs, thus training a substitute model using such\ninput-output pairs. In this paper, we propose a novel watermark to protect IP\nof DNN models against model extraction, named MEA-Defender. In particular, we\nobtain the watermark by combining two samples from two source classes in the\ninput domain and design a watermark loss function that makes the output domain\nof the watermark within that of the main task samples. Since both the input\ndomain and the output domain of our watermark are indispensable parts of those\nof the main task samples, the watermark will be extracted into the stolen model\nalong with the main task during model extraction. We conduct extensive\nexperiments on four model extraction attacks, using five datasets and six\nmodels trained based on supervised learning and self-supervised learning\nalgorithms. The experimental results demonstrate that MEA-Defender is highly\nrobust against different model extraction attacks, and various watermark\nremoval/detection approaches.\n', '  Deep Learning (DL) models have become crucial in digital transformation, thus\nraising concerns about their intellectual property rights. Different\nwatermarking techniques have been developed to protect Deep Neural Networks\n(DNNs) from IP infringement, creating a competitive field for DNN watermarking\nand removal methods. The predominant watermarking schemes use white-box\ntechniques, which involve modifying weights by adding a unique signature to\nspecific DNN layers. On the other hand, existing attacks on white-box\nwatermarking usually require knowledge of the specific deployed watermarking\nscheme or access to the underlying data for further training and fine-tuning.\nWe propose DeepEclipse, a novel and unified framework designed to remove\nwhite-box watermarks. We present obfuscation techniques that significantly\ndiffer from the existing white-box watermarking removal schemes. DeepEclipse\ncan evade watermark detection without prior knowledge of the underlying\nwatermarking scheme, additional data, or training and fine-tuning. Our\nevaluation reveals that DeepEclipse excels in breaking multiple white-box\nwatermarking schemes, reducing watermark detection to random guessing while\nmaintaining a similar model accuracy as the original one. Our framework\nshowcases a promising solution to address the ongoing DNN watermark protection\nand removal challenges.\n', '  Watermark has been widely deployed by industry to detect AI-generated images.\nThe robustness of such watermark-based detector against evasion attacks in the\nwhite-box and black-box settings is well understood in the literature. However,\nthe robustness in the no-box setting is much less understood. In particular,\nmultiple studies claimed that image watermark is robust in such setting. In\nthis work, we propose a new transfer evasion attack to image watermark in the\nno-box setting. Our transfer attack adds a perturbation to a watermarked image\nto evade multiple surrogate watermarking models trained by the attacker itself,\nand the perturbed watermarked image also evades the target watermarking model.\nOur major contribution is to show that, both theoretically and empirically,\nwatermark-based AI-generated image detector is not robust to evasion attacks\neven if the attacker does not have access to the watermarking model nor the\ndetection API.\n'] , ['  As large language models continue to scale up, the imperative for fault\ntolerance in distributed deep learning systems intensifies, becoming a focal\narea of AI infrastructure research. Checkpoint has emerged as the predominant\nfault tolerance strategy, with extensive studies dedicated to optimizing its\nefficiency. However, the advent of the sparse Mixture-of-Experts (MoE) model\npresents new challenges for traditional checkpoint techniques due to the\nsubstantial increase in model size, despite comparable computational demands to\ndense models. Breaking new ground in the realm of efficient fault tolerance for\nMoE model training, we introduce a novel Partial Experts Checkpoint (PEC)\nmechanism alongside a corresponding PEC fault-tolerant system. Our approach\nstrategically checkpoints a selected subset of experts, thereby significantly\nreducing the checkpoint size for MoE models to a level comparable with that of\ndense models. The empirical analysis on our 8-expert GPT-MoE model demonstrates\nthat the proposed PEC approach facilitates a substantial 54.2% decrease in the\nsize of non-redundant checkpoint (no data-parallel duplication), without\ncompromising the final model quality. Moreover, our PEC fault-tolerant system\nachieves a 76.9% reduction in checkpoint workload per data-parallel distributed\nrank, thereby correspondingly diminishing the checkpointing time and\nfacilitating complete overlap with the training process.\n', '  Existing checkpointing approaches seem ill-suited for distributed training\neven though hardware limitations make model parallelism, i.e., sharding model\nstate across multiple accelerators, a requirement for model scaling.\nConsolidating distributed model state into a single checkpoint unacceptably\nslows down training, and is impractical at extreme scales. Distributed\ncheckpoints, in contrast, are tightly coupled to the model parallelism and\nhardware configurations of the training run, and thus unusable on different\nconfigurations. To address this problem, we propose Universal Checkpointing, a\ntechnique that enables efficient checkpoint creation while providing the\nflexibility of resuming on arbitrary parallelism strategy and hardware\nconfigurations. Universal Checkpointing unlocks unprecedented capabilities for\nlarge-scale training such as improved resilience to hardware failures through\ncontinued training on remaining healthy hardware, and reduced training time\nthrough opportunistic exploitation of elastic capacity.\n  The key insight of Universal Checkpointing is the selection of the optimal\nrepresentation in each phase of the checkpointing life cycle: distributed\nrepresentation for saving, and consolidated representation for loading. This is\nachieved using two key mechanisms. First, the universal checkpoint format,\nwhich consists of a consolidated representation of each model parameter and\nmetadata for mapping parameter fragments into training ranks of arbitrary\nmodel-parallelism configuration. Second, the universal checkpoint language, a\nsimple but powerful specification language for converting distributed\ncheckpoints into the universal checkpoint format. Our evaluation demonstrates\nthe effectiveness and generality of Universal Checkpointing on state-of-the-art\nmodel architectures and a wide range of parallelism techniques.\n', ""  The development of real-world Large Language Models (LLMs) necessitates\ncheckpointing of training states in persistent storage to mitigate potential\nsoftware and hardware failures, as well as to facilitate checkpoint\ntransferring within the training pipeline and across various tasks. Due to the\nimmense size of LLMs, saving and loading checkpoints often incur intolerable\nminute-level stalls, significantly diminishing training efficiency. Besides,\nwhen transferring checkpoints across tasks, checkpoint resharding, defined as\nloading checkpoints into parallel configurations differing from those used for\nsaving, is often required according to the characteristics and resource quota\nof specific tasks. Previous checkpointing systems [16,3,33,6] assume consistent\nparallel configurations, failing to address the complexities of checkpoint\ntransformation during resharding. Furthermore, in the industry platform,\ndevelopers create checkpoints from different training frameworks[23,36,21,11],\neach with its own unique storage and I/O logic. This diversity complicates the\nimplementation of unified checkpoint management and optimization. To address\nthese challenges, we introduce ByteCheckpoint, a PyTorch-native multi-framework\nLLM checkpointing system that supports automatic online checkpoint resharding.\nByteCheckpoint employs a data/metadata disaggregated storage architecture,\ndecoupling checkpoint storage from the adopted parallelism strategies and\ntraining frameworks. We design an efficient asynchronous tensor merging\ntechnique to settle the irregular tensor sharding problem and propose several\nI/O performance optimizations to significantly enhance the efficiency of\ncheckpoint saving and loading. Experimental results demonstrate\nByteCheckpoint's substantial advantages in reducing checkpoint saving (by up to\n529.22X) and loading (by up to 3.51X) costs, compared to baseline methods.\n""]",Deep Learning Security and Efficiency,"""Deep Learning Watermarking and Protection"""
255,"""Efficient Deep Neural Network Accelerators"" , ""Efficient DNN Inference on Edge Devices"" , ""Efficient Deep Neural Networks for Edge Deployment"" , ""Efficient Deep Learning with Local Learning and Memory Reduction""","['accelerator', 'accelerators', 'hardware', 'cnn', 'dataflows', 'cores', 'throughput', 'dataflow', 'memory', 'dnns'] , ['edge', 'efficientnetv2b0', 'memory', 'swapnet', 'edgeai', 'mobilenetv2', 'deep', 'dnn', 'devices', 'batteryless'] , ['cnn', 'cnns', 'efficientnetv2b1', 'memory', 'optimized', 'networks', 'neural', 'deep', 'dnns', 'dnn'] , ['cnn', 'neural', 'backpropagation', 'imagenet', 'layers', 'layer', 'convolutional', 'neurons', 'networks', 'memory']","['  Deep Neural Networks (DNNs) excel in learning hierarchical representations\nfrom raw data, such as images, audio, and text. To compute these DNN models\nwith high performance and energy efficiency, these models are usually deployed\nonto customized hardware accelerators. Among various accelerator designs,\ndataflow architecture has shown promising performance due to its\nlayer-pipelined structure and its scalability in data parallelism.\n  Exploiting weights and activations sparsity can further enhance memory\nstorage and computation efficiency. However, existing approaches focus on\nexploiting sparsity in non-dataflow accelerators, which cannot be applied onto\ndataflow accelerators because of the large hardware design space introduced. As\nsuch, this could miss opportunities to find an optimal combination of sparsity\nfeatures and hardware designs.\n  In this paper, we propose a novel approach to exploit unstructured weights\nand activations sparsity for dataflow accelerators, using software and hardware\nco-optimization. We propose a Hardware-Aware Sparsity Search (HASS) to\nsystematically determine an efficient sparsity solution for dataflow\naccelerators. Over a set of models, we achieve an efficiency improvement\nranging from 1.3$\\times$ to 4.2$\\times$ compared to existing sparse designs,\nwhich are either non-dataflow or non-hardware-aware. Particularly, the\nthroughput of MobileNetV3 can be optimized to 4895 images per second. HASS is\nopen-source: \\url{https://github.com/Yu-Zhewen/HASS}\n', '  Systolic array has emerged as a prominent architecture for Deep Neural\nNetwork (DNN) hardware accelerators, providing high-throughput and low-latency\nperformance essential for deploying DNNs across diverse applications. However,\nwhen used in safety-critical applications, reliability assessment is mandatory\nto guarantee the correct behavior of DNN accelerators. While fault injection\nstands out as a well-established practical and robust method for reliability\nassessment, it is still a very time-consuming process. This paper addresses the\ntime efficiency issue by introducing a novel hierarchical software-based\nhardware-aware fault injection strategy tailored for systolic array-based DNN\naccelerators.\n', ""  The stringent requirements for the Deep Neural Networks (DNNs) accelerator's\nreliability stand along with the need for reducing the computational burden on\nthe hardware platforms, i.e. reducing the energy consumption and execution time\nas well as increasing the efficiency of DNN accelerators. Moreover, the growing\ndemand for specialized DNN accelerators with tailored requirements,\nparticularly for safety-critical applications, necessitates a comprehensive\ndesign space exploration to enable the development of efficient and robust\naccelerators that meet those requirements. Therefore, the trade-off between\nhardware performance, i.e. area and delay, and the reliability of the DNN\naccelerator implementation becomes critical and requires tools for analysis.\nThis paper presents a comprehensive methodology for exploring and enabling a\nholistic assessment of the trilateral impact of quantization on model accuracy,\nactivation fault reliability, and hardware efficiency. A fully automated\nframework is introduced that is capable of applying various quantization-aware\ntechniques, fault injection, and hardware implementation, thus enabling the\nmeasurement of hardware parameters. Moreover, this paper proposes a novel\nlightweight protection technique integrated within the framework to ensure the\ndependable deployment of the final systolic-array-based FPGA implementation.\nThe experiments on established benchmarks demonstrate the analysis flow and the\nprofound implications of quantization on reliability, hardware performance, and\nnetwork accuracy, particularly concerning the transient faults in the network's\nactivations.\n""] , ['  The proliferation of edge devices necessitates efficient computational\narchitectures for lightweight tasks, particularly deep neural network (DNN)\ninference. Traditional NPUs, though effective for such operations, face\nchallenges in power, cost, and area when integrated into lightweight edge\ndevices. The RISC-V architecture, known for its modularity and open-source\nnature, offers a viable alternative. This paper introduces the RISC-V\nR-extension, a novel approach to enhancing DNN process efficiency on edge\ndevices. The extension features rented-pipeline stages and architectural\npipeline registers (APR), which optimize critical operation execution, thereby\nreducing latency and memory access frequency. Furthermore, this extension\nincludes new custom instructions to support these architectural improvements.\nThrough comprehensive analysis, this study demonstrates the boost of\nR-extension in edge device processing, setting the stage for more responsive\nand intelligent edge applications.\n', '  Reducing inference time and energy usage while maintaining prediction\naccuracy has become a significant concern for deep neural networks (DNN)\ninference on resource-constrained edge devices. To address this problem, we\npropose a novel approach based on ""converting"" autoencoder and lightweight\nDNNs. This improves upon recent work such as early-exiting framework and DNN\npartitioning. Early-exiting frameworks spend different amounts of computation\npower for different input data depending upon their complexity. However, they\ncan be inefficient in real-world scenarios that deal with many hard image\nsamples. On the other hand, DNN partitioning algorithms that utilize the\ncomputation power of both the cloud and edge devices can be affected by network\ndelays and intermittent connections between the cloud and the edge. We present\nCBNet, a low-latency and energy-efficient DNN inference framework tailored for\nedge devices. It utilizes a ""converting"" autoencoder to efficiently transform\nhard images into easy ones, which are subsequently processed by a lightweight\nDNN for inference. To the best of our knowledge, such autoencoder has not been\nproposed earlier. Our experimental results using three popular\nimage-classification datasets on a Raspberry Pi 4, a Google Cloud instance, and\nan instance with Nvidia Tesla K80 GPU show that CBNet achieves up to 4.8x\nspeedup in inference latency and 79% reduction in energy usage compared to\ncompeting techniques while maintaining similar or higher accuracy.\n', '  Executing deep neural networks (DNNs) on edge artificial intelligence (AI)\ndevices enables various autonomous mobile computing applications. However, the\nmemory budget of edge AI devices restricts the number and complexity of DNNs\nallowed in such applications. Existing solutions, such as model compression or\ncloud offloading, reduce the memory footprint of DNN inference at the cost of\ndecreased model accuracy or autonomy. To avoid these drawbacks, we divide DNN\ninto blocks and swap them in and out in order, such that large DNNs can execute\nwithin a small memory budget. Nevertheless, naive swapping on edge AI devices\ninduces significant delays due to the redundant memory operations in the DNN\ndevelopment ecosystem for edge AI devices. To this end, we develop SwapNet, an\nefficient DNN block swapping middleware for edge AI devices. We systematically\neliminate the unnecessary memory operations during block swapping while\nretaining compatible with the deep learning frameworks, GPU backends, and\nhardware architectures of edge AI devices. We further showcase the utility of\nSwapNet via a multi-DNN scheduling scheme. Evaluations on eleven DNN inference\ntasks in three applications demonstrate that SwapNet achieves almost the same\nlatency as the case with sufficient memory even when DNNs demand 2.32x to 5.81x\nmemory beyond the available budget. The design of SwapNet also provides novel\nand feasible insights for deploying large language models (LLMs) on edge AI\ndevices in the future.\n'] , [""  Deploying Deep Neural Networks (DNNs) on microcontrollers (TinyML) is a\ncommon trend to process the increasing amount of sensor data generated at the\nedge, but in practice, resource and latency constraints make it difficult to\nfind optimal DNN candidates. Neural Architecture Search (NAS) is an excellent\napproach to automate this search and can easily be combined with DNN\ncompression techniques commonly used in TinyML. However, many NAS techniques\nare not only computationally expensive, especially hyperparameter optimization\n(HPO), but also often focus on optimizing only a single objective, e.g.,\nmaximizing accuracy, without considering additional objectives such as memory\nconsumption or computational complexity of a DNN, which are key to making\ndeployment at the edge feasible. In this paper, we propose a novel NAS strategy\nfor TinyML based on Multi-Objective Bayesian optimization (MOBOpt) and an\nensemble of competing parametric policies trained using Augmented Random Search\n(ARS) Reinforcement Learning (RL) agents. Our methodology aims at efficiently\nfinding tradeoffs between a DNN's predictive accuracy, memory consumption on a\ngiven target system, and computational complexity. Our experiments show that we\noutperform existing MOBOpt approaches consistently on different data sets and\narchitectures such as ResNet-18 and MobileNetV3.\n"", ""  While machine learning is traditionally a resource intensive task, embedded\nsystems, autonomous navigation, and the vision of the Internet of Things fuel\nthe interest in resource-efficient approaches. These approaches aim for a\ncarefully chosen trade-off between performance and resource consumption in\nterms of computation and energy. The development of such approaches is among\nthe major challenges in current machine learning research and key to ensure a\nsmooth transition of machine learning technology from a scientific environment\nwith virtually unlimited computing resources into everyday's applications. In\nthis article, we provide an overview of the current state of the art of machine\nlearning techniques facilitating these real-world requirements. In particular,\nwe focus on resource-efficient inference based on deep neural networks (DNNs),\nthe predominant machine learning models of the past decade. We give a\ncomprehensive overview of the vast literature that can be mainly split into\nthree non-mutually exclusive categories: (i) quantized neural networks, (ii)\nnetwork pruning, and (iii) structural efficiency. These techniques can be\napplied during training or as post-processing, and they are widely used to\nreduce the computational demands in terms of memory footprint, inference speed,\nand energy efficiency. We also briefly discuss different concepts of embedded\nhardware for DNNs and their compatibility with machine learning techniques as\nwell as potential for energy and latency reduction. We substantiate our\ndiscussion with experiments on well-known benchmark data sets using compression\ntechniques (quantization, pruning) for a set of resource-constrained embedded\nsystems, such as CPUs, GPUs and FPGAs. The obtained results highlight the\ndifficulty of finding good trade-offs between resource efficiency and\nprediction quality.\n"", ""  As artificial intelligence (AI) applications continue to expand, there is a\ngrowing need for deep neural network (DNN) models. Although DNN models deployed\nat the edge are promising to provide AI as a service with low latency, their\ncooperation is yet to be explored. In this paper, we consider the DNN service\nproviders share their computing resources as well as their models' parameters\nand allow other DNNs to offload their computations without mirroring. We\npropose a novel algorithm called coordinated DNNs on edge (\\textbf{CoDE}) that\nfacilitates coordination among DNN services by creating multi-task DNNs out of\nindividual models. CoDE aims to find the optimal path that results in the\nlowest possible cost, where the cost reflects the inference delay, model\naccuracy, and local computation workload. With CoDE, DNN models can make new\npaths for inference by using their own or other models' parameters. We then\nevaluate the performance of CoDE through numerical experiments. The results\ndemonstrate a $75\\%$ reduction in the local service computation workload while\ndegrading the accuracy by only $2\\%$ and having the same inference time in a\nbalanced load condition. Under heavy load, CoDE can further decrease the\ninference time by $30\\%$ while the accuracy is reduced by only $4\\%$.\n""] , [""  Deep neural networks are typically trained using global error signals that\nbackpropagate (BP) end-to-end, which is not only biologically implausible but\nalso suffers from the update locking problem and requires huge memory\nconsumption. Local learning, which updates each layer independently with a\ngradient-isolated auxiliary network, offers a promising alternative to address\nthe above problems. However, existing local learning methods are confronted\nwith a large accuracy gap with the BP counterpart, particularly for large-scale\nnetworks. This is due to the weak coupling between local layers and their\nsubsequent network layers, as there is no gradient communication across layers.\nTo tackle this issue, we put forward an augmented local learning method, dubbed\nAugLocal. AugLocal constructs each hidden layer's auxiliary network by\nuniformly selecting a small subset of layers from its subsequent network layers\nto enhance their synergy. We also propose to linearly reduce the depth of\nauxiliary networks as the hidden layer goes deeper, ensuring sufficient network\ncapacity while reducing the computational cost of auxiliary networks. Our\nextensive experiments on four image classification datasets (i.e., CIFAR-10,\nSVHN, STL-10, and ImageNet) demonstrate that AugLocal can effectively scale up\nto tens of local layers with a comparable accuracy to BP-trained networks while\nreducing GPU memory usage by around 40%. The proposed AugLocal method,\ntherefore, opens up a myriad of opportunities for training high-performance\ndeep neural networks on resource-constrained platforms.Code is available at\nhttps://github.com/ChenxiangMA/AugLocal.\n"", '  Different activation functions work best for different deep learning models.\nTo exploit this, we leverage recent advancements in gradient-based search\ntechniques for neural architectures to efficiently identify high-performing\nactivation functions for a given application. We propose a fine-grained search\ncell that combines basic mathematical operations to model activation functions,\nallowing for the exploration of novel activations. Our approach enables the\nidentification of specialized activations, leading to improved performance in\nevery model we tried, from image classification to language models. Moreover,\nthe identified activations exhibit strong transferability to larger models of\nthe same type, as well as new datasets. Importantly, our automated process for\ncreating customized activation functions is orders of magnitude more efficient\nthan previous approaches. It can easily be applied on top of arbitrary deep\nlearning pipelines and thus offers a promising practical avenue for enhancing\ndeep learning architectures.\n', '  Fine-tuning pretrained large models to downstream tasks is an important\nproblem, which however suffers from huge memory overhead due to large-scale\nparameters. This work strives to reduce memory overhead in fine-tuning from\nperspectives of activation function and layer normalization. To this end, we\npropose the Approximate Backpropagation (Approx-BP) theory, which provides the\ntheoretical feasibility of decoupling the forward and backward passes. We apply\nour Approx-BP theory to backpropagation training and derive memory-efficient\nalternatives of GELU and SiLU activation functions, which use derivative\nfunctions of ReLUs in the backward pass while keeping their forward pass\nunchanged. In addition, we introduce a Memory-Sharing Backpropagation strategy,\nwhich enables the activation memory to be shared by two adjacent layers,\nthereby removing activation memory usage redundancy. Our method neither induces\nextra computation nor reduces training efficiency. We conduct extensive\nexperiments with pretrained vision and language models, and the results\ndemonstrate that our proposal can reduce up to $\\sim$$30\\%$ of the peak memory\nusage. Our code is released at https://github.com/yyyyychen/LowMemoryBP.\n']",Efficient Deep Learning Architectures and Acceleration Techniques,"""Efficient Deep Neural Networks for Edge Deployment"""
256,"Hyperparameter Optimization for Deep Learning , Normalization Techniques for Deep Neural Networks , Sharpness-Aware Minimization for Deep Learning , Noisy Pre-training in Deep Learning , Regularization Techniques for Deep Neural Networks","['optimizing', 'optimization', 'hyperparameters', 'deepsdp', 'multitask', 'learning', 'hyperparameter', 'minimization', 'regularization', 'algorithms'] , ['normalization', 'regularization', 'neural', 'generalization', 'backpropagation', 'convolutional', 'neuron', 'learning', 'networks', 'deep'] , ['regularization', 'overfitting', 'gradients', 'minimization', 'minimizing', 'overparameterization', 'generalization', 'gradient', 'learning', 'overparameterized'] , ['pretraining', 'supervised', 'imagenet', 'trained', 'learning', 'pretrained', 'learned', 'training', 'classification', 'datasets'] , ['softmax', 'regularization', 'overfitting', 'underfitting', 'neural', 'classifiers', 'deep', 'memorization', 'networks', 'trained']","['  When training deep learning models, the performance depends largely on the\nselected hyperparameters. However, hyperparameter optimization (HPO) is often\none of the most expensive parts of model design. Classical HPO methods treat\nthis as a black-box optimization problem. However, gray-box HPO methods, which\nincorporate more information about the setup, have emerged as a promising\ndirection for more efficient optimization. For example, using intermediate loss\nevaluations to terminate bad selections. In this work, we propose an HPO method\nfor neural networks using logged checkpoints of the trained weights to guide\nfuture hyperparameter selections. Our method, Forecasting Model Search (FMS),\nembeds weights into a Gaussian process deep kernel surrogate model, using a\npermutation-invariant graph metanetwork to be data-efficient with the logged\nnetwork weights. To facilitate reproducibility and further research, we\nopen-source our code at https://github.com/NVlabs/forecasting-model-search.\n', '  Neural networks are central to many emerging technologies, but verifying\ntheir correctness remains a major challenge. It is known that network outputs\ncan be sensitive and fragile to even small input perturbations, thereby\nincreasing the risk of unpredictable and undesirable behavior. Fast and\naccurate verification of neural networks is therefore critical to their\nwidespread adoption, and in recent years, various methods have been developed\nas a response to this problem. In this paper, we focus on improving\nsemidefinite programming (SDP) based techniques for neural network\nverification. Such techniques offer the power of expressing complex geometric\nconstraints while retaining a convex problem formulation, but scalability\nremains a major issue in practice. Our starting point is the DeepSDP framework\nproposed by Fazlyab et al., which uses quadratic constraints to abstract the\nverification problem into a large-scale SDP. However, solving this SDP quickly\nbecomes intractable when the network grows. Our key observation is that by\nleveraging chordal sparsity, we can decompose the primary computational\nbottleneck of DeepSDP -- a large linear matrix inequality (LMI) -- into an\nequivalent collection of smaller LMIs. We call our chordally sparse\noptimization program Chordal-DeepSDP and prove that its construction is\nidentically expressive as that of DeepSDP. Moreover, we show that additional\nanalysis of Chordal-DeepSDP allows us to further rewrite its collection of LMIs\nin a second level of decomposition that we call Chordal-DeepSDP-2 -- which\nresults in another significant computational gain. Finally, we provide\nnumerical experiments on real networks of learned cart-pole dynamics,\nshowcasing the computational advantage of Chordal-DeepSDP and Chordal-DeepSDP-2\nover DeepSDP.\n', '  Hyperparameter optimization (HPO) is an important step in machine learning\n(ML) model development, but common practices are archaic -- primarily relying\non manual or grid searches. This is partly because adopting advanced HPO\nalgorithms introduces added complexity to the workflow, leading to longer\ncomputation times. This poses a notable challenge to ML applications, as\nsuboptimal hyperparameter selections curtail the potential of ML model\nperformance, ultimately obstructing the full exploitation of ML techniques. In\nthis article, we present a two-step HPO method as a strategic solution to\ncurbing computational demands and wait times, gleaned from practical\nexperiences in applied ML parameterization work. The initial phase involves a\npreliminary evaluation of hyperparameters on a small subset of the training\ndataset, followed by a re-evaluation of the top-performing candidate models\npost-retraining with the entire training dataset. This two-step HPO method is\nuniversally applicable across HPO search algorithms, and we argue it has\nattractive efficiency gains.\n  As a case study, we present our recent application of the two-step HPO method\nto the development of neural network emulators for aerosol activation. Although\nour primary use case is a data-rich limit with many millions of samples, we\nalso find that using up to 0.0025% of the data (a few thousand samples) in the\ninitial step is sufficient to find optimal hyperparameter configurations from\nmuch more extensive sampling, achieving up to 135-times speedup. The benefits\nof this method materialize through an assessment of hyperparameters and model\nperformance, revealing the minimal model complexity required to achieve the\nbest performance. The assortment of top-performing models harvested from the\nHPO process allows us to choose a high-performing model with a low inference\ncost for efficient use in global climate models (GCMs).\n'] , ['  Batch Normalization (BN), a widely-used technique in neural networks,\nenhances generalization and expedites training by normalizing each mini-batch\nto the same mean and variance. However, its effectiveness diminishes when\nconfronted with diverse data distributions. To address this challenge, we\npropose Supervised Batch Normalization (SBN), a pioneering approach. We expand\nnormalization beyond traditional single mean and variance parameters, enabling\nthe identification of data modes prior to training. This ensures effective\nnormalization for samples sharing common features. We define contexts as modes,\ncategorizing data with similar characteristics. These contexts are explicitly\ndefined, such as domains in domain adaptation or modalities in multimodal\nsystems, or implicitly defined through clustering algorithms based on data\nsimilarity. We illustrate the superiority of our approach over BN and other\ncommonly employed normalization techniques through various experiments on both\nsingle and multi-task datasets. Integrating SBN with Vision Transformer results\nin a remarkable \\textit{15.13}\\% accuracy enhancement on CIFAR-100.\nAdditionally, in domain adaptation scenarios, employing AdaMatch demonstrates\nan impressive \\textit{22.25}\\% accuracy improvement on MNIST and SVHN compared\nto BN.\n', ""  Normalization is a pre-processing step that converts the data into a more\nusable representation. As part of the deep neural networks (DNNs), the batch\nnormalization (BN) technique uses normalization to address the problem of\ninternal covariate shift. It can be packaged as general modules, which have\nbeen extensively integrated into various DNNs, to stabilize and accelerate\ntraining, presumably leading to improved generalization. However, the effect of\nBN is dependent on the mini-batch size and it does not take into account any\ngroups or clusters that may exist in the dataset when estimating population\nstatistics. This study proposes a new normalization technique, called context\nnormalization, for image data. This approach adjusts the scaling of features\nbased on the characteristics of each sample, which improves the model's\nconvergence speed and performance by adapting the data values to the context of\nthe target task. The effectiveness of context normalization is demonstrated on\nvarious datasets, and its performance is compared to other standard\nnormalization techniques.\n"", '  Deep learning grapples with challenges in training neural networks, notably\ninternal covariate shift and label shift. Conventional normalization techniques\nlike Batch Normalization (BN) partially mitigate these issues but are hindered\nby constraints such as dependency on batch size and distribution assumptions.\nSimilarly, mixture normalization (MN) encounters computational barriers in\nhandling diverse Gaussian distributions. This paper introduces Cluster-based\nNormalization (CB-Norm), presenting two variants: Supervised Cluster-based\nNormalization (SCB-Norm) and Unsupervised Cluster-based Normalization\n(UCB-Norm), offering a pioneering single-step normalization strategy. CB-Norm\nemploys a Gaussian mixture model to address gradient stability and learning\nacceleration challenges. SCB-Norm utilizes predefined data partitioning, termed\nclusters, for supervised normalization, while UCB-Norm adaptively clusters\nneuron activations during training, eliminating reliance on predefined\npartitions. This approach simultaneously tackles clustering and resolution\ntasks within neural networks, reducing computational complexity compared to\nexisting methods. CB-Norm outperforms traditional techniques like BN and MN,\nenhancing neural network performance across diverse learning scenarios.\n'] , ['  Recently, there has been a surge in interest in developing optimization\nalgorithms for overparameterized models as achieving generalization is believed\nto require algorithms with suitable biases. This interest centers on minimizing\nsharpness of the original loss function; the Sharpness-Aware Minimization (SAM)\nalgorithm has proven effective. However, most literature only considers a few\nsharpness measures, such as the maximum eigenvalue or trace of the training\nloss Hessian, which may not yield meaningful insights for non-convex\noptimization scenarios like neural networks. Additionally, many sharpness\nmeasures are sensitive to parameter invariances in neural networks, magnifying\nsignificantly under rescaling parameters. Motivated by these challenges, we\nintroduce a new class of sharpness measures in this paper, leading to new\nsharpness-aware objective functions. We prove that these measures are\n\\textit{universally expressive}, allowing any function of the training loss\nHessian matrix to be represented by appropriate hyperparameters. Furthermore,\nwe show that the proposed objective functions explicitly bias towards\nminimizing their corresponding sharpness measures, and how they allow\nmeaningful applications to models with parameter invariances (such as\nscale-invariances). Finally, as instances of our proposed general framework, we\npresent \\textit{Frob-SAM} and \\textit{Det-SAM}, which are specifically designed\nto minimize the Frobenius norm and the determinant of the Hessian of the\ntraining loss, respectively. We also demonstrate the advantages of our general\nframework through extensive experiments.\n', ""  Despite attaining high empirical generalization, the sharpness of models\ntrained with sharpness-aware minimization (SAM) do not always correlate with\ngeneralization error. Instead of viewing SAM as minimizing sharpness to improve\ngeneralization, our paper considers a new perspective based on SAM's training\ndynamics. We propose that perturbations in SAM perform perturbed forgetting,\nwhere they discard undesirable model biases to exhibit learning signals that\ngeneralize better. We relate our notion of forgetting to the information\nbottleneck principle, use it to explain observations like the better\ngeneralization of smaller perturbation batches, and show that perturbed\nforgetting can exhibit a stronger correlation with generalization than\nflatness. While standard SAM targets model biases exposed by the steepest\nascent directions, we propose a new perturbation that targets biases exposed\nthrough the model's outputs. Our output bias forgetting perturbations\noutperform standard SAM, GSAM, and ASAM on ImageNet, robustness benchmarks, and\ntransfer to CIFAR-{10,100}, while sometimes converging to sharper regions. Our\nresults suggest that the benefits of SAM can be explained by alternative\nmechanistic principles that do not require flatness of the loss surface.\n"", '  Sharpness-Aware Minimization (SAM) has been instrumental in improving deep\nneural network training by minimizing both training loss and loss sharpness.\nDespite the practical success, the mechanisms behind SAM\'s generalization\nenhancements remain elusive, limiting its progress in deep learning\noptimization. In this work, we investigate SAM\'s core components for\ngeneralization improvement and introduce ""Friendly-SAM"" (F-SAM) to further\nenhance SAM\'s generalization. Our investigation reveals the key role of\nbatch-specific stochastic gradient noise within the adversarial perturbation,\ni.e., the current minibatch gradient, which significantly influences SAM\'s\ngeneralization performance. By decomposing the adversarial perturbation in SAM\ninto full gradient and stochastic gradient noise components, we discover that\nrelying solely on the full gradient component degrades generalization while\nexcluding it leads to improved performance. The possible reason lies in the\nfull gradient component\'s increase in sharpness loss for the entire dataset,\ncreating inconsistencies with the subsequent sharpness minimization step solely\non the current minibatch data. Inspired by these insights, F-SAM aims to\nmitigate the negative effects of the full gradient component. It removes the\nfull gradient estimated by an exponentially moving average (EMA) of historical\nstochastic gradients, and then leverages stochastic gradient noise for improved\ngeneralization. Moreover, we provide theoretical validation for the EMA\napproximation and prove the convergence of F-SAM on non-convex problems.\nExtensive experiments demonstrate the superior generalization performance and\nrobustness of F-SAM over vanilla SAM. Code is available at\nhttps://github.com/nblt/F-SAM.\n'] , ['  Foundation models are usually pre-trained on large-scale datasets and then\nadapted to downstream tasks through tuning. However, the large-scale\npre-training datasets, often inaccessible or too expensive to handle, can\ncontain label noise that may adversely affect the generalization of the model\nand pose unexpected risks. This paper stands out as the first work to\ncomprehensively understand and analyze the nature of noise in pre-training\ndatasets and then effectively mitigate its impacts on downstream tasks.\nSpecifically, through extensive experiments of fully-supervised and image-text\ncontrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M\ndatasets, we demonstrate that, while slight noise in pre-training can benefit\nin-domain (ID) performance, where the training and testing data share a similar\ndistribution, it always deteriorates out-of-domain (OOD) performance, where\ntraining and testing distributions are significantly different. These\nobservations are agnostic to scales of pre-training datasets, pre-training\nnoise types, model architectures, pre-training objectives, downstream tuning\nmethods, and downstream applications. We empirically ascertain that the reason\nbehind this is that the pre-training noise shapes the feature space\ndifferently. We then propose a tuning method (NMTune) to affine the feature\nspace to mitigate the malignant effect of noise and improve generalization,\nwhich is applicable in both parameter-efficient and black-box tuning manners.\nWe additionally conduct extensive experiments on popular vision and language\nmodels, including APIs, which are supervised and self-supervised pre-trained on\nrealistic noisy data for evaluation. Our analysis and results demonstrate the\nimportance of this novel and fundamental research direction, which we term as\nNoisy Model Learning.\n', '  Fine-tuning is becoming widely used for leveraging the power of pre-trained\nfoundation models in new downstream tasks. While there are many successes of\nfine-tuning on various tasks, recent studies have observed challenges in the\ngeneralization of fine-tuned models to unseen distributions (i.e.,\nout-of-distribution; OOD). To improve OOD generalization, some previous studies\nidentify the limitations of fine-tuning data and regulate fine-tuning to\npreserve the general representation learned from pre-training data. However,\npotential limitations in the pre-training data and models are often ignored. In\nthis paper, we contend that overly relying on the pre-trained representation\nmay hinder fine-tuning from learning essential representations for downstream\ntasks and thus hurt its OOD generalization. It can be especially catastrophic\nwhen new tasks are from different (sub)domains compared to pre-training data.\nTo address the issues in both pre-training and fine-tuning data, we propose a\nnovel generalizable fine-tuning method LEVI (Layer-wise Ensemble of different\nVIews), where the pre-trained model is adaptively ensembled layer-wise with a\nsmall task-specific model, while preserving its efficiencies. By combining two\ncomplementing models, LEVI effectively suppresses problematic features in both\nthe fine-tuning data and pre-trained model and preserves useful features for\nnew tasks. Broad experiments with large language and vision models show that\nLEVI greatly improves fine-tuning generalization via emphasizing different\nviews from fine-tuning data and pre-trained features.\n', '  Pre-training on large-scale datasets and then fine-tuning on downstream tasks\nhave become a standard practice in deep learning. However, pre-training data\noften contain label noise that may adversely affect the generalization of the\nmodel. This paper aims to understand the nature of noise in pre-training\ndatasets and to mitigate its impact on downstream tasks. More specifically,\nthrough extensive experiments of supervised pre-training models on synthetic\nnoisy ImageNet-1K and YFCC15M datasets, we demonstrate that while slight noise\nin pre-training can benefit in-domain (ID) transfer performance, where the\ntraining and testing data share the same distribution, it always deteriorates\nout-of-domain (OOD) performance, where training and testing data distribution\nare different. We empirically verify that the reason behind is noise in\npre-training shapes the feature space differently. We then propose a\nlight-weight black-box tuning method (NMTune) to affine the feature space to\nmitigate the malignant effect of noise and improve generalization on both ID\nand OOD tasks, considering one may not be able to fully fine-tune or even\naccess the pre-trained models. We conduct practical experiments on popular\nvision and language models that are pre-trained on noisy data for evaluation of\nour approach. Our analysis and results show the importance of this interesting\nand novel research direction, which we term Noisy Model Learning.\n'] , ['  We present a latent variable model for classification that provides a novel\nprobabilistic interpretation of neural network softmax classifiers. We derive a\nvariational objective to train the model, analogous to the evidence lower bound\n(ELBO) used to train variational auto-encoders, that generalises the softmax\ncross-entropy loss. Treating inputs to the softmax layer as samples of a latent\nvariable, our abstracted perspective reveals a potential inconsistency between\ntheir anticipated distribution, required for accurate label predictions, and\ntheir empirical distribution found in practice. We augment the variational\nobjective to mitigate such inconsistency and induce a chosen latent\ndistribution, instead of the implicit assumption found in a standard softmax\nlayer. Overall, we provide new theoretical insight into the inner workings of\nwidely-used softmax classifiers. Empirical evaluation on image and text\nclassification datasets demonstrates that our proposed approach, variational\nclassification, maintains classification accuracy while the reshaped latent\nspace improves other desirable properties of a classifier, such as calibration,\nadversarial robustness, robustness to distribution shift and sample efficiency\nuseful in low data settings.\n', '  Overfitting commonly occurs when applying deep neural networks (DNNs) on\nsmall-scale datasets, where DNNs do not generalize well from existing data to\nunseen data. The main reason resulting in overfitting is that small-scale\ndatasets cannot reflect the situations of the real world. Label smoothing (LS)\nis an effective regularization method to prevent overfitting, avoiding it by\nmixing one-hot labels with uniform label vectors. However, LS only focuses on\nlabels while ignoring the distribution of existing data. In this paper, we\nintroduce the distributionally robust optimization (DRO) to LS, achieving shift\nthe existing data distribution flexibly to unseen domains when training DNNs.\nSpecifically, we prove that the regularization of LS can be extended to a\nregularization term for the DNNs parameters when integrating DRO. The\nregularization term can be utilized to shift existing data to unseen domains\nand generate new data. Furthermore, we propose an approximate\ngradient-iteration label smoothing algorithm (GI-LS) to achieve the findings\nand train DNNs. We prove that the shift for the existing data does not\ninfluence the convergence of GI-LS. Since GI-LS incorporates a series of\nhyperparameters, we further consider using Bayesian optimization (BO) to find\nthe relatively optimal combinations of these hyperparameters. Taking\nsmall-scale anomaly classification tasks as a case, we evaluate GI-LS, and the\nresults clearly demonstrate its superior performance.\n', '  Given data with noisy labels, over-parameterized deep networks suffer\noverfitting mislabeled data, resulting in poor generalization. The memorization\neffect of deep networks shows that although the networks have the ability to\nmemorize all noisy data, they would first memorize clean training data, and\nthen gradually memorize mislabeled training data. A simple and effective method\nthat exploits the memorization effect to combat noisy labels is early stopping.\nHowever, early stopping cannot distinguish the memorization of clean data and\nmislabeled data, resulting in the network still inevitably overfitting\nmislabeled data in the early training stage.In this paper, to decouple the\nmemorization of clean data and mislabeled data, and further reduce the side\neffect of mislabeled data, we perform additive decomposition on network\nparameters. Namely, all parameters are additively decomposed into two groups,\ni.e., parameters $\\mathbf{w}$ are decomposed as\n$\\mathbf{w}=\\bm{\\sigma}+\\bm{\\gamma}$. Afterward, the parameters $\\bm{\\sigma}$\nare considered to memorize clean data, while the parameters $\\bm{\\gamma}$ are\nconsidered to memorize mislabeled data. Benefiting from the memorization\neffect, the updates of the parameters $\\bm{\\sigma}$ are encouraged to fully\nmemorize clean data in early training, and then discouraged with the increase\nof training epochs to reduce interference of mislabeled data. The updates of\nthe parameters $\\bm{\\gamma}$ are the opposite. In testing, only the parameters\n$\\bm{\\sigma}$ are employed to enhance generalization. Extensive experiments on\nboth simulated and real-world benchmarks confirm the superior performance of\nour method.\n']",Deep Learning Optimization Techniques,Regularization Techniques for Deep Neural Networks
257,"Equivariant Neural Networks and Symmetries , Equivariant Neural Networks and Representations , Neural Network Symmetries and Alignment","['equivariance', 'symmetries', 'equivariant', 'invariance', 'invariants', 'symmetry', 'representations', 'cnns', 'convolutions', 'neural'] , ['encodenet', 'autoencoders', 'representations', 'embeddings', 'encoders', 'autoencoder', 'imagenet', 'neural', 'encoder', 'supervised'] , ['networks', 'symmetries', 'neural', 'symmetry', 'neurons', 'metanetworks', 'scalegmn', 'representations', 'layers', 'permuted']","['  We present a novel framework to overcome the limitations of equivariant\narchitectures in learning functions with group symmetries. In contrary to\nequivariant architectures, we use an arbitrary base model such as an MLP or a\ntransformer and symmetrize it to be equivariant to the given group by employing\na small equivariant network that parameterizes the probabilistic distribution\nunderlying the symmetrization. The distribution is end-to-end trained with the\nbase model which can maximize performance while reducing sample complexity of\nsymmetrization. We show that this approach ensures not only equivariance to\ngiven group but also universal approximation capability in expectation. We\nimplement our method on various base models, including patch-based transformers\nthat can be initialized from pretrained vision transformers, and test them for\na wide range of symmetry groups including permutation and Euclidean groups and\ntheir combinations. Empirical tests show competitive results against tailored\nequivariant architectures, suggesting the potential for learning equivariant\nfunctions for diverse groups using a non-equivariant universal base\narchitecture. We further show evidence of enhanced learning in symmetric\nmodalities, like graphs, when pretrained from non-symmetric modalities, like\nvision. Code is available at https://github.com/jw9730/lps.\n', '  Equivariant deep learning architectures exploit symmetries in learning\nproblems to improve the sample efficiency of neural-network-based models and\ntheir ability to generalise. However, when modelling real-world data, learning\nproblems are often not exactly equivariant, but only approximately. For\nexample, when estimating the global temperature field from weather station\nobservations, local topographical features like mountains break translation\nequivariance. In these scenarios, it is desirable to construct architectures\nthat can flexibly depart from exact equivariance in a data-driven way. In this\npaper, we develop a general approach to achieving this using existing\nequivariant architectures. Our approach is agnostic to both the choice of\nsymmetry group and model architecture, making it widely applicable. We consider\nthe use of approximately equivariant architectures in neural processes (NPs), a\npopular family of meta-learning models. We demonstrate the effectiveness of our\napproach on a number of synthetic and real-world regression experiments,\ndemonstrating that approximately equivariant NP models can outperform both\ntheir non-equivariant and strictly equivariant counterparts.\n', '  In this paper we develop a manifestly geometric framework for equivariant\nmanifold neural ordinary differential equations (NODEs), and use it to analyse\ntheir modelling capabilities for symmetric data. First, we consider the action\nof a Lie group $G$ on a smooth manifold $M$ and establish the equivalence\nbetween equivariance of vector fields, symmetries of the corresponding Cauchy\nproblems, and equivariance of the associated NODEs. We also propose a novel\nformulation of the equivariant NODEs in terms of the differential invariants of\nthe action of $G$ on $M$, based on Lie theory for symmetries of differential\nequations, which provides an efficient parameterisation of the space of\nequivariant vector fields in a way that is agnostic to both the manifold $M$\nand the symmetry group $G$. Second, we construct augmented manifold NODEs,\nthrough embeddings into equivariant flows, and show that they are universal\napproximators of equivariant diffeomorphisms on any path-connected $M$.\nFurthermore, we show that the augmented NODEs can be incorporated in the\ngeometric framework and parameterised using higher order differential\ninvariants. Finally, we consider the induced action of $G$ on different fields\non $M$ and show how it can be used to generalise previous work, on, e.g.,\ncontinuous normalizing flows, to equivariant models in any geometry.\n'] , ['  Recently, Neural Fields have emerged as a powerful modelling paradigm to\nrepresent continuous signals. In a conditional neural field, a field is\nrepresented by a latent variable that conditions the NeF, whose parametrisation\nis otherwise shared over an entire dataset. We propose Equivariant Neural\nFields based on cross attention transformers, in which NeFs are conditioned on\na geometric conditioning variable, a latent point cloud, that enables an\nequivariant decoding from latent to field. Our equivariant approach induces a\nsteerability property by which both field and latent are grounded in geometry\nand amenable to transformation laws if the field transforms, the latent\nrepresents transforms accordingly and vice versa. Crucially, the equivariance\nrelation ensures that the latent is capable of (1) representing geometric\npatterns faitfhully, allowing for geometric reasoning in latent space, (2)\nweightsharing over spatially similar patterns, allowing for efficient learning\nof datasets of fields. These main properties are validated using classification\nexperiments and a verification of the capability of fitting entire datasets, in\ncomparison to other non-equivariant NeF approaches. We further validate the\npotential of ENFs by demonstrate unique local field editing properties.\n', ""  At the core of self-supervised learning for vision is the idea of learning\ninvariant or equivariant representations with respect to a set of data\ntransformations. This approach, however, introduces strong inductive biases,\nwhich can render the representations fragile in downstream tasks that do not\nconform to these symmetries. In this work, drawing insights from world models,\nwe propose to instead learn a general representation that can adapt to be\ninvariant or equivariant to different transformations by paying attention to\ncontext -- a memory module that tracks task-specific states, actions, and\nfuture states. Here, the action is the transformation, while the current and\nfuture states respectively represent the input's representation before and\nafter the transformation. Our proposed algorithm, Contextual Self-Supervised\nLearning (ContextSSL), learns equivariance to all transformations (as opposed\nto invariance). In this way, the model can learn to encode all relevant\nfeatures as general representations while having the versatility to tail down\nto task-wise symmetries when given a few examples as the context. Empirically,\nwe demonstrate significant performance gains over existing methods on\nequivariance-related tasks, supported by both qualitative and quantitative\nevaluations.\n"", '  Latent representations are used extensively for downstream tasks, such as\nvisualization, interpolation or feature extraction of deep learning models.\nInvariant and equivariant neural networks are powerful and well-established\nmodels for enforcing inductive biases. In this paper, we demonstrate that the\ninductive bias imposed on the by an equivariant model must also be taken into\naccount when using latent representations. We show how not accounting for the\ninductive biases leads to decreased performance on downstream tasks, and vice\nversa, how accounting for inductive biases can be done effectively by using an\ninvariant projection of the latent representations. We propose principles for\nhow to choose such a projection, and show the impact of using these principles\nin two common examples: First, we study a permutation equivariant variational\nauto-encoder trained for molecule graph generation; here we show that invariant\nprojections can be designed that incur no loss of information in the resulting\ninvariant representation. Next, we study a rotation-equivariant representation\nused for image classification. Here, we illustrate how random invariant\nprojections can be used to obtain an invariant representation with a high\ndegree of retained information. In both cases, the analysis of invariant latent\nrepresentations proves superior to their equivariant counterparts. Finally, we\nillustrate that the phenomena documented here for equivariant neural networks\nhave counterparts in standard neural networks where invariance is encouraged\nvia augmentation. Thus, while these ambiguities may be known by experienced\ndevelopers of equivariant models, we make both the knowledge as well as\neffective tools to handle the ambiguities available to the broader community.\n'] , ['  Permutation symmetries of deep networks make basic operations like model\nmerging and similarity estimation challenging. In many cases, aligning the\nweights of the networks, i.e., finding optimal permutations between their\nweights, is necessary. Unfortunately, weight alignment is an NP-hard problem.\nPrior research has mainly focused on solving relaxed versions of the alignment\nproblem, leading to either time-consuming methods or sub-optimal solutions. To\naccelerate the alignment process and improve its quality, we propose a novel\nframework aimed at learning to solve the weight alignment problem, which we\nname Deep-Align. To that end, we first prove that weight alignment adheres to\ntwo fundamental symmetries and then, propose a deep architecture that respects\nthese symmetries. Notably, our framework does not require any labeled data. We\nprovide a theoretical analysis of our approach and evaluate Deep-Align on\nseveral types of network architectures and learning setups. Our experimental\nresults indicate that a feed-forward pass with Deep-Align produces better or\nequivalent alignments compared to those produced by current optimization\nalgorithms. Additionally, our alignments can be used as an effective\ninitialization for other methods, leading to improved solutions with a\nsignificant speedup in convergence.\n', '  Many algorithms and observed phenomena in deep learning appear to be affected\nby parameter symmetries -- transformations of neural network parameters that do\nnot change the underlying neural network function. These include linear mode\nconnectivity, model merging, Bayesian neural network inference, metanetworks,\nand several other characteristics of optimization or loss-landscapes. However,\ntheoretical analysis of the relationship between parameter space symmetries and\nthese phenomena is difficult. In this work, we empirically investigate the\nimpact of neural parameter symmetries by introducing new neural network\narchitectures that have reduced parameter space symmetries. We develop two\nmethods, with some provable guarantees, of modifying standard neural networks\nto reduce parameter space symmetries. With these new methods, we conduct a\ncomprehensive experimental study consisting of multiple tasks aimed at\nassessing the effect of removing parameter symmetries. Our experiments reveal\nseveral interesting observations on the empirical impact of parameter\nsymmetries; for instance, we observe linear mode connectivity between our\nnetworks without alignment of weight spaces, and we find that our networks\nallow for faster and more effective Bayesian neural network training.\n', '  Neural networks typically exhibit permutation symmetries which contribute to\nthe non-convexity of the networks\' loss landscapes, since linearly\ninterpolating between two permuted versions of a trained network tends to\nencounter a high loss barrier. Recent work has argued that permutation\nsymmetries are the only sources of non-convexity, meaning there are essentially\nno such barriers between trained networks if they are permuted appropriately.\nIn this work, we refine these arguments into three distinct claims of\nincreasing strength. We show that existing evidence only supports ""weak linear\nconnectivity""-that for each pair of networks belonging to a set of SGD\nsolutions, there exist (multiple) permutations that linearly connect it with\nthe other networks. In contrast, the claim ""strong linear connectivity""-that\nfor each network, there exists one permutation that simultaneously connects it\nwith the other networks-is both intuitively and practically more desirable.\nThis stronger claim would imply that the loss landscape is convex after\naccounting for permutation, and enable linear interpolation between three or\nmore independently trained models without increased loss. In this work, we\nintroduce an intermediate claim-that for certain sequences of networks, there\nexists one permutation that simultaneously aligns matching pairs of networks\nfrom these sequences. Specifically, we discover that a single permutation\naligns sequences of iteratively trained as well as iteratively pruned networks,\nmeaning that two networks exhibit low loss barriers at each step of their\noptimization and sparsification trajectories respectively. Finally, we provide\nthe first evidence that strong linear connectivity may be possible under\ncertain conditions, by showing that barriers decrease with increasing network\nwidth when interpolating among three networks.\n']",Equivariant Neural Networks and Symmetry in Deep Learning,Equivariant Neural Networks and Symmetries
258,"Neural Combinatorial Optimization for Routing Problems , Neural Network Pruning Techniques , Neural Architecture Search (NAS) Methods and Applications , Forward-Only Neural Network Learning Algorithms","['routes', 'planning', 'metaheuristics', 'salesman', 'heuristics', 'optimization', 'metaheuristic', 'heuristic', 'learning', 'routing'] , ['pruning', 'cnn', 'prune', 'pruned', 'cnns', 'imagenet', 'rnns', 'networks', 'neural', 'deep'] , ['imagenet', 'neural', 'rnn', 'supernet', 'architecture', 'architectures', 'nas', 'searching', 'networks', 'architectural'] , ['backpropagation', 'neural', 'forwardgnn', 'neurons', 'neuronal', 'backward', 'forward', 'cortex', 'backprop', 'learning']","['  Machine learning has been adapted to help solve NP-hard combinatorial\noptimization problems. One prevalent way is learning to construct solutions by\ndeep neural networks, which has been receiving more and more attention due to\nthe high efficiency and less requirement for expert knowledge. However, many\nneural construction methods for Vehicle Routing Problems~(VRPs) focus on\nsynthetic problem instances with specified node distributions and limited\nscales, leading to poor performance on real-world problems which usually\ninvolve complex and unknown node distributions together with large scales. To\nmake neural VRP solvers more practical, we design an auxiliary policy that\nlearns from the local transferable topological features, named local policy,\nand integrate it with a typical construction policy (which learns from the\nglobal information of VRP instances) to form an ensemble policy. With joint\ntraining, the aggregated policies perform cooperatively and complementarily to\nboost generalization. The experimental results on two well-known benchmarks,\nTSPLIB and CVRPLIB, of travelling salesman problem and capacitated VRP show\nthat the ensemble policy significantly improves both cross-distribution and\ncross-scale generalization performance, and even performs well on real-world\nproblems with several thousand nodes.\n', '  The neural combinatorial optimization (NCO) approach has shown great\npotential for solving routing problems without the requirement of expert\nknowledge. However, existing constructive NCO methods cannot directly solve\nlarge-scale instances, which significantly limits their application prospects.\nTo address these crucial shortcomings, this work proposes a novel\nInstance-Conditioned Adaptation Model (ICAM) for better large-scale\ngeneralization of neural combinatorial optimization. In particular, we design a\npowerful yet lightweight instance-conditioned adaptation module for the NCO\nmodel to generate better solutions for instances across different scales. In\naddition, we develop an efficient three-stage reinforcement learning-based\ntraining scheme that enables the model to learn cross-scale features without\nany labeled optimal solution. Experimental results show that our proposed\nmethod is capable of obtaining excellent results with a very fast inference\ntime in solving Traveling Salesman Problems (TSPs) and Capacitated Vehicle\nRouting Problems (CVRPs) across different scales. To the best of our knowledge,\nour model achieves state-of-the-art performance among all RL-based constructive\nmethods for TSP and CVRP with up to 1,000 nodes.\n', '  The end-to-end neural combinatorial optimization (NCO) method shows promising\nperformance in solving complex combinatorial optimization problems without the\nneed for expert design. However, existing methods struggle with large-scale\nproblems, hindering their practical applicability. To overcome this limitation,\nthis work proposes a novel Self-Improved Learning (SIL) method for better\nscalability of neural combinatorial optimization. Specifically, we develop an\nefficient self-improved mechanism that enables direct model training on\nlarge-scale problem instances without any labeled data. Powered by an\ninnovative local reconstruction approach, this method can iteratively generate\nbetter solutions by itself as pseudo-labels to guide efficient model training.\nIn addition, we design a linear complexity attention mechanism for the model to\nefficiently handle large-scale combinatorial problem instances with low\ncomputation overhead. Comprehensive experiments on the Travelling Salesman\nProblem (TSP) and the Capacitated Vehicle Routing Problem (CVRP) with up to\n100K nodes in both uniform and real-world distributions demonstrate the\nsuperior scalability of our method.\n'] , ['  Modern deep neural networks (DNNs) consist of millions of parameters,\nnecessitating high-performance computing during training and inference. Pruning\nis one solution that significantly reduces the space and time complexities of\nDNNs. Traditional pruning methods that are applied post-training focus on\nstreamlining inference, but there are recent efforts to leverage sparsity early\non by pruning before training. Pruning methods, such as iterative\nmagnitude-based pruning (IMP) achieve up to a 90% parameter reduction while\nretaining accuracy comparable to the original model. However, this leads to\nimpractical runtime as it relies on multiple train-prune-reset cycles to\nidentify and eliminate redundant parameters. In contrast, training agnostic\nearly pruning methods, such as SNIP and SynFlow offer fast pruning but fall\nshort of the accuracy achieved by IMP at high sparsities. To bridge this gap,\nwe present Dual Gradient-Based Rapid Iterative Pruning (DRIVE), which leverages\ndense training for initial epochs to counteract the randomness inherent at the\ninitialization. Subsequently, it employs a unique dual gradient-based metric\nfor parameter ranking. It has been experimentally demonstrated for VGG and\nResNet architectures on CIFAR-10/100 and Tiny ImageNet, and ResNet on ImageNet\nthat DRIVE consistently has superior performance over other training-agnostic\nearly pruning methods in accuracy. Notably, DRIVE is 43$\\times$ to 869$\\times$\nfaster than IMP for pruning.\n', ""  With the increasing size of datasets used for training neural networks, data\npruning becomes an attractive field of research. However, most current data\npruning algorithms are limited in their ability to preserve accuracy compared\nto models trained on the full data, especially in high pruning regimes. In this\npaper we explore the application of data pruning while incorporating knowledge\ndistillation (KD) when training on a pruned subset. That is, rather than\nrelying solely on ground-truth labels, we also use the soft predictions from a\nteacher network pre-trained on the complete data. By integrating KD into\ntraining, we demonstrate significant improvement across datasets, pruning\nmethods, and on all pruning fractions. We first establish a theoretical\nmotivation for employing self-distillation to improve training on pruned data.\nThen, we empirically make a compelling and highly practical observation: using\nKD, simple random pruning is comparable or superior to sophisticated pruning\nmethods across all pruning regimes. On ImageNet for example, we achieve\nsuperior accuracy despite training on a random subset of only 50% of the data.\nAdditionally, we demonstrate a crucial connection between the pruning factor\nand the optimal knowledge distillation weight. This helps mitigate the impact\nof samples with noisy labels and low-quality images retained by typical pruning\nalgorithms. Finally, we make an intriguing observation: when using lower\npruning fractions, larger teachers lead to accuracy degradation, while\nsurprisingly, employing teachers with a smaller capacity than the student's may\nimprove results. Our code will be made available.\n"", '  Deep neural networks (DNNs) have demonstrated remarkable success in various\nfields. However, the large number of floating-point operations (FLOPs) in DNNs\nposes challenges for their deployment in resource-constrained applications,\ne.g., edge devices. To address the problem, pruning has been introduced to\nreduce the computational cost in executing DNNs. Previous pruning strategies\nare based on weight values, gradient values and activation outputs. Different\nfrom previous pruning solutions, in this paper, we propose a class-aware\npruning technique to compress DNNs, which provides a novel perspective to\nreduce the computational cost of DNNs. In each iteration, the neural network\ntraining is modified to facilitate the class-aware pruning. Afterwards, the\nimportance of filters with respect to the number of classes is evaluated. The\nfilters that are only important for a few number of classes are removed. The\nneural network is then retrained to compensate for the incurred accuracy loss.\nThe pruning iterations end until no filter can be removed anymore, indicating\nthat the remaining filters are very important for many classes. This pruning\ntechnique outperforms previous pruning solutions in terms of accuracy, pruning\nratio and the reduction of FLOPs. Experimental results confirm that this\nclass-aware pruning technique can significantly reduce the number of weights\nand FLOPs, while maintaining a high inference accuracy.\n'] , ['  Efficient evaluation of a network architecture drawn from a large search\nspace remains a key challenge in Neural Architecture Search (NAS). Vanilla NAS\nevaluates each architecture by training from scratch, which gives the true\nperformance but is extremely time-consuming. Recently, one-shot NAS\nsubstantially reduces the computation cost by training only one supernetwork,\na.k.a. supernet, to approximate the performance of every architecture in the\nsearch space via weight-sharing. However, the performance estimation can be\nvery inaccurate due to the co-adaption among operations. In this paper, we\npropose few-shot NAS that uses multiple supernetworks, called sub-supernet,\neach covering different regions of the search space to alleviate the undesired\nco-adaption. Compared to one-shot NAS, few-shot NAS improves the accuracy of\narchitecture evaluation with a small increase of evaluation cost. With only up\nto 7 sub-supernets, few-shot NAS establishes new SoTAs: on ImageNet, it finds\nmodels that reach 80.5% top-1 accuracy at 600 MB FLOPS and 77.5% top-1 accuracy\nat 238 MFLOPS; on CIFAR10, it reaches 98.72% top-1 accuracy without using extra\ndata or transfer learning. In Auto-GAN, few-shot NAS outperforms the previously\npublished results by up to 20%. Extensive experiments show that few-shot NAS\nsignificantly improves various one-shot methods, including 4 gradient-based and\n6 search-based methods on 3 different tasks in NasBench-201 and\nNasBench1-shot-1.\n', ""  The paper provides a comprehensive overview of Neural Architecture Search\n(NAS), emphasizing its evolution from manual design to automated,\ncomputationally-driven approaches. It covers the inception and growth of NAS,\nhighlighting its application across various domains, including medical imaging\nand natural language processing. The document details the shift from\nexpert-driven design to algorithm-driven processes, exploring initial\nmethodologies like reinforcement learning and evolutionary algorithms. It also\ndiscusses the challenges of computational demands and the emergence of\nefficient NAS methodologies, such as Differentiable Architecture Search and\nhardware-aware NAS. The paper further elaborates on NAS's application in\ncomputer vision, NLP, and beyond, demonstrating its versatility and potential\nfor optimizing neural network architectures across different tasks. Future\ndirections and challenges, including computational efficiency and the\nintegration with emerging AI domains, are addressed, showcasing NAS's dynamic\nnature and its continued evolution towards more sophisticated and efficient\narchitecture search methods.\n"", '  Neural architecture search (NAS) has become a key component of AutoML and a\nstandard tool to automate the design of deep neural networks. Recently,\ntraining-free NAS as an emerging paradigm has successfully reduced the search\ncosts of standard training-based NAS by estimating the true architecture\nperformance with only training-free metrics. Nevertheless, the estimation\nability of these metrics typically varies across different tasks, making it\nchallenging to achieve robust and consistently good search performance on\ndiverse tasks with only a single training-free metric. Meanwhile, the\nestimation gap between training-free metrics and the true architecture\nperformances limits training-free NAS to achieve superior performance. To\naddress these challenges, we propose the robustifying and boosting\ntraining-free NAS (RoBoT) algorithm which (a) employs the optimized combination\nof existing training-free metrics explored from Bayesian optimization to\ndevelop a robust and consistently better-performing metric on diverse tasks,\nand (b) applies greedy search, i.e., the exploitation, on the newly developed\nmetric to bridge the aforementioned gap and consequently to boost the search\nperformance of standard training-free NAS further. Remarkably, the expected\nperformance of our RoBoT can be theoretically guaranteed, which improves over\nthe existing training-free NAS under mild conditions with additional\ninteresting insights. Our extensive experiments on various NAS benchmark tasks\nyield substantial empirical evidence to support our theoretical results.\n'] , ['  ""Forward-only"" algorithms, which train neural networks while avoiding a\nbackward pass, have recently gained attention as a way of solving the\nbiologically unrealistic aspects of backpropagation. Here, we first address\ncompelling challenges related to the ""forward-only"" rules, which include\nreducing the performance gap with backpropagation and providing an analytical\nunderstanding of their dynamics. To this end, we show that the forward-only\nalgorithm with top-down feedback is well-approximated by an\n""adaptive-feedback-alignment"" algorithm, and we analytically track its\nperformance during learning in a prototype high-dimensional setting. Then, we\ncompare different versions of forward-only algorithms, focusing on the\nForward-Forward and PEPITA frameworks, and we show that they share the same\nlearning principles. Overall, our work unveils the connections between three\nkey neuro-inspired learning rules, providing a link between ""forward-only""\nalgorithms, i.e., Forward-Forward and PEPITA, and an approximation of\nbackpropagation, i.e., Feedback Alignment.\n', '  Graph neural networks (GNNs) have achieved remarkable success across a wide\nrange of applications, such as recommendation, drug discovery, and question\nanswering. Behind the success of GNNs lies the backpropagation (BP) algorithm,\nwhich is the de facto standard for training deep neural networks (NNs).\nHowever, despite its effectiveness, BP imposes several constraints, which are\nnot only biologically implausible, but also limit the scalability, parallelism,\nand flexibility in learning NNs. Examples of such constraints include storage\nof neural activities computed in the forward pass for use in the subsequent\nbackward pass, and the dependence of parameter updates on non-local signals. To\naddress these limitations, the forward-forward algorithm (FF) was recently\nproposed as an alternative to BP in the image classification domain, which\ntrains NNs by performing two forward passes over positive and negative data.\nInspired by this advance, we propose ForwardGNN in this work, a new forward\nlearning procedure for GNNs, which avoids the constraints imposed by BP via an\neffective layer-wise local forward training. ForwardGNN extends the original FF\nto deal with graph data and GNNs, and makes it possible to operate without\ngenerating negative inputs (hence no longer forward-forward). Further,\nForwardGNN enables each layer to learn from both the bottom-up and top-down\nsignals without relying on the backpropagation of errors. Extensive experiments\non real-world datasets show the effectiveness and generality of the proposed\nforward graph learning framework. We release our code at\nhttps://github.com/facebookresearch/forwardgnn.\n', '  The Backpropagation algorithm has often been criticised for its lack of\nbiological realism. In an attempt to find a more biologically plausible\nalternative, the recently introduced Forward-Forward algorithm replaces the\nforward and backward passes of Backpropagation with two forward passes. In this\nwork, we show that the internal representations obtained by the Forward-Forward\nalgorithm can organise into category-specific ensembles exhibiting high\nsparsity - composed of a low number of active units. This situation is\nreminiscent of what has been observed in cortical sensory areas, where neuronal\nensembles are suggested to serve as the functional building blocks for\nperception and action. Interestingly, while this sparse pattern does not\ntypically arise in models trained with standard Backpropagation, it can emerge\nin networks trained with Backpropagation on the same objective proposed for the\nForward-Forward algorithm. These results suggest that the learning procedure\nproposed by Forward-Forward may be superior to Backpropagation in modelling\nlearning in the cortex, even when a backward pass is used.\n']",Neural Network Optimization Techniques,Neural Architecture Search (NAS) Methods and Applications
259,"ReLU Neural Networks Properties and Approximation , Theoretical Foundations of Convolutional Neural Networks , Neural Network Scaling Laws and Generalization , Neural Operator Approximation , Polysemantic Neurons in Deep Neural Networks , Neural Networks and Neuroscience-Inspired Learning","['neural', 'networks', 'regularization', 'neurons', 'relu', 'gradient', 'resnets', 'layers', 'weights', 'approximation'] , ['cnns', 'deepsets', 'neural', 'representations', 'deep', 'deepdrk', 'networks', 'convolutional', 'deeper', 'representational'] , ['scaling', 'generalization', 'neural', 'regularization', 'sparse', 'sgd', 'networks', 'learned', 'small', 'empirically'] , ['operators', 'operator', 'approximating', 'approximation', 'neural', 'approximate', 'learning', 'banach', 'kernel', 'networks'] , ['neural', 'neuron', 'neurons', 'networks', 'representations', 'deep', 'dnn', 'dnns', 'learning', 'backpropagation'] , ['neural', 'neuron', 'hippocampal', 'hippocampus', 'autoencoders', 'neuroscience', 'neocortical', 'memory', 'synaptic', 'brain']","['  We investigate the expressivity and learning dynamics of bias-free ReLU\nnetworks. We firstly show that two-layer bias-free ReLU networks have limited\nexpressivity: the only odd function two-layer bias-free ReLU networks can\nexpress is a linear one. We then show that, under symmetry conditions on the\ndata, these networks have the same learning dynamics as linear networks. This\nallows us to give closed-form time-course solutions to certain two-layer\nbias-free ReLU networks, which has not been done for nonlinear networks outside\nthe lazy learning regime. While deep bias-free ReLU networks are more\nexpressive than their two-layer counterparts, they still share a number of\nsimilarities with deep linear networks. These similarities enable us to\nleverage insights from linear networks, leading to a novel understanding of\nbias-free ReLU networks. Overall, our results show that some properties\nestablished for bias-free ReLU networks arise due to equivalence to linear\nnetworks, and suggest that including bias or considering asymmetric data are\navenues to engage with nonlinear behaviors.\n', '  Appropriate weight initialization settings, along with the ReLU activation\nfunction, have become cornerstones of modern deep learning, enabling the\ntraining and deployment of highly effective and efficient neural network models\nacross diverse areas of artificial intelligence. The problem of\n\\textquotedblleft dying ReLU,"" where ReLU neurons become inactive and yield\nzero output, presents a significant challenge in the training of deep neural\nnetworks with ReLU activation function. Theoretical research and various\nmethods have been introduced to address the problem. However, even with these\nmethods and research, training remains challenging for extremely deep and\nnarrow feedforward networks with ReLU activation function. In this paper, we\npropose a novel weight initialization method to address this issue. We\nestablish several properties of our initial weight matrix and demonstrate how\nthese properties enable the effective propagation of signal vectors. Through a\nseries of experiments and comparisons with existing methods, we demonstrate the\neffectiveness of the novel initialization method.\n', '  In this paper, we investigate the expressivity and approximation properties\nof deep neural networks employing the ReLU$^k$ activation function for $k \\geq\n2$. Although deep ReLU networks can approximate polynomials effectively, deep\nReLU$^k$ networks have the capability to represent higher-degree polynomials\nprecisely. Our initial contribution is a comprehensive, constructive proof for\npolynomial representation using deep ReLU$^k$ networks. This allows us to\nestablish an upper bound on both the size and count of network parameters.\nConsequently, we are able to demonstrate a suboptimal approximation rate for\nfunctions from Sobolev spaces as well as for analytic functions. Additionally,\nthrough an exploration of the representation power of deep ReLU$^k$ networks\nfor shallow networks, we reveal that deep ReLU$^k$ networks can approximate\nfunctions from a range of variation spaces, extending beyond those generated\nsolely by the ReLU$^k$ activation function. This finding demonstrates the\nadaptability of deep ReLU$^k$ networks in approximating functions within\nvarious variation spaces.\n'] , ['  Computer vision (CV) is one of the most crucial fields in artificial\nintelligence. In recent years, a variety of deep learning models based on\nconvolutional neural networks (CNNs) and Transformers have been designed to\ntackle diverse problems in CV. These algorithms have found practical\napplications in areas such as robotics and facial recognition. Despite the\nincreasing power of current CV models, several fundamental questions remain\nunresolved: Why do CNNs require deep layers? What ensures the generalization\nability of CNNs? Why do residual-based networks outperform fully convolutional\nnetworks like VGG? What is the fundamental difference between residual-based\nCNNs and Transformer-based networks? Why can CNNs utilize LoRA and pruning\ntechniques? The root cause of these questions lies in the lack of a robust\ntheoretical foundation for deep learning models in CV. To address these\ncritical issues and techniques, we employ the Universal Approximation Theorem\n(UAT) to provide a theoretical basis for convolution- and Transformer-based\nmodels in CV. By doing so, we aim to elucidate these questions from a\ntheoretical perspective.\n', '  In this paper, we provide a theoretical analysis of the inductive biases in\nconvolutional neural networks (CNNs). We start by examining the universality of\nCNNs, i.e., the ability to approximate any continuous functions. We prove that\na depth of $\\mathcal{O}(\\log d)$ suffices for deep CNNs to achieve this\nuniversality, where $d$ in the input dimension. Additionally, we establish that\nlearning sparse functions with CNNs requires only\n$\\widetilde{\\mathcal{O}}(\\log^2d)$ samples, indicating that deep CNNs can\nefficiently capture {\\em long-range} sparse correlations. These results are\nmade possible through a novel combination of the multichanneling and\ndownsampling when increasing the network depth. We also delve into the distinct\nroles of weight sharing and locality in CNNs. To this end, we compare the\nperformance of CNNs, locally-connected networks (LCNs), and fully-connected\nnetworks (FCNs) on a simple regression task, where LCNs can be viewed as CNNs\nwithout weight sharing. On the one hand, we prove that LCNs require\n${\\Omega}(d)$ samples while CNNs need only $\\widetilde{\\mathcal{O}}(\\log^2d)$\nsamples, highlighting the critical role of weight sharing. On the other hand,\nwe prove that FCNs require $\\Omega(d^2)$ samples, whereas LCNs need only\n$\\widetilde{\\mathcal{O}}(d)$ samples, underscoring the importance of locality.\nThese provable separations quantify the difference between the two biases, and\nthe major observation behind our proof is that weight sharing and locality\nbreak different symmetries in the learning process.\n', '  Analyzing the similarity of internal representations within and across\ndifferent models has been an important technique for understanding the behavior\nof deep neural networks. Most existing methods for analyzing the similarity\nbetween representations of high dimensions, such as those based on Canonical\nCorrelation Analysis (CCA) and widely used Centered Kernel Alignment (CKA),\nrely on statistical properties of the representations for a set of data points.\nIn this paper, we focus on transformer models and study the similarity of\nrepresentations between the hidden layers of individual transformers. In this\ncontext, we show that a simple sample-wise cosine similarity metric is capable\nof capturing the similarity and aligns with the complicated CKA. Our\nexperimental results on common transformers reveal that representations across\nlayers are positively correlated, albeit the similarity decreases when layers\nare far apart. We then propose an aligned training approach to enhance the\nsimilarity between internal representations, with trained models that enjoy the\nfollowing properties: (1) the last-layer classifier can be directly applied\nright after any hidden layers, yielding intermediate layer accuracies much\nhigher than those under standard training, (2) the layer-wise accuracies\nmonotonically increase and reveal the minimal depth needed for the given task,\n(3) when served as multi-exit models, they achieve on-par performance with\nstandard multi-exit architectures which consist of additional classifiers\ndesigned for early exiting in shallow layers. To our knowledge, our work is the\nfirst to show that one common classifier is sufficient for multi-exit models.\nWe conduct experiments on both vision and NLP tasks to demonstrate the\nperformance of the proposed aligned training.\n'] , ['  The population loss of trained deep neural networks often follows precise\npower-law scaling relations with either the size of the training dataset or the\nnumber of parameters in the network. We propose a theory that explains the\norigins of and connects these scaling laws. We identify variance-limited and\nresolution-limited scaling behavior for both dataset and model size, for a\ntotal of four scaling regimes. The variance-limited scaling follows simply from\nthe existence of a well-behaved infinite data or infinite width limit, while\nthe resolution-limited regime can be explained by positing that models are\neffectively resolving a smooth data manifold. In the large width limit, this\ncan be equivalently obtained from the spectrum of certain kernels, and we\npresent evidence that large width and large dataset resolution-limited scaling\nexponents are related by a duality. We exhibit all four scaling regimes in the\ncontrolled setting of large random feature and pretrained models and test the\npredictions empirically on a range of standard architectures and datasets. We\nalso observe several empirical relationships between datasets and scaling\nexponents under modifications of task and architecture aspect ratio. Our work\nprovides a taxonomy for classifying different scaling regimes, underscores that\nthere can be different mechanisms driving improvements in loss, and lends\ninsight into the microscopic origins of and relationships between scaling\nexponents.\n', '  Empirically, large-scale deep learning models often satisfy a neural scaling\nlaw: the test error of the trained model improves polynomially as the model\nsize and data size grow. However, conventional wisdom suggests the test error\nconsists of approximation, bias, and variance errors, where the variance error\nincreases with model size. This disagrees with the general form of neural\nscaling laws, which predict that increasing model size monotonically improves\nperformance.\n  We study the theory of scaling laws in an infinite dimensional linear\nregression setup. Specifically, we consider a model with $M$ parameters as a\nlinear function of sketched covariates. The model is trained by one-pass\nstochastic gradient descent (SGD) using $N$ data. Assuming the optimal\nparameter satisfies a Gaussian prior and the data covariance matrix has a\npower-law spectrum of degree $a>1$, we show that the reducible part of the test\nerror is $\\Theta(M^{-(a-1)} + N^{-(a-1)/a})$. The variance error, which\nincreases with $M$, is dominated by the other errors due to the implicit\nregularization of SGD, thus disappearing from the bound. Our theory is\nconsistent with the empirical neural scaling laws and verified by numerical\nsimulation.\n', '  On a variety of tasks, the performance of neural networks predictably\nimproves with training time, dataset size and model size across many orders of\nmagnitude. This phenomenon is known as a neural scaling law. Of fundamental\nimportance is the compute-optimal scaling law, which reports the performance as\na function of units of compute when choosing model sizes optimally. We analyze\na random feature model trained with gradient descent as a solvable model of\nnetwork training and generalization. This reproduces many observations about\nneural scaling laws. First, our model makes a prediction about why the scaling\nof performance with training time and with model size have different power law\nexponents. Consequently, the theory predicts an asymmetric compute-optimal\nscaling rule where the number of training steps are increased faster than model\nparameters, consistent with recent empirical observations. Second, it has been\nobserved that early in training, networks converge to their infinite-width\ndynamics at a rate $1/\\textit{width}$ but at late time exhibit a rate\n$\\textit{width}^{-c}$, where $c$ depends on the structure of the architecture\nand task. We show that our model exhibits this behavior. Lastly, our theory\nshows how the gap between training and test loss can gradually build up over\ntime due to repeated reuse of data.\n'] , [""  Neural operator architectures approximate operators between\ninfinite-dimensional Banach spaces of functions. They are gaining increased\nattention in computational science and engineering, due to their potential both\nto accelerate traditional numerical methods and to enable data-driven\ndiscovery. As the field is in its infancy basic questions about minimal\nrequirements for universal approximation remain open. It is clear that any\ngeneral approximation of operators between spaces of functions must be both\nnonlocal and nonlinear. In this paper we describe how these two attributes may\nbe combined in a simple way to deduce universal approximation. In so doing we\nunify the analysis of a wide range of neural operator architectures and open up\nconsideration of new ones.\n  A popular variant of neural operators is the Fourier neural operator (FNO).\nPrevious analysis proving universal operator approximation theorems for FNOs\nresorts to use of an unbounded number of Fourier modes, relying on intuition\nfrom traditional analysis of spectral methods. The present work challenges this\npoint of view: (i) the work reduces FNO to its core essence, resulting in a\nminimal architecture termed the ``averaging neural operator'' (ANO); and (ii)\nanalysis of the ANO shows that even this minimal ANO architecture benefits from\nuniversal approximation. This result is obtained based on only a spatial\naverage as its only nonlocal ingredient (corresponding to retaining only a\n\\emph{single} Fourier mode in the special case of the FNO). The analysis paves\nthe way for a more systematic exploration of nonlocality, both through the\ndevelopment of new operator learning architectures and the analysis of existing\nand new architectures. Numerical results are presented which give insight into\ncomplexity issues related to the roles of channel width (embedding dimension)\nand number of Fourier modes.\n"", '  Operator learning based on neural operators has emerged as a promising\nparadigm for the data-driven approximation of operators, mapping between\ninfinite-dimensional Banach spaces. Despite significant empirical progress, our\ntheoretical understanding regarding the efficiency of these approximations\nremains incomplete. This work addresses the parametric complexity of neural\noperator approximations for the general class of Lipschitz continuous\noperators. Motivated by recent findings on the limitations of specific\narchitectures, termed curse of parametric complexity, we here adopt an\ninformation-theoretic perspective. Our main contribution establishes lower\nbounds on the metric entropy of Lipschitz operators in two approximation\nsettings; uniform approximation over a compact set of input functions, and\napproximation in expectation, with input functions drawn from a probability\nmeasure. It is shown that these entropy bounds imply that, regardless of the\nactivation function used, neural operator architectures attaining an\napproximation accuracy $\\epsilon$ must have a size that is exponentially large\nin $\\epsilon^{-1}$. The size of architectures is here measured by counting the\nnumber of encoded bits necessary to store the given model in computational\nmemory. The results of this work elucidate fundamental trade-offs and\nlimitations in operator learning.\n', '  Operator learning refers to the application of ideas from machine learning to\napproximate (typically nonlinear) operators mapping between Banach spaces of\nfunctions. Such operators often arise from physical models expressed in terms\nof partial differential equations (PDEs). In this context, such approximate\noperators hold great potential as efficient surrogate models to complement\ntraditional numerical methods in many-query tasks. Being data-driven, they also\nenable model discovery when a mathematical description in terms of a PDE is not\navailable. This review focuses primarily on neural operators, built on the\nsuccess of deep neural networks in the approximation of functions defined on\nfinite dimensional Euclidean spaces. Empirically, neural operators have shown\nsuccess in a variety of applications, but our theoretical understanding remains\nincomplete. This review article summarizes recent progress and the current\nstate of our theoretical understanding of neural operators, focusing on an\napproximation theoretic point of view.\n'] , ['  Polysemantic neurons -- neurons that activate for a set of unrelated features\n-- have been seen as a significant obstacle towards interpretability of\ntask-optimized deep networks, with implications for AI safety. The classic\norigin story of polysemanticity is that the data contains more ``features"" than\nneurons, such that learning to perform a task forces the network to co-allocate\nmultiple unrelated features to the same neuron, endangering our ability to\nunderstand networks\' internal processing. In this work, we present a second and\nnon-mutually exclusive origin story of polysemanticity. We show that\npolysemanticity can arise incidentally, even when there are ample neurons to\nrepresent all features in the data, a phenomenon we term \\textit{incidental\npolysemanticity}. Using a combination of theory and experiments, we show that\nincidental polysemanticity can arise due to multiple reasons including\nregularization and neural noise; this incidental polysemanticity occurs because\nrandom initialization can, by chance alone, initially assign multiple features\nto the same neuron, and the training dynamics then strengthen such overlap. Our\npaper concludes by calling for further research quantifying the\nperformance-polysemanticity tradeoff in task-optimized deep neural networks to\nbetter understand to what extent polysemanticity is avoidable.\n', '  The field of mechanistic interpretability aims to study the role of\nindividual neurons in Deep Neural Networks. Single neurons, however, have the\ncapability to act polysemantically and encode for multiple (unrelated)\nfeatures, which renders their interpretation difficult. We present a method for\ndisentangling polysemanticity of any Deep Neural Network by decomposing a\npolysemantic neuron into multiple monosemantic ""virtual"" neurons. This is\nachieved by identifying the relevant sub-graph (""circuit"") for each ""pure""\nfeature. We demonstrate how our approach allows us to find and disentangle\nvarious polysemantic units of ResNet models trained on ImageNet. While\nevaluating feature visualizations using CLIP, our method effectively\ndisentangles representations, improving upon methods based on neuron\nactivations. Our code is available at https://github.com/maxdreyer/PURE.\n', ""  Despite substantial efforts, neural network interpretability remains an\nelusive goal, with previous research failing to provide succinct explanations\nof most single neurons' impact on the network output. This limitation is due to\nthe polysemantic nature of most neurons, whereby a given neuron is involved in\nmultiple unrelated network states, complicating the interpretation of that\nneuron. In this paper, we apply tools developed in neuroscience and information\ntheory to propose both a novel practical approach to network interpretability\nand theoretical insights into polysemanticity and the density of codes. We\ninfer levels of redundancy in the network's code by inspecting the\neigenspectrum of the activation's covariance matrix. Furthermore, we show how\nrandom projections can reveal whether a network exhibits a smooth or\nnon-differentiable code and hence how interpretable the code is. This same\nframework explains the advantages of polysemantic neurons to learning\nperformance and explains trends found in recent results by Elhage et\nal.~(2022). Our approach advances the pursuit of interpretability in neural\nnetworks, providing insights into their underlying structure and suggesting new\navenues for circuit-level interpretability.\n""] , ['  Recent years have witnessed a growing call for renewed emphasis on\nneuroscience-inspired approaches in artificial intelligence research, under the\nbanner of NeuroAI. A prime example of this is predictive coding networks\n(PCNs), based on the neuroscientific framework of predictive coding. This\nframework views the brain as a hierarchical Bayesian inference model that\nminimizes prediction errors through feedback connections. Unlike traditional\nneural networks trained with backpropagation (BP), PCNs utilize inference\nlearning (IL), a more biologically plausible algorithm that explains patterns\nof neural activity that BP cannot. Historically, IL has been more\ncomputationally intensive, but recent advancements have demonstrated that it\ncan achieve higher efficiency than BP with sufficient parallelization.\nFurthermore, PCNs can be mathematically considered a superset of traditional\nfeedforward neural networks (FNNs), significantly extending the range of\ntrainable architectures. As inherently probabilistic (graphical) latent\nvariable models, PCNs provide a versatile framework for both supervised\nlearning and unsupervised (generative) modeling that goes beyond traditional\nartificial neural networks. This work provides a comprehensive review and\ndetailed formal specification of PCNs, particularly situating them within the\ncontext of modern ML methods. Additionally, we introduce a Python library\n(PRECO) for practical implementation. This positions PC as a promising\nframework for future ML innovations.\n', ""  We propose Wake-Sleep Consolidated Learning (WSCL), a learning strategy\nleveraging Complementary Learning System theory and the wake-sleep phases of\nthe human brain to improve the performance of deep neural networks for visual\nclassification tasks in continual learning settings. Our method learns\ncontinually via the synchronization between distinct wake and sleep phases.\nDuring the wake phase, the model is exposed to sensory input and adapts its\nrepresentations, ensuring stability through a dynamic parameter freezing\nmechanism and storing episodic memories in a short-term temporary memory\n(similarly to what happens in the hippocampus). During the sleep phase, the\ntraining process is split into NREM and REM stages. In the NREM stage, the\nmodel's synaptic weights are consolidated using replayed samples from the\nshort-term and long-term memory and the synaptic plasticity mechanism is\nactivated, strengthening important connections and weakening unimportant ones.\nIn the REM stage, the model is exposed to previously-unseen realistic visual\nsensory experience, and the dreaming process is activated, which enables the\nmodel to explore the potential feature space, thus preparing synapses to future\nknowledge. We evaluate the effectiveness of our approach on three benchmark\ndatasets: CIFAR-10, Tiny-ImageNet and FG-ImageNet. In all cases, our method\noutperforms the baselines and prior work, yielding a significant performance\ngain on continual visual classification tasks. Furthermore, we demonstrate the\nusefulness of all processing stages and the importance of dreaming to enable\npositive forward transfer.\n"", '  Why do mammals need to sleep? Neuroscience treats sleep and wake as default\nand perturbation modes of the brain. It is hypothesized that the brain\nself-organizes neural activities without environmental inputs. This paper\npresents a new computational model of the sleep-wake cycle (SWC) for learning\nand memory. During the sleep mode, the memory consolidation by the\nthalamocortical system is abstracted by a disentangling operator that maps\ncontext-dependent representations (CDR) to context-independent representations\n(CIR) for generalization. Such a disentangling operator can be mathematically\nformalized by an integral transform that integrates the context variable from\nCDR. During the wake mode, the memory formation by the hippocampal-neocortical\nsystem is abstracted by an entangling operator from CIR to CDR where the\ncontext is introduced by physical motion. When designed as inductive bias,\nentangled CDR linearizes the problem of unsupervised learning for sensory\nmemory by direct-fit. The concatenation of disentangling and entangling\noperators forms a disentangling-entangling cycle (DEC) as the building block\nfor sensorimotor learning. We also discuss the relationship of DEC and SWC to\nthe perception-action cycle (PAC) for internal model learning and perceptual\ncontrol theory for the ecological origin of natural languages.\n']",Deep Learning Theory and Foundations,ReLU Neural Networks Properties and Approximation
260,"Quantum Machine Learning and Neural Networks , Spiking Neural Networks (SNNs) for Efficient Computing , Spin Systems and Neural Networks","['quantum', 'qubits', 'qubit', 'qcnns', 'qcnn', 'grover', 'entanglement', 'qnns', 'qiskit', 'qnn'] , ['spiking', 'neuron', 'neuromorphic', 'neurons', 'neural', 'neuronal', 'spike', 'dsnns', 'synapses', 'rsnns'] , ['spingnn', 'spin', 'ferromagnetic', 'isingnets', 'magnetization', 'boltzmann', 'lattice', 'magnetic', 'neural', 'networks']","['  This paper provides an introduction to quantum machine learning, exploring\nthe potential benefits of using quantum computing principles and algorithms\nthat may improve upon classical machine learning approaches. Quantum computing\nutilizes particles governed by quantum mechanics for computational purposes,\nleveraging properties like superposition and entanglement for information\nrepresentation and manipulation. Quantum machine learning applies these\nprinciples to enhance classical machine learning models, potentially reducing\nnetwork size and training time on quantum hardware. The paper covers basic\nquantum mechanics principles, including superposition, phase space, and\nentanglement, and introduces the concept of quantum gates that exploit these\nproperties. It also reviews classical deep learning concepts, such as\nartificial neural networks, gradient descent, and backpropagation, before\ndelving into trainable quantum circuits as neural networks. An example problem\ndemonstrates the potential advantages of quantum neural networks, and the\nappendices provide detailed derivations. The paper aims to help researchers new\nto quantum mechanics and machine learning develop their expertise more\nefficiently.\n', '  Graph states are used to represent mathematical graphs as quantum states on\nquantum computers. They can be formulated through stabilizer codes or directly\nquantum gates and quantum states. In this paper we show that a quantum graph\nneural network model can be understood and realized based on graph states. We\nshow that they can be used either as a parameterized quantum circuits to\nrepresent neural networks or as an underlying structure to construct graph\nneural networks on quantum computers.\n', '  In this work, quantum transformers are designed and analysed in detail by\nextending the state-of-the-art classical transformer neural network\narchitectures known to be very performant in natural language processing and\nimage analysis. Building upon the previous work, which uses parametrised\nquantum circuits for data loading and orthogonal neural layers, we introduce\nthree types of quantum transformers for training and inference, including a\nquantum transformer based on compound matrices, which guarantees a theoretical\nadvantage of the quantum attention mechanism compared to their classical\ncounterpart both in terms of asymptotic run time and the number of model\nparameters. These quantum architectures can be built using shallow quantum\ncircuits and produce qualitatively different classification models. The three\nproposed quantum attention layers vary on the spectrum between closely\nfollowing the classical transformers and exhibiting more quantum\ncharacteristics. As building blocks of the quantum transformer, we propose a\nnovel method for loading a matrix as quantum states as well as two new\ntrainable quantum orthogonal layers adaptable to different levels of\nconnectivity and quality of quantum computers. We performed extensive\nsimulations of the quantum transformers on standard medical image datasets that\nshowed competitively, and at times better performance compared to the classical\nbenchmarks, including the best-in-class classical vision transformers. The\nquantum transformers we trained on these small-scale datasets require fewer\nparameters compared to standard classical benchmarks. Finally, we implemented\nour quantum transformers on superconducting quantum computers and obtained\nencouraging results for up to six qubit experiments.\n'] , [""  This paper explores the synergistic potential of neuromorphic and edge\ncomputing to create a versatile machine learning (ML) system tailored for\nprocessing data captured by dynamic vision sensors. We construct and train\nhybrid models, blending spiking neural networks (SNNs) and artificial neural\nnetworks (ANNs) using PyTorch and Lava frameworks. Our hybrid architecture\nintegrates an SNN for temporal feature extraction and an ANN for\nclassification. We delve into the challenges of deploying such hybrid\nstructures on hardware. Specifically, we deploy individual components on\nIntel's Neuromorphic Processor Loihi (for SNN) and Jetson Nano (for ANN). We\nalso propose an accumulator circuit to transfer data from the spiking to the\nnon-spiking domain. Furthermore, we conduct comprehensive performance analyses\nof hybrid SNN-ANN models on a heterogeneous system of neuromorphic and edge AI\nhardware, evaluating accuracy, latency, power, and energy consumption. Our\nfindings demonstrate that the hybrid spiking networks surpass the baseline ANN\nmodel across all metrics and outperform the baseline SNN model in accuracy and\nlatency.\n"", '  Brain-inspired Spiking Neural Networks (SNNs) have bio-plausibility and\nlow-power advantages over Artificial Neural Networks (ANNs). Applications of\nSNNs are currently limited to simple classification tasks because of their poor\nperformance. In this work, we focus on bridging the performance gap between\nANNs and SNNs on object detection. Our design revolves around network\narchitecture and spiking neuron. First, the overly complex module design causes\nspike degradation when the YOLO series is converted to the corresponding\nspiking version. We design a SpikeYOLO architecture to solve this problem by\nsimplifying the vanilla YOLO and incorporating meta SNN blocks. Second, object\ndetection is more sensitive to quantization errors in the conversion of\nmembrane potentials into binary spikes by spiking neurons. To address this\nchallenge, we design a new spiking neuron that activates Integer values during\ntraining while maintaining spike-driven by extending virtual timesteps during\ninference. The proposed method is validated on both static and neuromorphic\nobject detection datasets. On the static COCO dataset, we obtain 66.2% mAP@50\nand 48.9% mAP@50:95, which is +15.0% and +18.7% higher than the prior\nstate-of-the-art SNN, respectively. On the neuromorphic Gen1 dataset, we\nachieve 67.2% mAP@50, which is +2.5% greater than the ANN with equivalent\narchitecture, and the energy efficiency is improved by 5.7*. Code:\nhttps://github.com/BICLab/SpikeYOLO\n', '  Spiking neural networks (SNNs) have low power consumption and\nbio-interpretable characteristics, and are considered to have tremendous\npotential for energy-efficient computing. However, the exploration of SNNs on\nimage generation tasks remains very limited, and a unified and effective\nstructure for SNN-based generative models has yet to be proposed. In this\npaper, we explore a novel diffusion model architecture within spiking neural\nnetworks. We utilize transformer to replace the commonly used U-net structure\nin mainstream diffusion models. It can generate higher quality images with\nrelatively lower computational cost and shorter sampling time. It aims to\nprovide an empirical baseline for research of generative models based on SNNs.\nExperiments on MNIST, Fashion-MNIST, and CIFAR-10 datasets demonstrate that our\nwork is highly competitive compared to existing SNN generative models.\n'] , ['  Frustrated itinerant magnets often exhibit complex noncollinear or\nnoncoplanar magnetic orders which support topological electronic structures. A\ncanonical example is the anomalous quantum Hall state with a chiral spin order\nstabilized by electron-spin interactions on a triangular lattice. While a\nlong-range magnetic order cannot survive thermal fluctuations in two\ndimensions, the chiral order which results from the breaking of a discrete\nIsing symmetry persists even at finite temperatures. We present a scalable\nmachine learning (ML) framework to model the complex electron-mediated\nspin-spin interactions that stabilize the chiral magnetic domains in a\ntriangular lattice. Large-scale dynamical simulations, enabled by the ML\nforce-field models, are performed to investigate the coarsening of chiral\ndomains after a thermal quench. While the chiral phase is described by a broken\n$Z_2$ Ising-type symmetry, we find that the characteristic size of chiral\ndomains increases linearly with time, in stark contrast to the expected\nAllen-Cahn domain growth law for a non-conserved Ising order parameter field.\nThe linear growth of the chiral domains is attributed to the orientational\nanisotropy of domain boundaries. Our work also demonstrates the promising\npotential of ML models for large-scale spin dynamics of itinerant magnets.\n', ""  We explore a one-to-one correspondence between a neural network (NN) and a\nstatistical mechanical spin model where neurons are mapped to Ising spins and\nweights to spin-spin couplings. The process of training an NN produces a family\nof spin Hamiltonians parameterized by training time. We study the magnetic\nphases and the melting transition temperature as training progresses. First, we\nprove analytically that the common initial state before training--an NN with\nindependent random weights--maps to a layered version of the classical\nSherrington-Kirkpatrick spin glass exhibiting a replica symmetry breaking. The\nspin-glass-to-paramagnet transition temperature is calculated. Further, we use\nthe Thouless-Anderson-Palmer (TAP) equations--a theoretical technique to\nanalyze the landscape of energy minima of random systems--to determine the\nevolution of the magnetic phases on two types of NNs (one with continuous and\none with binarized activations) trained on the MNIST dataset. The two NN types\ngive rise to similar results, showing a quick destruction of the spin glass and\nthe appearance of a phase with a hidden order, whose melting transition\ntemperature $T_c$ grows as a power law in training time. We also discuss the\nproperties of the spectrum of the spin system's bond matrix in the context of\nrich vs. lazy learning. We suggest that this statistical mechanical view of NNs\nprovides a useful unifying perspective on the training process, which can be\nviewed as selecting and strengthening a symmetry-broken state associated with\nthe training task.\n"", '  Many deep neural networks have been used to solve Ising models, including\nautoregressive neural networks, convolutional neural networks, recurrent neural\nnetworks, and graph neural networks. Learning a probability distribution of\nenergy configuration or finding the ground states of a disordered, fully\nconnected Ising model is essential for statistical mechanics and NP-hard\nproblems. Despite tremendous efforts, a neural network architecture with the\nability to high-accurately solve these fully connected and extremely\nintractable problems on larger systems is still lacking. Here we propose a\nvariational autoregressive architecture with a message passing mechanism, which\ncan effectively utilize the interactions between spin variables. The new\nnetwork trained under an annealing framework outperforms existing methods in\nsolving several prototypical Ising spin Hamiltonians, especially for larger\nspin systems at low temperatures. The advantages also come from the great\nmitigation of mode collapse during the training process of deep neural\nnetworks. Considering these extremely difficult problems to be solved, our\nmethod extends the current computational limits of unsupervised neural networks\nto solve combinatorial optimization problems.\n']",Quantum and Neuromorphic Computing,Spin Systems and Neural Networks
261,"Koopman Operator for Nonlinear Dynamics Analysis , Numerical Solvers for Nonlinear Fractional Differential Equations","['linearization', 'koopman', 'linearizations', 'observables', 'operators', 'eigenfunctions', 'dynamical', 'nonlinear', 'dynamics', 'spectral'] , ['nonlinear', 'fractional', 'sparse', 'solvers', 'odes', 'differential', 'pdes', 'rnns', 'solver', 'numerical']","['  Machine learning methods allow the prediction of nonlinear dynamical systems\nfrom data alone. The Koopman operator is one of them, which enables us to\nemploy linear analysis for nonlinear dynamical systems. The linear\ncharacteristics of the Koopman operator are hopeful to understand the nonlinear\ndynamics and perform rapid predictions. The extended dynamic mode decomposition\n(EDMD) is one of the methods to approximate the Koopman operator as a\nfinite-dimensional matrix. In this work, we propose a method to compress the\nKoopman matrix using hierarchical clustering. Numerical demonstrations for the\ncart-pole model and comparisons with the conventional singular value\ndecomposition (SVD) are shown; the results indicate that the hierarchical\nclustering performs better than the naive SVD compressions.\n', ""  Data-driven approximations of the Koopman operator are promising for\npredicting the time evolution of systems characterized by complex dynamics.\nAmong these methods, the approach known as extended dynamic mode decomposition\nwith dictionary learning (EDMD-DL) has garnered significant attention. Here we\npresent a modification of EDMD-DL that concurrently determines both the\ndictionary of observables and the corresponding approximation of the Koopman\noperator. This innovation leverages automatic differentiation to facilitate\ngradient descent computations through the pseudoinverse. We also address the\nperformance of several alternative methodologies. We assess a 'pure' Koopman\napproach, which involves the direct time-integration of a linear,\nhigh-dimensional system governing the dynamics within the space of observables.\nAdditionally, we explore a modified approach where the system alternates\nbetween spaces of states and observables at each time step -- this approach no\nlonger satisfies the linearity of the true Koopman operator representation. For\nfurther comparisons, we also apply a state space approach (neural ODEs). We\nconsider systems encompassing two and three-dimensional ordinary differential\nequation systems featuring steady, oscillatory, and chaotic attractors, as well\nas partial differential equations exhibiting increasingly complex and intricate\nbehaviors. Our framework significantly outperforms EDMD-DL. Furthermore, the\nstate space approach offers superior performance compared to the 'pure' Koopman\napproach where the entire time evolution occurs in the space of observables.\nWhen the temporal evolution of the Koopman approach alternates between states\nand observables at each time step, however, its predictions become comparable\nto those of the state space approach.\n"", '  The Koopman operator provides a linear perspective on non-linear dynamics by\nfocusing on the evolution of observables in an invariant subspace. Observables\nof interest are typically linearly reconstructed from the Koopman\neigenfunctions. Despite the broad use of Koopman operators over the past few\nyears, there exist some misconceptions about the applicability of Koopman\noperators to dynamical systems with more than one disjoint invariant sets\n(e.g., basins of attractions from isolated fixed points). In this work, we\nfirst provide a simple explanation for the mechanism of linear\nreconstruction-based Koopman operators of nonlinear systems with multiple\ndisjoint invariant sets. Next, we discuss the use of discrete symmetry among\nsuch invariant sets to construct Koopman eigenfunctions in a data efficient\nmanner. Finally, several numerical examples are provided to illustrate the\nbenefits of exploiting symmetry for learning the Koopman operator.\n'] , ['  This paper presents a novel operational matrix method to accelerate the\ntraining of fractional Physics-Informed Neural Networks (fPINNs). Our approach\ninvolves a non-uniform discretization of the fractional Caputo operator,\nfacilitating swift computation of fractional derivatives within Caputo-type\nfractional differential problems with $0<\\alpha<1$. In this methodology, the\noperational matrix is precomputed, and during the training phase, automatic\ndifferentiation is replaced with a matrix-vector product. While our methodology\nis compatible with any network, we particularly highlight its successful\nimplementation in PINNs, emphasizing the enhanced accuracy achieved when\nutilizing the Legendre Neural Block (LNB) architecture. LNB incorporates\nLegendre polynomials into the PINN structure, providing a significant boost in\naccuracy. The effectiveness of our proposed method is validated across diverse\ndifferential equations, including Delay Differential Equations (DDEs) and\nSystems of Differential Algebraic Equations (DAEs). To demonstrate its\nversatility, we extend the application of the method to systems of differential\nequations, specifically addressing nonlinear Pantograph fractional-order\nDDEs/DAEs. The results are supported by a comprehensive analysis of numerical\noutcomes.\n', '  The sparse identification of nonlinear dynamical systems (SINDy) is a\ndata-driven technique employed for uncovering and representing the fundamental\ndynamics of intricate systems based on observational data. However, a primary\nobstacle in the discovery of models for nonlinear partial differential\nequations (PDEs) lies in addressing the challenges posed by the curse of\ndimensionality and large datasets. Consequently, the strategic selection of the\nmost informative samples within a given dataset plays a crucial role in\nreducing computational costs and enhancing the effectiveness of SINDy-based\nalgorithms. To this aim, we employ a greedy sampling approach to the snapshot\nmatrix of a PDE to obtain its valuable samples, which are suitable to train a\ndeep neural network (DNN) in a SINDy framework. SINDy based algorithms often\nconsist of a data collection unit, constructing a dictionary of basis\nfunctions, computing the time derivative, and solving a sparse identification\nproblem which ends to regularised least squares minimization. In this paper, we\nextend the results of a SINDy based deep learning model discovery (DeePyMoD)\napproach by integrating greedy sampling technique in its data collection unit\nand new sparsity promoting algorithms in the least squares minimization unit.\nIn this regard we introduce the greedy sampling neural network in sparse\nidentification of nonlinear partial differential equations (GN-SINDy) which\nblends a greedy sampling method, the DNN, and the SINDy algorithm. In the\nimplementation phase, to show the effectiveness of GN-SINDy, we compare its\nresults with DeePyMoD by using a Python package that is prepared for this\npurpose on numerous PDE discovery\n', ""  We introduce an innovative approach for solving high-dimensional\nFokker-Planck-L\\'evy (FPL) equations in modeling non-Brownian processes across\ndisciplines such as physics, finance, and ecology. We utilize a fractional\nscore function and Physical-informed neural networks (PINN) to lift the curse\nof dimensionality (CoD) and alleviate numerical overflow from exponentially\ndecaying solutions with dimensions. The introduction of a fractional score\nfunction allows us to transform the FPL equation into a second-order partial\ndifferential equation without fractional Laplacian and thus can be readily\nsolved with standard physics-informed neural networks (PINNs). We propose two\nmethods to obtain a fractional score function: fractional score matching (FSM)\nand score-fPINN for fitting the fractional score function. While FSM is more\ncost-effective, it relies on known conditional distributions. On the other\nhand, score-fPINN is independent of specific stochastic differential equations\n(SDEs) but requires evaluating the PINN model's derivatives, which may be more\ncostly. We conduct our experiments on various SDEs and demonstrate numerical\nstability and effectiveness of our method in dealing with high-dimensional\nproblems, marking a significant advancement in addressing the CoD in FPL\nequations.\n""]",Nonlinear Dynamics and Differential Equations Analysis,Koopman Operator for Nonlinear Dynamics Analysis
262,"Physics-Informed Neural Networks for PDEs , Physics-Informed Machine Learning for Differential Equations , ""Physics-Informed Neural Networks for Fluid Dynamics"" , Physics-Informed Neural Networks for Fluid Flow Modeling","['pdes', 'pde', 'learning', 'neural', 'solvers', 'pinn', 'nonlinear', 'pinns', 'solver', 'networks'] , ['gradients', 'differential', 'pdes', 'nonlinear', 'pde', 'solvers', 'neural', 'parameterized', 'dde', 'nonlinearity'] , ['modeling', 'simulations', 'learning', 'neural', 'dynamics', 'networks', 'nonlinear', 'pdes', 'simulation', 'computational'] , ['reservoir', 'deep', 'flow', 'flows', 'predicting', 'modeling', 'predict', 'simulations', 'neural', 'prediction']","['  Learning and solving governing equations of a physical system, represented by\npartial differential equations (PDEs), from data is a central challenge in a\nvariety of areas of science and engineering. Traditional numerical methods for\nsolving PDEs can be computationally expensive for complex systems and require\nthe complete PDEs of the physical system. On the other hand, current\ndata-driven machine learning methods require a large amount of data to learn a\nsurrogate model of the PDE solution operator, which could be impractical. Here,\nwe propose the first solution operator learning method that only requires one\nPDE solution, i.e., one-shot learning. By leveraging the principle of locality\nof PDEs, we consider small local domains instead of the entire computational\ndomain and define a local solution operator. The local solution operator is\nthen trained using a neural network, and utilized to predict the solution of a\nnew input function via mesh-based fixed-point iteration (FPI), meshfree\nlocal-solution-operator informed neural network (LOINN) or\nlocal-solution-operator informed neural network with correction (cLOINN). We\ntest our method on diverse PDEs, including linear or nonlinear PDEs, PDEs\ndefined on complex geometries, and PDE systems, demonstrating the effectiveness\nand generalization capabilities of our method across these varied scenarios.\n', '  Physics-informed neural networks (PINNs) have recently emerged as a promising\nway to compute the solutions of partial differential equations (PDEs) using\ndeep neural networks. However, despite their significant success in various\nfields, it remains unclear in many aspects how to effectively train PINNs if\nthe solutions of PDEs exhibit stiff behaviors or high frequencies. In this\npaper, we propose a new method for training PINNs using variable-scaling\ntechniques. This method is simple and it can be applied to a wide range of\nproblems including PDEs with rapidly-varying solutions. Throughout various\nnumerical experiments, we will demonstrate the effectiveness of the proposed\nmethod for these problems and confirm that it can significantly improve the\ntraining efficiency and performance of PINNs. Furthermore, based on the\nanalysis of the neural tangent kernel (NTK), we will provide theoretical\nevidence for this phenomenon and show that our methods can indeed improve the\nperformance of PINNs.\n', '  Physics-informed neural networks (PINNs) have attracted significant attention\nfor solving partial differential equations (PDEs) in recent years because they\nalleviate the curse of dimensionality that appears in traditional methods.\nHowever, the most disadvantage of PINNs is that one neural network corresponds\nto one PDE. In practice, we usually need to solve a class of PDEs, not just\none. With the explosive growth of deep learning, many useful techniques in\ngeneral deep learning tasks are also suitable for PINNs. Transfer learning\nmethods may reduce the cost for PINNs in solving a class of PDEs. In this\npaper, we proposed a transfer learning method of PINNs via keeping singular\nvectors and optimizing singular values (namely SVD-PINNs). Numerical\nexperiments on high dimensional PDEs (10-d linear parabolic equations and 10-d\nAllen-Cahn equations) show that SVD-PINNs work for solving a class of PDEs with\ndifferent but close right-hand-side functions.\n'] , ['  Machine learning techniques have recently been of great interest for solving\ndifferential equations. Training these models is classically a data-fitting\ntask, but knowledge of the expression of the differential equation can be used\nto supplement the training objective, leading to the development of\nphysics-informed scientific machine learning. In this article, we focus on one\nclass of models called nonlinear vector autoregression (NVAR) to solve ordinary\ndifferential equations (ODEs). Motivated by connections to numerical\nintegration and physics-informed neural networks, we explicitly derive the\nphysics-informed NVAR (piNVAR) which enforces the right-hand side of the\nunderlying differential equation regardless of NVAR construction. Because NVAR\nand piNVAR completely share their learned parameters, we propose an augmented\nprocedure to jointly train the two models. Then, using both data-driven and\nODE-driven metrics, we evaluate the ability of the piNVAR model to predict\nsolutions to various ODE systems, such as the undamped spring, a Lotka-Volterra\npredator-prey nonlinear model, and the chaotic Lorenz system.\n', '  In this work, we present an adjoint-based method for discovering the\nunderlying governing partial differential equations (PDEs) given data. The idea\nis to consider a parameterized PDE in a general form and formulate a\nPDE-constrained optimization problem aimed at minimizing the error of the PDE\nsolution from data. Using variational calculus, we obtain an evolution equation\nfor the Lagrange multipliers (adjoint equations) allowing us to compute the\ngradient of the objective function with respect to the parameters of PDEs given\ndata in a straightforward manner. In particular, we consider a family of\nparameterized PDEs encompassing linear, nonlinear, and spatial derivative\ncandidate terms, and elegantly derive the corresponding adjoint equations. We\nshow the efficacy of the proposed approach in identifying the form of the PDE\nup to machine accuracy, enabling the accurate discovery of PDEs from data. We\nalso compare its performance with the famous PDE Functional Identification of\nNonlinear Dynamics method known as PDE-FIND (Rudy et al., 2017), on both smooth\nand noisy data sets. Even though the proposed adjoint method relies on\nforward/backward solvers, it outperforms PDE-FIND for large data sets thanks to\nthe analytic expressions for gradients of the cost function with respect to\neach PDE parameter.\n', ""  Delay Differential Equations (DDEs) are a class of differential equations\nthat can model diverse scientific phenomena. However, identifying the\nparameters, especially the time delay, that make a DDE's predictions match\nexperimental results can be challenging. We introduce DDE-Find, a data-driven\nframework for learning a DDE's parameters, time delay, and initial condition\nfunction. DDE-Find uses an adjoint-based approach to efficiently compute the\ngradient of a loss function with respect to the model parameters. We motivate\nand rigorously prove an expression for the gradients of the loss using the\nadjoint. DDE-Find builds upon recent developments in learning DDEs from data\nand delivers the first complete framework for learning DDEs from data. Through\na series of numerical experiments, we demonstrate that DDE-Find can learn DDEs\nfrom noisy, limited data.\n""] , [""  Multiscale phenomena manifest across various scientific domains, presenting a\nubiquitous challenge in accurately and effectively simulating multiscale\ndynamics in complex systems. In this paper, a novel decoupling solving paradigm\nis proposed through modelling large-scale dynamics independently and treating\nsmall-scale dynamics as a slaved system. A Spectral Physics-informed Neural\nNetwork (PINN) is developed to characterize the small-scale system in an\nefficient and accurate way, addressing the challenges posed by the\nrepresentation of multiscale dynamics in neural networks. The effectiveness of\nthe method is demonstrated through extensive numerical experiments, including\none-dimensional Kuramot-Sivashinsky equation, two- and three-dimensional\nNavier-Stokes equations, showcasing its versatility in addressing problems of\nfluid dynamics. Furthermore, we also delve into the application of the proposed\napproach to more complex problems, including non-uniform meshes, complex\ngeometries, large-scale data with noise, and high-dimensional small-scale\ndynamics. The discussions about these scenarios contribute to a comprehensive\nunderstanding of the method's capabilities and limitations. By enabling the\nacquisition of large-scale data with minimal computational demands, coupled\nwith the efficient and accurate characterization of small-scale dynamics via\nSpectral PINN, our approach offers a valuable and promising approach for\nresearchers seeking to tackle multiscale phenomena effectively.\n"", '  Finding the distribution of the velocities and pressures of a fluid by\nsolving the Navier-Stokes equations is a principal task in the chemical,\nenergy, and pharmaceutical industries, as well as in mechanical engineering and\nthe design of pipeline systems. With existing solvers, such as OpenFOAM and\nAnsys, simulations of fluid dynamics in intricate geometries are\ncomputationally expensive and require re-simulation whenever the geometric\nparameters or the initial and boundary conditions are altered. Physics-informed\nneural networks are a promising tool for simulating fluid flows in complex\ngeometries, as they can adapt to changes in the geometry and mesh definitions,\nallowing for generalization across fluid parameters and transfer learning\nacross different shapes. We present a hybrid quantum physics-informed neural\nnetwork that simulates laminar fluid flows in 3D Y-shaped mixers. Our approach\ncombines the expressive power of a quantum model with the flexibility of a\nphysics-informed neural network, resulting in a 21% higher accuracy compared to\na purely classical neural network. Our findings highlight the potential of\nmachine learning approaches, and in particular hybrid quantum physics-informed\nneural network, for complex shape optimization tasks in computational fluid\ndynamics. By improving the accuracy of fluid simulations in complex geometries,\nour research using hybrid quantum models contributes to the development of more\nefficient and reliable fluid dynamics solvers.\n', '  Scientific discovery and engineering design are currently limited by the time\nand cost of physical experiments, selected mostly through trial-and-error and\nintuition that require deep domain expertise. Numerical simulations present an\nalternative to physical experiments but are usually infeasible for complex\nreal-world domains due to the computational requirements of existing numerical\nmethods. Artificial intelligence (AI) presents a potential paradigm shift by\ndeveloping fast data-driven surrogate models. In particular, an AI framework,\nknown as Neural Operators, presents a principled framework for learning\nmappings between functions defined on continuous domains, e.g., spatiotemporal\nprocesses and partial differential equations (PDE). They can extrapolate and\npredict solutions at new locations unseen during training, i.e., perform\nzero-shot super-resolution. Neural Operators can augment or even replace\nexisting simulators in many applications, such as computational fluid dynamics,\nweather forecasting, and material modeling, while being 4-5 orders of magnitude\nfaster. Further, Neural Operators can be integrated with physics and other\ndomain constraints enforced at finer resolutions to obtain high-fidelity\nsolutions and good generalization. Since Neural Operators are differentiable,\nthey can directly optimize parameters for inverse design and other inverse\nproblems. We believe that Neural Operators present a transformative approach to\nsimulation and design, enabling rapid research and development.\n'] , [""  We developed a novel reservoir characterization workflow that addresses\nreservoir history matching by coupling a physics-informed neural operator\n(PINO) forward model with a mixture of experts' approach, termed cluster\nclassify regress (CCR). The inverse modelling is achieved via an adaptive\nRegularized Ensemble Kalman inversion (aREKI) method, ideal for rapid inverse\nuncertainty quantification during history matching. We parametrize unknown\npermeability and porosity fields for non-Gaussian posterior measures using a\nvariational convolution autoencoder and a denoising diffusion implicit model\n(DDIM) exotic priors. The CCR works as a supervised model with the PINO\nsurrogate to replicate nonlinear Peaceman well equations. The CCR's flexibility\nallows any independent machine-learning algorithm for each stage. The PINO\nreservoir surrogate's loss function is derived from supervised data loss and\nlosses from the initial conditions and residual of the governing black oil PDE.\nThe PINO-CCR surrogate outputs pressure, water, and gas saturations, along with\noil, water, and gas production rates. The methodology was compared to a\nstandard numerical black oil simulator for a waterflooding case on the Norne\nfield, showing similar outputs. This PINO-CCR surrogate was then used in the\naREKI history matching workflow, successfully recovering the unknown\npermeability, porosity and fault multiplier, with simulations up to 6000 times\nfaster than conventional methods. Training the PINO-CCR surrogate on an NVIDIA\nH100 with 80G memory takes about 5 hours for 100 samples of the Norne field.\nThis workflow is suitable for ensemble-based approaches, where posterior\ndensity sampling, given an expensive likelihood evaluation, is desirable for\nuncertainty quantification.\n"", '  Accurately predicting the long-term behavior of chaotic systems is crucial\nfor various applications such as climate modeling. However, achieving such\npredictions typically requires iterative computations over a dense\nspatiotemporal grid to account for the unstable nature of chaotic systems,\nwhich is expensive and impractical in many real-world situations. An\nalternative approach to such a full-resolved simulation is using a coarse grid\nand then correcting its errors through a \\textit{closure model}, which\napproximates the overall information from fine scales not captured in the\ncoarse-grid simulation. Recently, ML approaches have been used for closure\nmodeling, but they typically require a large number of training samples from\nexpensive fully-resolved simulations (FRS). In this work, we prove an even more\nfundamental limitation, i.e., the standard approach to learning closure models\nsuffers from a large approximation error for generic problems, no matter how\nlarge the model is, and it stems from the non-uniqueness of the mapping. We\npropose an alternative end-to-end learning approach using a physics-informed\nneural operator (PINO) that overcomes this limitation by not using a closure\nmodel or a coarse-grid solver. We first train the PINO model on data from a\ncoarse-grid solver and then fine-tune it with (a small amount of) FRS and\nphysics-based losses on a fine grid. The discretization-free nature of neural\noperators means that they do not suffer from the restriction of a coarse grid\nthat closure models face, and they can provably approximate the long-term\nstatistics of chaotic systems. In our experiments, our PINO model achieves a\n120x speedup compared to FRS with a relative error $\\sim 5\\%$. In contrast, the\nclosure model coupled with a coarse-grid solver is $58$x slower than PINO while\nhaving a much higher error $\\sim205\\%$ when the closure model is trained on the\nsame FRS dataset.\n', ""  High-resolution reconstruction of flow-field data from low-resolution and\nnoisy measurements is of interest due to the prevalence of such problems in\nexperimental fluid mechanics, where the measurement data are in general sparse,\nincomplete and noisy. Deep-learning approaches have been shown suitable for\nsuch super-resolution tasks. However, a high number of high-resolution examples\nis needed, which may not be available for many cases. Moreover, the obtained\npredictions may lack in complying with the physical principles, e.g. mass and\nmomentum conservation. Physics-informed deep learning provides frameworks for\nintegrating data and physical laws for learning. In this study, we apply\nphysics-informed neural networks (PINNs) for super-resolution of flow-field\ndata both in time and space from a limited set of noisy measurements without\nhaving any high-resolution reference data. Our objective is to obtain a\ncontinuous solution of the problem, providing a physically-consistent\nprediction at any point in the solution domain. We demonstrate the\napplicability of PINNs for the super-resolution of flow-field data in time and\nspace through three canonical cases: Burgers' equation, two-dimensional vortex\nshedding behind a circular cylinder and the minimal turbulent channel flow. The\nrobustness of the models is also investigated by adding synthetic Gaussian\nnoise. Furthermore, we show the capabilities of PINNs to improve the resolution\nand reduce the noise in a real experimental dataset consisting of\nhot-wire-anemometry measurements. Our results show the adequate capabilities of\nPINNs in the context of data augmentation for experiments in fluid mechanics.\n""]",Physics-Informed Machine Learning for Differential Equations and Fluid Dynamics,"""Physics-Informed Neural Networks for Fluid Dynamics"""
263,"Neural Networks for Dynamical Systems Modeling , ""Neural Operators for Dynamical Systems"" , ""Neural Operators for PDE Control and Stabilization"" , Recurrent Neural Networks and Oscillatory Dynamics","['dynamics', 'learning', 'dynamical', 'neural', 'modeling', 'odes', 'networks', 'dynamic', 'nonlinear', 'hamiltonian'] , ['variational', 'dynamical', 'neural', 'stochastic', 'dynamics', 'kalman', 'nonlinear', 'ensemble', 'spatiotemporal', 'simulations'] , ['pdes', 'pde', 'stabilization', 'lyapunov', 'control', 'stability', 'controller', 'controllers', 'nonlinear', 'stabilizing'] , ['backpropagation', 'rnns', 'neural', 'rnn', 'networks', 'neurons', 'gradients', 'recurrent', 'gradient', 'dynamical']","[""  Neural ordinary differential equations (Neural ODEs) is a class of machine\nlearning models that approximate the time derivative of hidden states using a\nneural network. They are powerful tools for modeling continuous-time dynamical\nsystems, enabling the analysis and prediction of complex temporal behaviors.\nHowever, how to improve the model's stability and physical interpretability\nremains a challenge. This paper introduces new conservation relations in Neural\nODEs using Lie symmetries in both the hidden state dynamics and the back\npropagation dynamics. These conservation laws are then incorporated into the\nloss function as additional regularization terms, potentially enhancing the\nphysical interpretability and generalizability of the model. To illustrate this\nmethod, the paper derives Lie symmetries and conservation laws in a simple\nNeural ODE designed to monitor charged particles in a sinusoidal electric\nfield. New loss functions are constructed from these conservation relations,\ndemonstrating the applicability symmetry-regularized Neural ODE in typical\nmodeling tasks, such as data-driven discovery of dynamical systems.\n"", ""  Differential equations are a ubiquitous tool to study dynamics, ranging from\nphysical systems to complex systems, where a large number of agents interact\nthrough a graph with non-trivial topological features. Data-driven\napproximations of differential equations present a promising alternative to\ntraditional methods for uncovering a model of dynamical systems, especially in\ncomplex systems that lack explicit first principles. A recently employed\nmachine learning tool for studying dynamics is neural networks, which can be\nused for data-driven solution finding or discovery of differential equations.\nSpecifically for the latter task, however, deploying deep learning models in\nunfamiliar settings - such as predicting dynamics in unobserved state space\nregions or on novel graphs - can lead to spurious results. Focusing on complex\nsystems whose dynamics are described with a system of first-order differential\nequations coupled through a graph, we show that extending the model's\ngeneralizability beyond traditional statistical learning theory limits is\nfeasible. However, achieving this advanced level of generalization requires\nneural network models to conform to fundamental assumptions about the dynamical\nmodel. Additionally, we propose a statistical significance test to assess\nprediction quality during inference, enabling the identification of a neural\nnetwork's confidence level in its predictions.\n"", '  Recent advances in deep learning for physics have focused on discovering\nshared representations of target systems by incorporating physics priors or\ninductive biases into neural networks. While effective, these methods are\nlimited to the system domain, where the type of system remains consistent and\nthus cannot ensure the adaptation to new, or unseen physical systems governed\nby different laws. For instance, a neural network trained on a mass-spring\nsystem cannot guarantee accurate predictions for the behavior of a two-body\nsystem or any other system with different physical laws. In this work, we take\na significant leap forward by targeting cross domain generalization within the\nfield of Hamiltonian dynamics. We model our system with a graph neural network\n(GNN) and employ a meta learning algorithm to enable the model to gain\nexperience over a distribution of systems and make it adapt to new physics. Our\napproach aims to learn a unified Hamiltonian representation that is\ngeneralizable across multiple system domains, thereby overcoming the\nlimitations of system-specific models. We demonstrate that the meta-trained\nmodel captures the generalized Hamiltonian representation that is consistent\nacross different physical domains. Overall, through the use of meta learning,\nwe offer a framework that achieves cross domain generalization, providing a\nstep towards a unified model for understanding a wide array of dynamical\nsystems via deep learning.\n'] , ['  Modeling dynamical systems, e.g. in climate and engineering sciences, often\nnecessitates solving partial differential equations. Neural operators are deep\nneural networks designed to learn nontrivial solution operators of such\ndifferential equations from data. As for all statistical models, the\npredictions of these models are imperfect and exhibit errors. Such errors are\nparticularly difficult to spot in the complex nonlinear behaviour of dynamical\nsystems. We introduce a new framework for approximate Bayesian uncertainty\nquantification in neural operators using function-valued Gaussian processes.\nOur approach can be interpreted as a probabilistic analogue of the concept of\ncurrying from functional programming and provides a practical yet theoretically\nsound way to apply the linearized Laplace approximation to neural operators. In\na case study on Fourier neural operators, we show that, even for a discretized\ninput, our method yields a Gaussian closure--a structured Gaussian process\nposterior capturing the uncertainty in the output function of the neural\noperator, which can be evaluated at an arbitrary set of points. The method adds\nminimal prediction overhead, can be applied post-hoc without retraining the\nneural operator, and scales to large models and datasets. We showcase the\nefficacy of our approach through applications to different types of partial\ndifferential equations.\n', '  With the increasing availability of large scale datasets, computational power\nand tools like automatic differentiation and expressive neural network\narchitectures, sequential data are now often treated in a data-driven way, with\na dynamical model trained from the observation data. While neural networks are\noften seen as uninterpretable black-box architectures, they can still benefit\nfrom physical priors on the data and from mathematical knowledge. In this\npaper, we use a neural network architecture which leverages the long-known\nKoopman operator theory to embed dynamical systems in latent spaces where their\ndynamics can be described linearly, enabling a number of appealing features. We\nintroduce methods that enable to train such a model for long-term continuous\nreconstruction, even in difficult contexts where the data comes in\nirregularly-sampled time series. The potential for self-supervised learning is\nalso demonstrated, as we show the promising use of trained dynamical models as\npriors for variational data assimilation techniques, with applications to e.g.\ntime series interpolation and forecasting.\n', '  Incorporating unstructured data into physical models is a challenging problem\nthat is emerging in data assimilation. Traditional approaches focus on\nwell-defined observation operators whose functional forms are typically assumed\nto be known. This prevents these methods from achieving a consistent model-data\nsynthesis in configurations where the mapping from data-space to model-space is\nunknown. To address these shortcomings, in this paper we develop a\nphysics-informed dynamical variational autoencoder ($\\Phi$-DVAE) to embed\ndiverse data streams into time-evolving physical systems described by\ndifferential equations. Our approach combines a standard, possibly nonlinear,\nfilter for the latent state-space model and a VAE, to assimilate the\nunstructured data into the latent dynamical system. Unstructured data, in our\nexample systems, comes in the form of video data and velocity field\nmeasurements, however the methodology is suitably generic to allow for\narbitrary unknown observation operators. A variational Bayesian framework is\nused for the joint estimation of the encoding, latent states, and unknown\nsystem parameters. To demonstrate the method, we provide case studies with the\nLorenz-63 ordinary differential equation, and the advection and Korteweg-de\nVries partial differential equations. Our results, with synthetic data, show\nthat $\\Phi$-DVAE provides a data efficient dynamics encoding methodology which\nis competitive with standard approaches. Unknown parameters are recovered with\nuncertainty quantification, and unseen data are accurately predicted.\n'] , ['  To stabilize PDE models, control laws require space-dependent functional\ngains mapped by nonlinear operators from the PDE functional coefficients. When\na PDE is nonlinear and its ""pseudo-coefficient"" functions are state-dependent,\na gain-scheduling (GS) nonlinear design is the simplest approach to the design\nof nonlinear feedback. The GS version of PDE backstepping employs gains\nobtained by solving a PDE at each value of the state. Performing such PDE\ncomputations in real time may be prohibitive. The recently introduced neural\noperators (NO) can be trained to produce the gain functions, rapidly in real\ntime, for each state value, without requiring a PDE solution. In this paper we\nintroduce NOs for GS-PDE backstepping. GS controllers act on the premise that\nthe state change is slow and, as a result, guarantee only local stability, even\nfor ODEs. We establish local stabilization of hyperbolic PDEs with nonlinear\nrecirculation using both a ""full-kernel"" approach and the ""gain-only"" approach\nto gain operator approximation. Numerical simulations illustrate stabilization\nand demonstrate speedup by three orders of magnitude over traditional PDE\ngain-scheduling. Code (Github) for the numerical implementation is published to\nenable exploration.\n', ""  The recently introduced DeepONet operator-learning framework for PDE control\nis extended from the results for basic hyperbolic and parabolic PDEs to an\nadvanced hyperbolic class that involves delays on both the state and the system\noutput or input. The PDE backstepping design produces gain functions that are\noutputs of a nonlinear operator, mapping functions on a spatial domain into\nfunctions on a spatial domain, and where this gain-generating operator's inputs\nare the PDE's coefficients. The operator is approximated with a DeepONet neural\nnetwork to a degree of accuracy that is provably arbitrarily tight. Once we\nproduce this approximation-theoretic result in infinite dimension, with it we\nestablish stability in closed loop under feedback that employs approximate\ngains. In addition to supplying such results under full-state feedback, we also\ndevelop DeepONet-approximated observers and output-feedback laws and prove\ntheir own stabilizing properties under neural operator approximations. With\nnumerical simulations we illustrate the theoretical results and quantify the\nnumerical effort savings, which are of two orders of magnitude, thanks to\nreplacing the numerical PDE solving with the DeepONet.\n"", ""  To stabilize PDEs, feedback controllers require gain kernel functions, which\nare themselves governed by PDEs. Furthermore, these gain-kernel PDEs depend on\nthe PDE plants' functional coefficients. The functional coefficients in PDE\nplants are often unknown. This requires an adaptive approach to PDE control,\ni.e., an estimation of the plant coefficients conducted concurrently with\ncontrol, where a separate PDE for the gain kernel must be solved at each\ntimestep upon the update in the plant coefficient function estimate. Solving a\nPDE at each timestep is computationally expensive and a barrier to the\nimplementation of real-time adaptive control of PDEs. Recently, results in\nneural operator (NO) approximations of functional mappings have been introduced\ninto PDE control, for replacing the computation of the gain kernel with a\nneural network that is trained, once offline, and reused in real-time for rapid\nsolution of the PDEs. In this paper, we present the first result on applying\nNOs in adaptive PDE control, presented for a benchmark 1-D hyperbolic PDE with\nrecirculation. We establish global stabilization via Lyapunov analysis, in the\nplant and parameter error states, and also present an alternative approach, via\npassive identifiers, which avoids the strong assumptions on kernel\ndifferentiability. We then present numerical simulations demonstrating\nstability and observe speedups up to three orders of magnitude, highlighting\nthe real-time efficacy of neural operators in adaptive control. Our code\n(Github) is made publicly available for future researchers.\n""] , ['  We propose a novel, brain-inspired deep neural network model known as the\nDeep Oscillatory Neural Network (DONN). Deep neural networks like the Recurrent\nNeural Networks indeed possess sequence processing capabilities but the\ninternal states of the network are not designed to exhibit brain-like\noscillatory activity. With this motivation, the DONN is designed to have\noscillatory internal dynamics. Neurons of the DONN are either nonlinear neural\noscillators or traditional neurons with sigmoidal or ReLU activation. The\nneural oscillator used in the model is the Hopf oscillator, with the dynamics\ndescribed in the complex domain. Input can be presented to the neural\noscillator in three possible modes. The sigmoid and ReLU neurons also use\ncomplex-valued extensions. All the weight stages are also complex-valued.\nTraining follows the general principle of weight change by minimizing the\noutput error and therefore has an overall resemblance to complex\nbackpropagation. A generalization of DONN to convolutional networks known as\nthe Oscillatory Convolutional Neural Network is also proposed. The two proposed\noscillatory networks are applied to a variety of benchmark problems in signal\nand image/video processing. The performance of the proposed models is either\ncomparable or superior to published results on the same data sets.\n', '  Residual connections have been proposed as an architecture-based inductive\nbias to mitigate the problem of exploding and vanishing gradients and increased\ntask performance in both feed-forward and recurrent networks (RNNs) when\ntrained with the backpropagation algorithm. Yet, little is known about how\nresidual connections in RNNs influence their dynamics and fading memory\nproperties. Here, we introduce weakly coupled residual recurrent networks\n(WCRNNs) in which residual connections result in well-defined Lyapunov\nexponents and allow for studying properties of fading memory. We investigate\nhow the residual connections of WCRNNs influence their performance, network\ndynamics, and memory properties on a set of benchmark tasks. We show that\nseveral distinct forms of residual connections yield effective inductive biases\nthat result in increased network expressivity. In particular, those are\nresidual connections that (i) result in network dynamics at the proximity of\nthe edge of chaos, (ii) allow networks to capitalize on characteristic spectral\nproperties of the data, and (iii) result in heterogeneous memory properties. In\naddition, we demonstrate how our results can be extended to non-linear\nresiduals and introduce a weakly coupled residual initialization scheme that\ncan be used for Elman RNNs.\n', '  We analyze recurrent neural networks trained with gradient descent in the\nsupervised learning setting for dynamical systems, and prove that gradient\ndescent can achieve optimality \\emph{without} massive overparameterization. Our\nin-depth nonasymptotic analysis (i) provides sharp bounds on the network size\n$m$ and iteration complexity $\\tau$ in terms of the sequence length $T$, sample\nsize $n$ and ambient dimension $d$, and (ii) identifies the significant impact\nof long-term dependencies in the dynamical system on the convergence and\nnetwork width bounds characterized by a cutoff point that depends on the\nLipschitz continuity of the activation function. Remarkably, this analysis\nreveals that an appropriately-initialized recurrent neural network trained with\n$n$ samples can achieve optimality with a network size $m$ that scales only\nlogarithmically with $n$. This sharply contrasts with the prior works that\nrequire high-order polynomial dependency of $m$ on $n$ to establish strong\nregularity conditions. Our results are based on an explicit characterization of\nthe class of dynamical systems that can be approximated and learned by\nrecurrent neural networks via norm-constrained transportation mappings, and\nestablishing local smoothness properties of the hidden state with respect to\nthe learnable parameters.\n']",Neural Networks for Modeling and Control of Dynamical Systems,"""Neural Operators for Dynamical Systems"""
