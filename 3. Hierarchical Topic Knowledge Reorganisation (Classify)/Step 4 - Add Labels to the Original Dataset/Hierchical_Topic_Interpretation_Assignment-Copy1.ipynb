{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "429d7569-688e-4513-bc81-2bbecb5fa5ca",
   "metadata": {},
   "source": [
    "### Overview\n",
    "This file focuses on organizing and generating higher-level topic labels from a set of subtopics (topic clsusters) using hierarchical clustering and natural language processing (NLP). The main steps involve embedding subtopic labels, clustering these embeddings, and generating human-readable topic labels for higher topic interpretation.\n",
    "\n",
    "### Main Components\n",
    "1. **Data Loading and Preparation**:\n",
    "   - The `load_and_merge_datasets` function loads multiple datasets and merges them into a single DataFrame, filtering out any records without a valid topic.\n",
    "\n",
    "2. **Embeddings**:\n",
    "   - The notebook utilizes the `SentenceTransformer` model to encode subtopics into vector representations (embeddings). This is handled by the `compute_embeddings` function.\n",
    "\n",
    "3. **Clustering**:\n",
    "   - **Hierarchical Clustering**:\n",
    "     - The `perform_clustering` function performs hierarchical clustering on the topic embeddings using the Ward method. This results in a dendrogram that visualizes the relationships between subtopics.\n",
    "   - **Cluster Assignment**:\n",
    "     - The `assign_cluster_labels` function assigns each subtopic to a cluster based on a specified cut height in the dendrogram.\n",
    "\n",
    "4. **Higher-Level Topic Generation**:\n",
    "   - **Grouping Subtopics**:\n",
    "     - `group_subtopics_by_cluster` aggregates subtopics within each cluster, preparing the data for generating higher-level topic labels.\n",
    "   - **Label Generation with Together AI**:\n",
    "     - The `generate_topic_label_together` function leverages a Together AI LLM to create concise and human-readable labels that represent each cluster.\n",
    "   - **Finding Representative Documents**:\n",
    "     - `find_representative_document` identifies a representative document for each cluster by finding the embedding that is most similar to the cluster centroid.\n",
    "\n",
    "5. **Final Processing**:\n",
    "   - The `process_clusters` function processes each cluster, generating a higher-level topic label and selecting a representative document for it.\n",
    "   - The final results are saved to a CSV file using the `save_results` function.\n",
    "\n",
    "6. **Main Workflow**:\n",
    "   - The `main` function orchestrates the entire workflow, starting from initializing the API client and loading data to clustering, topic label generation, and saving results.\n",
    "\n",
    "### Additional Details\n",
    "- The notebook uses the `SentenceTransformer` model (`all-MiniLM-L6-v2`) for generating embeddings.\n",
    "- It performs hierarchical clustering using the Ward method and determines cluster labels based on a specified cut height in the dendrogram. (baed on heuristics, 1.2 is chosen)\n",
    "- The Together AI LLM is used to generate human-readable, higher-level topic labels.\n",
    "\n",
    "This process aids in transforming subtopics into broader, more interpretable categories, making it easier to understand the content and structure of large datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2f102f-0797-4a65-805f-dc0f5abdb389",
   "metadata": {},
   "source": [
    "1. **Data Loading and Preparation**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a1a258-b2b4-46ec-913c-34f257308663",
   "metadata": {},
   "source": [
    "**First Step - Second Level Topic Knowledge**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1228b7c0-1113-40dd-b1cb-b232b5a4c56f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Aspect1</th>\n",
       "      <th>Aspect2</th>\n",
       "      <th>Representative_Docs</th>\n",
       "      <th>Human_Readable_Topic</th>\n",
       "      <th>Second_Level_Topic_Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>20415</td>\n",
       "      <td>-1_learning_knowledge_trained_tasks</td>\n",
       "      <td>['learning', 'knowledge', 'trained', 'tasks', ...</td>\n",
       "      <td>['data', 'models', 'learning', 'model', 'langu...</td>\n",
       "      <td>['knowledge', 'trained', 'tasks', 'ai', 'model...</td>\n",
       "      <td>[\"  In meta reinforcement learning (meta RL), ...</td>\n",
       "      <td>Artificial Intelligence and Machine Learning M...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0_molecular_molecule_molecules_ligands</td>\n",
       "      <td>['molecular', 'molecule', 'molecules', 'ligand...</td>\n",
       "      <td>['molecular', 'protein', 'drug', 'molecules', ...</td>\n",
       "      <td>['molecular', 'ligands', 'modeling', 'discover...</td>\n",
       "      <td>['  Generating molecules that bind to specific...</td>\n",
       "      <td>Molecular Generation and Modeling for Drug Dis...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>622</td>\n",
       "      <td>1_recommender_recommenders_personalized_recomm...</td>\n",
       "      <td>['recommender', 'recommenders', 'personalized'...</td>\n",
       "      <td>['recommendation', 'recommender', 'item', 'use...</td>\n",
       "      <td>['recommenders', 'personalized', 'factorizatio...</td>\n",
       "      <td>['  Contemporary recommender systems predomina...</td>\n",
       "      <td>Personalized Recommendation Systems</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>602</td>\n",
       "      <td>2_nlp_text_annotated_clinical</td>\n",
       "      <td>['nlp', 'text', 'annotated', 'clinical', 'medi...</td>\n",
       "      <td>['medical', 'clinical', 'biomedical', 'patient...</td>\n",
       "      <td>['nlp', 'text', 'annotated', 'hospital', 'retr...</td>\n",
       "      <td>['  In studies that rely on data from electron...</td>\n",
       "      <td>Natural Language Processing in Clinical Text A...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>583</td>\n",
       "      <td>3_retrieval_search_relevance_recall</td>\n",
       "      <td>['retrieval', 'search', 'relevance', 'recall',...</td>\n",
       "      <td>['retrieval', 'documents', 'query', 'document'...</td>\n",
       "      <td>['retrieval', 'recall', 'semantic', 'retriever...</td>\n",
       "      <td>['  Large Language Models (LLMs) excel in vari...</td>\n",
       "      <td>Improving Document Retrieval for Large Languag...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>517</td>\n",
       "      <td>516</td>\n",
       "      <td>10</td>\n",
       "      <td>516_nationalities_cultural_cultures_language</td>\n",
       "      <td>['nationalities', 'cultural', 'cultures', 'lan...</td>\n",
       "      <td>['cultural', 'debiasing', 'nationality', 'coun...</td>\n",
       "      <td>['nationalities', 'culturally', 'discourses', ...</td>\n",
       "      <td>[\"  Large Language Models (LLMs) attempt to im...</td>\n",
       "      <td>Cultural Sensitivity in Large Language Models</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>518</td>\n",
       "      <td>517</td>\n",
       "      <td>10</td>\n",
       "      <td>517_scheduling_prediction_predictions_queueing</td>\n",
       "      <td>['scheduling', 'prediction', 'predictions', 'q...</td>\n",
       "      <td>['predictions', 'jobs', 'skip', 'queues', 'lis...</td>\n",
       "      <td>['scheduling', 'predictions', 'queueing', 'alg...</td>\n",
       "      <td>[\"  Online decision-makers often obtain predic...</td>\n",
       "      <td>Scheduling and Queueing with Predictions</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>519</td>\n",
       "      <td>518</td>\n",
       "      <td>10</td>\n",
       "      <td>518_programming_solvers_solver_optimization</td>\n",
       "      <td>['programming', 'solvers', 'solver', 'optimiza...</td>\n",
       "      <td>['problems', 'programming', 'program', 'mathem...</td>\n",
       "      <td>['programming', 'solvers', 'optimization', 'op...</td>\n",
       "      <td>['  Optimization problems are pervasive in sec...</td>\n",
       "      <td>Optimization Problem Solving with Large Langua...</td>\n",
       "      <td>Optimization and Efficiency of Large Language ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>520</td>\n",
       "      <td>519</td>\n",
       "      <td>10</td>\n",
       "      <td>519_modeling_industrial_deep_flow</td>\n",
       "      <td>['modeling', 'industrial', 'deep', 'flow', 'st...</td>\n",
       "      <td>['soft', 'wells', 'sensor', 'sensing', 'sensor...</td>\n",
       "      <td>['modeling', 'industrial', 'flow', 'stochastic...</td>\n",
       "      <td>['  The modeling of multistage manufacturing s...</td>\n",
       "      <td>Industrial Process Modeling and Sensing with D...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>521</td>\n",
       "      <td>520</td>\n",
       "      <td>10</td>\n",
       "      <td>520_fingerprint_fingerprints_shoeprints_palmpr...</td>\n",
       "      <td>['fingerprint', 'fingerprints', 'shoeprints', ...</td>\n",
       "      <td>['fingerprint', 'shoeprint', 'morphing', 'prin...</td>\n",
       "      <td>['fingerprints', 'shoeprints', 'biometric', 'g...</td>\n",
       "      <td>['  In fingerprint matching, fixed-length desc...</td>\n",
       "      <td>Biometric Identification and Fingerprint Recog...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>523 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  Topic  Count  \\\n",
       "0             0     -1  20415   \n",
       "1             1      0   1000   \n",
       "2             2      1    622   \n",
       "3             3      2    602   \n",
       "4             4      3    583   \n",
       "..          ...    ...    ...   \n",
       "518         517    516     10   \n",
       "519         518    517     10   \n",
       "520         519    518     10   \n",
       "521         520    519     10   \n",
       "522         521    520     10   \n",
       "\n",
       "                                                  Name  \\\n",
       "0                  -1_learning_knowledge_trained_tasks   \n",
       "1               0_molecular_molecule_molecules_ligands   \n",
       "2    1_recommender_recommenders_personalized_recomm...   \n",
       "3                        2_nlp_text_annotated_clinical   \n",
       "4                  3_retrieval_search_relevance_recall   \n",
       "..                                                 ...   \n",
       "518       516_nationalities_cultural_cultures_language   \n",
       "519     517_scheduling_prediction_predictions_queueing   \n",
       "520        518_programming_solvers_solver_optimization   \n",
       "521                  519_modeling_industrial_deep_flow   \n",
       "522  520_fingerprint_fingerprints_shoeprints_palmpr...   \n",
       "\n",
       "                                        Representation  \\\n",
       "0    ['learning', 'knowledge', 'trained', 'tasks', ...   \n",
       "1    ['molecular', 'molecule', 'molecules', 'ligand...   \n",
       "2    ['recommender', 'recommenders', 'personalized'...   \n",
       "3    ['nlp', 'text', 'annotated', 'clinical', 'medi...   \n",
       "4    ['retrieval', 'search', 'relevance', 'recall',...   \n",
       "..                                                 ...   \n",
       "518  ['nationalities', 'cultural', 'cultures', 'lan...   \n",
       "519  ['scheduling', 'prediction', 'predictions', 'q...   \n",
       "520  ['programming', 'solvers', 'solver', 'optimiza...   \n",
       "521  ['modeling', 'industrial', 'deep', 'flow', 'st...   \n",
       "522  ['fingerprint', 'fingerprints', 'shoeprints', ...   \n",
       "\n",
       "                                               Aspect1  \\\n",
       "0    ['data', 'models', 'learning', 'model', 'langu...   \n",
       "1    ['molecular', 'protein', 'drug', 'molecules', ...   \n",
       "2    ['recommendation', 'recommender', 'item', 'use...   \n",
       "3    ['medical', 'clinical', 'biomedical', 'patient...   \n",
       "4    ['retrieval', 'documents', 'query', 'document'...   \n",
       "..                                                 ...   \n",
       "518  ['cultural', 'debiasing', 'nationality', 'coun...   \n",
       "519  ['predictions', 'jobs', 'skip', 'queues', 'lis...   \n",
       "520  ['problems', 'programming', 'program', 'mathem...   \n",
       "521  ['soft', 'wells', 'sensor', 'sensing', 'sensor...   \n",
       "522  ['fingerprint', 'shoeprint', 'morphing', 'prin...   \n",
       "\n",
       "                                               Aspect2  \\\n",
       "0    ['knowledge', 'trained', 'tasks', 'ai', 'model...   \n",
       "1    ['molecular', 'ligands', 'modeling', 'discover...   \n",
       "2    ['recommenders', 'personalized', 'factorizatio...   \n",
       "3    ['nlp', 'text', 'annotated', 'hospital', 'retr...   \n",
       "4    ['retrieval', 'recall', 'semantic', 'retriever...   \n",
       "..                                                 ...   \n",
       "518  ['nationalities', 'culturally', 'discourses', ...   \n",
       "519  ['scheduling', 'predictions', 'queueing', 'alg...   \n",
       "520  ['programming', 'solvers', 'optimization', 'op...   \n",
       "521  ['modeling', 'industrial', 'flow', 'stochastic...   \n",
       "522  ['fingerprints', 'shoeprints', 'biometric', 'g...   \n",
       "\n",
       "                                   Representative_Docs  \\\n",
       "0    [\"  In meta reinforcement learning (meta RL), ...   \n",
       "1    ['  Generating molecules that bind to specific...   \n",
       "2    ['  Contemporary recommender systems predomina...   \n",
       "3    ['  In studies that rely on data from electron...   \n",
       "4    ['  Large Language Models (LLMs) excel in vari...   \n",
       "..                                                 ...   \n",
       "518  [\"  Large Language Models (LLMs) attempt to im...   \n",
       "519  [\"  Online decision-makers often obtain predic...   \n",
       "520  ['  Optimization problems are pervasive in sec...   \n",
       "521  ['  The modeling of multistage manufacturing s...   \n",
       "522  ['  In fingerprint matching, fixed-length desc...   \n",
       "\n",
       "                                  Human_Readable_Topic  \\\n",
       "0    Artificial Intelligence and Machine Learning M...   \n",
       "1    Molecular Generation and Modeling for Drug Dis...   \n",
       "2                  Personalized Recommendation Systems   \n",
       "3    Natural Language Processing in Clinical Text A...   \n",
       "4    Improving Document Retrieval for Large Languag...   \n",
       "..                                                 ...   \n",
       "518      Cultural Sensitivity in Large Language Models   \n",
       "519           Scheduling and Queueing with Predictions   \n",
       "520  Optimization Problem Solving with Large Langua...   \n",
       "521  Industrial Process Modeling and Sensing with D...   \n",
       "522  Biometric Identification and Fingerprint Recog...   \n",
       "\n",
       "                              Second_Level_Topic_Label  \n",
       "0                                                  NaN  \n",
       "1                                                  NaN  \n",
       "2                                                  NaN  \n",
       "3                                                  NaN  \n",
       "4                                                  NaN  \n",
       "..                                                 ...  \n",
       "518                                                NaN  \n",
       "519                                                NaN  \n",
       "520  Optimization and Efficiency of Large Language ...  \n",
       "521                                                NaN  \n",
       "522                                                NaN  \n",
       "\n",
       "[523 rows x 10 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the two datasets\n",
    "first_step_df = pd.read_csv('First_Step_Topic_Clusters_with_human_readable_label.csv')\n",
    "higher_level_topics_df = pd.read_csv('Hierchical_Topics_Second_Level-Topics (1).csv')\n",
    "\n",
    "higher_level_topics_df.rename(columns={'Higher_Topic_Label': 'Second_Level_Topic_Label'}, inplace=True)\n",
    "higher_level_topics_df\n",
    "\n",
    "# Function to clean and split the topics\n",
    "def clean_and_split_topics(topic_str):\n",
    "    # Remove quotes and split by commas, then strip any extra spaces\n",
    "    topics = [topic.strip().replace('\"', '') for topic in topic_str.split(',')]\n",
    "    return topics\n",
    "\n",
    "\n",
    "# Apply the cleaning function to the 'Human_Readable_Topic' column\n",
    "first_step_df['Human_Readable_Topic'] = first_step_df['Human_Readable_Topic'].str.replace('\"', '', regex=False)\n",
    "higher_level_topics_df['Human_Readable_Topic'] = higher_level_topics_df['Human_Readable_Topic'].apply(clean_and_split_topics)\n",
    "\n",
    "# Explode the 'Human_Readable_Topic' to create a row for each topic\n",
    "higher_level_topics_df = higher_level_topics_df.explode('Human_Readable_Topic')\n",
    "\n",
    "first_step_df\n",
    "higher_level_topics_df\n",
    "# Merge the two datasets on the cleaned 'Human_Readable_Topic' column\n",
    "merged_df = pd.merge(first_step_df, higher_level_topics_df[['Human_Readable_Topic', 'Second_Level_Topic_Label']], \n",
    "                     on='Human_Readable_Topic', how='left')\n",
    "merged_df\n",
    "# # Save the merged dataframe to a new CSV file\n",
    "# merged_df.to_csv('BERTopic_First_Step_Second_Level_Topic_Knowledge.csv', index=False)\n",
    "\n",
    "# print(\"Merging completed and the result is saved as 'BERTopic_First_Step_Second_Level_Topic_Knowledge.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28af6cf-ef49-413a-ae19-7be8a15a5dc6",
   "metadata": {},
   "source": [
    "<!-- This is the code, if the regular String based matching doesn't work!!!! -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85e7a1e4-db7a-40a8-95f1-4156d5988858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging completed and the result is saved as 'BERTopic_First_Step_Second_Level_Topic_Knowledge.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the two datasets\n",
    "first_step_df = pd.read_csv('First_Step_Topic_Clusters_with_human_readable_label.csv')\n",
    "higher_level_topics_df = pd.read_csv('Hierchical_Topics_Second_Level-Topics (1).csv')\n",
    "\n",
    "# Rename column for consistency\n",
    "higher_level_topics_df.rename(columns={'Higher_Topic_Label': 'Second_Level_Topic_Label'}, inplace=True)\n",
    "\n",
    "# Function to clean and split the topics\n",
    "def clean_and_split_topics(topic_str):\n",
    "    # Remove quotes and split by commas, then strip any extra spaces\n",
    "    topics = [topic.strip().replace('\"', '') for topic in topic_str.split(',')]\n",
    "    return topics\n",
    "\n",
    "# Apply the cleaning function to the 'Human_Readable_Topic' column\n",
    "first_step_df['Human_Readable_Topic'] = first_step_df['Human_Readable_Topic'].str.replace('\"', '', regex=False)\n",
    "higher_level_topics_df['Human_Readable_Topic'] = higher_level_topics_df['Human_Readable_Topic'].apply(clean_and_split_topics)\n",
    "\n",
    "# Explode the 'Human_Readable_Topic' to create a row for each topic\n",
    "higher_level_topics_df = higher_level_topics_df.explode('Human_Readable_Topic')\n",
    "\n",
    "# Combine all human-readable topics for vectorization\n",
    "all_topics = pd.concat([first_step_df['Human_Readable_Topic'], higher_level_topics_df['Human_Readable_Topic']])\n",
    "\n",
    "# Vectorize the topics using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(all_topics)\n",
    "\n",
    "# Split the TF-IDF matrix back into the two parts\n",
    "first_step_tfidf = tfidf_matrix[:len(first_step_df)]\n",
    "higher_level_tfidf = tfidf_matrix[len(first_step_df):]\n",
    "\n",
    "# Calculate cosine similarity between each topic in first_step_df and all topics in higher_level_topics_df\n",
    "similarity_matrix = cosine_similarity(first_step_tfidf, higher_level_tfidf)\n",
    "\n",
    "# Find the index of the highest similarity for each topic\n",
    "best_match_indices = similarity_matrix.argmax(axis=1)\n",
    "\n",
    "# Get the best matching topics and their corresponding labels\n",
    "first_step_df['Matched_Topic'] = higher_level_topics_df.iloc[best_match_indices]['Human_Readable_Topic'].values\n",
    "first_step_df['Second_Level_Topic_Label'] = higher_level_topics_df.iloc[best_match_indices]['Second_Level_Topic_Label'].values\n",
    "\n",
    "# Save the merged dataframe to a new CSV file (if needed)\n",
    "first_step_df.to_csv('BERTopic_First_Step_Second_Level_Topic_Knowledge.csv', index=False)\n",
    "print(\"Merging completed and the result is saved as 'BERTopic_First_Step_Second_Level_Topic_Knowledge.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e841a16-0118-4f63-a325-3e6187bcfea7",
   "metadata": {},
   "source": [
    "First Step - Third Level Topic Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17b45340-29c6-42d8-b64b-eb7a7da23528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging completed and the result is saved as BERTopic_First_Step_Second_Level_Topic_Knowledge.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "first_step_df = pd.read_csv('BERTopic_First_Step_Second_Level_Topic_Knowledge.csv')\n",
    "highest_level_topics_df = pd.read_csv('Hierchical_Topics_Third_Level-Topics.csv')\n",
    "\n",
    "# Function to clean and split the 'Higher_Topic_Label' column\n",
    "def clean_and_split_labels(label_str):\n",
    "    # Split by commas and strip any extra spaces\n",
    "    labels = [label.strip() for label in label_str.split(';')]\n",
    "    return labels\n",
    "\n",
    "# Apply the cleaning and splitting function to the 'Higher_Topic_Label' column\n",
    "highest_level_topics_df['Second_Level_Topic_Label'] = highest_level_topics_df['Second_Level_Topic_Label'].apply(clean_and_split_labels)\n",
    "\n",
    "# Explode the 'Higher_Topic_Label' to create a row for each label\n",
    "highest_level_topics_df = highest_level_topics_df.explode('Second_Level_Topic_Label')\n",
    "\n",
    "# Merge the first step dataset with the highest level topics based on 'Higher_Topic_Label'\n",
    "final_merged_df = pd.merge(first_step_df, highest_level_topics_df[['Second_Level_Topic_Label', 'Highest_Topic_Label']], \n",
    "                           on='Second_Level_Topic_Label', how='left')\n",
    "\n",
    "# Save the final merged dataframe to a new CSV file\n",
    "final_merged_df.to_csv('BERTopic_First_Step_Third_Level_Topic_Knowledge.csv', index=False)\n",
    "\n",
    "print(\"Merging completed and the result is saved as BERTopic_First_Step_Second_Level_Topic_Knowledge.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1b5914-fa10-4ac3-85af-da24112eb13b",
   "metadata": {},
   "source": [
    "First Step - Final Topic Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e11011-d3e9-49c6-ad4a-dc53a7585ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "first_step_final_df = pd.read_csv('First_Step_BERTopic_topic_info_labelled_with_Highest_Topic_Label_final.csv')\n",
    "final_highest_level_topics_df = pd.read_csv('final_highest_level_topic_labels_with_representatives.csv')\n",
    "\n",
    "# Function to split the 'Highest_Topic_Label' based on semicolons or commas, handle NaN values\n",
    "def split_labels(label_str):\n",
    "    if isinstance(label_str, str):  # Check if the label_str is a string\n",
    "        # Split by semicolon or comma and strip any extra spaces\n",
    "        labels = [label.strip() for label in label_str.split(',')]\n",
    "    else:\n",
    "        labels = [None]  # Return a list with None if label_str is not a string\n",
    "    return labels\n",
    "\n",
    "# Apply the splitting function to the 'Highest_Topic_Label' in the first dataset\n",
    "first_step_final_df['Highest_Topic_Label_Split'] = first_step_final_df['Highest_Topic_Label'].apply(split_labels)\n",
    "\n",
    "# Explode the first dataframe to have one row per label\n",
    "first_step_final_df = first_step_final_df.explode('Highest_Topic_Label_Split')\n",
    "\n",
    "# Explode the final_highest_level_topics_df based on 'Highest_Topic_Label' for proper matching\n",
    "final_highest_level_topics_df['Highest_Topic_Label_Split'] = final_highest_level_topics_df['Highest_Topic_Label'].apply(split_labels)\n",
    "final_highest_level_topics_df = final_highest_level_topics_df.explode('Highest_Topic_Label_Split')\n",
    "\n",
    "# Merge the exploded first dataset with the final highest level topics on 'Highest_Topic_Label_Split'\n",
    "final_merged_df = pd.merge(first_step_final_df, final_highest_level_topics_df[['Highest_Topic_Label_Split', 'Final_Label']], \n",
    "                           left_on='Highest_Topic_Label_Split', right_on='Highest_Topic_Label_Split', how='left')\n",
    "\n",
    "# Drop the auxiliary column used for merging\n",
    "final_merged_df.drop(columns=['Highest_Topic_Label_Split'], inplace=True)\n",
    "\n",
    "# Save the final merged dataframe to a new CSV file\n",
    "final_merged_df.to_csv('First_Step_BERTopic_topic_info_labelled_with_Final_Label_Merged.csv', index=False)\n",
    "\n",
    "print(\"Merging completed and the result is saved as 'First_Step_BERTopic_topic_info_labelled_with_Final_Label_Merged.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ec9ac9-a6c5-488b-b3ed-b22ea8e8e08c",
   "metadata": {},
   "source": [
    "**Second Step - Topic Knowledge**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f032a6b1-8796-4363-8527-5d703c169dc5",
   "metadata": {},
   "source": [
    "Second Step Hierchical Topic Knowledge Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc5576f7-9135-4e4f-992b-2a91e428e48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging completed and the result is saved as 'BERTopic_Second_Step_Second_Level_Topic_Knowledge.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the two datasets\n",
    "first_step_df = pd.read_csv('Second_Step_Topic_Clusters_with_human_readable_label.csv')\n",
    "higher_level_topics_df = pd.read_csv('Hierchical_Topics_Second_Level-Topics.csv')\n",
    "\n",
    "# Function to clean and split the topics\n",
    "def clean_and_split_topics(topic_str):\n",
    "    # Remove quotes and split by commas, then strip any extra spaces\n",
    "    topics = [topic.strip().replace('\"', '') for topic in topic_str.split(',')]\n",
    "    return topics\n",
    "\n",
    "# Apply the cleaning function to the 'Human_Readable_Topic' column\n",
    "first_step_df['Human_Readable_Topic'] = first_step_df['Human_Readable_Topic'].str.replace('\"', '', regex=False)\n",
    "higher_level_topics_df['Human_Readable_Topic'] = higher_level_topics_df['Human_Readable_Topic'].apply(clean_and_split_topics)\n",
    "\n",
    "# Explode the 'Human_Readable_Topic' to create a row for each topic\n",
    "higher_level_topics_df = higher_level_topics_df.explode('Human_Readable_Topic')\n",
    "\n",
    "# Merge the two datasets on the cleaned 'Human_Readable_Topic' column\n",
    "merged_df = pd.merge(first_step_df, higher_level_topics_df[['Human_Readable_Topic', 'Second_Level_Topic_Label']], \n",
    "                     on='Human_Readable_Topic', how='left')\n",
    "\n",
    "# Save the merged dataframe to a new CSV file\n",
    "merged_df.to_csv('BERTopic_Second_Step_Second_Level_Topic_Knowledge.csv', index=False)\n",
    "\n",
    "print(\"Merging completed and the result is saved as 'BERTopic_Second_Step_Second_Level_Topic_Knowledge.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2774f78d-dae4-496b-879c-e5cdb3f4344c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on float64 and object columns for key 'Second_Level_Topic_Label'. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m highest_level_topics_df \u001b[38;5;241m=\u001b[39m highest_level_topics_df\u001b[38;5;241m.\u001b[39mexplode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSecond_Level_Topic_Label\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Merge the first step dataset with the highest level topics based on 'Higher_Topic_Label'\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m final_merged_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_step_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhighest_level_topics_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSecond_Level_Topic_Label\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHighest_Topic_Label\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSecond_Level_Topic_Label\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Save the final merged dataframe to a new CSV file\u001b[39;00m\n\u001b[1;32m     24\u001b[0m final_merged_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBERTopic_Second_Step_Third_Level_Topic_Knowledge.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/reshape/merge.py:170\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _cross_merge(\n\u001b[1;32m    156\u001b[0m         left_df,\n\u001b[1;32m    157\u001b[0m         right_df,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m    168\u001b[0m     )\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/reshape/merge.py:807\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_tolerance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys)\n\u001b[1;32m    805\u001b[0m \u001b[38;5;66;03m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;66;03m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[0;32m--> 807\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_coerce_merge_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;66;03m# If argument passed to validate,\u001b[39;00m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;66;03m# check if columns specified as unique\u001b[39;00m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;66;03m# are in fact unique.\u001b[39;00m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/reshape/merge.py:1508\u001b[0m, in \u001b[0;36m_MergeOperation._maybe_coerce_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1502\u001b[0m     \u001b[38;5;66;03m# unless we are merging non-string-like with string-like\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m   1504\u001b[0m         inferred_left \u001b[38;5;129;01min\u001b[39;00m string_types \u001b[38;5;129;01mand\u001b[39;00m inferred_right \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string_types\n\u001b[1;32m   1505\u001b[0m     ) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1506\u001b[0m         inferred_right \u001b[38;5;129;01min\u001b[39;00m string_types \u001b[38;5;129;01mand\u001b[39;00m inferred_left \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string_types\n\u001b[1;32m   1507\u001b[0m     ):\n\u001b[0;32m-> 1508\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;66;03m# datetimelikes must match exactly\u001b[39;00m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m needs_i8_conversion(lk\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m needs_i8_conversion(rk\u001b[38;5;241m.\u001b[39mdtype):\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to merge on float64 and object columns for key 'Second_Level_Topic_Label'. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "first_step_df = pd.read_csv('BERTopic_Second_Step_Second_Level_Topic_Knowledge.csv')\n",
    "highest_level_topics_df = pd.read_csv('Hierchical_Topics_Third_Level-Topics.csv')\n",
    "\n",
    "# Function to clean and split the 'Higher_Topic_Label' column\n",
    "def clean_and_split_labels(label_str):\n",
    "    # Split by commas and strip any extra spaces\n",
    "    labels = [label.strip() for label in label_str.split(';')]\n",
    "    return labels\n",
    "\n",
    "# Apply the cleaning and splitting function to the 'Higher_Topic_Label' column\n",
    "highest_level_topics_df['Second_Level_Topic_Label'] = highest_level_topics_df['Second_Level_Topic_Label'].apply(clean_and_split_labels)\n",
    "\n",
    "# Explode the 'Higher_Topic_Label' to create a row for each label\n",
    "highest_level_topics_df = highest_level_topics_df.explode('Second_Level_Topic_Label')\n",
    "\n",
    "# Merge the first step dataset with the highest level topics based on 'Higher_Topic_Label'\n",
    "final_merged_df = pd.merge(first_step_df, highest_level_topics_df[['Second_Level_Topic_Label', 'Highest_Topic_Label']], \n",
    "                           on='Second_Level_Topic_Label', how='left')\n",
    "\n",
    "# Save the final merged dataframe to a new CSV file\n",
    "final_merged_df.to_csv('BERTopic_Second_Step_Third_Level_Topic_Knowledge.csv', index=False)\n",
    "\n",
    "print(\"Merging completed and the result is saved as BERTopic_Second_Step_Third_Level_Topic_Knowledge.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56981ec7-2649-4d31-9b15-8f6f8aa37ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "first_step_final_df = pd.read_csv('BERTopic_Second_Step_Third_Level_Topic_Knowledge.csv')\n",
    "final_highest_level_topics_df = pd.read_csv('final_highest_level_topic_labels_with_representatives.csv')\n",
    "\n",
    "# Function to split the 'Highest_Topic_Label' based on semicolons or commas, handle NaN values\n",
    "def split_labels(label_str):\n",
    "    if isinstance(label_str, str):  # Check if the label_str is a string\n",
    "        # Split by semicolon or comma and strip any extra spaces\n",
    "        labels = [label.strip() for label in label_str.split(',')]\n",
    "    else:\n",
    "        labels = [None]  # Return a list with None if label_str is not a string\n",
    "    return labels\n",
    "\n",
    "# Apply the splitting function to the 'Highest_Topic_Label' in the first dataset\n",
    "first_step_final_df['Highest_Topic_Label_Split'] = first_step_final_df['Highest_Topic_Label'].apply(split_labels)\n",
    "\n",
    "# Explode the first dataframe to have one row per label\n",
    "first_step_final_df = first_step_final_df.explode('Highest_Topic_Label_Split')\n",
    "\n",
    "# Explode the final_highest_level_topics_df based on 'Highest_Topic_Label' for proper matching\n",
    "final_highest_level_topics_df['Highest_Topic_Label_Split'] = final_highest_level_topics_df['Highest_Topic_Label'].apply(split_labels)\n",
    "final_highest_level_topics_df = final_highest_level_topics_df.explode('Highest_Topic_Label_Split')\n",
    "\n",
    "# Merge the exploded first dataset with the final highest level topics on 'Highest_Topic_Label_Split'\n",
    "final_merged_df = pd.merge(first_step_final_df, final_highest_level_topics_df[['Highest_Topic_Label_Split', 'Final_Label']], \n",
    "                           left_on='Highest_Topic_Label_Split', right_on='Highest_Topic_Label_Split', how='left')\n",
    "\n",
    "# Drop the auxiliary column used for merging\n",
    "final_merged_df.drop(columns=['Highest_Topic_Label_Split'], inplace=True)\n",
    "\n",
    "# Save the final merged dataframe to a new CSV file\n",
    "final_merged_df.to_csv('Second_Step_BERTopic_topic_info_labelled_with_Final_Label_Merged.csv', index=False)\n",
    "\n",
    "print(\"Merging completed and the result is saved as 'Second_Step_BERTopic_topic_info_labelled_with_Final_Label_Merged.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe61ca59-d49e-416b-b129-d4ac719d4056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging completed and the result is saved as 'Second_Step_BERTopic_topic_info_labelled_with_Higher_Topic_Label_cleaned.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the two datasets\n",
    "first_step_df = pd.read_csv('Second_step_clustering_results_labelled.csv')\n",
    "higher_level_topics_df = pd.read_csv('higherr_level_topic_labels_with_representatives.csv')\n",
    "\n",
    "# Function to clean and split the topics\n",
    "def clean_and_split_topics(topic_str):\n",
    "    # Remove quotes and split by commas, then strip any extra spaces\n",
    "    topics = [topic.strip().replace('\"', '') for topic in topic_str.split(',')]\n",
    "    return topics\n",
    "\n",
    "# Apply the cleaning function to the 'Human_Readable_Topic' column\n",
    "first_step_df['Human_Readable_Topic'] = first_step_df['Human_Readable_Topic'].str.replace('\"', '', regex=False)\n",
    "higher_level_topics_df['Human_Readable_Topic'] = higher_level_topics_df['Human_Readable_Topic'].apply(clean_and_split_topics)\n",
    "\n",
    "# Explode the 'Human_Readable_Topic' to create a row for each topic\n",
    "higher_level_topics_df = higher_level_topics_df.explode('Human_Readable_Topic')\n",
    "\n",
    "# Merge the two datasets on the cleaned 'Human_Readable_Topic' column\n",
    "merged_df = pd.merge(first_step_df, higher_level_topics_df[['Human_Readable_Topic', 'Higher_Topic_Label']], \n",
    "                     on='Human_Readable_Topic', how='left')\n",
    "\n",
    "# Save the merged dataframe to a new CSV file\n",
    "merged_df.to_csv('Second_Step_BERTopic_topic_info_labelled_with_Higher_Topic_Label_cleaned.csv', index=False)\n",
    "\n",
    "print(\"Merging completed and the result is saved as 'Second_Step_BERTopic_topic_info_labelled_with_Higher_Topic_Label_cleaned.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a42b90f-e8ff-4954-9f1d-af466bc5e492",
   "metadata": {},
   "outputs": [],
   "source": [
    "Assign the Highest Topic Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c94af861-cb97-4bec-876e-0d8853d44eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging completed and the result is saved as 'Second_Step_BERTopic_topic_info_labelled_with_Highest_Topic_Label_final.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "first_step_df = pd.read_csv('Second_Step_BERTopic_topic_info_labelled_with_Higher_Topic_Label_cleaned.csv')\n",
    "highest_level_topics_df = pd.read_csv('highest_level_topic_labels_with_representatives.csv')\n",
    "\n",
    "# Function to clean and split the 'Higher_Topic_Label' column\n",
    "def clean_and_split_labels(label_str):\n",
    "    # Split by commas and strip any extra spaces\n",
    "    labels = [label.strip() for label in label_str.split(';')]\n",
    "    return labels\n",
    "\n",
    "# Apply the cleaning and splitting function to the 'Higher_Topic_Label' column\n",
    "highest_level_topics_df['Higher_Topic_Label'] = highest_level_topics_df['Higher_Topic_Label'].apply(clean_and_split_labels)\n",
    "\n",
    "# Explode the 'Higher_Topic_Label' to create a row for each label\n",
    "highest_level_topics_df = highest_level_topics_df.explode('Higher_Topic_Label')\n",
    "\n",
    "# Merge the first step dataset with the highest level topics based on 'Higher_Topic_Label'\n",
    "final_merged_df = pd.merge(first_step_df, highest_level_topics_df[['Higher_Topic_Label', 'Highest_Topic_Label']], \n",
    "                           on='Higher_Topic_Label', how='left')\n",
    "\n",
    "# Save the final merged dataframe to a new CSV file\n",
    "final_merged_df.to_csv('Second_Step_BERTopic_topic_info_labelled_with_Highest_Topic_Label_final.csv', index=False)\n",
    "\n",
    "print(\"Merging completed and the result is saved as 'Second_Step_BERTopic_topic_info_labelled_with_Highest_Topic_Label_final.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f8d2e9-07db-4b5d-91b5-ec687f8eee80",
   "metadata": {},
   "source": [
    "Assign the Final Labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a934fabc-676f-4a2a-be1c-ecaa009131c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging completed and the result is saved as 'First_Step_BERTopic_topic_info_labelled_with_Final_Label_Merged.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "first_step_final_df = pd.read_csv('First_Step_BERTopic_topic_info_labelled_with_Highest_Topic_Label_final.csv')\n",
    "final_highest_level_topics_df = pd.read_csv('final_highest_level_topic_labels_with_representatives.csv')\n",
    "\n",
    "# Merge the first step dataset with the final highest level topics based on 'Highest_Topic_Label'\n",
    "final_merged_df = pd.merge(first_step_final_df, final_highest_level_topics_df[['Highest_Topic_Label', 'Final_Label']], \n",
    "                           on='Highest_Topic_Label', how='left')\n",
    "\n",
    "# Save the final merged dataframe to a new CSV file\n",
    "final_merged_df.to_csv('First_Step_BERTopic_topic_info_labelled_with_Final_Label_Merged.csv', index=False)\n",
    "\n",
    "print(\"Merging completed and the result is saved as 'First_Step_BERTopic_topic_info_labelled_with_Final_Label_Merged.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "693a6485-07a7-47af-b82c-95693bcdd4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging completed and the result is saved as 'First_Step_BERTopic_topic_info_labelled_with_Final_Label_Merged.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "first_step_final_df = pd.read_csv('First_Step_BERTopic_topic_info_labelled_with_Highest_Topic_Label_final.csv')\n",
    "final_highest_level_topics_df = pd.read_csv('final_highest_level_topic_labels_with_representatives.csv')\n",
    "\n",
    "# Function to split the 'Highest_Topic_Label' based on semicolons or commas, handle NaN values\n",
    "def split_labels(label_str):\n",
    "    if isinstance(label_str, str):  # Check if the label_str is a string\n",
    "        # Split by semicolon and strip any extra spaces\n",
    "        labels = [label.strip() for label in label_str.split(';')]\n",
    "    else:\n",
    "        labels = [None]  # Return a list with None if label_str is not a string\n",
    "    return labels\n",
    "\n",
    "# Apply the splitting function to the 'Highest_Topic_Label' in the first dataset\n",
    "first_step_final_df['Highest_Topic_Label_Split'] = first_step_final_df['Highest_Topic_Label'].apply(split_labels)\n",
    "\n",
    "# Explode the first dataframe to have one row per label\n",
    "first_step_final_df = first_step_final_df.explode('Highest_Topic_Label_Split')\n",
    "\n",
    "# Merge the exploded first dataset with the final highest level topics on 'Highest_Topic_Label_Split'\n",
    "final_merged_df = pd.merge(first_step_final_df, final_highest_level_topics_df[['Highest_Topic_Label', 'Final_Label']], \n",
    "                           left_on='Highest_Topic_Label_Split', right_on='Highest_Topic_Label', how='left')\n",
    "\n",
    "# Drop the auxiliary column used for merging\n",
    "final_merged_df.drop(columns=['Highest_Topic_Label_Split'], inplace=True)\n",
    "\n",
    "# Save the final merged dataframe to a new CSV file\n",
    "final_merged_df.to_csv('First_Step_BERTopic_topic_info_labelled_with_Final_Label_Merged.csv', index=False)\n",
    "\n",
    "print(\"Merging completed and the result is saved as 'First_Step_BERTopic_topic_info_labelled_with_Final_Label_Merged.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "777d2fa7-c3be-49b8-bac6-4ba017ed9358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging completed and the result is saved as 'First_Step_BERTopic_topic_info_labelled_with_Final_Label_Merged.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "first_step_final_df = pd.read_csv('First_Step_BERTopic_topic_info_labelled_with_Highest_Topic_Label_final.csv')\n",
    "final_highest_level_topics_df = pd.read_csv('final_highest_level_topic_labels_with_representatives.csv')\n",
    "\n",
    "# Function to split the 'Highest_Topic_Label' based on semicolons or commas, handle NaN values\n",
    "def split_labels(label_str):\n",
    "    if isinstance(label_str, str):  # Check if the label_str is a string\n",
    "        # Split by semicolon or comma and strip any extra spaces\n",
    "        labels = [label.strip() for label in label_str.split(',')]\n",
    "    else:\n",
    "        labels = [None]  # Return a list with None if label_str is not a string\n",
    "    return labels\n",
    "\n",
    "# Apply the splitting function to the 'Highest_Topic_Label' in the first dataset\n",
    "first_step_final_df['Highest_Topic_Label_Split'] = first_step_final_df['Highest_Topic_Label'].apply(split_labels)\n",
    "\n",
    "# Explode the first dataframe to have one row per label\n",
    "first_step_final_df = first_step_final_df.explode('Highest_Topic_Label_Split')\n",
    "\n",
    "# Explode the final_highest_level_topics_df based on 'Highest_Topic_Label' for proper matching\n",
    "final_highest_level_topics_df['Highest_Topic_Label_Split'] = final_highest_level_topics_df['Highest_Topic_Label'].apply(split_labels)\n",
    "final_highest_level_topics_df = final_highest_level_topics_df.explode('Highest_Topic_Label_Split')\n",
    "\n",
    "# Merge the exploded first dataset with the final highest level topics on 'Highest_Topic_Label_Split'\n",
    "final_merged_df = pd.merge(first_step_final_df, final_highest_level_topics_df[['Highest_Topic_Label_Split', 'Final_Label']], \n",
    "                           left_on='Highest_Topic_Label_Split', right_on='Highest_Topic_Label_Split', how='left')\n",
    "\n",
    "# Drop the auxiliary column used for merging\n",
    "final_merged_df.drop(columns=['Highest_Topic_Label_Split'], inplace=True)\n",
    "\n",
    "# Save the final merged dataframe to a new CSV file\n",
    "final_merged_df.to_csv('First_Step_BERTopic_topic_info_labelled_with_Final_Label_Merged.csv', index=False)\n",
    "\n",
    "print(\"Merging completed and the result is saved as 'First_Step_BERTopic_topic_info_labelled_with_Final_Label_Merged.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2ffaa63-1f4a-4072-956c-0df163a613fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Second_Step_BERTopic_topic_info_labelled_with_Highest_Topic_Label_final.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the datasets\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m first_step_final_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSecond_Step_BERTopic_topic_info_labelled_with_Highest_Topic_Label_final.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m final_highest_level_topics_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_highest_level_topic_labels_with_representatives.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Function to split the 'Highest_Topic_Label' based on semicolons or commas, handle NaN values\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Second_Step_BERTopic_topic_info_labelled_with_Highest_Topic_Label_final.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "first_step_final_df = pd.read_csv('Second_Step_BERTopic_topic_info_labelled_with_Highest_Topic_Label_final.csv')\n",
    "final_highest_level_topics_df = pd.read_csv('final_highest_level_topic_labels_with_representatives.csv')\n",
    "\n",
    "# Function to split the 'Highest_Topic_Label' based on semicolons or commas, handle NaN values\n",
    "def split_labels(label_str):\n",
    "    if isinstance(label_str, str):  # Check if the label_str is a string\n",
    "        # Split by semicolon or comma and strip any extra spaces\n",
    "        labels = [label.strip() for label in label_str.split(',')]\n",
    "    else:\n",
    "        labels = [None]  # Return a list with None if label_str is not a string\n",
    "    return labels\n",
    "\n",
    "# Apply the splitting function to the 'Highest_Topic_Label' in the first dataset\n",
    "first_step_final_df['Highest_Topic_Label_Split'] = first_step_final_df['Highest_Topic_Label'].apply(split_labels)\n",
    "\n",
    "# Explode the first dataframe to have one row per label\n",
    "first_step_final_df = first_step_final_df.explode('Highest_Topic_Label_Split')\n",
    "\n",
    "# Explode the final_highest_level_topics_df based on 'Highest_Topic_Label' for proper matching\n",
    "final_highest_level_topics_df['Highest_Topic_Label_Split'] = final_highest_level_topics_df['Highest_Topic_Label'].apply(split_labels)\n",
    "final_highest_level_topics_df = final_highest_level_topics_df.explode('Highest_Topic_Label_Split')\n",
    "\n",
    "# Merge the exploded first dataset with the final highest level topics on 'Highest_Topic_Label_Split'\n",
    "final_merged_df = pd.merge(first_step_final_df, final_highest_level_topics_df[['Highest_Topic_Label_Split', 'Final_Label']], \n",
    "                           left_on='Highest_Topic_Label_Split', right_on='Highest_Topic_Label_Split', how='left')\n",
    "\n",
    "# Drop the auxiliary column used for merging\n",
    "final_merged_df.drop(columns=['Highest_Topic_Label_Split'], inplace=True)\n",
    "\n",
    "# Save the final merged dataframe to a new CSV file\n",
    "final_merged_df.to_csv('Second_Step_BERTopic_topic_info_labelled_with_Final_Label_Merged.csv', index=False)\n",
    "\n",
    "print(\"Merging completed and the result is saved as 'Second_Step_BERTopic_topic_info_labelled_with_Final_Label_Merged.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b048d873-0970-4e47-8826-fb52fdd90391",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
